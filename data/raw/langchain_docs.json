[
  [
    "https://python.langchain.com/docs/",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#__docusaurus_skipToContent_fallback",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/integrations/providers/",
    "Providers\nOn this page\nProviders\ninfo\nIf you'd like to write your own integration, see\nExtending LangChain\n.\nIf you'd like to contribute an integration, see\nContributing integrations\n.\nIntegration Packages\nâ€‹\nThese providers have standalone\nlangchain-{provider}\npackages for improved versioning, dependency management and testing.\nProvider\nPackage\nDownloads\nLatest\nJS\nOpenAI\nlangchain-openai\nâœ…\nGoogle VertexAI\nlangchain-google-vertexai\nâœ…\nAWS\nlangchain-aws\nâœ…\nAnthropic\nlangchain-anthropic\nâœ…\nGoogle Generative AI\nlangchain-google-genai\nâœ…\nGoogle Community\nlangchain-google-community\nâŒ\nOllama\nlangchain-ollama\nâœ…\nGroq\nlangchain-groq\nâœ…\nChroma\nlangchain-chroma\nâœ…\nCohere\nlangchain-cohere\nâœ…\nHuggingface\nlangchain-huggingface\nâœ…\nMistralAI\nlangchain-mistralai\nâœ…\nPostgres\nlangchain-postgres\nâŒ\nPinecone\nlangchain-pinecone\nâœ…\nDeepseek\nlangchain-deepseek\nâœ…\nPerplexity\nlangchain-perplexity\nâœ…\nNvidia AI Endpoints\nlangchain-nvidia-ai-endpoints\nâŒ\nIbm\nlangchain-ibm\nâœ…\nMilvus\nlangchain-milvus\nâŒ\nMongoDB\nlangchain-mongodb\nâœ…\nDatabricks\ndatabricks-langchain\nâŒ\nQdrant\nlangchain-qdrant\nâœ…\nFireworks\nlangchain-fireworks\nâœ…\nTavily\nlangchain-tavily\nâœ…\nElasticsearch\nlangchain-elasticsearch\nâœ…\nUnstructured\nlangchain-unstructured\nâŒ\nDataStax Astra DB\nlangchain-astradb\nâœ…\nNeo4J\nlangchain-neo4j\nâœ…\nTogether\nlangchain-together\nâœ…\nRedis\nlangchain-redis\nâœ…\nXAI\nlangchain-xai\nâœ…\nSambanova\nlangchain-sambanova\nâŒ\nGraph RAG\nlangchain-graph-retriever\nâŒ\nAzure AI\nlangchain-azure-ai\nâœ…\nWeaviate\nlangchain-weaviate\nâœ…\nVoyageAI\nlangchain-voyageai\nâŒ\nCerebras\nlangchain-cerebras\nâœ…\nLitellm\nlangchain-litellm\nâŒ\nDocling\nlangchain-docling\nâŒ\nUpstage\nlangchain-upstage\nâŒ\nNomic\nlangchain-nomic\nâœ…\nAzure Dynamic Sessions\nlangchain-azure-dynamic-sessions\nâœ…\nPymupdf4Llm\nlangchain-pymupdf4llm\nâŒ\nAI21\nlangchain-ai21\nâŒ\nExa\nlangchain-exa\nâœ…\nPredictionguard\nlangchain-predictionguard\nâŒ\nWriter\nlangchain-writer\nâŒ\nMemgraph\nlangchain-memgraph\nâŒ\nQwq\nlangchain-qwq\nâŒ\nDB2\nlangchain-db2\nâŒ\nPrompty\nlangchain-prompty\nâŒ\nSqlserver\nlangchain-sqlserver\nâŒ\nHyperbrowser\nlangchain-hyperbrowser\nâŒ\nApify\nlangchain-apify\nâŒ\nLangFair\nlangfair\nâŒ\nVDMS\nlangchain-vdms\nâŒ\nNaver\nlangchain-naver\nâŒ\nMariaDB\nlangchain-mariadb\nâŒ\nSAP HANA Cloud\nlangchain-hana\nâŒ\nScrapegraph\nlangchain-scrapegraph\nâŒ\nSema4\nlangchain-sema4\nâŒ\nSnowflake\nlangchain-snowflake\nâŒ\nCouchbase\nlangchain-couchbase\nâŒ\nLinkup\nlangchain-linkup\nâŒ\nADS4GPTs\nads4gpts-langchain\nâŒ\nLocalAI\nlangchain-localai\nâŒ\nSalesforce\nlangchain-salesforce\nâŒ\nDeeplake\nlangchain-deeplake\nâŒ\nProlog\nlangchain-prolog\nâŒ\nTableau\nlangchain-tableau\nâŒ\nContextual AI\nlangchain-contextual\nâŒ\nCloudflare\nlangchain-cloudflare\nâœ…\nYDB\nlangchain-ydb\nâŒ\nValyu\nlangchain-valyu\nâŒ\nBox\nlangchain-box\nâŒ\nTilores\ntilores-langchain\nâŒ\nRecallio\nlangchain-recallio\nâŒ\nSurrealDB\nlangchain-surrealdb\nâŒ\nKuzu\nlangchain-kuzu\nâŒ\nNaver\nlangchain-naver-community\nâŒ\nBrightdata\nlangchain-brightdata\nâŒ\nDigitalOcean Gradient\nlangchain-gradient\nâŒ\nAgentql\nlangchain-agentql\nâŒ\nNebius\nlangchain-nebius\nâŒ\nNimble\nlangchain-nimble\nâŒ\nDappier\nlangchain-dappier\nâŒ\nRunPod\nlangchain-runpod\nâŒ\nGoodfire\nlangchain-goodfire\nâŒ\nTaiga\nlangchain-taiga\nâŒ\nXinference\nlangchain-xinference\nâŒ\nJenkins\nlangchain-jenkins\nâŒ\nGOAT SDK\ngoat-sdk-adapter-langchain\nâŒ\nOxylabs\nlangchain-oxylabs\nâŒ\nCrateDB\nlangchain-cratedb\nâŒ\nCognee\nlangchain-cognee\nâŒ\nAbso\nlangchain-abso\nâŒ\nValthera\nlangchain-valthera\nâŒ\nFeatherless AI\nlangchain-featherless-ai\nâŒ\nOceanbase\nlangchain-oceanbase\nâŒ\nPull Md\nlangchain-pull-md\nâŒ\nGalaxia Retriever\nlangchain-galaxia-retriever\nâŒ\nGel\nlangchain-gel\nâŒ\nVectara\nlangchain-vectara\nâŒ\nOpengradient\nlangchain-opengradient\nâŒ\nPermit\nlangchain-permit\nâŒ\nFalkorDB\nlangchain-falkordb\nâŒ\nDiscord (Shikenso)\nlangchain-discord-shikenso\nâŒ\nSingleStore\nlangchain-singlestore\nâŒ\nGreennode\nlangchain-greennode\nâŒ\nModelscope\nlangchain-modelscope\nâŒ\nPipeshift\nlangchain-pipeshift\nâŒ\nFmp Data\nlangchain-fmp-data\nâŒ\nNetmind\nlangchain-netmind\nâŒ\nZotero\nlangchain-zotero-retriever\nâŒ\nAerospike\nlangchain-aerospike\nâŒ\nPowerScale RAG Connector\npowerscale-rag-connector\nâŒ\nLindorm\nlangchain-lindorm-integration\nâŒ\nPayman Tool\nlangchain-payman-tool\nâŒ\nTensorlake\nlangchain-tensorlake\nâŒ\nAnchorbrowser\nlangchain-anchorbrowser\nâŒ\nMCP Toolbox\ntoolbox-langchain\nâŒ\nScrapeless\nlangchain-scrapeless\nâŒ\nAll Providers\nâ€‹\nClick\nhere\nto see all providers. Or search for a\nprovider using the Search field in the top-right corner of the screen.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/contributing/",
    "Welcome Contributors\nOn this page\nWelcome Contributors\nHi there! Thank you for your interest in contributing to LangChain.\nAs an open-source project in a fast developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes.\nTutorials\nâ€‹\nMore coming soon! We are working on tutorials to help you make your first contribution to the project.\nMake your first docs PR\nHow-to guides\nâ€‹\nDocumentation\n: Help improve our docs, including this one!\nCode\n: Help us write code, fix bugs, or improve our infrastructure.\nIntegrations\n: Help us integrate with your favorite vendors and tools.\nStandard Tests\n: Ensure your integration passes an expected set of tests.\nReference\nâ€‹\nRepository Structure\n: Understand the high level structure of the repository.\nReview Process\n: Learn about the review process for pull requests.\nFrequently Asked Questions (FAQ)\n: Get answers to common questions about contributing.\nCommunity\nâ€‹\nðŸ’­ Forum\nâ€‹\nWe have a\nLangChain Forum\nwhere users can ask usage questions, discuss design decisions, and propose new features.\nIf you are able to help answer questions, please do so! This will allow the maintainers to spend more time focused on development and bug fixing.\nðŸš© GitHub Issues\nâ€‹\nOur\nissues\npage is kept up to date with bugs, docs improvements, and triaged feature requests that are being worked on.\nThere is a\ntaxonomy of labels\nto help with sorting and discovery of issues of interest. Please use these to help\norganize issues. Check out the\nHelp Wanted\nand\nGood First Issue\ntags for recommendations.\nIf you start working on an issue, please assign it to yourself.\nIf you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature.\nIf two issues are related, or blocking, please link them rather than combining them.\nWe will try to keep these issues as up-to-date as possible, though\nwith the rapid rate of development in this field some may get out of date.\nIf you notice this happening, please let us know.\nðŸ“¢ Community Slack\nâ€‹\nWe have a\ncommunity slack\nwhere you can ask questions, get help, and discuss the project with other contributors and users.\nðŸ™‹ Getting Help\nâ€‹\nOur goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please\nask in\ncommunity slack\nor open a\nforum post\n.\nIn a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase.\nIf you are finding these difficult (or even just annoying) to work with, feel free to ask in\ncommunity slack\n!\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/people/",
    "People\nThere are some incredible humans from all over the world who have been instrumental in helping the LangChain community flourish ðŸŒ!\nThis page highlights a few of those folks who have dedicated their time to the open-source repo in the form of direct contributions and reviews.\nTop reviewers\nâ€‹\nAs LangChain has grown, the amount of surface area that maintainers cover has grown as well.\nThank you to the following folks who have gone above and beyond in reviewing incoming PRs ðŸ™!\n@\nleo-gan\n@\ncbornet\n@\nlkuligin\n@\n3coins\n@\njexp\n@\ntomasonjo\n@\nliugddx\n@\njoemcelroy\n@\nUndertone0809\n@\nmspronesti\n@\nJohnNay\n@\ntjaffri\n@\nsjwhitmore\n@\nholtskinner\n@\nskcoirz\n@\nharupy\n@\ntylerhutcherson\n@\nkeenborder786\n@\nAnush008\n@\nmaang-h\n@\nSpartee\n@\nRaj725\n@\nMthwRobinson\n@\nnicoloboschi\n@\nJibola\n@\nShaneHarvey\n@\nsepiatone\n@\nscadEfUr\n@\njeffchuber\n@\nkacperlukawski\n@\npranjaldoshi96\n@\nblink1073\n@\nB-Step62\n@\neavanvalkenburg\n@\ngengliangwang\n@\ndbczumar\n@\nBenWilson2\n@\njmorganca\n@\npprados\n@\nhemidactylus\n@\nRafaelXokito\n@\nbjchambers\n@\nandychenmagrathea-06e0a82f-34fc-48ca\n@\nsam-h-bean\n@\nnickscamara\n@\nnaveentatikonda\n@\nkylehh\n@\nofermend\n@\nnavneet1v\n@\nbillytrend-cohere\n@\nopenvino-dev-samples\n@\nserena-ruan\n@\nmaxjakob\nTop recent contributors\nâ€‹\nThe list below contains contributors who have had the most PRs merged in the last three months, weighted (imperfectly) by impact.\nThank you all so much for your time and efforts in making LangChain better â¤ï¸!\n@\nleo-gan\n@\ncbornet\n@\nmaang-h\n@\nAnush008\n@\nRaj725\n@\nZhangShenao\n@\nshurrey\n@\nbjchambers\n@\ntomasonjo\n@\nB-Step62\n@\njhpiedrahitao\n@\nnithishr\n@\ncaseyclements\n@\nConiferish\n@\nzc277584121\n@\nserena-ruan\n@\nkeenborder786\n@\ndristysrivastava\n@\ngbaian10\n@\nthedavgar\nCore maintainers\nâ€‹\nHello there ðŸ‘‹!\nWe're LangChain's core maintainers. If you've spent time in the community, you've probably crossed paths\nwith at least one of us already.\n@\nbaskaryan\n@\nccurme\n@\nhinthornw\n@\nrlancemartin\n@\nnfcampos\n@\nagola11\n@\nhwchase17\n@\nvbarda\n@\nefriis\n@\neyurtsev\nTop all-time contributors\nâ€‹\nAnd finally, this is an all-time list of all-stars who have made significant contributions to the framework ðŸŒŸ:\n@\nleo-gan\n@\ncbornet\n@\ntomasonjo\n@\nlkuligin\n@\nmaang-h\n@\nMthwRobinson\n@\nkacperlukawski\n@\nhemidactylus\n@\n3coins\n@\nliugddx\n@\ntimothyasp\n@\nsjwhitmore\n@\nMateuszOssGit\n@\nkeenborder786\n@\nmbchang\n@\ndanielchalef\n@\nAnush008\n@\nmspronesti\n@\nchyroc\n@\neavanvalkenburg\n@\nshibuiwilliam\n@\nholtskinner\n@\nsepiatone\n@\nofermend\n@\nfpingham\n@\npprados\n@\n169\n@\ntjaffri\n@\njhpiedrahitao\n@\nRaj725\n@\nbillytrend-cohere\n@\nnickscamara\n@\nmaxjakob\n@\nmaks-operlejn-ds\n@\nvolodymyr-memsql\n@\nnithishr\n@\nJibola\n@\nAdi8885\n@\nB-Step62\n@\nsergerdn\n@\nopenvino-dev-samples\n@\naverikitsch\n@\nnaveentatikonda\n@\ntyumentsev4\n@\nUmerHA\n@\njoshuasundance-swca\n@\nadolkhan\n@\njamesbraza\n@\nseamusp\n@\nmichaelfeil\n@\nrahul-trip\n@\nvirattt\n@\nJosephasafg\n@\nblob42\n@\nmalandis\n@\nmpskex\n@\ndavidbuniat\n@\nShreyaR\n@\nlalanikarim\n@\nzc277584121\n@\nmaiqingqiang\n@\ntylerhutcherson\n@\nskcoirz\n@\nZhangShenao\n@\nharupy\n@\nmanuel-soria\n@\nCG80499\n@\noutday29\n@\nharry-cohere\n@\nGMartin-dev\n@\nljeagle\n@\njoemcelroy\n@\nIANTHEREAL\n@\nwangxuqi\n@\nshurrey\n@\nmackong\n@\ngengliangwang\n@\njzluo\n@\nAnindyadeep\n@\nmateusz-wosinski-ds\n@\nJped\n@\nhughcrt\n@\ncs0lar\n@\nShorthillsAI\n@\nbjchambers\n@\nkylehh\n@\neltociear\n@\njeffvestal\n@\nVKudlay\n@\nconceptofmind\n@\nwenngong\n@\nraveharpaz\n@\nruoccofabrizio\n@\naayush3011\n@\nConiferish\n@\nKyrianC\n@\naxiangcoding\n@\nhmasdev\n@\nhomanp\n@\nyakigac\n@\nHunterGerlach\n@\ngkorland\n@\nskozlovf\n@\nGordon-BP\n@\nkzk-maeda\n@\nparambharat\n@\ncaseyclements\n@\nsaginawj\n@\nfilip-halt\n@\nzachschillaci27\n@\ncwlacewe\n@\nnelly-hateva\n@\nserena-ruan\n@\nwemysschen\n@\nalexsherstinsky\n@\nzanderchase\n@\ndglogo\n@\ndanielhjz\n@\nos1ma\n@\ncevian\n@\ncharliermarsh\n@\nmaximeperrindev\n@\nbborn\n@\nraghavdixit99\n@\njunkeon\n@\njj701\n@\ncauwulixuan\n@\nmarkcusack\n@\nrohanaggarwal7997\n@\ndelip\n@\nichernev\n@\nMartinKolbAtWork\n@\nkennethchoe\n@\namiaxys\n@\njeffchuber\n@\nshane-huang\n@\ncbh123\n@\nsdelgadoc\n@\nNutlope\n@\njirimoravcik\n@\nkitrak-rev\n@\nchadj2\n@\ntazarov\n@\nmattgotteiner\n@\nam-kinetica\n@\nbaichuan-assistant\n@\nsfvaroglu\n@\nsfc-gh-jcarroll\n@\njeffzwang\n@\nBeatrixCohere\n@\nmainred\n@\nCahidArda\n@\nP-E-B\n@\nsam-h-bean\n@\nwilliamdevena\n@\nfilip-michalsky\n@\nk8si\n@\nedwardzjl\n@\npaul-paliychuk\n@\ngregnr\n@\nasamant21\n@\nsudranga\n@\nsseide\n@\nscottnath\n@\nAI-Bassem\n@\nBeautyyuYanli\n@\ngradenr\n@\nzhaoshengbo\n@\nhakantekgul\n@\neryk-dsai\n@\nmrtj\n@\npcliupc\n@\nalvarobartt\n@\nrogerserper\n@\nekzhu\n@\nashleyxuu\n@\nbhalder\n@\nZixinYang\n@\nnikhilkjha\n@\nDominastorm\n@\nraunakshrivastava7\n@\nrodrigo-f-nogueira\n@\nbenjibc\n@\nhoyungcher\n@\nOwenPendrighElliott\n@\nMikelarg\n@\nfreemso\n@\nnetoferraz\n@\nzizhong\n@\namicus-veritatis\n@\nMikeNitsenko\n@\nliangz1\n@\nmikelambert\n@\nnicoloboschi\n@\nmkorpela\n@\nlinancn\n@\ntsg\n@\nanar2706\n@\nyifeis7\n@\nwhitead\n@\nbenitoThree\n@\nruze00\n@\nHeChangHaoGary\n@\nxiaoyuxee\n@\njerwelborn\n@\nvairodp\n@\naletna\n@\nhsm207\n@\nDayuanJiang\n@\nrigazilla\n@\napepkuss\n@\ngadhagod\n@\nLastMonopoly\n@\nmatthewdeguzman\n@\nTokkiu\n@\nsoftboyjimbo\n@\nDobiichi-Origami\n@\nzhanghexian\n@\nrajtilakjee\n@\nashvardanian\n@\nplv\n@\nTomTom101\n@\nlucas-tucker\n@\nJoanFM\n@\nmengxr\n@\nerika-cardenas\n@\njuliuslipp\n@\npors\n@\nshivanimodi16\n@\nthomas0809\n@\nazamiftikhar1000\n@\nalecf\n@\nprakul\n@\nOscilloscope98\n@\necneladis\n@\nUndertone0809\n@\nhetaoBackend\n@\nRichmondAlake\n@\nyackermann\n@\nlesters\n@\nmax-arthurai\n@\nphilipkiely-baseten\n@\ndristysrivastava\n@\nschadem\n@\nAratako\n@\nanubhav94N\n@\nrithwik-db\n@\nkartheekyakkala\n@\njiayini1119\n@\nshufanhao\n@\nzcgeng\n@\nash0ts\n@\nHonkware\n@\ndwhitena\n@\nSagarBM396\n@\njamie256\n@\nyanghua\n@\nmiri-bar\n@\nklein-t\n@\nAyan-Bandyopadhyay\n@\ntugot17\n@\nDaveDeCaprio\n@\nSpartee\n@\nJflick58\n@\nJuHyung-Son\n@\nstewartjarod\n@\nmkhludnev\n@\ncxumol\n@\nrihardsgravis\n@\nkouroshHakha\n@\nsamnoyes\n@\nByronHsu\n@\nO-Roma\n@\nrowillia\n@\nlesterpjy\n@\njunefish\n@\n2jimoo\n@\npetervandenabeele\n@\nshahrin014\n@\nshoelsch\n@\nh0rv\n@\nasai95\n@\nmgoin\n@\nBlaizzy\n@\nakmhmgc\n@\ngmpetrov\n@\naarnphm\n@\naMahanna\n@\nhp0404\n@\nliushuaikobe\n@\nfserv\n@\nseanmavley\n@\ncloudscool\n@\nLothiraldan\n@\nAther23\n@\nmogith-pn\n@\nJohnnyDeuss\n@\neunhye1kim\n@\ndakinggg\n@\njackwotherspoon\n@\nklaudialemiec\n@\nphilippe2803\n@\nwnleao\n@\nfzowl\n@\nkdcokenny\n@\nqtangs\n@\nwey-gu\n@\nSukitly\n@\nsamber\n@\nAtry\n@\nchosh0615\n@\navsolatorio\n@\n19374242\n@\nleedotpang\n@\nyarikoptic\n@\nJofthomas\n@\nmarlenezw\n@\nrancomp\n@\nmorganda\n@\natroyn\n@\ndmenini\n@\nbrotchie\n@\nangeligareta\n@\novuruska\n@\nmmajewsk\n@\nhaydeniw\n@\nwangwei1237\n@\nnimimeht\n@\nalexiri\n@\nrjanardhan3\n@\nmsaelices\n@\nSimFG\n@\nStankoKuveljic\n@\nquchuyuan\n@\nsirjan-ws-ext\n@\nanentropic\n@\nEricLiclair\n@\nhsuyuming\n@\nasofter\n@\nThatsJustCheesy\n@\nMacanPN\n@\nkristapratico\n@\nimeckr\n@\nchristeefy\n@\nrc19\n@\nanthonychu\n@\nh3l\n@\nJensMadsen\n@\nakiradev0x\n@\nJonZeolla\n@\nmlejva\n@\nmsetbar\n@\nj-space-b\n@\nchrispy-snps\n@\namosjyng\n@\nninjapenguin\n@\ndvonthenen\n@\nJoffref\n@\nHamJaw1432\n@\nAnirudh31415926535\n@\ncristobalcl\n@\nkrrishdholakia\n@\nsamhita-alla\n@\nralewis85\n@\nfinnless\n@\nfelixocker\n@\nbrendancol\n@\ngbaian10\n@\njuliensalinas\n@\nmuntaqamahmood\n@\nFei-Wang\n@\njupyterjazz\n@\nkooyunmo\n@\ndonbr\n@\njdogmcsteezy\n@\nborisdev\n@\nthedavgar\n@\njasonwcfan\n@\nyilmaz-burak\n@\nyessenzhar\n@\npjb157\n@\nkrasserm\n@\nNickL77\n@\nmishushakov\n@\nflash1293\n@\nCode-Hex\n@\njnis23\n@\ncgalo5758\n@\nraymond-yuan\n@\nsunishsheth2009\n@\nklae01\n@\nLunarECL\n@\nwhiskyboy\n@\nyuskhan\n@\nakashAD98\n@\nShrined\n@\nDavidLMS\n@\nrmkraus\n@\nrawwar\n@\npmcfadin\n@\ntricktreat\n@\nfzliu\n@\ndongreenberg\n@\naledelunap\n@\nstonekim\n@\nchip-davis\n@\ntonyabracadabra\n@\nmachulav\n@\nshauryr\n@\nPawelFaron\n@\nlvliang-intel\n@\nbalvisio\n@\nxinqiu\n@\nMikeMcGarry\n@\nrobcaulk\n@\njagilley\n@\nprrao87\n@\nlujingxuansc\n@\nmplachter\n@\njvelezmagic\n@\npatrickloeber\n@\ntrancethehuman\n@\nvadimgu\n@\nhulitaitai\n@\ncjcjameson\n@\naymeric-roucher\n@\nSandy247\n@\nzoltan-fedor\n@\nberkedilekoglu\n@\nrodrigo-clickup\n@\nnumb3r3\n@\nsvdeepak99\n@\nZyeG\n@\nitok01\n@\nNoahStapp\n@\ntconkling\n@\nthehapyone\n@\ntoshish\n@\ndremeika\n@\nmingkang111\n@\nliaokongVFX\n@\n0xRaduan\n@\napeng-singlestore\n@\njeffkit\n@\nxsai9101\n@\nissam9\n@\nCogniJT\n@\nivyas21\n@\nflorian-morel22\n@\ngdj0nes\n@\nsdan\n@\nsamching\n@\nlukestanley\n@\nIlyaKIS1\n@\ndosuken123\n@\nwietsevenema\n@\ngustavo-yt\n@\nalexander1999-hub\n@\njonathanalgar\n@\nvsxd\n@\namirai21\n@\nvar77\n@\nL-cloud\n@\nmatiasjacob25\n@\nHaijian06\n@\nIlyaMichlin\n@\ndzmitry-kankalovich\n@\nEniasCailliau\n@\nkreneskyp\n@\nrsharath\n@\nizapolsk\n@\nrjadr\n@\nLord-Haji\n@\nwoodworker\n@\nphilschmid\n@\nChrKahl\n@\nbongsang\n@\nclwillhuang\n@\nBidhanRoy\n@\nfinger-bone\n@\nhiigao\n@\nsamkhano1\n@\nireneisdoomed\n@\nmahaddad\n@\nnicolasnk\n@\nbovlb\n@\ntomhamer\n@\nhaoch\n@\nSlapDrone\n@\ntaranjeet\n@\nPixeladed\n@\nmlot\n@\nJGalego\n@\nxieqihui\n@\nmhavey\n@\npraveenv\n@\nsrics\n@\n16BitNarwhal\n@\nzanussbaum\n@\nzhangch9\n@\npaulonasc\n@\nrubell\n@\nizzymsft\n@\nricharda23\n@\nzifeiq\n@\nliuyonghengheng\n@\ntomaspiaggio\n@\nklaus-xiong\n@\nalallema\n@\nfengjial\n@\nsimon824\n@\nAksAman\n@\nmewim\n@\nruanwz\n@\ngdedrouas\n@\nmariokostelac\n@\nmosheber\n@\nlaplaceon\n@\nthepycoder\n@\ntoddkim95\n@\nRafaelXokito\n@\nagamble\n@\nThanhNguye-n\n@\nigor-drozdov\n@\nKastanDay\n@\nseanaedmiston\n@\nRandl\n@\nNikolaosPapailiou\n@\nebrehault\n@\nwlleiiwang\n@\nsantiagxf\n@\nthehappydinoa\n@\nabhiaagarwal\n@\nLMC117\n@\nWilliamEspegren\n@\nsunbc0120\n@\nSimon-Stone\n@\nAmyh102\n@\nshumway743\n@\ngcheron\n@\nzachdj\n@\nehsanmok\n@\nbsbodden\n@\nTrevato\n@\nraoufchebri\n@\ndelgermurun\n@\njcjc712\n@\nyonarw\n@\nEvilFreelancer\n@\nzywilliamli\n@\nthaiminhpv\n@\npaperMoose\n@\nyounis-bash\n@\nrajib76\n@\nTejaHara\n@\nihpolash\n@\nscadEfUr\n@\nSauhaardW\n@\npranava-amzn\n@\nfynnfluegge\n@\nadilansari\n@\nbstadt\n@\ndependabot\n@\nPenghuiCheng\n@\ngiannis2two\n@\nanilaltuner\n@\nbu2kx\n@\nAmineDjeghri\n@\nfrancesco-kruk\n@\nbakebrain\n@\nbburgin\n@\ncoolbeevip\n@\nsreiswig\n@\nvrushankportkey\n@\njxnl\n@\narron2003\n@\nHashemAlsaket\n@\nea-open-source\n@\nconstantinmusca\n@\nandrewmbenton\n@\nSubsegment\n@\nzrcni\n@\npiizei\n@\nRohanDey02\n@\ntherontau0054\n@\ndemjened\n@\nkillinsun\n@\nsanzgiri\n@\nsp35\n@\nYash-1511\n@\nabdalrohman\n@\ncoyotespike\n@\nzchenyu\n@\nyuwenzho\n@\nricki-epsilla\n@\nHassanOuda\n@\ns-udhaya\n@\ntesfagabir\n@\nchocolate4\n@\njasondotparse\n@\nbwmatson\n@\nDaggx\n@\nseth-hg\n@\nNolanTrem\n@\nmpb159753\n@\nmikeknoop\n@\ndatelier\n@\njakerachleff\n@\nJamsheedMistri\n@\natherfawaz\n@\nHugoberry\n@\nHaris-Ali007\n@\nAlpinDale\n@\njjovalle99\n@\nDN6\n@\nspike-spiegel-21\n@\nmziru\n@\nDylan20XX\n@\nxingfanxia\n@\nNoahBPeterson\n@\n0xJord4n\n@\nyuncliu\n@\ntabbyl21\n@\nnaman-modi\n@\nsokolgood\n@\nharelix\n@\nstandby24x7\n@\nchangliu-0520\n@\nlts-rad\n@\nnuric\n@\nakshaya-a\n@\nedreisMD\n@\nar-mccabe\n@\nBobMerkus\n@\nNavanit-git\n@\ndavid-huge\n@\nrotemweiss57\n@\nsharmisthasg\n@\nhmilkovi\n@\nvreyespue\n@\ndeepblue\n@\nmrugank-wadekar\n@\nniklub\n@\ndirtysalt\n@\nzeiler\n@\nmoyid\n@\nsachinparyani\n@\nlsloan\n@\nju-bezdek\n@\nColabDog\n@\nhanit-com\n@\nkevaldekivadiya2415\n@\nmanmax31\n@\nimrehg\n@\nldorigo\n@\njanchorowski\n@\nAthulVincent\n@\ntamohannes\n@\nboazwasserman\n@\ngkermit\n@\ndsummersl\n@\nidvorkin\n@\nvempaliakhil96\n@\nC-K-Loan\n@\ndaniel-brenot\n@\njwbeck97\n@\ngiacbrd\nWe're so thankful for your support!\nAnd one more thank you to\n@tiangolo\nfor inspiration via FastAPI's\nexcellent people page\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/troubleshooting/errors/",
    "Error reference\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an\nlc_error_code\nproperty corresponding to one of the below codes when they are thrown in code.\nINVALID_PROMPT_INPUT\nINVALID_TOOL_RESULTS\nMESSAGE_COERCION_FAILURE\nMODEL_AUTHENTICATION\nMODEL_NOT_FOUND\nMODEL_RATE_LIMIT\nOUTPUT_PARSING_FAILURE\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/introduction/",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/",
    "Tutorials\nOn this page\nTutorials\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\nGet started\nâ€‹\nFamiliarize yourself with LangChain's open-source components by building simple applications.\nIf you're looking to get started with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our supported\nintegrations\n.\nChat models and prompts\n: Build a simple LLM application with\nprompt templates\nand\nchat models\n.\nSemantic search\n: Build a semantic search engine over a PDF with\ndocument loaders\n,\nembedding models\n, and\nvector stores\n.\nClassification\n: Classify text into categories or labels using\nchat models\nwith\nstructured outputs\n.\nExtraction\n: Extract structured data from text and other unstructured media using\nchat models\nand\nfew-shot examples\n.\nRefer to the\nhow-to guides\nfor more detail on using all LangChain components.\nOrchestration\nâ€‹\nGet started using\nLangGraph\nto assemble LangChain components into full-featured applications.\nChatbots\n: Build a chatbot that incorporates memory.\nAgents\n: Build an agent that interacts with external tools.\nRetrieval Augmented Generation (RAG) Part 1\n: Build an application that uses your own documents to inform its responses.\nRetrieval Augmented Generation (RAG) Part 2\n: Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\nQuestion-Answering with SQL\n: Build a question-answering system that executes SQL queries to inform its responses.\nSummarization\n: Generate summaries of (potentially long) texts.\nQuestion-Answering with Graph Databases\n: Build a question-answering system that queries a graph database to inform its responses.\nLangSmith\nâ€‹\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\nLangSmith documentation is hosted on a separate site.\nYou can peruse\nLangSmith tutorials here\n.\nEvaluation\nâ€‹\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\nEvaluate your LLM application\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/graph/",
    "Tutorials\nBuild a Question Answering application over a Graph Database\nOn this page\nBuild a Question Answering application over a Graph Database\nIn this guide we'll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph.\nâš ï¸ Security note âš ï¸\nâ€‹\nBuilding Q&A systems of graph databases requires executing model-generated graph queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices,\nsee here\n.\nArchitecture\nâ€‹\nAt a high-level, the steps of most graph chains are:\nConvert question to a graph database query\n: Model converts user input to a graph database query (e.g. Cypher).\nExecute graph database query\n: Execute the graph database query.\nAnswer the question\n: Model responds to user input using the query results.\nSetup\nâ€‹\nFirst, get required packages and set environment variables.\nIn this example, we will be using Neo4j graph database.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain langchain\n-\nneo4j langchain\n-\nopenai langgraph\nWe default to OpenAI models in this guide.\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter your OpenAI API key: \"\n)\n# Uncomment the below to use LangSmith. Not required.\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\nEnter your OpenAI API key:  Â·Â·Â·Â·Â·Â·Â·Â·\nNext, we need to define Neo4j credentials.\nFollow\nthese installation steps\nto set up a Neo4j database.\nos\n.\nenviron\n[\n\"NEO4J_URI\"\n]\n=\n\"bolt://localhost:7687\"\nos\n.\nenviron\n[\n\"NEO4J_USERNAME\"\n]\n=\n\"neo4j\"\nos\n.\nenviron\n[\n\"NEO4J_PASSWORD\"\n]\n=\n\"password\"\nThe below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.\nfrom\nlangchain_neo4j\nimport\nNeo4jGraph\ngraph\n=\nNeo4jGraph\n(\n)\n# Import movie information\nmovies_query\n=\n\"\"\"\nLOAD CSV WITH HEADERS FROM\n'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'\nAS row\nMERGE (m:Movie {id:row.movieId})\nSET m.released = date(row.released),\nm.title = row.title,\nm.imdbRating = toFloat(row.imdbRating)\nFOREACH (director in split(row.director, '|') |\nMERGE (p:Person {name:trim(director)})\nMERGE (p)-[:DIRECTED]->(m))\nFOREACH (actor in split(row.actors, '|') |\nMERGE (p:Person {name:trim(actor)})\nMERGE (p)-[:ACTED_IN]->(m))\nFOREACH (genre in split(row.genres, '|') |\nMERGE (g:Genre {name:trim(genre)})\nMERGE (m)-[:IN_GENRE]->(g))\n\"\"\"\ngraph\n.\nquery\n(\nmovies_query\n)\n[]\nGraph schema\nâ€‹\nIn order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the\nrefresh_schema\nmethod to refresh the schema information.\ngraph\n.\nrefresh_schema\n(\n)\nprint\n(\ngraph\n.\nschema\n)\nNode properties:\nPerson {name: STRING}\nMovie {id: STRING, released: DATE, title: STRING, imdbRating: FLOAT}\nGenre {name: STRING}\nChunk {id: STRING, embedding: LIST, text: STRING, question: STRING, query: STRING}\nRelationship properties:\nThe relationships:\n(:Person)-[:DIRECTED]->(:Movie)\n(:Person)-[:ACTED_IN]->(:Movie)\n(:Movie)-[:IN_GENRE]->(:Genre)\nFor more involved schema information, you can use\nenhanced_schema\noption.\nenhanced_graph\n=\nNeo4jGraph\n(\nenhanced_schema\n=\nTrue\n)\nprint\n(\nenhanced_graph\n.\nschema\n)\nReceived notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n``````output\nNode properties:\n- **Person**\n- `name`: STRING Example: \"John Lasseter\"\n- **Movie**\n- `id`: STRING Example: \"1\"\n- `released`: DATE Min: 1964-12-16, Max: 1996-09-15\n- `title`: STRING Example: \"Toy Story\"\n- `imdbRating`: FLOAT Min: 2.4, Max: 9.3\n- **Genre**\n- `name`: STRING Example: \"Adventure\"\n- **Chunk**\n- `id`: STRING Available options: ['d66006059fd78d63f3df90cc1059639a', '0e3dcb4502853979d12357690a95ec17', 'c438c6bcdcf8e4fab227f29f8e7ff204', '97fe701ec38057594464beaa2df0710e', 'b54f9286e684373498c4504b4edd9910', '5b50a72c3a4954b0ff7a0421be4f99b9', 'fb28d41771e717255f0d8f6c799ede32', '58e6f14dd2e6c6702cf333f2335c499c']\n- `text`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', \"List all the genres of the movie Schindler's List\", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i']\n- `question`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', \"List all the genres of the movie Schindler's List\", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i']\n- `query`: STRING Available options: ['MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN coun', \"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a)\", \"MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->\", \"MATCH (m:Movie {title: 'Schindler's List'})-[:IN_G\", 'MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]', 'MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_I', 'MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACT', 'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.na']\nRelationship properties:\nThe relationships:\n(:Person)-[:DIRECTED]->(:Movie)\n(:Person)-[:ACTED_IN]->(:Movie)\n(:Movie)-[:IN_GENRE]->(:Genre)\nThe\nenhanced_schema\noption enriches property information by including details such as minimum and maximum values for floats and dates, as well as example values for string properties. This additional context helps guide the LLM toward generating more accurate and effective queries.\nGreat! We've got a graph database that we can query. Now let's try hooking it up to an LLM.\nGraphQACypherChain\nâ€‹\nLet's use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question.\nLangChain comes with a built-in chain for this workflow that is designed to work with Neo4j:\nGraphCypherQAChain\nfrom\nlangchain_neo4j\nimport\nGraphCypherQAChain\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n,\ntemperature\n=\n0\n)\nchain\n=\nGraphCypherQAChain\n.\nfrom_llm\n(\ngraph\n=\nenhanced_graph\n,\nllm\n=\nllm\n,\nverbose\n=\nTrue\n,\nallow_dangerous_requests\n=\nTrue\n)\nresponse\n=\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\n\"What was the cast of the Casino?\"\n}\n)\nresponse\n\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\nGenerated Cypher:\n\u001b[32;1m\u001b[1;3mcypher\nMATCH (p:Person)-[:ACTED_IN]->(m:Movie {title: \"Casino\"})\nRETURN p.name\n\u001b[0m\nFull Context:\n\u001b[32;1m\u001b[1;3m[{'p.name': 'Robert De Niro'}, {'p.name': 'Joe Pesci'}, {'p.name': 'Sharon Stone'}, {'p.name': 'James Woods'}]\u001b[0m\n\u001b[1m> Finished chain.\u001b[0m\n{'query': 'What was the cast of the Casino?',\n'result': 'Robert De Niro, Joe Pesci, Sharon Stone, and James Woods were the cast of Casino.'}\nAdvanced implementation with LangGraph\nâ€‹\nWhile the GraphCypherQAChain is effective for quick demonstrations, it may face challenges in production environments. Transitioning to LangGraph can enhance the workflow, but implementing natural language to query flows in production remains a complex task. Nevertheless, there are several strategies to significantly improve accuracy and reliability, which we will explore next.\nHere is the visualized LangGraph flow we will implement:\nWe will begin by defining the Input, Output, and Overall state of the LangGraph application.\nfrom\noperator\nimport\nadd\nfrom\ntyping\nimport\nAnnotated\n,\nList\nfrom\ntyping_extensions\nimport\nTypedDict\nclass\nInputState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\nclass\nOverallState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\nnext_action\n:\nstr\ncypher_statement\n:\nstr\ncypher_errors\n:\nList\n[\nstr\n]\ndatabase_records\n:\nList\n[\ndict\n]\nsteps\n:\nAnnotated\n[\nList\n[\nstr\n]\n,\nadd\n]\nclass\nOutputState\n(\nTypedDict\n)\n:\nanswer\n:\nstr\nsteps\n:\nList\n[\nstr\n]\ncypher_statement\n:\nstr\nThe first step is a simple\nguardrails\nstep, where we validate whether the question pertains to movies or their cast. If it doesn't, we notify the user that we cannot answer any other questions. Otherwise, we move on to the Cypher generation step.\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\npydantic\nimport\nBaseModel\n,\nField\nguardrails_system\n=\n\"\"\"\nAs an intelligent assistant, your primary objective is to decide whether a given question is related to movies or not.\nIf the question is related to movies, output \"movie\". Otherwise, output \"end\".\nTo make this decision, assess the content of the question and determine if it refers to any movie, actor, director, film industry,\nor related topics. Provide only the specified output: \"movie\" or \"end\".\n\"\"\"\nguardrails_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nguardrails_system\n,\n)\n,\n(\n\"human\"\n,\n(\n\"{question}\"\n)\n,\n)\n,\n]\n)\nclass\nGuardrailsOutput\n(\nBaseModel\n)\n:\ndecision\n:\nLiteral\n[\n\"movie\"\n,\n\"end\"\n]\n=\nField\n(\ndescription\n=\n\"Decision on whether the question is related to movies\"\n)\nguardrails_chain\n=\nguardrails_prompt\n|\nllm\n.\nwith_structured_output\n(\nGuardrailsOutput\n)\ndef\nguardrails\n(\nstate\n:\nInputState\n)\n-\n>\nOverallState\n:\n\"\"\"\nDecides if the question is related to movies or not.\n\"\"\"\nguardrails_output\n=\nguardrails_chain\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n.\nget\n(\n\"question\"\n)\n}\n)\ndatabase_records\n=\nNone\nif\nguardrails_output\n.\ndecision\n==\n\"end\"\n:\ndatabase_records\n=\n\"This questions is not about movies or their cast. Therefore I cannot answer this question.\"\nreturn\n{\n\"next_action\"\n:\nguardrails_output\n.\ndecision\n,\n\"database_records\"\n:\ndatabase_records\n,\n\"steps\"\n:\n[\n\"guardrail\"\n]\n,\n}\nAPI Reference:\nChatPromptTemplate\nFew-shot prompting\nâ€‹\nConverting natural language into accurate queries is challenging. One way to enhance this process is by providing relevant few-shot examples to guide the LLM in query generation. To achieve this, we will use the\nSemanticSimilarityExampleSelector\nto dynamically select the most relevant examples.\nfrom\nlangchain_core\n.\nexample_selectors\nimport\nSemanticSimilarityExampleSelector\nfrom\nlangchain_neo4j\nimport\nNeo4jVector\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nexamples\n=\n[\n{\n\"question\"\n:\n\"How many artists are there?\"\n,\n\"query\"\n:\n\"MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)\"\n,\n}\n,\n{\n\"question\"\n:\n\"Which actors played in the movie Casino?\"\n,\n\"query\"\n:\n\"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a) RETURN a.name\"\n,\n}\n,\n{\n\"question\"\n:\n\"How many movies has Tom Hanks acted in?\"\n,\n\"query\"\n:\n\"MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->(m:Movie) RETURN count(m)\"\n,\n}\n,\n{\n\"question\"\n:\n\"List all the genres of the movie Schindler's List\"\n,\n\"query\"\n:\n\"MATCH (m:Movie {title: 'Schindler's List'})-[:IN_GENRE]->(g:Genre) RETURN g.name\"\n,\n}\n,\n{\n\"question\"\n:\n\"Which actors have worked in movies from both the comedy and action genres?\"\n,\n\"query\"\n:\n\"MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = 'Comedy' AND g2.name = 'Action' RETURN DISTINCT a.name\"\n,\n}\n,\n{\n\"question\"\n:\n\"Which directors have made movies with at least three different actors named 'John'?\"\n,\n\"query\"\n:\n\"MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH 'John' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name\"\n,\n}\n,\n{\n\"question\"\n:\n\"Identify movies where directors also played a role in the film.\"\n,\n\"query\"\n:\n\"MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name\"\n,\n}\n,\n{\n\"question\"\n:\n\"Find the actor with the highest number of movies in the database.\"\n,\n\"query\"\n:\n\"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1\"\n,\n}\n,\n]\nexample_selector\n=\nSemanticSimilarityExampleSelector\n.\nfrom_examples\n(\nexamples\n,\nOpenAIEmbeddings\n(\n)\n,\nNeo4jVector\n,\nk\n=\n5\n,\ninput_keys\n=\n[\n\"question\"\n]\n)\nAPI Reference:\nSemanticSimilarityExampleSelector\nNext, we implement the Cypher generation chain, also known as\ntext2cypher\n. The prompt includes an enhanced graph schema, dynamically selected few-shot examples, and the userâ€™s question. This combination enables the generation of a Cypher query to retrieve relevant information from the database.\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\ntext2cypher_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n(\n\"Given an input question, convert it to a Cypher query. No pre-amble.\"\n\"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"\n)\n,\n)\n,\n(\n\"human\"\n,\n(\n\"\"\"You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.\nDo not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\nHere is the schema information\n{schema}\nBelow are a number of examples of questions and their corresponding Cypher queries.\n{fewshot_examples}\nUser input: {question}\nCypher query:\"\"\"\n)\n,\n)\n,\n]\n)\ntext2cypher_chain\n=\ntext2cypher_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\ndef\ngenerate_cypher\n(\nstate\n:\nOverallState\n)\n-\n>\nOverallState\n:\n\"\"\"\nGenerates a cypher statement based on the provided schema and user input\n\"\"\"\nNL\n=\n\"\\n\"\nfewshot_examples\n=\n(\nNL\n*\n2\n)\n.\njoin\n(\n[\nf\"Question:\n{\nel\n[\n'question'\n]\n}\n{\nNL\n}\nCypher:\n{\nel\n[\n'query'\n]\n}\n\"\nfor\nel\nin\nexample_selector\n.\nselect_examples\n(\n{\n\"question\"\n:\nstate\n.\nget\n(\n\"question\"\n)\n}\n)\n]\n)\ngenerated_cypher\n=\ntext2cypher_chain\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n.\nget\n(\n\"question\"\n)\n,\n\"fewshot_examples\"\n:\nfewshot_examples\n,\n\"schema\"\n:\nenhanced_graph\n.\nschema\n,\n}\n)\nreturn\n{\n\"cypher_statement\"\n:\ngenerated_cypher\n,\n\"steps\"\n:\n[\n\"generate_cypher\"\n]\n}\nAPI Reference:\nStrOutputParser\nQuery validation\nâ€‹\nThe next step is to validate the generated Cypher statement and ensuring that all property values are accurate. While numbers and dates typically donâ€™t require validation, strings such as movie titles or peopleâ€™s names do. In this example, weâ€™ll use a basic\nCONTAINS\nclause for validation, though more advanced mapping and validation techniques can be implemented if needed.\nFirst, we will create a chain that detects any errors in the Cypher statement and extracts the property values it references.\nfrom\ntyping\nimport\nList\n,\nOptional\nvalidate_cypher_system\n=\n\"\"\"\nYou are a Cypher expert reviewing a statement written by a junior developer.\n\"\"\"\nvalidate_cypher_user\n=\n\"\"\"You must check the following:\n* Are there any syntax errors in the Cypher statement?\n* Are there any missing or undefined variables in the Cypher statement?\n* Are any node labels missing from the schema?\n* Are any relationship types missing from the schema?\n* Are any of the properties not included in the schema?\n* Does the Cypher statement include enough information to answer the question?\nExamples of good errors:\n* Label (:Foo) does not exist, did you mean (:Bar)?\n* Property bar does not exist for label Foo, did you mean baz?\n* Relationship FOO does not exist, did you mean FOO_BAR?\nSchema:\n{schema}\nThe question is:\n{question}\nThe Cypher statement is:\n{cypher}\nMake sure you don't make any mistakes!\"\"\"\nvalidate_cypher_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nvalidate_cypher_system\n,\n)\n,\n(\n\"human\"\n,\n(\nvalidate_cypher_user\n)\n,\n)\n,\n]\n)\nclass\nProperty\n(\nBaseModel\n)\n:\n\"\"\"\nRepresents a filter condition based on a specific node property in a graph in a Cypher statement.\n\"\"\"\nnode_label\n:\nstr\n=\nField\n(\ndescription\n=\n\"The label of the node to which this property belongs.\"\n)\nproperty_key\n:\nstr\n=\nField\n(\ndescription\n=\n\"The key of the property being filtered.\"\n)\nproperty_value\n:\nstr\n=\nField\n(\ndescription\n=\n\"The value that the property is being matched against.\"\n)\nclass\nValidateCypherOutput\n(\nBaseModel\n)\n:\n\"\"\"\nRepresents the validation result of a Cypher query's output,\nincluding any errors and applied filters.\n\"\"\"\nerrors\n:\nOptional\n[\nList\n[\nstr\n]\n]\n=\nField\n(\ndescription\n=\n\"A list of syntax or semantical errors in the Cypher statement. Always explain the discrepancy between schema and Cypher statement\"\n)\nfilters\n:\nOptional\n[\nList\n[\nProperty\n]\n]\n=\nField\n(\ndescription\n=\n\"A list of property-based filters applied in the Cypher statement.\"\n)\nvalidate_cypher_chain\n=\nvalidate_cypher_prompt\n|\nllm\n.\nwith_structured_output\n(\nValidateCypherOutput\n)\nLLMs often struggle with correctly determining relationship directions in generated Cypher statements. Since we have access to the schema, we can deterministically correct these directions using the\nCypherQueryCorrector\n.\nNote: The\nCypherQueryCorrector\nis an experimental feature and doesn't support all the newest Cypher syntax.\nfrom\nlangchain_neo4j\n.\nchains\n.\ngraph_qa\n.\ncypher_utils\nimport\nCypherQueryCorrector\n,\nSchema\n# Cypher query corrector is experimental\ncorrector_schema\n=\n[\nSchema\n(\nel\n[\n\"start\"\n]\n,\nel\n[\n\"type\"\n]\n,\nel\n[\n\"end\"\n]\n)\nfor\nel\nin\nenhanced_graph\n.\nstructured_schema\n.\nget\n(\n\"relationships\"\n)\n]\ncypher_query_corrector\n=\nCypherQueryCorrector\n(\ncorrector_schema\n)\nNow we can implement the Cypher validation step. First, we use the\nEXPLAIN\nmethod to detect any syntax errors. Next, we leverage the LLM to identify potential issues and extract the properties used for filtering. For string properties, we validate them against the database using a simple\nCONTAINS\nclause.\nBased on the validation results, the process can take the following paths:\nIf value mapping fails, we end the conversation and inform the user that we couldn't identify a specific property value (e.g., a person or movie title).\nIf errors are found, we route the query for correction.\nIf no issues are detected, we proceed to the Cypher execution step.\nfrom\nneo4j\n.\nexceptions\nimport\nCypherSyntaxError\ndef\nvalidate_cypher\n(\nstate\n:\nOverallState\n)\n-\n>\nOverallState\n:\n\"\"\"\nValidates the Cypher statements and maps any property values to the database.\n\"\"\"\nerrors\n=\n[\n]\nmapping_errors\n=\n[\n]\n# Check for syntax errors\ntry\n:\nenhanced_graph\n.\nquery\n(\nf\"EXPLAIN\n{\nstate\n.\nget\n(\n'cypher_statement'\n)\n}\n\"\n)\nexcept\nCypherSyntaxError\nas\ne\n:\nerrors\n.\nappend\n(\ne\n.\nmessage\n)\n# Experimental feature for correcting relationship directions\ncorrected_cypher\n=\ncypher_query_corrector\n(\nstate\n.\nget\n(\n\"cypher_statement\"\n)\n)\nif\nnot\ncorrected_cypher\n:\nerrors\n.\nappend\n(\n\"The generated Cypher statement doesn't fit the graph schema\"\n)\nif\nnot\ncorrected_cypher\n==\nstate\n.\nget\n(\n\"cypher_statement\"\n)\n:\nprint\n(\n\"Relationship direction was corrected\"\n)\n# Use LLM to find additional potential errors and get the mapping for values\nllm_output\n=\nvalidate_cypher_chain\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n.\nget\n(\n\"question\"\n)\n,\n\"schema\"\n:\nenhanced_graph\n.\nschema\n,\n\"cypher\"\n:\nstate\n.\nget\n(\n\"cypher_statement\"\n)\n,\n}\n)\nif\nllm_output\n.\nerrors\n:\nerrors\n.\nextend\n(\nllm_output\n.\nerrors\n)\nif\nllm_output\n.\nfilters\n:\nfor\nfilter\nin\nllm_output\n.\nfilters\n:\n# Do mapping only for string values\nif\n(\nnot\n[\nprop\nfor\nprop\nin\nenhanced_graph\n.\nstructured_schema\n[\n\"node_props\"\n]\n[\nfilter\n.\nnode_label\n]\nif\nprop\n[\n\"property\"\n]\n==\nfilter\n.\nproperty_key\n]\n[\n0\n]\n[\n\"type\"\n]\n==\n\"STRING\"\n)\n:\ncontinue\nmapping\n=\nenhanced_graph\n.\nquery\n(\nf\"MATCH (n:\n{\nfilter\n.\nnode_label\n}\n) WHERE toLower(n.`\n{\nfilter\n.\nproperty_key\n}\n`) = toLower($value) RETURN 'yes' LIMIT 1\"\n,\n{\n\"value\"\n:\nfilter\n.\nproperty_value\n}\n,\n)\nif\nnot\nmapping\n:\nprint\n(\nf\"Missing value mapping for\n{\nfilter\n.\nnode_label\n}\non property\n{\nfilter\n.\nproperty_key\n}\nwith value\n{\nfilter\n.\nproperty_value\n}\n\"\n)\nmapping_errors\n.\nappend\n(\nf\"Missing value mapping for\n{\nfilter\n.\nnode_label\n}\non property\n{\nfilter\n.\nproperty_key\n}\nwith value\n{\nfilter\n.\nproperty_value\n}\n\"\n)\nif\nmapping_errors\n:\nnext_action\n=\n\"end\"\nelif\nerrors\n:\nnext_action\n=\n\"correct_cypher\"\nelse\n:\nnext_action\n=\n\"execute_cypher\"\nreturn\n{\n\"next_action\"\n:\nnext_action\n,\n\"cypher_statement\"\n:\ncorrected_cypher\n,\n\"cypher_errors\"\n:\nerrors\n,\n\"steps\"\n:\n[\n\"validate_cypher\"\n]\n,\n}\nThe Cypher correction step takes the existing Cypher statement, any identified errors, and the original question to generate a corrected version of the query.\ncorrect_cypher_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n(\n\"You are a Cypher expert reviewing a statement written by a junior developer. \"\n\"You need to correct the Cypher statement based on the provided errors. No pre-amble.\"\n\"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"\n)\n,\n)\n,\n(\n\"human\"\n,\n(\n\"\"\"Check for invalid syntax or semantics and return a corrected Cypher statement.\nSchema:\n{schema}\nNote: Do not include any explanations or apologies in your responses.\nDo not wrap the response in any backticks or anything else.\nRespond with a Cypher statement only!\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\nThe question is:\n{question}\nThe Cypher statement is:\n{cypher}\nThe errors are:\n{errors}\nCorrected Cypher statement: \"\"\"\n)\n,\n)\n,\n]\n)\ncorrect_cypher_chain\n=\ncorrect_cypher_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\ndef\ncorrect_cypher\n(\nstate\n:\nOverallState\n)\n-\n>\nOverallState\n:\n\"\"\"\nCorrect the Cypher statement based on the provided errors.\n\"\"\"\ncorrected_cypher\n=\ncorrect_cypher_chain\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n.\nget\n(\n\"question\"\n)\n,\n\"errors\"\n:\nstate\n.\nget\n(\n\"cypher_errors\"\n)\n,\n\"cypher\"\n:\nstate\n.\nget\n(\n\"cypher_statement\"\n)\n,\n\"schema\"\n:\nenhanced_graph\n.\nschema\n,\n}\n)\nreturn\n{\n\"next_action\"\n:\n\"validate_cypher\"\n,\n\"cypher_statement\"\n:\ncorrected_cypher\n,\n\"steps\"\n:\n[\n\"correct_cypher\"\n]\n,\n}\nWe need to add a step that executes the given Cypher statement. If no results are returned, we should explicitly handle this scenario, as leaving the context empty can sometimes lead to LLM hallucinations.\nno_results\n=\n\"I couldn't find any relevant information in the database\"\ndef\nexecute_cypher\n(\nstate\n:\nOverallState\n)\n-\n>\nOverallState\n:\n\"\"\"\nExecutes the given Cypher statement.\n\"\"\"\nrecords\n=\nenhanced_graph\n.\nquery\n(\nstate\n.\nget\n(\n\"cypher_statement\"\n)\n)\nreturn\n{\n\"database_records\"\n:\nrecords\nif\nrecords\nelse\nno_results\n,\n\"next_action\"\n:\n\"end\"\n,\n\"steps\"\n:\n[\n\"execute_cypher\"\n]\n,\n}\nThe final step is to generate the answer. This involves combining the initial question with the database output to produce a relevant response.\ngenerate_final_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant\"\n,\n)\n,\n(\n\"human\"\n,\n(\n\"\"\"Use the following results retrieved from a database to provide\na succinct, definitive answer to the user's question.\nRespond as if you are answering the question directly.\nResults: {results}\nQuestion: {question}\"\"\"\n)\n,\n)\n,\n]\n)\ngenerate_final_chain\n=\ngenerate_final_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\ndef\ngenerate_final_answer\n(\nstate\n:\nOverallState\n)\n-\n>\nOutputState\n:\n\"\"\"\nDecides if the question is related to movies.\n\"\"\"\nfinal_answer\n=\ngenerate_final_chain\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n.\nget\n(\n\"question\"\n)\n,\n\"results\"\n:\nstate\n.\nget\n(\n\"database_records\"\n)\n}\n)\nreturn\n{\n\"answer\"\n:\nfinal_answer\n,\n\"steps\"\n:\n[\n\"generate_final_answer\"\n]\n}\nNext, we will implement the LangGraph workflow, starting with defining the conditional edge functions.\ndef\nguardrails_condition\n(\nstate\n:\nOverallState\n,\n)\n-\n>\nLiteral\n[\n\"generate_cypher\"\n,\n\"generate_final_answer\"\n]\n:\nif\nstate\n.\nget\n(\n\"next_action\"\n)\n==\n\"end\"\n:\nreturn\n\"generate_final_answer\"\nelif\nstate\n.\nget\n(\n\"next_action\"\n)\n==\n\"movie\"\n:\nreturn\n\"generate_cypher\"\ndef\nvalidate_cypher_condition\n(\nstate\n:\nOverallState\n,\n)\n-\n>\nLiteral\n[\n\"generate_final_answer\"\n,\n\"correct_cypher\"\n,\n\"execute_cypher\"\n]\n:\nif\nstate\n.\nget\n(\n\"next_action\"\n)\n==\n\"end\"\n:\nreturn\n\"generate_final_answer\"\nelif\nstate\n.\nget\n(\n\"next_action\"\n)\n==\n\"correct_cypher\"\n:\nreturn\n\"correct_cypher\"\nelif\nstate\n.\nget\n(\n\"next_action\"\n)\n==\n\"execute_cypher\"\n:\nreturn\n\"execute_cypher\"\nLet's put it all together now.\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\nlanggraph\n=\nStateGraph\n(\nOverallState\n,\ninput\n=\nInputState\n,\noutput\n=\nOutputState\n)\nlanggraph\n.\nadd_node\n(\nguardrails\n)\nlanggraph\n.\nadd_node\n(\ngenerate_cypher\n)\nlanggraph\n.\nadd_node\n(\nvalidate_cypher\n)\nlanggraph\n.\nadd_node\n(\ncorrect_cypher\n)\nlanggraph\n.\nadd_node\n(\nexecute_cypher\n)\nlanggraph\n.\nadd_node\n(\ngenerate_final_answer\n)\nlanggraph\n.\nadd_edge\n(\nSTART\n,\n\"guardrails\"\n)\nlanggraph\n.\nadd_conditional_edges\n(\n\"guardrails\"\n,\nguardrails_condition\n,\n)\nlanggraph\n.\nadd_edge\n(\n\"generate_cypher\"\n,\n\"validate_cypher\"\n)\nlanggraph\n.\nadd_conditional_edges\n(\n\"validate_cypher\"\n,\nvalidate_cypher_condition\n,\n)\nlanggraph\n.\nadd_edge\n(\n\"execute_cypher\"\n,\n\"generate_final_answer\"\n)\nlanggraph\n.\nadd_edge\n(\n\"correct_cypher\"\n,\n\"validate_cypher\"\n)\nlanggraph\n.\nadd_edge\n(\n\"generate_final_answer\"\n,\nEND\n)\nlanggraph\n=\nlanggraph\n.\ncompile\n(\n)\n# View\ndisplay\n(\nImage\n(\nlanggraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nAPI Reference:\nStateGraph\nWe can now test the application by asking an irrelevant question.\nlanggraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What's the weather in Spain?\"\n}\n)\n{'answer': \"I'm sorry, but I cannot provide current weather information. Please check a reliable weather website or app for the latest updates on the weather in Spain.\",\n'steps': ['guardrail', 'generate_final_answer']}\nLet's now ask something relevant about the movies.\nlanggraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What was the cast of the Casino?\"\n}\n)\n{'answer': 'The cast of \"Casino\" includes Robert De Niro, Joe Pesci, Sharon Stone, and James Woods.',\n'steps': ['guardrail',\n'generate_cypher',\n'validate_cypher',\n'execute_cypher',\n'generate_final_answer'],\n'cypher_statement': \"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a:Person) RETURN a.name\"}\nNext steps\nâ€‹\nFor other graph techniques like this and more check out:\nSemantic layer\n: Techniques for implementing semantic layers.\nConstructing graphs\n: Techniques for constructing knowledge graphs.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/llm_chain/",
    "Tutorials\nBuild a simple LLM application with chat models and prompt templates\nOn this page\nBuild a simple LLM application with chat models and prompt templates\nIn this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\nAfter reading this tutorial, you'll have a high level overview of:\nUsing\nlanguage models\nUsing\nprompt templates\nDebugging and tracing your application using\nLangSmith\nLet's dive in!\nSetup\nâ€‹\nJupyter Notebook\nâ€‹\nThis and other tutorials are perhaps most conveniently run in a\nJupyter notebooks\n. Going through guides in an interactive environment is a great way to better understand them. See\nhere\nfor instructions on how to install.\nInstallation\nâ€‹\nTo install LangChain run:\nPip\nConda\npip install langchain\nconda install langchain -c conda-forge\nFor more details, see our\nInstallation guide\n.\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nexport LANGSMITH_PROJECT=\"default\" # or any other project name\nOr, if in a notebook, you can set them with:\nimport\ngetpass\nimport\nos\ntry\n:\n# load environment variables from .env file (requires `python-dotenv`)\nfrom\ndotenv\nimport\nload_dotenv\nload_dotenv\n(\n)\nexcept\nImportError\n:\npass\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nif\n\"LANGSMITH_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\nprompt\n=\n\"Enter your LangSmith API key (optional): \"\n)\nif\n\"LANGSMITH_PROJECT\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"LANGSMITH_PROJECT\"\n]\n=\ngetpass\n.\ngetpass\n(\nprompt\n=\n'Enter your LangSmith Project Name (default = \"default\"): '\n)\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"LANGSMITH_PROJECT\"\n)\n:\nos\n.\nenviron\n[\n\"LANGSMITH_PROJECT\"\n]\n=\n\"default\"\nUsing Language Models\nâ€‹\nFirst up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to\nsupported integrations\n.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nLet's first use the model directly.\nChatModels\nare instances of LangChain\nRunnables\n, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of\nmessages\nto the\n.invoke\nmethod.\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\n,\nSystemMessage\nmessages\n=\n[\nSystemMessage\n(\ncontent\n=\n\"Translate the following from English into Italian\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"hi!\"\n)\n,\n]\nmodel\n.\ninvoke\n(\nmessages\n)\nAPI Reference:\nHumanMessage\n|\nSystemMessage\nAIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\ntip\nIf we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the\nLangSmith trace\n. The LangSmith trace reports\ntoken\nusage information, latency,\nstandard model parameters\n(such as temperature), and other information.\nNote that ChatModels receive\nmessage\nobjects as input and generate message objects as output. In addition to text content, message objects convey conversational\nroles\nand hold important data, such as\ntool calls\nand token usage counts.\nLangChain also supports chat model inputs via strings or\nOpenAI format\n. The following are equivalent:\nmodel\n.\ninvoke\n(\n\"Hello\"\n)\nmodel\n.\ninvoke\n(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hello\"\n}\n]\n)\nmodel\n.\ninvoke\n(\n[\nHumanMessage\n(\n\"Hello\"\n)\n]\n)\nStreaming\nâ€‹\nBecause chat models are\nRunnables\n, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\nfor\ntoken\nin\nmodel\n.\nstream\n(\nmessages\n)\n:\nprint\n(\ntoken\n.\ncontent\n,\nend\n=\n\"|\"\n)\n|C|iao|!||\nYou can find more details on streaming chat model outputs in\nthis guide\n.\nPrompt Templates\nâ€‹\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\nPrompt templates\nare a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\nLet's create a prompt template here. It will take in two user variables:\nlanguage\n: The language to translate text into\ntext\n: The text to translate\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nsystem_template\n=\n\"Translate the following from English into {language}\"\nprompt_template\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem_template\n)\n,\n(\n\"user\"\n,\n\"{text}\"\n)\n]\n)\nAPI Reference:\nChatPromptTemplate\nNote that\nChatPromptTemplate\nsupports multiple\nmessage roles\nin a single template. We format the\nlanguage\nparameter into the system message, and the user\ntext\ninto a user message.\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\nprompt\n=\nprompt_template\n.\ninvoke\n(\n{\n\"language\"\n:\n\"Italian\"\n,\n\"text\"\n:\n\"hi!\"\n}\n)\nprompt\nChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])\nWe can see that it returns a\nChatPromptValue\nthat consists of two messages. If we want to access the messages directly we do:\nprompt\n.\nto_messages\n(\n)\n[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\nHumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\nFinally, we can invoke the chat model on the formatted prompt:\nresponse\n=\nmodel\n.\ninvoke\n(\nprompt\n)\nprint\n(\nresponse\n.\ncontent\n)\nCiao!\ntip\nMessage\ncontent\ncan contain both text and\ncontent blocks\nwith additional structure. See\nthis guide\nfor more information.\nIf we take a look at the\nLangSmith trace\n, we can see exactly what prompt the chat model receives, along with\ntoken\nusage information, latency,\nstandard model parameters\n(such as temperature), and other information.\nConclusion\nâ€‹\nThat's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!\nFor further reading on the core concepts of LangChain, we've got detailed\nConceptual Guides\n.\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\nChat models\nPrompt templates\nAnd the LangSmith docs:\nLangSmith\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/chatbot/",
    "Tutorials\nBuild a Chatbot\nOn this page\nBuild a Chatbot\nnote\nThis tutorial previously used the\nRunnableWithMessageHistory\nabstraction. You can access that version of the documentation in the\nv0.2 docs\n.\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of\nLangGraph persistence\nto incorporate\nmemory\ninto new LangChain applications.\nIf your code is already relying on\nRunnableWithMessageHistory\nor\nBaseChatMessageHistory\n, you do\nnot\nneed to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses\nRunnableWithMessageHistory\nwill continue to work as expected.\nPlease see\nHow to migrate to LangGraph Memory\nfor more details.\nOverview\nâ€‹\nWe'll go over an example of how to design and implement an LLM-powered chatbot.\nThis chatbot will be able to have a conversation and remember previous interactions with a\nchat model\n.\nNote that this chatbot that we build will only use the language model to have a conversation.\nThere are several other related concepts that you may be looking for:\nConversational RAG\n: Enable a chatbot experience over an external source of data\nAgents\n: Build a chatbot that can take actions\nThis tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\nSetup\nâ€‹\nJupyter Notebook\nâ€‹\nThis guide (and most of the other guides in the documentation) uses\nJupyter notebooks\nand assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See\nhere\nfor instructions on how to install.\nInstallation\nâ€‹\nFor this tutorial we will need\nlangchain-core\nand\nlanggraph\n. This guide requires\nlanggraph >= 0.2.28\n.\nPip\nConda\npip install langchain-core langgraph>0.2.27\nconda install langchain-core langgraph>0.2.27 -c conda-forge\nFor more details, see our\nInstallation guide\n.\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above,\n(you'll need to create an API key from the Settings -> API Keys page on the LangSmith website)\n, make sure to set your environment variables to start logging traces:\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nOr, if in a notebook, you can set them with:\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nQuickstart\nâ€‹\nFirst up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nLet's first use the model directly.\nChatModel\ns are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the\n.invoke\nmethod.\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nmodel\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"Hi! I'm Bob\"\n)\n]\n)\nAPI Reference:\nHumanMessage\nAIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\nThe model on its own does not have any concept of state. For example, if you ask a followup question:\nmodel\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"What's my name?\"\n)\n]\n)\nAIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 11, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2d13a18-7022-4784-b54f-f85c097d1075-0', usage_metadata={'input_tokens': 11, 'output_tokens': 34, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\nLet's take a look at the example\nLangSmith trace\nWe can see that it doesn't take the previous conversation turn into context, and cannot answer the question.\nThis makes for a terrible chatbot experience!\nTo get around this, we need to pass the entire\nconversation history\ninto the model. Let's see what happens when we do that:\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\nmodel\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"Hi! I'm Bob\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Hello Bob! How can I assist you today?\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"What's my name?\"\n)\n,\n]\n)\nAPI Reference:\nAIMessage\nAIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\nAnd now we can see that we get a good response!\nThis is the basic idea underpinning a chatbot's ability to interact conversationally.\nSo how do we best implement this?\nMessage persistence\nâ€‹\nLangGraph\nimplements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its\ndocumentation\nfor more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nMessagesState\n,\nStateGraph\n# Define a new graph\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define the function that calls the model\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\nresponse\n=\nmodel\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\nreturn\n{\n\"messages\"\n:\nresponse\n}\n# Define the (single) node in the graph\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\n# Add memory\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nAPI Reference:\nMemorySaver\n|\nStateGraph\nWe now need to create a\nconfig\nthat we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a\nthread_id\n. This should look like:\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc123\"\n}\n}\nThis enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\nWe can then invoke the application:\nquery\n=\n\"Hi! I'm Bob.\"\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n# output contains all messages in state\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHi Bob! How can I assist you today?\nquery\n=\n\"What's my name?\"\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYour name is Bob! How can I help you today, Bob?\nGreat! Our chatbot now remembers things about us. If we change the config to reference a different\nthread_id\n, we can see that it starts the conversation fresh.\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc234\"\n}\n}\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nI'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today?\nHowever, we can always go back to the original conversation (since we are persisting it in a database)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc123\"\n}\n}\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYour name is Bob. What would you like to discuss today?\nThis is how we can support a chatbot having conversations with many users!\ntip\nFor async support, update the\ncall_model\nnode to be an async function and use\n.ainvoke\nwhen invoking the application:\n# Async function for node:\nasync\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\nresponse\n=\nawait\nmodel\n.\nainvoke\n(\nstate\n[\n\"messages\"\n]\n)\nreturn\n{\n\"messages\"\n:\nresponse\n}\n# Define graph as before:\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nMemorySaver\n(\n)\n)\n# Async invocation:\noutput\n=\nawait\napp\n.\nainvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nRight now, all we've done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\nPrompt templates\nâ€‹\nPrompt Templates\nhelp to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\nTo add in a system message, we will create a\nChatPromptTemplate\n. We will utilize\nMessagesPlaceholder\nto pass all the messages in.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nMessagesPlaceholder\nprompt_template\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You talk like a pirate. Answer all questions to the best of your ability.\"\n,\n)\n,\nMessagesPlaceholder\n(\nvariable_name\n=\n\"messages\"\n)\n,\n]\n)\nAPI Reference:\nChatPromptTemplate\n|\nMessagesPlaceholder\nWe can now update our application to incorporate this template:\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\nprompt\n=\nprompt_template\n.\ninvoke\n(\nstate\n)\nresponse\n=\nmodel\n.\ninvoke\n(\nprompt\n)\nreturn\n{\n\"messages\"\n:\nresponse\n}\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nWe invoke the application in the same way:\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc345\"\n}\n}\nquery\n=\n\"Hi! I'm Jim.\"\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nAhoy there, Jim! What brings ye to these waters today? Be ye seekin' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\nquery\n=\n\"What is my name?\"\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYe be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\nAwesome! Let's now make our prompt a little bit more complicated. Let's assume that the prompt template now looks something like this:\nprompt_template\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\"\n,\n)\n,\nMessagesPlaceholder\n(\nvariable_name\n=\n\"messages\"\n)\n,\n]\n)\nNote that we have added a new\nlanguage\ninput to the prompt. Our application now has two parameters-- the input\nmessages\nand\nlanguage\n. We should update our application's state to reflect this:\nfrom\ntyping\nimport\nSequence\nfrom\nlangchain_core\n.\nmessages\nimport\nBaseMessage\nfrom\nlanggraph\n.\ngraph\n.\nmessage\nimport\nadd_messages\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\nclass\nState\n(\nTypedDict\n)\n:\nmessages\n:\nAnnotated\n[\nSequence\n[\nBaseMessage\n]\n,\nadd_messages\n]\nlanguage\n:\nstr\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nState\n)\ndef\ncall_model\n(\nstate\n:\nState\n)\n:\nprompt\n=\nprompt_template\n.\ninvoke\n(\nstate\n)\nresponse\n=\nmodel\n.\ninvoke\n(\nprompt\n)\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nAPI Reference:\nBaseMessage\n|\nadd_messages\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc456\"\n}\n}\nquery\n=\n\"Hi! I'm Bob.\"\nlanguage\n=\n\"Spanish\"\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n,\n\"language\"\n:\nlanguage\n}\n,\nconfig\n,\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nÂ¡Hola, Bob! Â¿CÃ³mo puedo ayudarte hoy?\nNote that the entire state is persisted, so we can omit parameters like\nlanguage\nif no changes are desired:\nquery\n=\n\"What is my name?\"\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n,\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTu nombre es Bob. Â¿Hay algo mÃ¡s en lo que pueda ayudarte?\nTo help you understand what's happening internally, check out\nthis LangSmith trace\n.\nManaging Conversation History\nâ€‹\nOne important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\nImportantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\nWe can do this by adding a simple step in front of the prompt that modifies the\nmessages\nkey appropriately, and then wrap that new chain in the Message History class.\nLangChain comes with a few built-in helpers for\nmanaging a list of messages\n. In this case we'll use the\ntrim_messages\nhelper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:\nfrom\nlangchain_core\n.\nmessages\nimport\nSystemMessage\n,\ntrim_messages\ntrimmer\n=\ntrim_messages\n(\nmax_tokens\n=\n65\n,\nstrategy\n=\n\"last\"\n,\ntoken_counter\n=\nmodel\n,\ninclude_system\n=\nTrue\n,\nallow_partial\n=\nFalse\n,\nstart_on\n=\n\"human\"\n,\n)\nmessages\n=\n[\nSystemMessage\n(\ncontent\n=\n\"you're a good assistant\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"hi! I'm bob\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"hi!\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"I like vanilla ice cream\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"nice\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"whats 2 + 2\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"4\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"thanks\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"no problem!\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"having fun?\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"yes!\"\n)\n,\n]\ntrimmer\n.\ninvoke\n(\nmessages\n)\nAPI Reference:\nSystemMessage\n|\ntrim_messages\n[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\nAIMessage(content='4', additional_kwargs={}, response_metadata={}),\nHumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\nAIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\nHumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\nAIMessage(content='yes!', additional_kwargs={}, response_metadata={})]\nTo  use it in our chain, we just need to run the trimmer before we pass the\nmessages\ninput to our prompt.\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nState\n)\ndef\ncall_model\n(\nstate\n:\nState\n)\n:\nprint\n(\nf\"Messages before trimming:\n{\nlen\n(\nstate\n[\n'messages'\n]\n)\n}\n\"\n)\ntrimmed_messages\n=\ntrimmer\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\nprint\n(\nf\"Messages after trimming:\n{\nlen\n(\ntrimmed_messages\n)\n}\n\"\n)\nprint\n(\n\"Remaining messages:\"\n)\nfor\nmsg\nin\ntrimmed_messages\n:\nprint\n(\nf\"\n{\ntype\n(\nmsg\n)\n.\n__name__\n}\n:\n{\nmsg\n.\ncontent\n}\n\"\n)\nprompt\n=\nprompt_template\n.\ninvoke\n(\n{\n\"messages\"\n:\ntrimmed_messages\n,\n\"language\"\n:\nstate\n[\n\"language\"\n]\n}\n)\nresponse\n=\nmodel\n.\ninvoke\n(\nprompt\n)\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nNow if we try asking the model our name, it won't know it since we trimmed that part of the chat history. (By defining our trim stragegy as\n'last'\n, we are only keeping the most recent messages that fit within the\nmax_tokens\n.)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc567\"\n}\n}\nquery\n=\n\"What is my name?\"\nlanguage\n=\n\"English\"\ninput_messages\n=\nmessages\n+\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n,\n\"language\"\n:\nlanguage\n}\n,\nconfig\n,\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nMessages before trimming: 12\nMessages after trimming: 8\nRemaining messages:\nSystemMessage: you're a good assistant\nHumanMessage: whats 2 + 2\nAIMessage: 4\nHumanMessage: thanks\nAIMessage: no problem!\nHumanMessage: having fun?\nAIMessage: yes!\nHumanMessage: What is my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nI don't know your name. If you'd like to share it, feel free!\nBut if we ask about information that is within the last few messages, it remembers:\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc678\"\n}\n}\nquery\n=\n\"What math problem was asked?\"\nlanguage\n=\n\"English\"\ninput_messages\n=\nmessages\n+\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n,\n\"language\"\n:\nlanguage\n}\n,\nconfig\n,\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nMessages before trimming: 12\nMessages after trimming: 8\nRemaining messages:\nSystemMessage: you're a good assistant\nHumanMessage: whats 2 + 2\nAIMessage: 4\nHumanMessage: thanks\nAIMessage: no problem!\nHumanMessage: having fun?\nAIMessage: yes!\nHumanMessage: What math problem was asked?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe math problem that was asked was \"what's 2 + 2.\"\nIf you take a look at LangSmith, you can see exactly what is happening under the hood in the\nLangSmith trace\n.\nStreaming\nâ€‹\nNow we've got a functioning chatbot. However, one\nreally\nimportant UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\nIt's actually super easy to do this!\nBy default,\n.stream\nin our LangGraph application streams application steps-- in this case, the single step of the model response. Setting\nstream_mode=\"messages\"\nallows us to stream output tokens instead:\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc789\"\n}\n}\nquery\n=\n\"Hi I'm Todd, please tell me a joke.\"\nlanguage\n=\n\"English\"\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\nfor\nchunk\n,\nmetadata\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\ninput_messages\n,\n\"language\"\n:\nlanguage\n}\n,\nconfig\n,\nstream_mode\n=\n\"messages\"\n,\n)\n:\nif\nisinstance\n(\nchunk\n,\nAIMessage\n)\n:\n# Filter to just model responses\nprint\n(\nchunk\n.\ncontent\n,\nend\n=\n\"|\"\n)\n|Hi| Todd|!| Here|â€™s| a| joke| for| you|:\n|Why| don't| scientists| trust| atoms|?\n|Because| they| make| up| everything|!||\nNext Steps\nâ€‹\nNow that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\nConversational RAG\n: Enable a chatbot experience over an external source of data\nAgents\n: Build a chatbot that can take actions\nIf you want to dive deeper on specifics, some things worth checking out are:\nStreaming\n: streaming is\ncrucial\nfor chat applications\nHow to add message history\n: for a deeper dive into all things related to message history\nHow to manage large message history\n: more techniques for managing a large chat history\nLangGraph main docs\n: for more detail on building with LangGraph\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/qa_chat_history/",
    "Tutorials\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nOn this page\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nIn many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\nThis is the second part of a multi-part tutorial:\nPart 1\nintroduces RAG and walks through a minimal implementation.\nPart 2\n(this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\nHere we focus on\nadding logic for incorporating historical messages.\nThis involves the management of a\nchat history\n.\nWe will cover two approaches:\nChains\n, in which we execute at most one retrieval step;\nAgents\n, in which we give an LLM discretion to execute multiple retrieval steps.\nnote\nThe methods presented here leverage\ntool-calling\ncapabilities in modern\nchat models\n. See\nthis page\nfor a table of models supporting tool calling features.\nFor the external knowledge source, we will use the same\nLLM Powered Autonomous Agents\nblog post by Lilian Weng from the\nPart 1\nof the RAG tutorial.\nSetup\nâ€‹\nComponents\nâ€‹\nWe will need to select three components from LangChain's suite of integrations.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nSelect\nvector store\n:\nIn-memory\nâ–¾\nIn-memory\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\npip install -qU langchain-core\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\nDependencies\nâ€‹\nIn addition, we'll use the following packages:\n%\n%\ncapture\n-\n-\nno\n-\nstderr\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langgraph langchain\n-\ncommunity beautifulsoup4\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"LANGSMITH_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nChains\nâ€‹\nLet's first revisit the vector store we built in\nPart 1\n, which indexes an\nLLM Powered Autonomous Agents\nblog post by Lilian Weng.\nimport\nbs4\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader\n(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,\n)\n,\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4\n.\nSoupStrainer\n(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n)\n,\n)\ndocs\n=\nloader\n.\nload\n(\n)\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\nAPI Reference:\nDocument\n# Index chunks\n_\n=\nvector_store\n.\nadd_documents\n(\ndocuments\n=\nall_splits\n)\nIn the\nPart 1\nof the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of\nmessages\n. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via\ntool messages\n. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have\nUser input as a\nHumanMessage\n;\nVector store query as an\nAIMessage\nwith tool calls;\nRetrieved documents as a\nToolMessage\n;\nFinal response as a\nAIMessage\n.\nThis model for state is so versatile that LangGraph offers a built-in version for convenience:\nfrom\nlanggraph\n.\ngraph\nimport\nMessagesState\n,\nStateGraph\ngraph_builder\n=\nStateGraph\n(\nMessagesState\n)\nAPI Reference:\nStateGraph\nLeveraging\ntool-calling\nto interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:\nHuman: \"What is Task Decomposition?\"\nAI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\nHuman: \"What are common ways of doing it?\"\nIn this scenario, a model could generate a query such as\n\"common approaches to task decomposition\"\n. Tool-calling facilitates this naturally. As in the\nquery analysis\nsection of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).\nLet's turn our retrieval step into a\ntool\n:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve\n(\nquery\n:\nstr\n)\n:\n\"\"\"Retrieve information related to a query.\"\"\"\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nquery\n,\nk\n=\n2\n)\nserialized\n=\n\"\\n\\n\"\n.\njoin\n(\n(\nf\"Source:\n{\ndoc\n.\nmetadata\n}\n\\nContent:\n{\ndoc\n.\npage_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized\n,\nretrieved_docs\nAPI Reference:\ntool\nSee\nthis guide\nfor more detail on creating tools.\nOur graph will consist of three nodes:\nA node that fields the user input, either generating a query for the retriever or responding directly;\nA node for the retriever tool that executes the retrieval step;\nA node that generates the final response using the retrieved context.\nWe build them below. Note that we leverage another pre-built LangGraph component,\nToolNode\n, that executes the tool and adds the result as a\nToolMessage\nto the state.\nfrom\nlangchain_core\n.\nmessages\nimport\nSystemMessage\nfrom\nlanggraph\n.\nprebuilt\nimport\nToolNode\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef\nquery_or_respond\n(\nstate\n:\nMessagesState\n)\n:\n\"\"\"Generate tool call for retrieval or respond.\"\"\"\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nretrieve\n]\n)\nresponse\n=\nllm_with_tools\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\n# MessagesState appends messages to state instead of overwriting\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\n# Step 2: Execute the retrieval.\ntools\n=\nToolNode\n(\n[\nretrieve\n]\n)\n# Step 3: Generate a response using the retrieved content.\ndef\ngenerate\n(\nstate\n:\nMessagesState\n)\n:\n\"\"\"Generate answer.\"\"\"\n# Get generated ToolMessages\nrecent_tool_messages\n=\n[\n]\nfor\nmessage\nin\nreversed\n(\nstate\n[\n\"messages\"\n]\n)\n:\nif\nmessage\n.\ntype\n==\n\"tool\"\n:\nrecent_tool_messages\n.\nappend\n(\nmessage\n)\nelse\n:\nbreak\ntool_messages\n=\nrecent_tool_messages\n[\n:\n:\n-\n1\n]\n# Format into prompt\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\ncontent\nfor\ndoc\nin\ntool_messages\n)\nsystem_message_content\n=\n(\n\"You are an assistant for question-answering tasks. \"\n\"Use the following pieces of retrieved context to answer \"\n\"the question. If you don't know the answer, say that you \"\n\"don't know. Use three sentences maximum and keep the \"\n\"answer concise.\"\n\"\\n\\n\"\nf\"\n{\ndocs_content\n}\n\"\n)\nconversation_messages\n=\n[\nmessage\nfor\nmessage\nin\nstate\n[\n\"messages\"\n]\nif\nmessage\n.\ntype\nin\n(\n\"human\"\n,\n\"system\"\n)\nor\n(\nmessage\n.\ntype\n==\n\"ai\"\nand\nnot\nmessage\n.\ntool_calls\n)\n]\nprompt\n=\n[\nSystemMessage\n(\nsystem_message_content\n)\n]\n+\nconversation_messages\n# Run\nresponse\n=\nllm\n.\ninvoke\n(\nprompt\n)\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\nAPI Reference:\nSystemMessage\n|\nToolNode\nFinally, we compile our application into a single\ngraph\nobject. In this case, we are just connecting the steps into a sequence. We also allow the first\nquery_or_respond\nstep to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\nfrom\nlanggraph\n.\ngraph\nimport\nEND\nfrom\nlanggraph\n.\nprebuilt\nimport\nToolNode\n,\ntools_condition\ngraph_builder\n.\nadd_node\n(\nquery_or_respond\n)\ngraph_builder\n.\nadd_node\n(\ntools\n)\ngraph_builder\n.\nadd_node\n(\ngenerate\n)\ngraph_builder\n.\nset_entry_point\n(\n\"query_or_respond\"\n)\ngraph_builder\n.\nadd_conditional_edges\n(\n\"query_or_respond\"\n,\ntools_condition\n,\n{\nEND\n:\nEND\n,\n\"tools\"\n:\n\"tools\"\n}\n,\n)\ngraph_builder\n.\nadd_edge\n(\n\"tools\"\n,\n\"generate\"\n)\ngraph_builder\n.\nadd_edge\n(\n\"generate\"\n,\nEND\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nToolNode\n|\ntools_condition\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nLet's test our application.\nNote that it responds appropriately to messages that do not require an additional retrieval step:\ninput_message\n=\n\"Hello\"\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nHello\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello! How can I assist you today?\nAnd when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:\ninput_message\n=\n\"What is Task Decomposition?\"\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat is Task Decomposition?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_dLjB3rkMoxZZxwUGXi33UBeh)\nCall ID: call_dLjB3rkMoxZZxwUGXi33UBeh\nArgs:\nquery: Task Decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTask Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often involves techniques like Chain of Thought (CoT), which encourages models to think step by step, enhancing performance on complex tasks. This approach allows for a clearer understanding of the task and aids in structuring the problem-solving process.\nCheck out the LangSmith trace\nhere\n.\nStateful management of chat history\nâ€‹\nnote\nThis section of the tutorial previously used the\nRunnableWithMessageHistory\nabstraction. You can access that version of the documentation in the\nv0.2 docs\n.\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of\nLangGraph persistence\nto incorporate\nmemory\ninto new LangChain applications.\nIf your code is already relying on\nRunnableWithMessageHistory\nor\nBaseChatMessageHistory\n, you do\nnot\nneed to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses\nRunnableWithMessageHistory\nwill continue to work as expected.\nPlease see\nHow to migrate to LangGraph Memory\nfor more details.\nIn production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately.\nLangGraph\nimplements a built-in\npersistence layer\n, making it ideal for chat applications that support multiple conversational turns.\nTo manage multiple conversational turns and threads, all we have to do is specify a\ncheckpointer\nwhen compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its\ndocumentation\nfor more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\nFor a detailed walkthrough of how to manage message history, head to the\nHow to add message history (memory)\nguide.\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nmemory\n=\nMemorySaver\n(\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\n# Specify an ID for the thread\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc123\"\n}\n}\nAPI Reference:\nMemorySaver\nWe can now invoke similar to before:\ninput_message\n=\n\"What is Task Decomposition?\"\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\nconfig\n=\nconfig\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat is Task Decomposition?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_JZb6GLD812bW2mQsJ5EJQDnN)\nCall ID: call_JZb6GLD812bW2mQsJ5EJQDnN\nArgs:\nquery: Task Decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTask Decomposition is a technique used to break down complicated tasks into smaller, manageable steps. It involves using methods like Chain of Thought (CoT) prompting, which encourages the model to think step by step, enhancing performance on complex tasks. This process helps to clarify the model's reasoning and makes it easier to tackle difficult problems.\ninput_message\n=\n\"Can you look up some common ways of doing it?\"\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\nconfig\n=\nconfig\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nCan you look up some common ways of doing it?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_kjRI4Y5cJOiB73yvd7dmb6ux)\nCall ID: call_kjRI4Y5cJOiB73yvd7dmb6ux\nArgs:\nquery: common methods of task decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nCommon ways of performing Task Decomposition include: (1) using Large Language Models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", (2) employing task-specific instructions such as \"Write a story outline\" for specific tasks, and (3) incorporating human inputs to guide the decomposition process.\nNote that the query generated by the model in the second question incorporates the conversational context.\nThe\nLangSmith\ntrace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step.\nAgents\nâ€‹\nAgents\nleverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\nBelow we assemble a minimal RAG agent. Using LangGraph's\npre-built ReAct agent constructor\n, we can do this in one line.\ntip\nCheck out LangGraph's\nAgentic RAG\ntutorial for more advanced formulations.\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nagent_executor\n=\ncreate_react_agent\n(\nllm\n,\n[\nretrieve\n]\n,\ncheckpointer\n=\nmemory\n)\nAPI Reference:\ncreate_react_agent\nLet's inspect the graph:\ndisplay\n(\nImage\n(\nagent_executor\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nThe key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.\nLet's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"def234\"\n}\n}\ninput_message\n=\n(\n\"What is the standard method for Task Decomposition?\\n\\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\nconfig\n=\nconfig\n,\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_Y3YaIzL71B83Cjqa8d2G0O8N)\nCall ID: call_Y3YaIzL71B83Cjqa8d2G0O8N\nArgs:\nquery: standard method for Task Decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_2JntP1x4XQMWwgVpYurE12ff)\nCall ID: call_2JntP1x4XQMWwgVpYurE12ff\nArgs:\nquery: common extensions of Task Decomposition methods\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe standard method for task decomposition involves using techniques such as Chain of Thought (CoT), where a model is instructed to \"think step by step\" to break down complex tasks into smaller, more manageable components. This approach enhances model performance by allowing for more thorough reasoning and planning. Task decomposition can be accomplished through various means, including:\n1. Simple prompting (e.g., asking for steps to achieve a goal).\n2. Task-specific instructions (e.g., asking for a story outline).\n3. Human inputs to guide the decomposition process.\n### Common Extensions of Task Decomposition Methods:\n1. **Tree of Thoughts**: This extension builds on CoT by not only decomposing the problem into thought steps but also generating multiple thoughts at each step, creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.\nThese extensions aim to enhance reasoning capabilities and improve the effectiveness of task decomposition in various contexts.\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nNext steps\nâ€‹\nWe've covered the steps to build a basic conversational Q&A application:\nWe used chains to build a predictable application that generates at most one query per user input;\nWe used agents to build an application that can iterate on a sequence of queries.\nTo explore different types of retrievers and retrieval strategies, visit the\nretrievers\nsection of the how-to guides.\nFor a detailed walkthrough of LangChain's conversation memory abstractions, visit the\nHow to add message history (memory)\nguide.\nTo learn more about agents, check out the\nconceptual guide\nand LangGraph\nagent architectures\npage.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/extraction/",
    "Tutorials\nBuild an Extraction Chain\nOn this page\nBuild an Extraction Chain\nIn this tutorial, we will use\ntool-calling\nfeatures of\nchat models\nto extract structured information from unstructured text. We will also demonstrate how to use\nfew-shot prompting\nin this context to improve performance.\nimportant\nThis tutorial requires\nlangchain-core>=0.3.20\nand will only work with models that support\ntool calling\n.\nSetup\nâ€‹\nJupyter Notebook\nâ€‹\nThis and other tutorials are perhaps most conveniently run in a\nJupyter notebooks\n. Going through guides in an interactive environment is a great way to better understand them. See\nhere\nfor instructions on how to install.\nInstallation\nâ€‹\nTo install LangChain run:\nPip\nConda\npip install --upgrade langchain-core\nconda install langchain-core -c conda-forge\nFor more details, see our\nInstallation guide\n.\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nOr, if in a notebook, you can set them with:\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nThe Schema\nâ€‹\nFirst, we need to describe what information we want to extract from the text.\nWe'll use Pydantic to define an example schema  to extract personal information.\nfrom\ntyping\nimport\nOptional\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nPerson\n(\nBaseModel\n)\n:\n\"\"\"Information about a person.\"\"\"\n# ^ Doc-string for the entity Person.\n# This doc-string is sent to the LLM as the description of the schema Person,\n# and it can help to improve extraction results.\n# Note that:\n# 1. Each field is an `optional` -- this allows the model to decline to extract it!\n# 2. Each field has a `description` -- this description is used by the LLM.\n# Having a good description can help improve extraction results.\nname\n:\nOptional\n[\nstr\n]\n=\nField\n(\ndefault\n=\nNone\n,\ndescription\n=\n\"The name of the person\"\n)\nhair_color\n:\nOptional\n[\nstr\n]\n=\nField\n(\ndefault\n=\nNone\n,\ndescription\n=\n\"The color of the person's hair if known\"\n)\nheight_in_meters\n:\nOptional\n[\nstr\n]\n=\nField\n(\ndefault\n=\nNone\n,\ndescription\n=\n\"Height measured in meters\"\n)\nThere are two best practices when defining schema:\nDocument the\nattributes\nand the\nschema\nitself: This information is sent to the LLM and is used to improve the quality of information extraction.\nDo not force the LLM to make up information! Above we used\nOptional\nfor the attributes allowing the LLM to output\nNone\nif it doesn't know the answer.\nimportant\nFor best performance, document the schema well and make sure the model isn't forced to return results if there's no information to be extracted in the text.\nThe Extractor\nâ€‹\nLet's create an information extractor using the schema we defined above.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nMessagesPlaceholder\n# Define a custom prompt to provide instructions and any additional context.\n# 1) You can add examples into the prompt template to improve extraction quality\n# 2) Introduce additional parameters to take context into account (e.g., include metadata\n#    about the document from which the text was extracted.)\nprompt_template\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are an expert extraction algorithm. \"\n\"Only extract relevant information from the text. \"\n\"If you do not know the value of an attribute asked to extract, \"\n\"return null for the attribute's value.\"\n,\n)\n,\n# Please see the how-to about improving performance with\n# reference examples.\n# MessagesPlaceholder('examples'),\n(\n\"human\"\n,\n\"{text}\"\n)\n,\n]\n)\nAPI Reference:\nChatPromptTemplate\n|\nMessagesPlaceholder\nWe need to use a model that supports function/tool calling.\nPlease review\nthe documentation\nfor all models that can be used with this API.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nschema\n=\nPerson\n)\nLet's test it out:\ntext\n=\n\"Alan Smith is 6 feet tall and has blond hair.\"\nprompt\n=\nprompt_template\n.\ninvoke\n(\n{\n\"text\"\n:\ntext\n}\n)\nstructured_llm\n.\ninvoke\n(\nprompt\n)\nPerson(name='Alan Smith', hair_color='blond', height_in_meters='1.83')\nimportant\nExtraction is Generative ðŸ¤¯\nLLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters\neven though it was provided in feet!\nWe can see the LangSmith trace\nhere\n. Note that the\nchat model portion of the trace\nreveals the exact sequence of messages sent to the model, tools invoked, and other metadata.\nMultiple Entities\nâ€‹\nIn\nmost cases\n, you should be extracting a list of entities rather than a single entity.\nThis can be easily achieved using pydantic by nesting models inside one another.\nfrom\ntyping\nimport\nList\n,\nOptional\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nPerson\n(\nBaseModel\n)\n:\n\"\"\"Information about a person.\"\"\"\n# ^ Doc-string for the entity Person.\n# This doc-string is sent to the LLM as the description of the schema Person,\n# and it can help to improve extraction results.\n# Note that:\n# 1. Each field is an `optional` -- this allows the model to decline to extract it!\n# 2. Each field has a `description` -- this description is used by the LLM.\n# Having a good description can help improve extraction results.\nname\n:\nOptional\n[\nstr\n]\n=\nField\n(\ndefault\n=\nNone\n,\ndescription\n=\n\"The name of the person\"\n)\nhair_color\n:\nOptional\n[\nstr\n]\n=\nField\n(\ndefault\n=\nNone\n,\ndescription\n=\n\"The color of the person's hair if known\"\n)\nheight_in_meters\n:\nOptional\n[\nstr\n]\n=\nField\n(\ndefault\n=\nNone\n,\ndescription\n=\n\"Height measured in meters\"\n)\nclass\nData\n(\nBaseModel\n)\n:\n\"\"\"Extracted data about people.\"\"\"\n# Creates a model so that we can extract multiple entities.\npeople\n:\nList\n[\nPerson\n]\nimportant\nExtraction results might not be perfect here. Read on to see how to use\nReference Examples\nto improve the quality of extraction, and check out our extraction\nhow-to\nguides for more detail.\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nschema\n=\nData\n)\ntext\n=\n\"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\nprompt\n=\nprompt_template\n.\ninvoke\n(\n{\n\"text\"\n:\ntext\n}\n)\nstructured_llm\n.\ninvoke\n(\nprompt\n)\nData(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])\ntip\nWhen the schema accommodates the extraction of\nmultiple entities\n, it also allows the model to extract\nno entities\nif no relevant information\nis in the text by providing an empty list.\nThis is usually a\ngood\nthing! It allows specifying\nrequired\nattributes on an entity without necessarily forcing the model to detect this entity.\nWe can see the LangSmith trace\nhere\n.\nReference examples\nâ€‹\nThe behavior of LLM applications can be steered using\nfew-shot prompting\n. For\nchat models\n, this can take the form of a sequence of pairs of input and response messages demonstrating desired behaviors.\nFor example, we can convey the meaning of a symbol with alternating\nuser\nand\nassistant\nmessages\n:\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"2 ðŸ¦œ 2\"\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"4\"\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"2 ðŸ¦œ 3\"\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"5\"\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"3 ðŸ¦œ 4\"\n}\n,\n]\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nprint\n(\nresponse\n.\ncontent\n)\n7\nStructured output\noften uses\ntool calling\nunder-the-hood. This typically involves the generation of\nAI messages\ncontaining tool calls, as well as\ntool messages\ncontaining the results of tool calls. What should a sequence of messages look like in this case?\nDifferent\nchat model providers\nimpose different requirements for valid message sequences. Some will accept a (repeating) message sequence of the form:\nUser message\nAI message with tool call\nTool message with result\nOthers require a final AI message containing some sort of response.\nLangChain includes a utility function\ntool_example_to_messages\nthat will generate a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the corresponding tool calls.\nLet's try this out. We can convert pairs of input strings and desired Pydantic objects to a sequence of messages that can be provided to a chat model. Under the hood, LangChain will format the tool calls to each provider's required format.\nNote: this version of\ntool_example_to_messages\nrequires\nlangchain-core>=0.3.20\n.\nfrom\nlangchain_core\n.\nutils\n.\nfunction_calling\nimport\ntool_example_to_messages\nexamples\n=\n[\n(\n\"The ocean is vast and blue. It's more than 20,000 feet deep.\"\n,\nData\n(\npeople\n=\n[\n]\n)\n,\n)\n,\n(\n\"Fiona traveled far from France to Spain.\"\n,\nData\n(\npeople\n=\n[\nPerson\n(\nname\n=\n\"Fiona\"\n,\nheight_in_meters\n=\nNone\n,\nhair_color\n=\nNone\n)\n]\n)\n,\n)\n,\n]\nmessages\n=\n[\n]\nfor\ntxt\n,\ntool_call\nin\nexamples\n:\nif\ntool_call\n.\npeople\n:\n# This final message is optional for some providers\nai_response\n=\n\"Detected people.\"\nelse\n:\nai_response\n=\n\"Detected no people.\"\nmessages\n.\nextend\n(\ntool_example_to_messages\n(\ntxt\n,\n[\ntool_call\n]\n,\nai_response\n=\nai_response\n)\n)\nAPI Reference:\ntool_example_to_messages\nInspecting the result, we see these two example pairs generated eight messages:\nfor\nmessage\nin\nmessages\n:\nmessage\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nThe ocean is vast and blue. It's more than 20,000 feet deep.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nData (d8f2e054-7fb9-417f-b28f-0447a775b2c3)\nCall ID: d8f2e054-7fb9-417f-b28f-0447a775b2c3\nArgs:\npeople: []\n=================================\u001b[1m Tool Message \u001b[0m=================================\nYou have correctly called this tool.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nDetected no people.\n================================\u001b[1m Human Message \u001b[0m=================================\nFiona traveled far from France to Spain.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nData (0178939e-a4b1-4d2a-a93e-b87f665cdfd6)\nCall ID: 0178939e-a4b1-4d2a-a93e-b87f665cdfd6\nArgs:\npeople: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]\n=================================\u001b[1m Tool Message \u001b[0m=================================\nYou have correctly called this tool.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nDetected people.\nLet's compare performance with and without these messages. For example, let's pass a message for which we intend no people to be extracted:\nmessage_no_extraction\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"The solar system is large, but earth has only 1 moon.\"\n,\n}\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nschema\n=\nData\n)\nstructured_llm\n.\ninvoke\n(\n[\nmessage_no_extraction\n]\n)\nData(people=[Person(name='Earth', hair_color='None', height_in_meters='0.00')])\nIn this example, the model is liable to erroneously generate records of people.\nBecause our few-shot examples contain examples of \"negatives\", we encourage the model to behave correctly in this case:\nstructured_llm\n.\ninvoke\n(\nmessages\n+\n[\nmessage_no_extraction\n]\n)\nData(people=[])\ntip\nThe\nLangSmith\ntrace for the run reveals the exact sequence of messages sent to the chat model, tool calls generated, latency, token counts, and other metadata.\nSee\nthis guide\nfor more detail on extraction workflows with reference examples, including how to incorporate prompt templates and customize the generation of example messages.\nNext steps\nâ€‹\nNow that you understand the basics of extraction with LangChain, you're ready to proceed to the rest of the how-to guides:\nAdd Examples\n: More detail on using\nreference examples\nto improve performance.\nHandle Long Text\n: What should you do if the text does not fit into the context window of the LLM?\nUse a Parsing Approach\n: Use a prompt based approach to extract with models that do not support\ntool/function calling\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/agents/",
    "Tutorials\nBuild an Agent\nOn this page\nBuild an Agent\nLangChain supports the creation of\nagents\n, or systems that use\nLLMs\nas reasoning engines to determine which actions to take and the inputs necessary to perform the action.\nAfter executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish. This is often achieved via\ntool-calling\n.\nIn this tutorial we will build an agent that can interact with a search engine. You will be able to ask this agent questions, watch it call the search tool, and have conversations with it.\nEnd-to-end agent\nâ€‹\nThe code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot.\nIn the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this!\n# Import relevant functionality\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nfrom\nlangchain_tavily\nimport\nTavilySearch\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\n# Create the agent\nmemory\n=\nMemorySaver\n(\n)\nmodel\n=\ninit_chat_model\n(\n\"anthropic:claude-3-5-sonnet-latest\"\n)\nsearch\n=\nTavilySearch\n(\nmax_results\n=\n2\n)\ntools\n=\n[\nsearch\n]\nagent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n,\ncheckpointer\n=\nmemory\n)\nAPI Reference:\nMemorySaver\n|\ncreate_react_agent\n# Use the agent\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc123\"\n}\n}\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi, I'm Bob and I live in SF.\"\n,\n}\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nHi, I'm Bob and I live in SF.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello Bob! I notice you've introduced yourself and mentioned you live in SF (San Francisco), but you haven't asked a specific question or made a request that requires the use of any tools. Is there something specific you'd like to know about San Francisco or any other topic? I'd be happy to help you find information using the available search tools.\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the weather where I live?\"\n,\n}\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat's the weather where I live?\n==================================\u001b[1m Ai Message \u001b[0m==================================\n[{'text': 'Let me search for current weather information in San Francisco.', 'type': 'text'}, {'id': 'toolu_011kSdheoJp8THURoLmeLtZo', 'input': {'query': 'current weather San Francisco CA'}, 'name': 'tavily_search', 'type': 'tool_use'}]\nTool Calls:\ntavily_search (toolu_011kSdheoJp8THURoLmeLtZo)\nCall ID: toolu_011kSdheoJp8THURoLmeLtZo\nArgs:\nquery: current weather San Francisco CA\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: tavily_search\n{\"query\": \"current weather San Francisco CA\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.944705, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed âš¡ San Francisco Weather Forecast for June 2025 - day/night ðŸŒ¡ï¸ temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget Â°F. World; United States; California; Weather in San Francisco; ... 17 +64Â° +54Â° 18 +61Â° +54Â° 19\", \"score\": 0.86441374, \"raw_content\": null}], \"response_time\": 2.34}\n==================================\u001b[1m Ai Message \u001b[0m==================================\nBased on the search results, here's the current weather in San Francisco:\n- Temperature: 53.1Â°F (11.7Â°C)\n- Condition: Foggy\n- Wind: 4.0 mph from the Southwest\n- Humidity: 86%\n- Visibility: 9 miles\nThis is quite typical weather for San Francisco, with the characteristic fog that the city is known for. Would you like to know anything else about the weather or San Francisco in general?\nSetup\nâ€‹\nJupyter Notebook\nâ€‹\nThis guide (and most of the other guides in the documentation) uses\nJupyter notebooks\nand assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs.\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See\nhere\nfor instructions on how to install.\nInstallation\nâ€‹\nTo install LangChain run:\n%\npip install\n-\nU langgraph langchain\n-\ntavily langgraph\n-\ncheckpoint\n-\nsqlite\nFor more details, see our\nInstallation guide\n.\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nOr, if in a notebook, you can set them with:\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nTavily\nâ€‹\nWe will be using\nTavily\n(a search engine) as a tool.\nIn order to use it, you will need to get and set an API key:\nexport TAVILY_API_KEY=\"...\"\nOr, if in a notebook, you can set it with:\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"TAVILY_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nDefine tools\nâ€‹\nWe first need to create the tools we want to use. Our main tool of choice will be\nTavily\n- a search engine. We can use the dedicated\nlangchain-tavily\nintegration package\nto easily use Tavily search engine as tool with LangChain.\nfrom\nlangchain_tavily\nimport\nTavilySearch\nsearch\n=\nTavilySearch\n(\nmax_results\n=\n2\n)\nsearch_results\n=\nsearch\n.\ninvoke\n(\n\"What is the weather in SF\"\n)\nprint\n(\nsearch_results\n)\n# If we want, we can create other tools.\n# Once we have all the tools we want, we can put them in a list that we will reference later.\ntools\n=\n[\nsearch\n]\n{'query': 'What is the weather in SF', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in San Francisco, CA', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", 'score': 0.9185379, 'raw_content': None}, {'title': 'Weather in San Francisco in June 2025', 'url': 'https://world-weather.info/forecast/usa/san_francisco/june-2025/', 'content': \"Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month *   Weather in San Francisco Weather in San Francisco in June 2025 *   1 +63Â° +55Â° *   2 +66Â° +54Â° *   3 +66Â° +55Â° *   4 +66Â° +54Â° *   5 +66Â° +55Â° *   6 +66Â° +57Â° *   7 +64Â° +55Â° *   8 +63Â° +55Â° *   9 +63Â° +54Â° *   10 +59Â° +54Â° *   11 +59Â° +54Â° *   12 +61Â° +54Â° Weather in Washington, D.C.**+68Â°** Sacramento**+81Â°** Pleasanton**+72Â°** Redwood City**+68Â°** San Leandro**+61Â°** San Mateo**+64Â°** San Rafael**+70Â°** San Ramon**+64Â°** South San Francisco**+61Â°** Daly City**+59Â°** Wilder**+66Â°** Woodacre**+70Â°** world's temperature today Colchani day+50Â°F night+16Â°F Az Zubayr day+124Â°F night+93Â°F Weather forecast on your site Install _San Francisco_ +61Â° Temperature units\", 'score': 0.7978881, 'raw_content': None}], 'response_time': 2.62}\ntip\nIn many applications, you may want to define custom tools. LangChain supports custom\ntool creation via Python functions and other means. Refer to the\nHow to create tools\nguide for details.\nUsing Language Models\nâ€‹\nNext, let's learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nYou can call the language model by passing in a list of messages. By default, the response is a\ncontent\nstring.\nquery\n=\n\"Hi!\"\nresponse\n=\nmodel\n.\ninvoke\n(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nquery\n}\n]\n)\nresponse\n.\ntext\n(\n)\n'Hello! How can I help you today?'\nWe can now see what it is like to enable this model to do tool calling. In order to enable that we use\n.bind_tools\nto give the language model knowledge of these tools\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\ntools\n)\nWe can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the\ncontent\nfield as well as the\ntool_calls\nfield.\nquery\n=\n\"Hi!\"\nresponse\n=\nmodel_with_tools\n.\ninvoke\n(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nquery\n}\n]\n)\nprint\n(\nf\"Message content:\n{\nresponse\n.\ntext\n(\n)\n}\n\\n\"\n)\nprint\n(\nf\"Tool calls:\n{\nresponse\n.\ntool_calls\n}\n\"\n)\nMessage content: Hello! I'm here to help you. I have access to a powerful search tool that can help answer questions and find information about various topics. What would you like to know about?\nFeel free to ask any question or request information, and I'll do my best to assist you using the available tools.\nTool calls: []\nNow, let's try calling it with some input that would expect a tool to be called.\nquery\n=\n\"Search for the weather in SF\"\nresponse\n=\nmodel_with_tools\n.\ninvoke\n(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nquery\n}\n]\n)\nprint\n(\nf\"Message content:\n{\nresponse\n.\ntext\n(\n)\n}\n\\n\"\n)\nprint\n(\nf\"Tool calls:\n{\nresponse\n.\ntool_calls\n}\n\"\n)\nMessage content: I'll help you search for information about the weather in San Francisco.\nTool calls: [{'name': 'tavily_search', 'args': {'query': 'current weather San Francisco'}, 'id': 'toolu_015gdPn1jbB2Z21DmN2RAnti', 'type': 'tool_call'}]\nWe can see that there's now no text content, but there is a tool call! It wants us to call the Tavily Search tool.\nThis isn't calling that tool yet - it's just telling us to. In order to actually call it, we'll want to create our agent.\nCreate the agent\nâ€‹\nNow that we have defined the tools and the LLM, we can create the agent. We will be using\nLangGraph\nto construct the agent.\nCurrently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\nNow, we can initialize the agent with the LLM and the tools.\nNote that we are passing in the\nmodel\n, not\nmodel_with_tools\n. That is because\ncreate_react_agent\nwill call\n.bind_tools\nfor us under the hood.\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nagent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n)\nAPI Reference:\ncreate_react_agent\nRun the agent\nâ€‹\nWe can now run the agent with a few queries! Note that for now, these are all\nstateless\nqueries (it won't remember previous interactions). Note that the agent will return the\nfinal\nstate at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).\nFirst up, let's see how it responds when there's no need to call a tool:\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi!\"\n}\nresponse\n=\nagent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n)\nfor\nmessage\nin\nresponse\n[\n\"messages\"\n]\n:\nmessage\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nHi!\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello! I'm here to help you with your questions using the available search tools. Please feel free to ask any question, and I'll do my best to find relevant and accurate information for you.\nIn order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the\nLangSmith trace\nLet's now try it out on an example where it should be invoking the tool\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Search for the weather in SF\"\n}\nresponse\n=\nagent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n)\nfor\nmessage\nin\nresponse\n[\n\"messages\"\n]\n:\nmessage\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nSearch for the weather in SF\n==================================\u001b[1m Ai Message \u001b[0m==================================\n[{'text': \"I'll help you search for weather information in San Francisco. Let me use the search engine to find current weather conditions.\", 'type': 'text'}, {'id': 'toolu_01WWcXGnArosybujpKzdmARZ', 'input': {'query': 'current weather San Francisco SF'}, 'name': 'tavily_search', 'type': 'tool_use'}]\nTool Calls:\ntavily_search (toolu_01WWcXGnArosybujpKzdmARZ)\nCall ID: toolu_01WWcXGnArosybujpKzdmARZ\nArgs:\nquery: current weather San Francisco SF\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: tavily_search\n{\"query\": \"current weather San Francisco SF\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168606, 'localtime': '2025-06-17 06:56'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.885373, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed âš¡ San Francisco Weather Forecast for June 2025 - day/night ðŸŒ¡ï¸ temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget Â°F. World; United States; California; Weather in San Francisco; ... 17 +64Â° +54Â° 18 +61Â° +54Â° 19\", \"score\": 0.8830044, \"raw_content\": null}], \"response_time\": 2.6}\n==================================\u001b[1m Ai Message \u001b[0m==================================\nBased on the search results, here's the current weather in San Francisco:\n- Temperature: 53.1Â°F (11.7Â°C)\n- Conditions: Foggy\n- Wind: 4.0 mph from the SW\n- Humidity: 86%\n- Visibility: 9.0 miles\nThe weather appears to be typical for San Francisco, with morning fog and mild temperatures. The \"feels like\" temperature is 52.4Â°F (11.3Â°C).\nWe can check out the\nLangSmith trace\nto make sure it's calling the search tool effectively.\nStreaming Messages\nâ€‹\nWe've seen how the agent can be called with\n.invoke\nto get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nstream_mode\n=\n\"values\"\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nSearch for the weather in SF\n==================================\u001b[1m Ai Message \u001b[0m==================================\n[{'text': \"I'll help you search for information about the weather in San Francisco.\", 'type': 'text'}, {'id': 'toolu_01DCPnJES53Fcr7YWnZ47kDG', 'input': {'query': 'current weather San Francisco'}, 'name': 'tavily_search', 'type': 'tool_use'}]\nTool Calls:\ntavily_search (toolu_01DCPnJES53Fcr7YWnZ47kDG)\nCall ID: toolu_01DCPnJES53Fcr7YWnZ47kDG\nArgs:\nquery: current weather San Francisco\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: tavily_search\n{\"query\": \"current weather San Francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750168506, 'localtime': '2025-06-17 06:55'}, 'current': {'last_updated_epoch': 1750167900, 'last_updated': '2025-06-17 06:45', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Fog', 'icon': '//cdn.weatherapi.com/weather/64x64/day/248.png', 'code': 1135}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 215, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 8.7, 'windchill_f': 47.7, 'heatindex_c': 9.8, 'heatindex_f': 49.7, 'dewpoint_c': 9.6, 'dewpoint_f': 49.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 6.3, 'gust_kph': 10.2}}\", \"score\": 0.9542825, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed âš¡ San Francisco Weather Forecast for June 2025 - day/night ðŸŒ¡ï¸ temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget Â°F. World; United States; California; Weather in San Francisco; ... 17 +64Â° +54Â° 18 +61Â° +54Â° 19\", \"score\": 0.8638634, \"raw_content\": null}], \"response_time\": 2.57}\n==================================\u001b[1m Ai Message \u001b[0m==================================\nBased on the search results, here's the current weather in San Francisco:\n- Temperature: 53.1Â°F (11.7Â°C)\n- Condition: Foggy\n- Wind: 4.0 mph from the Southwest\n- Humidity: 86%\n- Visibility: 9.0 miles\n- Feels like: 52.4Â°F (11.3Â°C)\nThis is quite typical weather for San Francisco, which is known for its fog, especially during the morning hours. The city's proximity to the ocean and unique geographical features often result in mild temperatures and foggy conditions.\nStreaming tokens\nâ€‹\nIn addition to streaming back messages, it is also useful to stream back tokens.\nWe can do this by specifying\nstream_mode=\"messages\"\n.\n::: note\nBelow we use\nmessage.text()\n, which requires\nlangchain-core>=0.3.37\n.\n:::\nfor\nstep\n,\nmetadata\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nstream_mode\n=\n\"messages\"\n)\n:\nif\nmetadata\n[\n\"langgraph_node\"\n]\n==\n\"agent\"\nand\n(\ntext\n:=\nstep\n.\ntext\n(\n)\n)\n:\nprint\n(\ntext\n,\nend\n=\n\"|\"\n)\nI|'ll help you search for information| about the weather in San Francisco.|Base|d on the search results, here|'s the current weather in| San Francisco:\n-| Temperature: 53.1Â°F (|11.7Â°C)\n-| Condition: Foggy\n- Wind:| 4.0 mph from| the Southwest\n- Humidity|: 86%|\n- Visibility: 9|.0 miles\n- Pressure: |30.02 in|Hg\nThe weather| is characteristic of San Francisco, with| foggy conditions and mild temperatures|. The \"feels like\" temperature is slightly| lower at 52.4|Â°F (11.|3Â°C)| due to the wind chill effect|.|\nAdding in memory\nâ€‹\nAs mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a\nthread_id\nwhen invoking the agent (so it knows which thread/conversation to resume from).\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nmemory\n=\nMemorySaver\n(\n)\nAPI Reference:\nMemorySaver\nagent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n,\ncheckpointer\n=\nmemory\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc123\"\n}\n}\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi, I'm Bob!\"\n}\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nHi, I'm Bob!\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello Bob! I'm an AI assistant who can help you search for information using specialized search tools. Is there anything specific you'd like to know about or search for? I'm happy to help you find accurate and up-to-date information on various topics.\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's my name?\"\n}\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat's my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYour name is Bob, as you introduced yourself earlier. I can remember information shared within our conversation without needing to search for it.\nExample\nLangSmith trace\nIf you want to start a new conversation, all you have to do is change the\nthread_id\nused\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"xyz123\"\n}\n}\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's my name?\"\n}\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat's my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nI apologize, but I don't have access to any tools that would tell me your name. I can only assist you with searching for publicly available information using the tavily_search function. I don't have access to personal information about users. If you'd like to tell me your name, I'll be happy to address you by it.\nConclusion\nâ€‹\nThat's a wrap! In this quick start we covered how to create a simple agent.\nWe've then shown how to stream back a response - not only with the intermediate steps, but also tokens!\nWe've also added in memory so you can have a conversation with them.\nAgents are a complex topic with lots to learn!\nFor more information on Agents, please check out the\nLangGraph\ndocumentation. This has it's own set of concepts, tutorials, and how-to guides.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/classification/",
    "Tutorials\nTagging\nOn this page\nClassify Text into Labels\nTagging means labeling a document with classes such as:\nSentiment\nLanguage\nStyle (formal, informal etc.)\nCovered topics\nPolitical tendency\nOverview\nâ€‹\nTagging has a few components:\nfunction\n: Like\nextraction\n, tagging uses\nfunctions\nto specify how the model should tag a document\nschema\n: defines how we want to tag the document\nQuickstart\nâ€‹\nLet's see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We'll use the\nwith_structured_output\nmethod supported by OpenAI models.\npip install\n-\nU langchain\n-\ncore\nWe'll need to load a\nchat model\n:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nLet's specify a Pydantic model with a few properties and their expected type in our schema.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\npydantic\nimport\nBaseModel\n,\nField\ntagging_prompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"\"\"\nExtract the desired information from the following passage.\nOnly extract the properties mentioned in the 'Classification' function.\nPassage:\n{input}\n\"\"\"\n)\nclass\nClassification\n(\nBaseModel\n)\n:\nsentiment\n:\nstr\n=\nField\n(\ndescription\n=\n\"The sentiment of the text\"\n)\naggressiveness\n:\nint\n=\nField\n(\ndescription\n=\n\"How aggressive the text is on a scale from 1 to 10\"\n)\nlanguage\n:\nstr\n=\nField\n(\ndescription\n=\n\"The language the text is written in\"\n)\n# Structured LLM\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nClassification\n)\nAPI Reference:\nChatPromptTemplate\ninp\n=\n\"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\nprompt\n=\ntagging_prompt\n.\ninvoke\n(\n{\n\"input\"\n:\ninp\n}\n)\nresponse\n=\nstructured_llm\n.\ninvoke\n(\nprompt\n)\nresponse\nClassification(sentiment='positive', aggressiveness=1, language='Spanish')\nIf we want dictionary output, we can just call\n.model_dump()\ninp\n=\n\"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\nprompt\n=\ntagging_prompt\n.\ninvoke\n(\n{\n\"input\"\n:\ninp\n}\n)\nresponse\n=\nstructured_llm\n.\ninvoke\n(\nprompt\n)\nresponse\n.\nmodel_dump\n(\n)\n{'sentiment': 'angry', 'aggressiveness': 8, 'language': 'Spanish'}\nAs we can see in the examples, it correctly interprets what we want.\nThe results vary so that we may get, for example, sentiments in different languages ('positive', 'enojado' etc.).\nWe will see how to control these results in the next section.\nFiner control\nâ€‹\nCareful schema definition gives us more control over the model's output.\nSpecifically, we can define:\nPossible values for each property\nDescription to make sure that the model understands the property\nRequired properties to be returned\nLet's redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:\nclass\nClassification\n(\nBaseModel\n)\n:\nsentiment\n:\nstr\n=\nField\n(\n.\n.\n.\n,\nenum\n=\n[\n\"happy\"\n,\n\"neutral\"\n,\n\"sad\"\n]\n)\naggressiveness\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"describes how aggressive the statement is, the higher the number the more aggressive\"\n,\nenum\n=\n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n]\n,\n)\nlanguage\n:\nstr\n=\nField\n(\n.\n.\n.\n,\nenum\n=\n[\n\"spanish\"\n,\n\"english\"\n,\n\"french\"\n,\n\"german\"\n,\n\"italian\"\n]\n)\ntagging_prompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"\"\"\nExtract the desired information from the following passage.\nOnly extract the properties mentioned in the 'Classification' function.\nPassage:\n{input}\n\"\"\"\n)\nllm\n=\nChatOpenAI\n(\ntemperature\n=\n0\n,\nmodel\n=\n\"gpt-4o-mini\"\n)\n.\nwith_structured_output\n(\nClassification\n)\nNow the answers will be restricted in a way we expect!\ninp\n=\n\"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\nprompt\n=\ntagging_prompt\n.\ninvoke\n(\n{\n\"input\"\n:\ninp\n}\n)\nllm\n.\ninvoke\n(\nprompt\n)\nClassification(sentiment='happy', aggressiveness=1, language='spanish')\ninp\n=\n\"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\nprompt\n=\ntagging_prompt\n.\ninvoke\n(\n{\n\"input\"\n:\ninp\n}\n)\nllm\n.\ninvoke\n(\nprompt\n)\nClassification(sentiment='sad', aggressiveness=4, language='spanish')\ninp\n=\n\"Weather is ok here, I can go outside without much more than a coat\"\nprompt\n=\ntagging_prompt\n.\ninvoke\n(\n{\n\"input\"\n:\ninp\n}\n)\nllm\n.\ninvoke\n(\nprompt\n)\nClassification(sentiment='happy', aggressiveness=1, language='english')\nThe\nLangSmith trace\nlets us peek under the hood:\nGoing deeper\nâ€‹\nYou can use the\nmetadata tagger\ndocument transformer to extract metadata from a LangChain\nDocument\n.\nThis covers the same basic functionality as the tagging chain, only applied to a LangChain\nDocument\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/rag/",
    "Tutorials\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nOn this page\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis is a multi-part tutorial:\nPart 1\n(this guide) introduces RAG and walks through a minimal implementation.\nPart 2\nextends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\nThis tutorial will show how to build a simple Q&A application\nover a text data source. Along the way weâ€™ll go over a typical Q&A\narchitecture and highlight additional resources for more advanced Q&A techniques. Weâ€™ll also see\nhow LangSmith can help us trace and understand our application.\nLangSmith will become increasingly helpful as our application grows in\ncomplexity.\nIf you're already familiar with basic retrieval, you might also be interested in\nthis\nhigh-level overview of different retrieval techniques\n.\nNote\n: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing\nquestion/answering over SQL data\n.\nOverview\nâ€‹\nA typical RAG application has two main components:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens offline.\nRetrieval and generation\n: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nNote: the indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nThe most common full sequence from raw data to answer looks like:\nIndexing\nâ€‹\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\nRetrieval and generation\nâ€‹\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nChatModel\n/\nLLM\nproduces an answer using a prompt that includes both the question with the retrieved data\nOnce we've indexed our data, we will use\nLangGraph\nas our orchestration framework to implement the retrieval and generation steps.\nSetup\nâ€‹\nJupyter Notebook\nâ€‹\nThis and other tutorials are perhaps most conveniently run in a\nJupyter notebooks\n. Going through guides in an interactive environment is a great way to better understand them. See\nhere\nfor instructions on how to install.\nInstallation\nâ€‹\nThis tutorial requires these langchain dependencies:\nPip\nConda\n%\npip install\n-\n-\nquiet\n-\n-\nupgrade langchain\n-\ntext\n-\nsplitters langchain\n-\ncommunity langgraph\nconda install langchain-text-splitters langchain-community langgraph -c conda-forge\nFor more details, see our\nInstallation guide\n.\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nOr, if in a notebook, you can set them with:\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nComponents\nâ€‹\nWe will need to select three components from LangChain's suite of integrations.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nSelect\nvector store\n:\nIn-memory\nâ–¾\nIn-memory\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\npip install -qU langchain-core\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\nPreview\nâ€‹\nIn this guide weâ€™ll build an app that answers questions about the website's content. The specific website we will use is the\nLLM Powered Autonomous\nAgents\nblog post\nby Lilian Weng, which allows us to ask questions about the contents of\nthe post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~50\nlines of code.\nimport\nbs4\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nStateGraph\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader\n(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,\n)\n,\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4\n.\nSoupStrainer\n(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n)\n,\n)\ndocs\n=\nloader\n.\nload\n(\n)\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\n# Index chunks\n_\n=\nvector_store\n.\nadd_documents\n(\ndocuments\n=\nall_splits\n)\n# Define prompt for question-answering\n# N.B. for non-US LangSmith endpoints, you may need to specify\n# api_url=\"https://api.smith.langchain.com\" in hub.pull.\nprompt\n=\nhub\n.\npull\n(\n\"rlm/rag-prompt\"\n)\n# Define state for application\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\n# Define application steps\ndef\nretrieve\n(\nstate\n:\nState\n)\n:\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nstate\n[\n\"question\"\n]\n)\nreturn\n{\n\"context\"\n:\nretrieved_docs\n}\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\n# Compile application and test\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nDocument\n|\nStateGraph\nresponse\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What is Task Decomposition?\"\n}\n)\nprint\n(\nresponse\n[\n\"answer\"\n]\n)\nTask Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model's thinking process.\nCheck out the\nLangSmith\ntrace\n.\nDetailed walkthrough\nâ€‹\nLetâ€™s go through the above code step-by-step to really understand whatâ€™s\ngoing on.\n1. Indexing\nâ€‹\nnote\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf you're comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n,\nfeel free to skip to the next section on\nretrieval and generation\n.\nLoading documents\nâ€‹\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a\nlist of\nDocument\nobjects.\nIn this case weâ€™ll use the\nWebBaseLoader\n,\nwhich uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto\nparse it to text. We can customize the HTML -> text parsing by passing\nin parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup\ndocs\n).\nIn this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or\nâ€œpost-headerâ€ are relevant, so weâ€™ll remove all others.\nimport\nbs4\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4\n.\nSoupStrainer\n(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n)\n)\nloader\n=\nWebBaseLoader\n(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,\n)\n,\nbs_kwargs\n=\n{\n\"parse_only\"\n:\nbs4_strainer\n}\n,\n)\ndocs\n=\nloader\n.\nload\n(\n)\nassert\nlen\n(\ndocs\n)\n==\n1\nprint\n(\nf\"Total characters:\n{\nlen\n(\ndocs\n[\n0\n]\n.\npage_content\n)\n}\n\"\n)\nTotal characters: 43131\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n[\n:\n500\n]\n)\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nâ€‹\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nDocs\n:\nDetailed documentation on how to use\nDocumentLoaders\n.\nIntegrations\n: 160+\nintegrations to choose from.\nInterface\n:\nAPI reference for the base interface.\nSplitting documents\nâ€‹\nOur loaded document is over 42k characters which is too long to fit\ninto the context window of many models. Even for those models that could\nfit the full post in their context window, models can struggle to find\ninformation in very long inputs.\nTo handle this weâ€™ll split the\nDocument\ninto chunks for embedding and\nvector storage. This should help us retrieve only the most relevant parts\nof the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n,\nwhich will recursively split the document using common separators like\nnew lines until each chunk is the appropriate size. This is the\nrecommended text splitter for generic text use cases.\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\nprint\n(\nf\"Split blog post into\n{\nlen\n(\nall_splits\n)\n}\nsub-documents.\"\n)\nSplit blog post into 66 sub-documents.\nGo deeper\nâ€‹\nTextSplitter\n: Object that splits a list of\nDocument\ns into smaller\nchunks. Subclass of\nDocumentTransformer\ns.\nLearn more about splitting text using different methods by reading the\nhow-to docs\nCode (py or js)\nScientific papers\nInterface\n: API reference for the base interface.\nDocumentTransformer\n: Object that performs a transformation on a list\nof\nDocument\nobjects.\nDocs\n: Detailed documentation on how to use\nDocumentTransformers\nIntegrations\nInterface\n: API reference for the base interface.\nStoring documents\nâ€‹\nNow we need to index our 66 text chunks so that we can search over them\nat runtime. Following the\nsemantic search tutorial\n,\nour approach is to\nembed\nthe contents of each document split and insert these embeddings\ninto a\nvector store\n. Given an input query, we can then use\nvector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command\nusing the vector store and embeddings model selected at the\nstart of the tutorial\n.\ndocument_ids\n=\nvector_store\n.\nadd_documents\n(\ndocuments\n=\nall_splits\n)\nprint\n(\ndocument_ids\n[\n:\n3\n]\n)\n['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\nGo deeper\nâ€‹\nEmbeddings\n: Wrapper around a text embedding model, used for converting\ntext to embeddings.\nDocs\n: Detailed documentation on how to use embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and\nquerying embeddings.\nDocs\n: Detailed documentation on how to use vector stores.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point\nwe have a query-able vector store containing the chunked contents of our\nblog post. Given a user question, we should ideally be able to return\nthe snippets of the blog post that answer the question.\n2. Retrieval and Generation\nâ€‹\nNow letâ€™s write the actual application logic. We want to create a simple\napplication that takes a user question, searches for documents relevant\nto that question, passes the retrieved documents and initial question to\na model, and returns an answer.\nFor generation, we will use the chat model selected at the\nstart of the tutorial\n.\nWeâ€™ll use a prompt for RAG that is checked into the LangChain prompt hub\n(\nhere\n).\nfrom\nlangchain\nimport\nhub\n# N.B. for non-US LangSmith endpoints, you may need to specify\n# api_url=\"https://api.smith.langchain.com\" in hub.pull.\nprompt\n=\nhub\n.\npull\n(\n\"rlm/rag-prompt\"\n)\nexample_messages\n=\nprompt\n.\ninvoke\n(\n{\n\"context\"\n:\n\"(context goes here)\"\n,\n\"question\"\n:\n\"(question goes here)\"\n}\n)\n.\nto_messages\n(\n)\nassert\nlen\n(\nexample_messages\n)\n==\n1\nprint\n(\nexample_messages\n[\n0\n]\n.\ncontent\n)\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: (question goes here)\nContext: (context goes here)\nAnswer:\nWe'll use\nLangGraph\nto tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\nWe can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\nWe get streamlined deployments via\nLangGraph Platform\n.\nLangSmith will automatically trace the steps of our application together.\nWe can easily add key features to our application, including\npersistence\nand\nhuman-in-the-loop approval\n, with minimal code changes.\nTo use LangGraph, we need to define three things:\nThe state of our application;\nThe nodes of our application (i.e., application steps);\nThe \"control flow\" of our application (e.g., the ordering of the steps).\nState:\nâ€‹\nThe\nstate\nof our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a\nTypedDict\n, but can also be a\nPydantic BaseModel\n.\nFor a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\nAPI Reference:\nDocument\nNodes (application steps)\nâ€‹\nLet's start with a simple sequence of two steps: retrieval and generation.\ndef\nretrieve\n(\nstate\n:\nState\n)\n:\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nstate\n[\n\"question\"\n]\n)\nreturn\n{\n\"context\"\n:\nretrieved_docs\n}\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\nOur retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.\nControl flow\nâ€‹\nFinally, we compile our application into a single\ngraph\nobject. In this case, we are just connecting the retrieval and generation steps into a single sequence.\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nStateGraph\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nStateGraph\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nDo I need to use LangGraph?\nLangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\nquestion\n=\n\"...\"\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nquestion\n)\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nretrieved_docs\n)\nprompt\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nquestion\n,\n\"context\"\n:\ndocs_content\n}\n)\nanswer\n=\nllm\n.\ninvoke\n(\nprompt\n)\nThe benefits of LangGraph include:\nSupport for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\nAutomatic support for tracing via\nLangSmith\nand deployments via\nLangGraph Platform\n;\nSupport for persistence, human-in-the-loop, and other features.\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in\nPart 2\nof the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\nUsage\nâ€‹\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\nInvoke:\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What is Task Decomposition?\"\n}\n)\nprint\n(\nf\"Context:\n{\nresult\n[\n'context'\n]\n}\n\\n\\n\"\n)\nprint\n(\nf\"Answer:\n{\nresult\n[\n'answer'\n]\n}\n\"\n)\nContext: [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]\nAnswer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.\nStream steps:\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"question\"\n:\n\"What is Task Decomposition?\"\n}\n,\nstream_mode\n=\n\"updates\"\n)\n:\nprint\n(\nf\"\n{\nstep\n}\n\\n\\n----------------\\n\"\n)\n{'retrieve': {'context': [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}\n----------------\n{'generate': {'answer': 'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.'}}\n----------------\nStream\ntokens\n:\nfor\nmessage\n,\nmetadata\nin\ngraph\n.\nstream\n(\n{\n\"question\"\n:\n\"What is Task Decomposition?\"\n}\n,\nstream_mode\n=\n\"messages\"\n)\n:\nprint\n(\nmessage\n.\ncontent\n,\nend\n=\n\"|\"\n)\n|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|'s| reasoning| process|.||\ntip\nFor async invocations, use:\nresult\n=\nawait\ngraph\n.\nainvoke\n(\n.\n.\n.\n)\nand\nasync\nfor\nstep\nin\ngraph\n.\nastream\n(\n.\n.\n.\n)\n:\nReturning sources\nâ€‹\nNote that by storing the retrieved context in the state of the graph, we recover sources for the model's generated answer in the\n\"context\"\nfield of the state. See\nthis guide\non returning sources for more detail.\nGo deeper\nâ€‹\nChat models\ntake in a sequence of messages and return a message.\nDocs\nIntegrations\n: 25+ integrations to choose from.\nInterface\n: API reference for the base interface.\nCustomizing the prompt\nAs shown above, we can load prompts (e.g.,\nthis RAG\nprompt\n) from the prompt\nhub. The prompt can also be easily customized. For example:\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\ntemplate\n=\n\"\"\"Use the following pieces of context to answer the question at the end.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\nUse three sentences maximum and keep the answer as concise as possible.\nAlways say \"thanks for asking!\" at the end of the answer.\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\ncustom_rag_prompt\n=\nPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nAPI Reference:\nPromptTemplate\nQuery analysis\nâ€‹\nSo far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\nIn addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\nThe model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.\nQuery analysis\nemploys models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let's add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.\ntotal_documents\n=\nlen\n(\nall_splits\n)\nthird\n=\ntotal_documents\n//\n3\nfor\ni\n,\ndocument\nin\nenumerate\n(\nall_splits\n)\n:\nif\ni\n<\nthird\n:\ndocument\n.\nmetadata\n[\n\"section\"\n]\n=\n\"beginning\"\nelif\ni\n<\n2\n*\nthird\n:\ndocument\n.\nmetadata\n[\n\"section\"\n]\n=\n\"middle\"\nelse\n:\ndocument\n.\nmetadata\n[\n\"section\"\n]\n=\n\"end\"\nall_splits\n[\n0\n]\n.\nmetadata\n{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n'start_index': 8,\n'section': 'beginning'}\nWe will need to update the documents in our vector store. We will use a simple\nInMemoryVectorStore\nfor this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store\nintegration documentation\nfor relevant features of your chosen vector store.\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\n_\n=\nvector_store\n.\nadd_documents\n(\nall_splits\n)\nAPI Reference:\nInMemoryVectorStore\nLet's next define a schema for our search query. We will use\nstructured output\nfor this purpose. Here we define a query as containing a string query and a document section (either \"beginning\", \"middle\", or \"end\"), but this can be defined however you like.\nfrom\ntyping\nimport\nLiteral\nfrom\ntyping_extensions\nimport\nAnnotated\nclass\nSearch\n(\nTypedDict\n)\n:\n\"\"\"Search query.\"\"\"\nquery\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"Search query to run.\"\n]\nsection\n:\nAnnotated\n[\nLiteral\n[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]\n,\n.\n.\n.\n,\n\"Section to query.\"\n,\n]\nFinally, we add a step to our LangGraph application to generate a query from the user's raw input:\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\nquery\n:\nSearch\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\ndef\nanalyze_query\n(\nstate\n:\nState\n)\n:\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nSearch\n)\nquery\n=\nstructured_llm\n.\ninvoke\n(\nstate\n[\n\"question\"\n]\n)\nreturn\n{\n\"query\"\n:\nquery\n}\ndef\nretrieve\n(\nstate\n:\nState\n)\n:\nquery\n=\nstate\n[\n\"query\"\n]\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nquery\n[\n\"query\"\n]\n,\nfilter\n=\nlambda\ndoc\n:\ndoc\n.\nmetadata\n.\nget\n(\n\"section\"\n)\n==\nquery\n[\n\"section\"\n]\n,\n)\nreturn\n{\n\"context\"\n:\nretrieved_docs\n}\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nanalyze_query\n,\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"analyze_query\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nFull Code:\nfrom\ntyping\nimport\nLiteral\nimport\nbs4\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nStateGraph\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nList\n,\nTypedDict\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader\n(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,\n)\n,\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4\n.\nSoupStrainer\n(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n)\n,\n)\ndocs\n=\nloader\n.\nload\n(\n)\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\n# Update metadata (illustration purposes)\ntotal_documents\n=\nlen\n(\nall_splits\n)\nthird\n=\ntotal_documents\n//\n3\nfor\ni\n,\ndocument\nin\nenumerate\n(\nall_splits\n)\n:\nif\ni\n<\nthird\n:\ndocument\n.\nmetadata\n[\n\"section\"\n]\n=\n\"beginning\"\nelif\ni\n<\n2\n*\nthird\n:\ndocument\n.\nmetadata\n[\n\"section\"\n]\n=\n\"middle\"\nelse\n:\ndocument\n.\nmetadata\n[\n\"section\"\n]\n=\n\"end\"\n# Index chunks\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\n_\n=\nvector_store\n.\nadd_documents\n(\nall_splits\n)\n# Define schema for search\nclass\nSearch\n(\nTypedDict\n)\n:\n\"\"\"Search query.\"\"\"\nquery\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"Search query to run.\"\n]\nsection\n:\nAnnotated\n[\nLiteral\n[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]\n,\n.\n.\n.\n,\n\"Section to query.\"\n,\n]\n# Define prompt for question-answering\nprompt\n=\nhub\n.\npull\n(\n\"rlm/rag-prompt\"\n)\n# Define state for application\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\nquery\n:\nSearch\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\ndef\nanalyze_query\n(\nstate\n:\nState\n)\n:\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nSearch\n)\nquery\n=\nstructured_llm\n.\ninvoke\n(\nstate\n[\n\"question\"\n]\n)\nreturn\n{\n\"query\"\n:\nquery\n}\ndef\nretrieve\n(\nstate\n:\nState\n)\n:\nquery\n=\nstate\n[\n\"query\"\n]\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nquery\n[\n\"query\"\n]\n,\nfilter\n=\nlambda\ndoc\n:\ndoc\n.\nmetadata\n.\nget\n(\n\"section\"\n)\n==\nquery\n[\n\"section\"\n]\n,\n)\nreturn\n{\n\"context\"\n:\nretrieved_docs\n}\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nanalyze_query\n,\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"analyze_query\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nDocument\n|\nInMemoryVectorStore\n|\nStateGraph\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nWe can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"question\"\n:\n\"What does the end of the post say about Task Decomposition?\"\n}\n,\nstream_mode\n=\n\"updates\"\n,\n)\n:\nprint\n(\nf\"\n{\nstep\n}\n\\n\\n----------------\\n\"\n)\n{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}\n----------------\n{'retrieve': {'context': [Document(id='d6cef137-e1e8-4ddc-91dc-b62bd33c6020', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39221, 'section': 'end'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'), Document(id='d1834ae1-eb6a-43d7-a023-08dfa5028799', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39086, 'section': 'end'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='ca7f06e4-2c2e-4788-9a81-2418d82213d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32942, 'section': 'end'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(id='1fcc2736-30f4-4ef6-90f2-c64af92118cb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35127, 'section': 'end'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease')]}}\n----------------\n{'generate': {'answer': 'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.'}}\n----------------\nIn both the streamed steps and the\nLangSmith trace\n, we can now observe the structured query that was fed into the retrieval step.\nQuery Analysis is a rich problem with a wide range of approaches. Refer to the\nhow-to guides\nfor more examples.\nNext steps\nâ€‹\nWe've covered the steps to build a basic Q&A app over data:\nLoading data with a\nDocument Loader\nChunking the indexed data with a\nText Splitter\nto make it more easily usable by a model\nEmbedding the data\nand storing the data in a\nvectorstore\nRetrieving\nthe previously stored chunks in response to incoming questions\nGenerating an answer using the retrieved chunks as context.\nIn\nPart 2\nof the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.\nFurther reading:\nReturn sources\n: Learn how to return source documents\nStreaming\n: Learn how to stream outputs and intermediate steps\nAdd chat history\n: Learn how to add chat history to your app\nRetrieval conceptual guide\n: A high-level overview of specific retrieval techniques\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/retrievers/",
    "Tutorials\nBuild a semantic search engine\nOn this page\nBuild a semantic search engine\nThis tutorial will familiarize you with LangChain's\ndocument loader\n,\nembedding\n, and\nvector store\nabstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or\nRAG\n(see our RAG tutorial\nhere\n).\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.\nConcepts\nâ€‹\nThis guide focuses on retrieval of text data. We will cover the following concepts:\nDocuments and document loaders;\nText splitters;\nEmbeddings;\nVector stores and retrievers.\nSetup\nâ€‹\nJupyter Notebook\nâ€‹\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See\nhere\nfor instructions on how to install.\nInstallation\nâ€‹\nThis tutorial requires the\nlangchain-community\nand\npypdf\npackages:\nPip\nConda\npip install langchain-community pypdf\nconda install langchain-community pypdf -c conda-forge\nFor more details, see our\nInstallation guide\n.\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nOr, if in a notebook, you can set them with:\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nDocuments and Document Loaders\nâ€‹\nLangChain implements a\nDocument\nabstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\npage_content\n: a string representing the content;\nmetadata\n: a dict containing arbitrary metadata;\nid\n: (optional) a string identifier for the document.\nThe\nmetadata\nattribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual\nDocument\nobject often represents a chunk of a larger document.\nWe can generate sample documents when desired:\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\ndocuments\n=\n[\nDocument\n(\npage_content\n=\n\"Dogs are great companions, known for their loyalty and friendliness.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"mammal-pets-doc\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Cats are independent pets that often enjoy their own space.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"mammal-pets-doc\"\n}\n,\n)\n,\n]\nAPI Reference:\nDocument\nHowever, the LangChain ecosystem implements\ndocument loaders\nthat\nintegrate with hundreds of common sources\n. This makes it easy to incorporate data from these sources into your AI application.\nLoading documents\nâ€‹\nLet's load a PDF into a sequence of\nDocument\nobjects. There is a sample PDF in the LangChain repo\nhere\n-- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for\navailable PDF document loaders\n. Let's select\nPyPDFLoader\n, which is fairly lightweight.\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nPyPDFLoader\nfile_path\n=\n\"../example_data/nke-10k-2023.pdf\"\nloader\n=\nPyPDFLoader\n(\nfile_path\n)\ndocs\n=\nloader\n.\nload\n(\n)\nprint\n(\nlen\n(\ndocs\n)\n)\n107\ntip\nSee\nthis guide\nfor more detail on PDF document loaders.\nPyPDFLoader\nloads one\nDocument\nobject per PDF page. For each, we can easily access:\nThe string content of the page;\nMetadata containing the file name and page number.\nprint\n(\nf\"\n{\ndocs\n[\n0\n]\n.\npage_content\n[\n:\n200]\n}\n\\n\"\n)\nprint\n(\ndocs\n[\n0\n]\n.\nmetadata\n)\nTable of Contents\nUNITED STATES\nSECURITIES AND EXCHANGE COMMISSION\nWashington, D.C. 20549\nFORM 10-K\n(Mark One)\nâ˜‘ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\nFO\n{'source': '../example_data/nke-10k-2023.pdf', 'page': 0}\nSplitting\nâ€‹\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve\nDocument\nobjects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\nWe can use\ntext splitters\nfor this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\nwith 200 characters of overlap between chunks. The overlap helps\nmitigate the possibility of separating a statement from important\ncontext related to it. We use the\nRecursiveCharacterTextSplitter\n,\nwhich will recursively split the document using common separators like\nnew lines until each chunk is the appropriate size. This is the\nrecommended text splitter for generic text use cases.\nWe set\nadd_start_index=True\nso that the character index where each\nsplit Document starts within the initial Document is preserved as\nmetadata attribute â€œstart_indexâ€.\nSee\nthis guide\nfor more detail about working with PDFs, including how to extract text from specific sections and images.\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n,\nadd_start_index\n=\nTrue\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\nlen\n(\nall_splits\n)\n514\nEmbeddings\nâ€‹\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can\nembed\nit as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\nLangChain supports embeddings from\ndozens of providers\n. These models specify how text should be converted into a numeric vector. Let's select a model:\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nvector_1\n=\nembeddings\n.\nembed_query\n(\nall_splits\n[\n0\n]\n.\npage_content\n)\nvector_2\n=\nembeddings\n.\nembed_query\n(\nall_splits\n[\n1\n]\n.\npage_content\n)\nassert\nlen\n(\nvector_1\n)\n==\nlen\n(\nvector_2\n)\nprint\n(\nf\"Generated vectors of length\n{\nlen\n(\nvector_1\n)\n}\n\\n\"\n)\nprint\n(\nvector_1\n[\n:\n10\n]\n)\nGenerated vectors of length 1536\n[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\nVector stores\nâ€‹\nLangChain\nVectorStore\nobjects contain methods for adding text and\nDocument\nobjects to the store, and querying them using various similarity metrics. They are often initialized with\nembedding\nmodels, which determine how text data is translated to numeric vectors.\nLangChain includes a suite of\nintegrations\nwith different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as\nPostgres\n) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:\nSelect\nvector store\n:\nIn-memory\nâ–¾\nIn-memory\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\npip install -qU langchain-core\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\nHaving instantiated our vector store, we can now index the documents.\nids\n=\nvector_store\n.\nadd_documents\n(\ndocuments\n=\nall_splits\n)\nNote that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific\nintegration\nfor more detail.\nOnce we've instantiated a\nVectorStore\nthat contains documents, we can query it.\nVectorStore\nincludes methods for querying:\nSynchronously and asynchronously;\nBy string query and by vector;\nWith and without returning similarity scores;\nBy similarity and\nmaximum marginal relevance\n(to balance similarity with query to diversity in retrieved results).\nThe methods will generally include a list of\nDocument\nobjects in their outputs.\nUsage\nâ€‹\nEmbeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\nReturn documents based on similarity to a string query:\nresults\n=\nvector_store\n.\nsimilarity_search\n(\n\"How many distribution centers does Nike have in the US?\"\n)\nprint\n(\nresults\n[\n0\n]\n)\npage_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213\nNIKE Brand in-line stores (including employee-only stores) 74\nConverse stores (including factory stores) 82\nTOTAL 369\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}\nAsync query:\nresults\n=\nawait\nvector_store\n.\nasimilarity_search\n(\n\"When was Nike incorporated?\"\n)\nprint\n(\nresults\n[\n0\n]\n)\npage_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\nReturn scores:\n# Note that providers implement different scores; the score here\n# is a distance metric that varies inversely with similarity.\nresults\n=\nvector_store\n.\nsimilarity_search_with_score\n(\n\"What was Nike's revenue in 2023?\"\n)\ndoc\n,\nscore\n=\nresults\n[\n0\n]\nprint\n(\nf\"Score:\n{\nscore\n}\n\\n\"\n)\nprint\n(\ndoc\n)\nScore: 0.23699893057346344\npage_content='Table of Contents\nFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\nThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\nFISCAL 2023 COMPARED TO FISCAL 2022\nâ€¢NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\nThe increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\n2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\nâ€¢NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\nincrease was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\nequivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\nReturn documents based on similarity to an embedded query:\nembedding\n=\nembeddings\n.\nembed_query\n(\n\"How were Nike's margins impacted in 2023?\"\n)\nresults\n=\nvector_store\n.\nsimilarity_search_by_vector\n(\nembedding\n)\nprint\n(\nresults\n[\n0\n]\n)\npage_content='Table of Contents\nGROSS MARGIN\nFISCAL 2023 COMPARED TO FISCAL 2022\nFor fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\n43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\n*Wholesale equivalent\nThe decrease in gross margin for fiscal 2023 was primarily due to:\nâ€¢Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\nproduct mix;\nâ€¢Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\nthe prior period resulting from lower available inventory supply;\nâ€¢Unfavorable changes in net foreign currency exchange rates, including hedges; and\nâ€¢Lower off-price margin, on a wholesale equivalent basis.\nThis was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\nLearn more:\nAPI reference\nHow-to guide\nIntegration-specific docs\nRetrievers\nâ€‹\nLangChain\nVectorStore\nobjects do not subclass\nRunnable\n. LangChain\nRetrievers\nare Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous\ninvoke\nand\nbatch\noperations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\nWe can create a simple version of this ourselves, without subclassing\nRetriever\n. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the\nsimilarity_search\nmethod:\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_core\n.\nrunnables\nimport\nchain\n@chain\ndef\nretriever\n(\nquery\n:\nstr\n)\n-\n>\nList\n[\nDocument\n]\n:\nreturn\nvector_store\n.\nsimilarity_search\n(\nquery\n,\nk\n=\n1\n)\nretriever\n.\nbatch\n(\n[\n\"How many distribution centers does Nike have in the US?\"\n,\n\"When was Nike incorporated?\"\n,\n]\n,\n)\nAPI Reference:\nDocument\n|\nchain\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n[Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\nVectorstores implement an\nas_retriever\nmethod that will generate a Retriever, specifically a\nVectorStoreRetriever\n. These retrievers include specific\nsearch_type\nand\nsearch_kwargs\nattributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\nretriever\n=\nvector_store\n.\nas_retriever\n(\nsearch_type\n=\n\"similarity\"\n,\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n}\n,\n)\nretriever\n.\nbatch\n(\n[\n\"How many distribution centers does Nike have in the US?\"\n,\n\"When was Nike incorporated?\"\n,\n]\n,\n)\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n[Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\nVectorStoreRetriever\nsupports search types of\n\"similarity\"\n(default),\n\"mmr\"\n(maximum marginal relevance, described above), and\n\"similarity_score_threshold\"\n. We can use the latter to threshold documents output by the retriever by similarity score.\nRetrievers can easily be incorporated into more complex applications, such as\nretrieval-augmented generation (RAG)\napplications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the\nRAG tutorial\ntutorial.\nLearn more:\nâ€‹\nRetrieval strategies can be rich and complex. For example:\nWe can\ninfer hard rules and filters\nfrom a query (e.g., \"using documents published after 2020\");\nWe can\nreturn documents that are linked\nto the retrieved context in some way (e.g., via some document taxonomy);\nWe can generate\nmultiple embeddings\nfor each unit of context;\nWe can\nensemble results\nfrom multiple retrievers;\nWe can assign weights to documents, e.g., to weigh\nrecent documents\nhigher.\nThe\nretrievers\nsection of the how-to guides covers these and other built-in retrieval strategies.\nIt is also straightforward to extend the\nBaseRetriever\nclass in order to implement custom retrievers. See our how-to guide\nhere\n.\nNext steps\nâ€‹\nYou've now seen how to build a semantic search engine over a PDF document.\nFor more on document loaders:\nConceptual guide\nHow-to guides\nAvailable integrations\nFor more on embeddings:\nConceptual guide\nHow-to guides\nAvailable integrations\nFor more on vector stores:\nConceptual guide\nHow-to guides\nAvailable integrations\nFor more on RAG, see:\nBuild a Retrieval Augmented Generation (RAG) App\nRelated how-to guides\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/sql_qa/",
    "Tutorials\nBuild a Question/Answering system over SQL data\nOn this page\nBuild a Question/Answering system over SQL data\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nTools\nAgents\nLangGraph\nEnabling a LLM system to query structured data can be qualitatively different from unstructured text data. Whereas in the latter it is common to generate text that can be searched against a vector database, the approach for structured data is often for the LLM to write and execute queries in a DSL, such as SQL. In this guide we'll go over the basic ways to create a Q&A system over tabular data in databases. We will cover implementations using both\nchains\nand\nagents\n. These systems will allow us to ask a question about the data in a database and get back a natural language answer. The main difference between the two is that our agent can query the database in a loop as many times as it needs to answer the question.\nâš ï¸ Security note âš ï¸\nâ€‹\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices,\nsee here\n.\nArchitecture\nâ€‹\nAt a high-level, the steps of these systems are:\nConvert question to SQL query\n: Model converts user input to a SQL query.\nExecute SQL query\n: Execute the query.\nAnswer the question\n: Model responds to user input using the query results.\nNote that querying data in CSVs can follow a similar approach. See our\nhow-to guide\non question-answering over CSV data for more detail.\nSetup\nâ€‹\nFirst, get required packages and set environment variables:\n%\n%\ncapture\n-\n-\nno\n-\nstderr\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\ncommunity langgraph\n# Comment out the below to opt-out of using LangSmith in this notebook. Not required.\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"LANGSMITH_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nSample data\nâ€‹\nThe below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow\nthese installation steps\nto create\nChinook.db\nin the same directory as this notebook. You can also download and build the database via the command line:\ncurl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db\nNow,\nChinook.db\nis in our directory and we can interface with it using the SQLAlchemy-driven\nSQLDatabase\nclass:\nfrom\nlangchain_community\n.\nutilities\nimport\nSQLDatabase\ndb\n=\nSQLDatabase\n.\nfrom_uri\n(\n\"sqlite:///Chinook.db\"\n)\nprint\n(\ndb\n.\ndialect\n)\nprint\n(\ndb\n.\nget_usable_table_names\n(\n)\n)\ndb\n.\nrun\n(\n\"SELECT * FROM Artist LIMIT 10;\"\n)\nsqlite\n['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n\"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\"\nGreat! We've got a SQL database that we can query. Now let's try hooking it up to an LLM.\nChains\nâ€‹\nChains are compositions of predictable steps. In\nLangGraph\n, we can represent a chain via simple sequence of nodes. Let's create a sequence of steps that, given a question, does the following:\nconverts the question into a SQL query;\nexecutes the query;\nuses the result to answer the original question.\nThere are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input-- even \"hello\". Importantly, as we'll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section.\nApplication state\nâ€‹\nThe LangGraph\nstate\nof our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a\nTypedDict\n, but can also be a\nPydantic BaseModel\n.\nFor this application, we can just keep track of the input question, generated query, query result, and generated answer:\nfrom\ntyping_extensions\nimport\nTypedDict\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\nquery\n:\nstr\nresult\n:\nstr\nanswer\n:\nstr\nNow we just need functions that operate on this state and populate its contents.\nConvert question to SQL query\nâ€‹\nThe first step is to take the user input and convert it to a SQL query. To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain's\nstructured output\nabstraction.\nLet's select a chat model for our application:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nLet's provide some instructions for our model:\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nsystem_message\n=\n\"\"\"\nGiven an input question, create a syntactically correct {dialect} query to\nrun to help find the answer. Unless the user specifies in his question a\nspecific number of examples they wish to obtain, always limit your query to\nat most {top_k} results. You can order the results by a relevant column to\nreturn the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for a the\nfew relevant columns given the question.\nPay attention to use only the column names that you can see in the schema\ndescription. Be careful to not query for columns that do not exist. Also,\npay attention to which column is in which table.\nOnly use the following tables:\n{table_info}\n\"\"\"\nuser_prompt\n=\n\"Question: {input}\"\nquery_prompt_template\n=\nChatPromptTemplate\n(\n[\n(\n\"system\"\n,\nsystem_message\n)\n,\n(\n\"user\"\n,\nuser_prompt\n)\n]\n)\nfor\nmessage\nin\nquery_prompt_template\n.\nmessages\n:\nmessage\n.\npretty_print\n(\n)\nAPI Reference:\nChatPromptTemplate\n================================\u001b[1m System Message \u001b[0m================================\nGiven an input question, create a syntactically correct \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m query to\nrun to help find the answer. Unless the user specifies in his question a\nspecific number of examples they wish to obtain, always limit your query to\nat most \u001b[33;1m\u001b[1;3m{top_k}\u001b[0m results. You can order the results by a relevant column to\nreturn the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for a the\nfew relevant columns given the question.\nPay attention to use only the column names that you can see in the schema\ndescription. Be careful to not query for columns that do not exist. Also,\npay attention to which column is in which table.\nOnly use the following tables:\n\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m\n================================\u001b[1m Human Message \u001b[0m=================================\nQuestion: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\nThe prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain's\nSQLDatabase\nobject includes methods to help with this. Our\nwrite_query\nstep will just populate these parameters and prompt a model to generate the SQL query:\nfrom\ntyping_extensions\nimport\nAnnotated\nclass\nQueryOutput\n(\nTypedDict\n)\n:\n\"\"\"Generated SQL query.\"\"\"\nquery\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"Syntactically valid SQL query.\"\n]\ndef\nwrite_query\n(\nstate\n:\nState\n)\n:\n\"\"\"Generate SQL query to fetch information.\"\"\"\nprompt\n=\nquery_prompt_template\n.\ninvoke\n(\n{\n\"dialect\"\n:\ndb\n.\ndialect\n,\n\"top_k\"\n:\n10\n,\n\"table_info\"\n:\ndb\n.\nget_table_info\n(\n)\n,\n\"input\"\n:\nstate\n[\n\"question\"\n]\n,\n}\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nQueryOutput\n)\nresult\n=\nstructured_llm\n.\ninvoke\n(\nprompt\n)\nreturn\n{\n\"query\"\n:\nresult\n[\n\"query\"\n]\n}\nLet's test it out:\nwrite_query\n(\n{\n\"question\"\n:\n\"How many Employees are there?\"\n}\n)\n{'query': 'SELECT COUNT(*) as employee_count FROM Employee;'}\nExecute query\nâ€‹\nThis is the most dangerous part of creating a SQL chain.\nConsider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below).\nTo execute the query, we will load a tool from\nlangchain-community\n. Our\nexecute_query\nnode will just wrap this tool:\nfrom\nlangchain_community\n.\ntools\n.\nsql_database\n.\ntool\nimport\nQuerySQLDatabaseTool\ndef\nexecute_query\n(\nstate\n:\nState\n)\n:\n\"\"\"Execute SQL query.\"\"\"\nexecute_query_tool\n=\nQuerySQLDatabaseTool\n(\ndb\n=\ndb\n)\nreturn\n{\n\"result\"\n:\nexecute_query_tool\n.\ninvoke\n(\nstate\n[\n\"query\"\n]\n)\n}\nTesting this step:\nexecute_query\n(\n{\n\"query\"\n:\n\"SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;\"\n}\n)\n{'result': '[(8,)]'}\nGenerate answer\nâ€‹\nFinally, our last step generates an answer to the question given the information pulled from the database:\ndef\ngenerate_answer\n(\nstate\n:\nState\n)\n:\n\"\"\"Answer question using retrieved information as context.\"\"\"\nprompt\n=\n(\n\"Given the following user question, corresponding SQL query, \"\n\"and SQL result, answer the user question.\\n\\n\"\nf\"Question:\n{\nstate\n[\n'question'\n]\n}\n\\n\"\nf\"SQL Query:\n{\nstate\n[\n'query'\n]\n}\n\\n\"\nf\"SQL Result:\n{\nstate\n[\n'result'\n]\n}\n\"\n)\nresponse\n=\nllm\n.\ninvoke\n(\nprompt\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\nOrchestrating with LangGraph\nâ€‹\nFinally, we compile our application into a single\ngraph\nobject. In this case, we are just connecting the three steps into a single sequence.\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nStateGraph\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nwrite_query\n,\nexecute_query\n,\ngenerate_answer\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"write_query\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nStateGraph\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nLet's test our application! Note that we can stream the results of individual steps:\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"question\"\n:\n\"How many employees are there?\"\n}\n,\nstream_mode\n=\n\"updates\"\n)\n:\nprint\n(\nstep\n)\n{'write_query': {'query': 'SELECT COUNT(*) as employee_count FROM Employee;'}}\n{'execute_query': {'result': '[(8,)]'}}\n{'generate_answer': {'answer': 'There are 8 employees in total.'}}\nCheck out the\nLangSmith trace\n.\nHuman-in-the-loop\nâ€‹\nLangGraph supports a number of features that can be useful for this workflow. One of them is\nhuman-in-the-loop\n: we can interrupt our application before sensitive steps (such as the execution of a SQL query) for human review. This is enabled by LangGraph's\npersistence\nlayer, which saves run progress to your storage of choice. Below, we specify storage in-memory:\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nmemory\n=\nMemorySaver\n(\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\ncheckpointer\n=\nmemory\n,\ninterrupt_before\n=\n[\n\"execute_query\"\n]\n)\n# Now that we're using persistence, we need to specify a thread ID\n# so that we can continue the run after review.\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}\n}\nAPI Reference:\nMemorySaver\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nLet's repeat the same run, adding in a simple yes/no approval step:\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"question\"\n:\n\"How many employees are there?\"\n}\n,\nconfig\n,\nstream_mode\n=\n\"updates\"\n,\n)\n:\nprint\n(\nstep\n)\ntry\n:\nuser_approval\n=\ninput\n(\n\"Do you want to go to execute query? (yes/no): \"\n)\nexcept\nException\n:\nuser_approval\n=\n\"no\"\nif\nuser_approval\n.\nlower\n(\n)\n==\n\"yes\"\n:\n# If approved, continue the graph execution\nfor\nstep\nin\ngraph\n.\nstream\n(\nNone\n,\nconfig\n,\nstream_mode\n=\n\"updates\"\n)\n:\nprint\n(\nstep\n)\nelse\n:\nprint\n(\n\"Operation cancelled by user.\"\n)\n{'write_query': {'query': 'SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;'}}\n{'__interrupt__': ()}\n``````output\nDo you want to go to execute query? (yes/no):  yes\n``````output\n{'execute_query': {'result': '[(8,)]'}}\n{'generate_answer': {'answer': 'There are 8 employees.'}}\nSee\nthis\nLangGraph guide for more detail and examples.\nNext steps\nâ€‹\nFor more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:\nPrompting strategies\n: Advanced prompt engineering techniques.\nQuery checking\n: Add query validation and error handling.\nLarge databases\n: Techniques for working with large databases.\nAgents\nâ€‹\nAgents\nleverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the query generation and execution process. Although their behavior is less predictable than the above \"chain\", they feature some advantages:\nThey can query the database as many times as needed to answer the user question.\nThey can recover from errors by running a generated query, catching the traceback and regenerating it correctly.\nThey can answer questions based on the databases' schema as well as on the databases' content (like describing a specific table).\nBelow we assemble a minimal SQL agent. We will equip it with a set of tools using LangChain's\nSQLDatabaseToolkit\n. Using LangGraph's\npre-built ReAct agent constructor\n, we can do this in one line.\ntip\nCheck out LangGraph's\nSQL Agent Tutorial\nfor a more advanced formulation of a SQL agent.\nThe\nSQLDatabaseToolkit\nincludes tools that can:\nCreate and execute queries\nCheck query syntax\nRetrieve table descriptions\n... and more\nfrom\nlangchain_community\n.\nagent_toolkits\nimport\nSQLDatabaseToolkit\ntoolkit\n=\nSQLDatabaseToolkit\n(\ndb\n=\ndb\n,\nllm\n=\nllm\n)\ntools\n=\ntoolkit\n.\nget_tools\n(\n)\ntools\n[QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>),\nInfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>),\nListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>),\nQuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\\n{query}\\nDouble check the {dialect} query above for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\n\\nOutput the final SQL query only.\\n\\nSQL Query: '), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))]\nSystem Prompt\nâ€‹\nWe will also want to load a system prompt for our agent. This will consist of instructions for how to behave. Note that the prompt below has several parameters, which we assign below.\nsystem_message\n=\n\"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\nThen you should query the schema of the most relevant tables.\n\"\"\"\n.\nformat\n(\ndialect\n=\n\"SQLite\"\n,\ntop_k\n=\n5\n,\n)\nInitializing agent\nâ€‹\nWe will use a prebuilt\nLangGraph\nagent to build our agent\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nagent_executor\n=\ncreate_react_agent\n(\nllm\n,\ntools\n,\nprompt\n=\nsystem_message\n)\nAPI Reference:\nHumanMessage\n|\ncreate_react_agent\nConsider how the agent responds to the below question:\nquestion\n=\n\"Which country's customers spent the most?\"\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nquestion\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhich country's customers spent the most?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsql_db_list_tables (call_tFp7HYD6sAAmCShgeqkVZH6Q)\nCall ID: call_tFp7HYD6sAAmCShgeqkVZH6Q\nArgs:\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: sql_db_list_tables\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsql_db_schema (call_KJZ1Jx6JazyDdJa0uH1UeiOz)\nCall ID: call_KJZ1Jx6JazyDdJa0uH1UeiOz\nArgs:\ntable_names: Customer, Invoice\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: sql_db_schema\nCREATE TABLE \"Customer\" (\n\"CustomerId\" INTEGER NOT NULL,\n\"FirstName\" NVARCHAR(40) NOT NULL,\n\"LastName\" NVARCHAR(20) NOT NULL,\n\"Company\" NVARCHAR(80),\n\"Address\" NVARCHAR(70),\n\"City\" NVARCHAR(40),\n\"State\" NVARCHAR(40),\n\"Country\" NVARCHAR(40),\n\"PostalCode\" NVARCHAR(10),\n\"Phone\" NVARCHAR(24),\n\"Fax\" NVARCHAR(24),\n\"Email\" NVARCHAR(60) NOT NULL,\n\"SupportRepId\" INTEGER,\nPRIMARY KEY (\"CustomerId\"),\nFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\n)\n/*\n3 rows from Customer table:\nCustomerId\tFirstName\tLastName\tCompany\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\tSupportRepId\n1\tLuÃ­s\tGonÃ§alves\tEmbraer - Empresa Brasileira de AeronÃ¡utica S.A.\tAv. Brigadeiro Faria Lima, 2170\tSÃ£o JosÃ© dos Campos\tSP\tBrazil\t12227-000\t+55 (12) 3923-5555\t+55 (12) 3923-5566\tluisg@embraer.com.br\t3\n2\tLeonie\tKÃ¶hler\tNone\tTheodor-Heuss-StraÃŸe 34\tStuttgart\tNone\tGermany\t70174\t+49 0711 2842222\tNone\tleonekohler@surfeu.de\t5\n3\tFranÃ§ois\tTremblay\tNone\t1498 rue BÃ©langer\tMontrÃ©al\tQC\tCanada\tH2G 1A7\t+1 (514) 721-4711\tNone\tftremblay@gmail.com\t3\n*/\nCREATE TABLE \"Invoice\" (\n\"InvoiceId\" INTEGER NOT NULL,\n\"CustomerId\" INTEGER NOT NULL,\n\"InvoiceDate\" DATETIME NOT NULL,\n\"BillingAddress\" NVARCHAR(70),\n\"BillingCity\" NVARCHAR(40),\n\"BillingState\" NVARCHAR(40),\n\"BillingCountry\" NVARCHAR(40),\n\"BillingPostalCode\" NVARCHAR(10),\n\"Total\" NUMERIC(10, 2) NOT NULL,\nPRIMARY KEY (\"InvoiceId\"),\nFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\n)\n/*\n3 rows from Invoice table:\nInvoiceId\tCustomerId\tInvoiceDate\tBillingAddress\tBillingCity\tBillingState\tBillingCountry\tBillingPostalCode\tTotal\n1\t2\t2021-01-01 00:00:00\tTheodor-Heuss-StraÃŸe 34\tStuttgart\tNone\tGermany\t70174\t1.98\n2\t4\t2021-01-02 00:00:00\tUllevÃ¥lsveien 14\tOslo\tNone\tNorway\t0171\t3.96\n3\t8\t2021-01-03 00:00:00\tGrÃ©trystraat 63\tBrussels\tNone\tBelgium\t1000\t5.94\n*/\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsql_db_query_checker (call_AQuTGbgH63u4gPgyV723yrjX)\nCall ID: call_AQuTGbgH63u4gPgyV723yrjX\nArgs:\nquery: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: sql_db_query_checker\n\\`\\`\\`sql\nSELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\n\\`\\`\\`\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsql_db_query (call_B88EwU44nwwpQL5M9nlcemSU)\nCall ID: call_B88EwU44nwwpQL5M9nlcemSU\nArgs:\nquery: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: sql_db_query\n[('USA', 523.06)]\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe country whose customers spent the most is the USA, with a total spending of 523.06.\nYou can also use the\nLangSmith trace\nto visualize these steps and associated metadata.\nNote that the agent executes multiple queries until it has the information it needs:\nList available tables;\nRetrieves the schema for three tables;\nQueries multiple of the tables via a join operation.\nThe agent is then able to use the result of the final query to generate an answer to the original question.\nThe agent can similarly handle qualitative questions:\nquestion\n=\n\"Describe the playlisttrack table\"\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nquestion\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nDescribe the playlisttrack table\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsql_db_list_tables (call_fMF8eTmX5TJDJjc3Mhdg52TI)\nCall ID: call_fMF8eTmX5TJDJjc3Mhdg52TI\nArgs:\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: sql_db_list_tables\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsql_db_schema (call_W8Vkk4NEodkAAIg8nexAszUH)\nCall ID: call_W8Vkk4NEodkAAIg8nexAszUH\nArgs:\ntable_names: PlaylistTrack\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: sql_db_schema\nCREATE TABLE \"PlaylistTrack\" (\n\"PlaylistId\" INTEGER NOT NULL,\n\"TrackId\" INTEGER NOT NULL,\nPRIMARY KEY (\"PlaylistId\", \"TrackId\"),\nFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"),\nFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\")\n)\n/*\n3 rows from PlaylistTrack table:\nPlaylistId\tTrackId\n1\t3402\n1\t3389\n1\t3390\n*/\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe `PlaylistTrack` table is designed to associate tracks with playlists. It has the following structure:\n- **PlaylistId**: An integer that serves as a foreign key referencing the `Playlist` table. It is part of the composite primary key.\n- **TrackId**: An integer that serves as a foreign key referencing the `Track` table. It is also part of the composite primary key.\nThe primary key for this table is a composite key consisting of both `PlaylistId` and `TrackId`, ensuring that each track can be uniquely associated with a playlist. The table enforces referential integrity by linking to the `Track` and `Playlist` tables through foreign keys.\nDealing with high-cardinality columns\nâ€‹\nIn order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.\nWe can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.\nFirst we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:\nimport\nast\nimport\nre\ndef\nquery_as_list\n(\ndb\n,\nquery\n)\n:\nres\n=\ndb\n.\nrun\n(\nquery\n)\nres\n=\n[\nel\nfor\nsub\nin\nast\n.\nliteral_eval\n(\nres\n)\nfor\nel\nin\nsub\nif\nel\n]\nres\n=\n[\nre\n.\nsub\n(\nr\"\\b\\d+\\b\"\n,\n\"\"\n,\nstring\n)\n.\nstrip\n(\n)\nfor\nstring\nin\nres\n]\nreturn\nlist\n(\nset\n(\nres\n)\n)\nartists\n=\nquery_as_list\n(\ndb\n,\n\"SELECT Name FROM Artist\"\n)\nalbums\n=\nquery_as_list\n(\ndb\n,\n\"SELECT Title FROM Album\"\n)\nalbums\n[\n:\n5\n]\n['In Through The Out Door',\n'Transmission',\n'Battlestar Galactica (Classic), Season',\n'A Copland Celebration, Vol. I',\n'Quiet Songs']\nUsing this function, we can create a\nretriever tool\nthat the agent can execute at its discretion.\nLet's select an\nembeddings model\nand\nvector store\nfor this step:\nSelect an embedding model\n:\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nSelect a vector store\n:\nSelect\nvector store\n:\nIn-memory\nâ–¾\nIn-memory\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\npip install -qU langchain-core\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\nWe can now construct a retrieval tool that can search over relevant proper nouns in the database:\nfrom\nlangchain\n.\nagents\n.\nagent_toolkits\nimport\ncreate_retriever_tool\n_\n=\nvector_store\n.\nadd_texts\n(\nartists\n+\nalbums\n)\nretriever\n=\nvector_store\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n5\n}\n)\ndescription\n=\n(\n\"Use to look up values to filter on. Input is an approximate spelling \"\n\"of the proper noun, output is valid proper nouns. Use the noun most \"\n\"similar to the search.\"\n)\nretriever_tool\n=\ncreate_retriever_tool\n(\nretriever\n,\nname\n=\n\"search_proper_nouns\"\n,\ndescription\n=\ndescription\n,\n)\nLet's try it out:\nprint\n(\nretriever_tool\n.\ninvoke\n(\n\"Alice Chains\"\n)\n)\nAlice In Chains\nAlanis Morissette\nPearl Jam\nPearl Jam\nAudioslave\nThis way, if the agent determines it needs to write a filter based on an artist along the lines of \"Alice Chains\", it can first use the retriever tool to observe relevant values of a column.\nPutting this together:\n# Add to system message\nsuffix\n=\n(\n\"If you need to filter on a proper noun like a Name, you must ALWAYS first look up \"\n\"the filter value using the 'search_proper_nouns' tool! Do not try to \"\n\"guess at the proper name - use this function to find similar ones.\"\n)\nsystem\n=\nf\"\n{\nsystem_message\n}\n\\n\\n\n{\nsuffix\n}\n\"\ntools\n.\nappend\n(\nretriever_tool\n)\nagent\n=\ncreate_react_agent\n(\nllm\n,\ntools\n,\nprompt\n=\nsystem\n)\nquestion\n=\n\"How many albums does alis in chain have?\"\nfor\nstep\nin\nagent\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nquestion\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nHow many albums does alis in chain have?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsearch_proper_nouns (call_8ryjsRPLAr79mM3Qvnq6gTOH)\nCall ID: call_8ryjsRPLAr79mM3Qvnq6gTOH\nArgs:\nquery: alis in chain\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: search_proper_nouns\nAlice In Chains\nAisha Duo\nXis\nDa Lama Ao Caos\nA-Sides\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsql_db_list_tables (call_NJjtCpU89MBMplssjn1z0xzq)\nCall ID: call_NJjtCpU89MBMplssjn1z0xzq\nArgs:\nsearch_proper_nouns (call_1BfrueC9koSIyi4OfMu2Ao8q)\nCall ID: call_1BfrueC9koSIyi4OfMu2Ao8q\nArgs:\nquery: Alice In Chains\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: search_proper_nouns\nAlice In Chains\nPearl Jam\nPearl Jam\nFoo Fighters\nSoundgarden\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsql_db_schema (call_Kn09w9jd9swcNzIZ1b5MlKID)\nCall ID: call_Kn09w9jd9swcNzIZ1b5MlKID\nArgs:\ntable_names: Album, Artist\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: sql_db_schema\nCREATE TABLE \"Album\" (\n\"AlbumId\" INTEGER NOT NULL,\n\"Title\" NVARCHAR(160) NOT NULL,\n\"ArtistId\" INTEGER NOT NULL,\nPRIMARY KEY (\"AlbumId\"),\nFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\n)\n/*\n3 rows from Album table:\nAlbumId\tTitle\tArtistId\n1\tFor Those About To Rock We Salute You\t1\n2\tBalls to the Wall\t2\n3\tRestless and Wild\t2\n*/\nCREATE TABLE \"Artist\" (\n\"ArtistId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(120),\nPRIMARY KEY (\"ArtistId\")\n)\n/*\n3 rows from Artist table:\nArtistId\tName\n1\tAC/DC\n2\tAccept\n3\tAerosmith\n*/\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsql_db_query (call_WkHRiPcBoGN9bc58MIupRHKP)\nCall ID: call_WkHRiPcBoGN9bc58MIupRHKP\nArgs:\nquery: SELECT COUNT(*) FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'Alice In Chains')\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: sql_db_query\n[(1,)]\n==================================\u001b[1m Ai Message \u001b[0m==================================\nAlice In Chains has released 1 album in the database.\nAs we can see, both in the streamed steps and in the\nLangSmith trace\n, the agent used the\nsearch_proper_nouns\ntool in order to check how to correctly query the database for this specific artist.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/tutorials/summarization/",
    "Tutorials\nSummarize Text\nOn this page\nSummarize Text\ninfo\nThis tutorial demonstrates text summarization using built-in chains and\nLangGraph\n.\nA\nprevious version\nof this page showcased the legacy chains\nStuffDocumentsChain\n,\nMapReduceDocumentsChain\n, and\nRefineDocumentsChain\n. See\nhere\nfor information on using those abstractions and a comparison with the methods demonstrated in this tutorial.\nSuppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.\nLLMs are a great tool for this given their proficiency in understanding and synthesizing text.\nIn the context of\nretrieval-augmented generation\n, summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM.\nIn this walkthrough we'll go over how to summarize content from multiple documents using LLMs.\nConcepts\nâ€‹\nConcepts we will cover are:\nUsing\nlanguage models\n.\nUsing\ndocument loaders\n, specifically the\nWebBaseLoader\nto load content from an HTML webpage.\nTwo ways to summarize or otherwise combine documents.\nStuff\n, which simply concatenates documents into a prompt;\nMap-reduce\n, for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries.\nShorter, targeted guides on these strategies and others, including\niterative refinement\n, can be found in the\nhow-to guides\n.\nSetup\nâ€‹\nJupyter Notebook\nâ€‹\nThis guide (and most of the other guides in the documentation) uses\nJupyter notebooks\nand assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See\nhere\nfor instructions on how to install.\nInstallation\nâ€‹\nTo install LangChain run:\nPip\nConda\npip install langchain\nconda install langchain -c conda-forge\nFor more details, see our\nInstallation guide\n.\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nOr, if in a notebook, you can set them with:\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nOverview\nâ€‹\nA central question for building a summarizer is how to pass your documents into the LLM's context window. Two common approaches for this are:\nStuff\n: Simply \"stuff\" all your documents into a single prompt. This is the simplest approach (see\nhere\nfor more on the\ncreate_stuff_documents_chain\nconstructor, which is used for this method).\nMap-reduce\n: Summarize each document on its own in a \"map\" step and then \"reduce\" the summaries into a final summary (see\nhere\nfor more on the\nMapReduceDocumentsChain\n, which is used for this method).\nNote that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence,\niterative refinement\nmay be more effective.\nSetup\nâ€‹\nFirst set environment variables and install packages:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet tiktoken langchain langgraph beautifulsoup4 langchain\n-\ncommunity\n# Set env var OPENAI_API_KEY or load from a .env file\n# import dotenv\n# dotenv.load_dotenv()\nimport\nos\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nFirst we load in our documents. We will use\nWebBaseLoader\nto load a blog post:\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nloader\n=\nWebBaseLoader\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n)\ndocs\n=\nloader\n.\nload\n(\n)\nLet's next select a LLM:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nStuff: summarize in a single LLM call\nâ€‹\nWe can use\ncreate_stuff_documents_chain\n, especially if using larger context window models such as:\n128k token OpenAI\ngpt-4o\n200k token Anthropic\nclaude-3-5-sonnet-latest\nThe chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\nimport\ncreate_stuff_documents_chain\nfrom\nlangchain\n.\nchains\n.\nllm\nimport\nLLMChain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n# Define prompt\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"Write a concise summary of the following:\\\\n\\\\n{context}\"\n)\n]\n)\n# Instantiate chain\nchain\n=\ncreate_stuff_documents_chain\n(\nllm\n,\nprompt\n)\n# Invoke chain\nresult\n=\nchain\n.\ninvoke\n(\n{\n\"context\"\n:\ndocs\n}\n)\nprint\n(\nresult\n)\nAPI Reference:\nChatPromptTemplate\nThe article \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture that includes three main components: Planning, Memory, and Tool Use.\n1. **Planning** involves task decomposition, where complex tasks are broken down into manageable subgoals, and self-reflection, allowing agents to learn from past actions to improve future performance. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for enhancing reasoning and planning.\n2. **Memory** is categorized into short-term and long-term memory, with mechanisms for fast retrieval using Maximum Inner Product Search (MIPS) algorithms. This allows agents to retain and recall information effectively.\n3. **Tool Use** enables agents to interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and frameworks like HuggingGPT, which facilitate task planning and execution.\nThe article also addresses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with case studies demonstrating the practical applications of these concepts in scientific discovery and interactive simulations. Overall, the article emphasizes the potential of LLMs as powerful problem solvers in autonomous agent systems.\nStreaming\nâ€‹\nNote that we can also stream the result token-by-token:\nfor\ntoken\nin\nchain\n.\nstream\n(\n{\n\"context\"\n:\ndocs\n}\n)\n:\nprint\n(\ntoken\n,\nend\n=\n\"|\"\n)\n|The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| architecture| that| includes| three| main| components|:| Planning|,| Memory|,| and| Tool| Use|.|\n|1|.| **|Planning|**| involves| task| decomposition|,| where| complex| tasks| are| broken| down| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| enhancing| reasoning| and| planning|.\n|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| mechanisms| for| fast| retrieval| using| Maximum| Inner| Product| Search| (|M|IPS|)| algorithms|.| This| allows| agents| to| retain| and| recall| information| effectively|.\n|3|.| **|Tool| Use|**| emphasizes| the| integration| of| external| APIs| and| tools| to| extend| the| capabilities| of| L|LM|s|,| enabling| them| to| perform| tasks| beyond| their| inherent| limitations|.| Examples| include| MR|KL| systems| and| frameworks| like| Hug|ging|GPT|,| which| facilitate| task| planning| and| execution|.\n|The| article| also| addresses| challenges| such| as| finite| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| interfaces|.| It| concludes| with| case| studies| demonstrating| the| practical| applications| of| L|LM|-powered| agents| in| scientific| discovery| and| interactive| simulations|.| Overall|,| the| piece| illustrates| the| potential| of| L|LM|s| as| general| problem| sol|vers| and| their| evolving| role| in| autonomous| systems|.||\nGo deeper\nâ€‹\nYou can easily customize the prompt.\nYou can easily try different LLMs, (e.g.,\nClaude\n) via the\nllm\nparameter.\nMap-Reduce: summarize long texts via parallelization\nâ€‹\nLet's unpack the map reduce approach. For this, we'll first map each document to an individual summary using an LLM. Then we'll reduce or consolidate those summaries into a single global summary.\nNote that the map step is typically parallelized over the input documents.\nLangGraph\n, built on top of\nlangchain-core\n, supports\nmap-reduce\nworkflows and is well-suited to this problem:\nLangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;\nLangGraph's\ncheckpointing\nsupports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.\nThe LangGraph implementation is straightforward to modify and extend, as we will see below.\nMap\nâ€‹\nLet's first define the prompt associated with the map step. We can use the same summarization prompt as in the\nstuff\napproach, above:\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nmap_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"Write a concise summary of the following:\\\\n\\\\n{context}\"\n)\n]\n)\nAPI Reference:\nChatPromptTemplate\nWe can also use the Prompt Hub to store and fetch prompts.\nThis will work with your\nLangSmith API key\n.\nFor example, see the map prompt\nhere\n.\nfrom\nlangchain\nimport\nhub\nmap_prompt\n=\nhub\n.\npull\n(\n\"rlm/map-prompt\"\n)\nReduce\nâ€‹\nWe also define a prompt that takes the document mapping results and reduces them into a single output.\n# Also available via the hub: `hub.pull(\"rlm/reduce-prompt\")`\nreduce_template\n=\n\"\"\"\nThe following is a set of summaries:\n{docs}\nTake these and distill it into a final, consolidated summary\nof the main themes.\n\"\"\"\nreduce_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\nreduce_template\n)\n]\n)\nOrchestration via LangGraph\nâ€‹\nBelow we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.\nMap-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive \"collapsing\" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.\nFirst we chunk the blog post into smaller \"sub documents\" to be mapped:\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\ntext_splitter\n=\nCharacterTextSplitter\n.\nfrom_tiktoken_encoder\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n0\n)\nsplit_docs\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\nprint\n(\nf\"Generated\n{\nlen\n(\nsplit_docs\n)\n}\ndocuments.\"\n)\nCreated a chunk of size 1003, which is longer than the specified 1000\n``````output\nGenerated 14 documents.\nNext, we define our graph. Note that we define an artificially low maximum token length of 1,000 tokens to illustrate the \"collapsing\" step.\nimport\noperator\nfrom\ntyping\nimport\nAnnotated\n,\nList\n,\nLiteral\n,\nTypedDict\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\n.\nreduce\nimport\n(\nacollapse_docs\n,\nsplit_list_of_docs\n,\n)\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlanggraph\n.\nconstants\nimport\nSend\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\ntoken_max\n=\n1000\ndef\nlength_function\n(\ndocuments\n:\nList\n[\nDocument\n]\n)\n-\n>\nint\n:\n\"\"\"Get number of tokens for input contents.\"\"\"\nreturn\nsum\n(\nllm\n.\nget_num_tokens\n(\ndoc\n.\npage_content\n)\nfor\ndoc\nin\ndocuments\n)\n# This will be the overall state of the main graph.\n# It will contain the input document contents, corresponding\n# summaries, and a final summary.\nclass\nOverallState\n(\nTypedDict\n)\n:\n# Notice here we use the operator.add\n# This is because we want combine all the summaries we generate\n# from individual nodes back into one list - this is essentially\n# the \"reduce\" part\ncontents\n:\nList\n[\nstr\n]\nsummaries\n:\nAnnotated\n[\nlist\n,\noperator\n.\nadd\n]\ncollapsed_summaries\n:\nList\n[\nDocument\n]\nfinal_summary\n:\nstr\n# This will be the state of the node that we will \"map\" all\n# documents to in order to generate summaries\nclass\nSummaryState\n(\nTypedDict\n)\n:\ncontent\n:\nstr\n# Here we generate a summary, given a document\nasync\ndef\ngenerate_summary\n(\nstate\n:\nSummaryState\n)\n:\nprompt\n=\nmap_prompt\n.\ninvoke\n(\nstate\n[\n\"content\"\n]\n)\nresponse\n=\nawait\nllm\n.\nainvoke\n(\nprompt\n)\nreturn\n{\n\"summaries\"\n:\n[\nresponse\n.\ncontent\n]\n}\n# Here we define the logic to map out over the documents\n# We will use this an edge in the graph\ndef\nmap_summaries\n(\nstate\n:\nOverallState\n)\n:\n# We will return a list of `Send` objects\n# Each `Send` object consists of the name of a node in the graph\n# as well as the state to send to that node\nreturn\n[\nSend\n(\n\"generate_summary\"\n,\n{\n\"content\"\n:\ncontent\n}\n)\nfor\ncontent\nin\nstate\n[\n\"contents\"\n]\n]\ndef\ncollect_summaries\n(\nstate\n:\nOverallState\n)\n:\nreturn\n{\n\"collapsed_summaries\"\n:\n[\nDocument\n(\nsummary\n)\nfor\nsummary\nin\nstate\n[\n\"summaries\"\n]\n]\n}\nasync\ndef\n_reduce\n(\ninput\n:\ndict\n)\n-\n>\nstr\n:\nprompt\n=\nreduce_prompt\n.\ninvoke\n(\ninput\n)\nresponse\n=\nawait\nllm\n.\nainvoke\n(\nprompt\n)\nreturn\nresponse\n.\ncontent\n# Add node to collapse summaries\nasync\ndef\ncollapse_summaries\n(\nstate\n:\nOverallState\n)\n:\ndoc_lists\n=\nsplit_list_of_docs\n(\nstate\n[\n\"collapsed_summaries\"\n]\n,\nlength_function\n,\ntoken_max\n)\nresults\n=\n[\n]\nfor\ndoc_list\nin\ndoc_lists\n:\nresults\n.\nappend\n(\nawait\nacollapse_docs\n(\ndoc_list\n,\n_reduce\n)\n)\nreturn\n{\n\"collapsed_summaries\"\n:\nresults\n}\n# This represents a conditional edge in the graph that determines\n# if we should collapse the summaries or not\ndef\nshould_collapse\n(\nstate\n:\nOverallState\n,\n)\n-\n>\nLiteral\n[\n\"collapse_summaries\"\n,\n\"generate_final_summary\"\n]\n:\nnum_tokens\n=\nlength_function\n(\nstate\n[\n\"collapsed_summaries\"\n]\n)\nif\nnum_tokens\n>\ntoken_max\n:\nreturn\n\"collapse_summaries\"\nelse\n:\nreturn\n\"generate_final_summary\"\n# Here we will generate the final summary\nasync\ndef\ngenerate_final_summary\n(\nstate\n:\nOverallState\n)\n:\nresponse\n=\nawait\n_reduce\n(\nstate\n[\n\"collapsed_summaries\"\n]\n)\nreturn\n{\n\"final_summary\"\n:\nresponse\n}\n# Construct the graph\n# Nodes:\ngraph\n=\nStateGraph\n(\nOverallState\n)\ngraph\n.\nadd_node\n(\n\"generate_summary\"\n,\ngenerate_summary\n)\n# same as before\ngraph\n.\nadd_node\n(\n\"collect_summaries\"\n,\ncollect_summaries\n)\ngraph\n.\nadd_node\n(\n\"collapse_summaries\"\n,\ncollapse_summaries\n)\ngraph\n.\nadd_node\n(\n\"generate_final_summary\"\n,\ngenerate_final_summary\n)\n# Edges:\ngraph\n.\nadd_conditional_edges\n(\nSTART\n,\nmap_summaries\n,\n[\n\"generate_summary\"\n]\n)\ngraph\n.\nadd_edge\n(\n\"generate_summary\"\n,\n\"collect_summaries\"\n)\ngraph\n.\nadd_conditional_edges\n(\n\"collect_summaries\"\n,\nshould_collapse\n)\ngraph\n.\nadd_conditional_edges\n(\n\"collapse_summaries\"\n,\nshould_collapse\n)\ngraph\n.\nadd_edge\n(\n\"generate_final_summary\"\n,\nEND\n)\napp\n=\ngraph\n.\ncompile\n(\n)\nAPI Reference:\nDocument\n|\nSend\n|\nStateGraph\nLangGraph allows the graph structure to be plotted to help visualize its function:\nfrom\nIPython\n.\ndisplay\nimport\nImage\nImage\n(\napp\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\nWhen running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.\nNote that because we have a loop in the graph, it can be helpful to specify a\nrecursion_limit\non its execution. This will raise a specific error when the specified limit is exceeded.\nasync\nfor\nstep\nin\napp\n.\nastream\n(\n{\n\"contents\"\n:\n[\ndoc\n.\npage_content\nfor\ndoc\nin\nsplit_docs\n]\n}\n,\n{\n\"recursion_limit\"\n:\n10\n}\n,\n)\n:\nprint\n(\nlist\n(\nstep\n.\nkeys\n(\n)\n)\n)\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['collect_summaries']\n['collapse_summaries']\n['collapse_summaries']\n['generate_final_summary']\nprint\n(\nstep\n)\n{'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the provided documents is as follows:\\n\\n1. **Integration of Large Language Models (LLMs) in Autonomous Agents**: The documents explore the evolving role of LLMs in autonomous systems, emphasizing their enhanced reasoning and acting capabilities through methodologies that incorporate structured planning, memory systems, and tool use.\\n\\n2. **Core Components of Autonomous Agents**:\\n   - **Planning**: Techniques like task decomposition (e.g., Chain of Thought) and external classical planners are utilized to facilitate long-term planning by breaking down complex tasks.\\n   - **Memory**: The memory system is divided into short-term (in-context learning) and long-term memory, with parallels drawn between human memory and machine learning to improve agent performance.\\n   - **Tool Use**: Agents utilize external APIs and algorithms to enhance problem-solving abilities, exemplified by frameworks like HuggingGPT that manage task workflows.\\n\\n3. **Neuro-Symbolic Architectures**: The integration of MRKL (Modular Reasoning, Knowledge, and Language) systems combines neural and symbolic expert modules with LLMs, addressing challenges in tasks such as verbal math problem-solving.\\n\\n4. **Specialized Applications**: Case studies, such as ChemCrow and projects in anticancer drug discovery, demonstrate the advantages of LLMs augmented with expert tools in specialized domains.\\n\\n5. **Challenges and Limitations**: The documents highlight challenges such as hallucination in model outputs and the finite context length of LLMs, which affects their ability to incorporate historical information and perform self-reflection. Techniques like Chain of Hindsight and Algorithm Distillation are discussed to enhance model performance through iterative learning.\\n\\n6. **Structured Software Development**: A systematic approach to creating Python software projects is emphasized, focusing on defining core components, managing dependencies, and adhering to best practices for documentation.\\n\\nOverall, the integration of structured planning, memory systems, and advanced tool use aims to enhance the capabilities of LLM-powered autonomous agents while addressing the challenges and limitations these technologies face in real-world applications.'}}\nIn the corresponding\nLangSmith trace\nwe can see the individual LLM calls, grouped under their respective nodes.\nGo deeper\nâ€‹\nCustomization\nAs shown above, you can customize the LLMs and prompts for map and reduce stages.\nReal-world use-case\nSee\nthis blog post\ncase-study on analyzing user interactions (questions about LangChain documentation)!\nThe blog post and associated\nrepo\nalso introduce clustering as a means of summarization.\nThis opens up another path beyond the\nstuff\nor\nmap-reduce\napproaches that is worth considering.\nNext steps\nâ€‹\nWe encourage you to check out the\nhow-to guides\nfor more detail on:\nOther summarization strategies, such as\niterative refinement\nBuilt-in\ndocument loaders\nand\ntext-splitters\nIntegrating various combine-document chains into a\nRAG application\nIncorporating retrieval into a\nchatbot\nand other concepts.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/",
    "How-to guides\nOn this page\nHow-to guides\nHere youâ€™ll find answers to \"How do Iâ€¦.?\" types of questions.\nThese guides are\ngoal-oriented\nand\nconcrete\n; they're meant to help you complete a specific task.\nFor conceptual explanations see the\nConceptual guide\n.\nFor end-to-end walkthroughs see\nTutorials\n.\nFor comprehensive descriptions of every class and function see the\nAPI Reference\n.\nInstallation\nâ€‹\nHow to: install LangChain packages\nHow to: use LangChain with different Pydantic versions\nKey features\nâ€‹\nThis highlights functionality that is core to using LangChain.\nHow to: return structured data from a model\nHow to: use a model to call tools\nHow to: stream runnables\nHow to: debug your LLM apps\nComponents\nâ€‹\nThese are the core building blocks you can use when building applications.\nChat models\nâ€‹\nChat Models\nare newer forms of language models that take messages in and output a message.\nSee\nsupported integrations\nfor details on getting started with chat models from a specific provider.\nHow to: initialize any model in one line\nHow to: work with local models\nHow to: do function/tool calling\nHow to: get models to return structured output\nHow to: cache model responses\nHow to: get log probabilities\nHow to: create a custom chat model class\nHow to: stream a response back\nHow to: track token usage\nHow to: track response metadata across providers\nHow to: use chat model to call tools\nHow to: stream tool calls\nHow to: handle rate limits\nHow to: few-shot prompt tool behavior\nHow to: bind model-specific formatted tools\nHow to: force a specific tool call\nHow to: pass multimodal data directly to models\nMessages\nâ€‹\nMessages\nare the input and output of chat models. They have some\ncontent\nand a\nrole\n, which describes the source of the message.\nHow to: trim messages\nHow to: filter messages\nHow to: merge consecutive messages of the same type\nPrompt templates\nâ€‹\nPrompt Templates\nare responsible for formatting user input into a format that can be passed to a language model.\nHow to: use few-shot examples\nHow to: use few-shot examples in chat models\nHow to: partially format prompt templates\nHow to: compose prompts together\nHow to: use multimodal prompts\nExample selectors\nâ€‹\nExample Selectors\nare responsible for selecting the correct few shot examples to pass to the prompt.\nHow to: use example selectors\nHow to: select examples by length\nHow to: select examples by semantic similarity\nHow to: select examples by semantic ngram overlap\nHow to: select examples by maximal marginal relevance\nHow to: select examples from LangSmith few-shot datasets\nLLMs\nâ€‹\nWhat LangChain calls\nLLMs\nare older forms of language models that take a string in and output a string.\nHow to: cache model responses\nHow to: create a custom LLM class\nHow to: stream a response back\nHow to: track token usage\nHow to: work with local models\nOutput parsers\nâ€‹\nOutput Parsers\nare responsible for taking the output of an LLM and parsing into more structured format.\nHow to: parse text from message objects\nHow to: use output parsers to parse an LLM response into structured format\nHow to: parse JSON output\nHow to: parse XML output\nHow to: parse YAML output\nHow to: retry when output parsing errors occur\nHow to: try to fix errors in output parsing\nHow to: write a custom output parser class\nDocument loaders\nâ€‹\nDocument Loaders\nare responsible for loading documents from a variety of sources.\nHow to: load PDF files\nHow to: load web pages\nHow to: load CSV data\nHow to: load data from a directory\nHow to: load HTML data\nHow to: load JSON data\nHow to: load Markdown data\nHow to: load Microsoft Office data\nHow to: write a custom document loader\nText splitters\nâ€‹\nText Splitters\ntake a document and split into chunks that can be used for retrieval.\nHow to: recursively split text\nHow to: split HTML\nHow to: split by character\nHow to: split code\nHow to: split Markdown by headers\nHow to: recursively split JSON\nHow to: split text into semantic chunks\nHow to: split by tokens\nEmbedding models\nâ€‹\nEmbedding Models\ntake a piece of text and create a numerical representation of it.\nSee\nsupported integrations\nfor details on getting started with embedding models from a specific provider.\nHow to: embed text data\nHow to: cache embedding results\nHow to: create a custom embeddings class\nVector stores\nâ€‹\nVector stores\nare databases that can efficiently store and retrieve embeddings.\nSee\nsupported integrations\nfor details on getting started with vector stores from a specific provider.\nHow to: use a vector store to retrieve data\nRetrievers\nâ€‹\nRetrievers\nare responsible for taking a query and returning relevant documents.\nHow to: use a vector store to retrieve data\nHow to: generate multiple queries to retrieve data for\nHow to: use contextual compression to compress the data retrieved\nHow to: write a custom retriever class\nHow to: add similarity scores to retriever results\nHow to: combine the results from multiple retrievers\nHow to: reorder retrieved results to mitigate the \"lost in the middle\" effect\nHow to: generate multiple embeddings per document\nHow to: retrieve the whole document for a chunk\nHow to: generate metadata filters\nHow to: create a time-weighted retriever\nHow to: use hybrid vector and keyword retrieval\nIndexing\nâ€‹\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\nHow to: reindex data to keep your vectorstore in sync with the underlying data source\nTools\nâ€‹\nLangChain\nTools\ncontain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer\nhere\nfor a list of pre-built tools.\nHow to: create tools\nHow to: use built-in tools and toolkits\nHow to: use chat models to call tools\nHow to: pass tool outputs to chat models\nHow to: pass runtime values to tools\nHow to: add a human-in-the-loop for tools\nHow to: handle tool errors\nHow to: force models to call a tool\nHow to: disable parallel tool calling\nHow to: access the\nRunnableConfig\nfrom a tool\nHow to: stream events from a tool\nHow to: return artifacts from a tool\nHow to: convert Runnables to tools\nHow to: add ad-hoc tool calling capability to models\nHow to: pass in runtime secrets\nMultimodal\nâ€‹\nHow to: pass multimodal data directly to models\nHow to: use multimodal prompts\nAgents\nâ€‹\nnote\nFor in depth how-to guides for agents, please check out\nLangGraph\ndocumentation.\nHow to: use legacy LangChain Agents (AgentExecutor)\nHow to: migrate from legacy LangChain agents to LangGraph\nCallbacks\nâ€‹\nCallbacks\nallow you to hook into the various stages of your LLM application's execution.\nHow to: pass in callbacks at runtime\nHow to: attach callbacks to a module\nHow to: pass callbacks into a module constructor\nHow to: create custom callback handlers\nHow to: use callbacks in async environments\nHow to: dispatch custom callback events\nCustom\nâ€‹\nAll of LangChain components can easily be extended to support your own versions.\nHow to: create a custom chat model class\nHow to: create a custom LLM class\nHow to: create a custom embeddings class\nHow to: write a custom retriever class\nHow to: write a custom document loader\nHow to: write a custom output parser class\nHow to: create custom callback handlers\nHow to: define a custom tool\nHow to: dispatch custom callback events\nSerialization\nâ€‹\nHow to: save and load LangChain objects\nUse cases\nâ€‹\nThese guides cover use-case specific details.\nQ&A with RAG\nâ€‹\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\nFor a high-level tutorial on RAG, check out\nthis guide\n.\nHow to: add chat history\nHow to: stream\nHow to: return sources\nHow to: return citations\nHow to: do per-user retrieval\nExtraction\nâ€‹\nExtraction is when you use LLMs to extract structured information from unstructured text.\nFor a high level tutorial on extraction, check out\nthis guide\n.\nHow to: use reference examples\nHow to: handle long text\nHow to: do extraction without using function calling\nChatbots\nâ€‹\nChatbots involve using an LLM to have a conversation.\nFor a high-level tutorial on building chatbots, check out\nthis guide\n.\nHow to: manage memory\nHow to: do retrieval\nHow to: use tools\nHow to: manage large chat history\nQuery analysis\nâ€‹\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\nFor a high-level tutorial on query analysis, check out\nthis guide\n.\nHow to: add examples to the prompt\nHow to: handle cases where no queries are generated\nHow to: handle multiple queries\nHow to: handle multiple retrievers\nHow to: construct filters\nHow to: deal with high cardinality categorical variables\nQ&A over SQL + CSV\nâ€‹\nYou can use LLMs to do question answering over tabular data.\nFor a high-level tutorial, check out\nthis guide\n.\nHow to: use prompting to improve results\nHow to: do query validation\nHow to: deal with large databases\nHow to: deal with CSV files\nQ&A over graph databases\nâ€‹\nYou can use an LLM to do question answering over graph databases.\nFor a high-level tutorial, check out\nthis guide\n.\nHow to: add a semantic layer over a database\nHow to: construct knowledge graphs\nSummarization\nâ€‹\nLLMs can summarize and otherwise distill desired information from text, including\nlarge volumes of text. For a high-level tutorial, check out\nthis guide\n.\nHow to: summarize text in a single LLM call\nHow to: summarize text through parallelization\nHow to: summarize text through iterative refinement\nLangChain Expression Language (LCEL)\nâ€‹\nShould I use LCEL?\nLCEL is an orchestration solution. See our\nconcepts page\nfor recommendations on when to\nuse LCEL.\nLangChain Expression Language\nis a way to create arbitrary custom chains. It is built on the\nRunnable\nprotocol.\nLCEL cheatsheet\n: For a quick overview of how to use the main LCEL primitives.\nMigration guide\n: For migrating legacy chain abstractions to LCEL.\nHow to: chain runnables\nHow to: stream runnables\nHow to: invoke runnables in parallel\nHow to: add default invocation args to runnables\nHow to: turn any function into a runnable\nHow to: pass through inputs from one chain step to the next\nHow to: configure runnable behavior at runtime\nHow to: add message history (memory) to a chain\nHow to: route between sub-chains\nHow to: create a dynamic (self-constructing) chain\nHow to: inspect runnables\nHow to: add fallbacks to a runnable\nHow to: pass runtime secrets to a runnable\nLangGraph\nâ€‹\nLangGraph is an extension of LangChain aimed at\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\nLangGraph documentation is currently hosted on a separate site.\nYou can find the\nLangGraph guides here\n.\nLangSmith\nâ€‹\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\nLangSmith documentation is hosted on a separate site.\nYou can peruse\nLangSmith how-to guides here\n, but we'll highlight a few sections that are particularly\nrelevant to LangChain below:\nEvaluation\nâ€‹\nEvaluating performance is a vital part of building LLM-powered applications.\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\nTo learn more, check out the\nLangSmith evaluation how-to guides\n.\nTracing\nâ€‹\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\nHow to: trace with LangChain\nHow to: add metadata and tags to traces\nYou can see general tracing-related how-tos\nin this section of the LangSmith docs\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tools_chain/",
    "How-to guides\nHow to use tools in a chain\nOn this page\nHow to use tools in a chain\nIn this guide, we will go over the basic ways to create Chains and Agents that call\nTools\n. Tools can be just about anything â€”Â APIs, functions, databases, etc. Tools allow us to extend the capabilities of a model beyond just outputting text/messages. The key to using models with tools is correctly prompting a model and parsing its response so that it chooses the right tools and provides the right inputs for them.\nSetup\nâ€‹\nWe'll need to install the following packages for this guide:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\nIf you'd like to trace your runs in\nLangSmith\nuncomment and set the following environment variables:\nimport\ngetpass\nimport\nos\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nCreate a tool\nâ€‹\nFirst, we need to create a tool to call. For this example, we will create a custom tool from a function. For more information on creating custom tools, please see\nthis guide\n.\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nmultiply\n(\nfirst_int\n:\nint\n,\nsecond_int\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two integers together.\"\"\"\nreturn\nfirst_int\n*\nsecond_int\nAPI Reference:\ntool\nprint\n(\nmultiply\n.\nname\n)\nprint\n(\nmultiply\n.\ndescription\n)\nprint\n(\nmultiply\n.\nargs\n)\nmultiply\nMultiply two integers together.\n{'first_int': {'title': 'First Int', 'type': 'integer'}, 'second_int': {'title': 'Second Int', 'type': 'integer'}}\nmultiply\n.\ninvoke\n(\n{\n\"first_int\"\n:\n4\n,\n\"second_int\"\n:\n5\n}\n)\n20\nChains\nâ€‹\nIf we know that we only need to use a tool a fixed number of times, we can create a chain for doing so. Let's create a simple chain that just multiplies user-specified numbers.\nTool/function calling\nâ€‹\nOne of the most reliable ways to use tools with LLMs is with\ntool calling\nAPIs (also sometimes called function calling). This only works with models that explicitly support tool calling. You can see which models support tool calling\nhere\n, and learn more about how to use tool calling in\nthis guide\n.\nFirst we'll define our model and tools. We'll start with just a single tool,\nmultiply\n.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nWe'll use\nbind_tools\nto pass the definition of our tool in as part of each call to the model, so that the model can invoke the tool when appropriate:\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nmultiply\n]\n)\nWhen the model invokes the tool, this will show up in the\nAIMessage.tool_calls\nattribute of the output:\nmsg\n=\nllm_with_tools\n.\ninvoke\n(\n\"whats 5 times forty two\"\n)\nmsg\n.\ntool_calls\n[{'name': 'multiply',\n'args': {'first_int': 5, 'second_int': 42},\n'id': 'call_8QIg4QVFVAEeC1orWAgB2036',\n'type': 'tool_call'}]\nCheck out the\nLangSmith trace here\n.\nInvoking the tool\nâ€‹\nGreat! We're able to generate tool invocations. But what if we want to actually call the tool? To do so we'll need to pass the generated tool args to our tool. As a simple example we'll just extract the arguments of the first tool_call:\nfrom\noperator\nimport\nitemgetter\nchain\n=\nllm_with_tools\n|\n(\nlambda\nx\n:\nx\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\n|\nmultiply\nchain\n.\ninvoke\n(\n\"What's four times 23\"\n)\n92\nCheck out the\nLangSmith trace here\n.\nAgents\nâ€‹\nChains are great when we know the specific sequence of tool usage needed for any user input. But for certain use cases, how many times we use tools depends on the input. In these cases, we want to let the model itself decide how many times to use tools and in what order.\nAgents\nlet us do just this.\nWe'll demonstrate a simple example using a LangGraph agent. See\nthis tutorial\nfor more detail.\n!pip install\n-\nqU langgraph\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nAPI Reference:\ncreate_react_agent\nAgents are also great because they make it easy to use multiple tools.\n@tool\ndef\nadd\n(\nfirst_int\n:\nint\n,\nsecond_int\n:\nint\n)\n-\n>\nint\n:\n\"Add two integers.\"\nreturn\nfirst_int\n+\nsecond_int\n@tool\ndef\nexponentiate\n(\nbase\n:\nint\n,\nexponent\n:\nint\n)\n-\n>\nint\n:\n\"Exponentiate the base to the exponent power.\"\nreturn\nbase\n**\nexponent\ntools\n=\n[\nmultiply\n,\nadd\n,\nexponentiate\n]\n# Construct the tool calling agent\nagent\n=\ncreate_react_agent\n(\nllm\n,\ntools\n)\nWith an agent, we can ask questions that require arbitrarily-many uses of our tools:\n# Use the agent\nquery\n=\n(\n\"Take 3 to the fifth power and multiply that by the sum of twelve and \"\n\"three, then square the whole result.\"\n)\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nquery\n}\nfor\nstep\nin\nagent\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nstream_mode\n=\n\"values\"\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nTake 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nexponentiate (call_EHGS8gnEVNCJQ9rVOk11KCQH)\nCall ID: call_EHGS8gnEVNCJQ9rVOk11KCQH\nArgs:\nbase: 3\nexponent: 5\nadd (call_s2cxOrXEKqI6z7LWbMUG6s8c)\nCall ID: call_s2cxOrXEKqI6z7LWbMUG6s8c\nArgs:\nfirst_int: 12\nsecond_int: 3\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: add\n15\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nmultiply (call_25v5JEfDWuKNgmVoGBan0d7J)\nCall ID: call_25v5JEfDWuKNgmVoGBan0d7J\nArgs:\nfirst_int: 243\nsecond_int: 15\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: multiply\n3645\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nexponentiate (call_x1yKEeBPrFYmCp2z5Kn8705r)\nCall ID: call_x1yKEeBPrFYmCp2z5Kn8705r\nArgs:\nbase: 3645\nexponent: 2\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: exponentiate\n13286025\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe final result of taking 3 to the fifth power, multiplying it by the sum of twelve and three, and then squaring the whole result is **13,286,025**.\nCheck out the\nLangSmith trace here\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/vectorstore_retriever/",
    "How-to guides\nHow to use a vectorstore as a retriever\nOn this page\nHow to use a vectorstore as a retriever\nA vector store retriever is a\nretriever\nthat uses a\nvector store\nto retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever\ninterface\n.\nIt uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.\nIn this guide we will cover:\nHow to instantiate a retriever from a vectorstore;\nHow to specify the search type for the retriever;\nHow to specify additional search parameters, such as threshold scores and top-k.\nCreating a retriever from a vectorstore\nâ€‹\nYou can build a retriever from a vectorstore using its\n.as_retriever\nmethod. Let's walk through an example.\nFirst we instantiate a vectorstore. We will use an in-memory\nFAISS\nvectorstore:\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nTextLoader\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\nloader\n=\nTextLoader\n(\n\"state_of_the_union.txt\"\n)\ndocuments\n=\nloader\n.\nload\n(\n)\ntext_splitter\n=\nCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n0\n)\ntexts\n=\ntext_splitter\n.\nsplit_documents\n(\ndocuments\n)\nembeddings\n=\nOpenAIEmbeddings\n(\n)\nvectorstore\n=\nFAISS\n.\nfrom_documents\n(\ntexts\n,\nembeddings\n)\nWe can then instantiate a retriever:\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\nThis creates a retriever (specifically a\nVectorStoreRetriever\n), which we can use in the usual way:\ndocs\n=\nretriever\n.\ninvoke\n(\n\"what did the president say about ketanji brown jackson?\"\n)\nMaximum marginal relevance retrieval\nâ€‹\nBy default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type.\nThis effectively specifies what method on the underlying vectorstore is used (e.g.,\nsimilarity_search\n,\nmax_marginal_relevance_search\n, etc.).\nretriever\n=\nvectorstore\n.\nas_retriever\n(\nsearch_type\n=\n\"mmr\"\n)\ndocs\n=\nretriever\n.\ninvoke\n(\n\"what did the president say about ketanji brown jackson?\"\n)\nPassing search parameters\nâ€‹\nWe can pass parameters to the underlying vectorstore's search methods using\nsearch_kwargs\n.\nSimilarity score threshold retrieval\nâ€‹\nFor example, we can set a similarity score threshold and only return documents with a score above that threshold.\nretriever\n=\nvectorstore\n.\nas_retriever\n(\nsearch_type\n=\n\"similarity_score_threshold\"\n,\nsearch_kwargs\n=\n{\n\"score_threshold\"\n:\n0.5\n}\n)\ndocs\n=\nretriever\n.\ninvoke\n(\n\"what did the president say about ketanji brown jackson?\"\n)\nSpecifying top k\nâ€‹\nWe can also limit the number of documents\nk\nreturned by the retriever.\nretriever\n=\nvectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n}\n)\ndocs\n=\nretriever\n.\ninvoke\n(\n\"what did the president say about ketanji brown jackson?\"\n)\nlen\n(\ndocs\n)\n1\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/chatbots_memory/",
    "How-to guides\nHow to add memory to chatbots\nOn this page\nHow to add memory to chatbots\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\nSimply stuffing previous messages into a chat model prompt.\nThe above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\nMore complex modifications like synthesizing summaries for long running conversations.\nWe'll go into more detail on a few techniques below!\nnote\nThis how-to guide previously built a chatbot using\nRunnableWithMessageHistory\n. You can access this version of the guide in the\nv0.2 docs\n.\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of\nLangGraph persistence\nto incorporate\nmemory\ninto new LangChain applications.\nIf your code is already relying on\nRunnableWithMessageHistory\nor\nBaseChatMessageHistory\n, you do\nnot\nneed to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses\nRunnableWithMessageHistory\nwill continue to work as expected.\nPlease see\nHow to migrate to LangGraph Memory\nfor more details.\nSetup\nâ€‹\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named\nOPENAI_API_KEY\n:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain langchain\n-\nopenai langgraph\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"OpenAI API Key:\"\n)\nOpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·\nLet's also set up a chat model that we'll use for the below examples.\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nMessage passing\nâ€‹\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nHumanMessage\n,\nSystemMessage\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nMessagesPlaceholder\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\nSystemMessage\n(\ncontent\n=\n\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n)\n,\nMessagesPlaceholder\n(\nvariable_name\n=\n\"messages\"\n)\n,\n]\n)\nchain\n=\nprompt\n|\nmodel\nai_msg\n=\nchain\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"Translate from English to French: I love programming.\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"J'adore la programmation.\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"What did you just say?\"\n)\n,\n]\n,\n}\n)\nprint\n(\nai_msg\n.\ncontent\n)\nAPI Reference:\nAIMessage\n|\nHumanMessage\n|\nSystemMessage\n|\nChatPromptTemplate\n|\nMessagesPlaceholder\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\nWe can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.\nAutomatic history management\nâ€‹\nThe previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's\npersistence\n. You can\nenable persistence\nin LangGraph applications by providing a\ncheckpointer\nwhen compiling the graph.\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nMessagesState\n,\nStateGraph\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define the function that calls the model\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\nsystem_prompt\n=\n(\n\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n)\nmessages\n=\n[\nSystemMessage\n(\ncontent\n=\nsystem_prompt\n)\n]\n+\nstate\n[\n\"messages\"\n]\nresponse\n=\nmodel\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"messages\"\n:\nresponse\n}\n# Define the node and edge\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\n# Add simple in-memory checkpointer\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nAPI Reference:\nMemorySaver\n|\nStateGraph\nWe'll pass the latest input to the conversation here and let LangGraph keep track of the conversation history using the checkpointer:\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"Translate to French: I love programming.\"\n)\n]\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}\n}\n,\n)\n{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\nAIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"What did I just ask you?\"\n)\n]\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}\n}\n,\n)\n{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\nAIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}),\nHumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'),\nAIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]}\nModifying chat history\nâ€‹\nModifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples:\nTrimming messages\nâ€‹\nLLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the\napp\nwe declared above:\ndemo_ephemeral_chat_history\n=\n[\nHumanMessage\n(\ncontent\n=\n\"Hey there! I'm Nemo.\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Hello!\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"How are you today?\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Fine thanks!\"\n)\n,\n]\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ndemo_ephemeral_chat_history\n+\n[\nHumanMessage\n(\ncontent\n=\n\"What's my name?\"\n)\n]\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"2\"\n}\n}\n,\n)\n{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\nAIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\nHumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\nAIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\nHumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'),\nAIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]}\nWe can see the app remembers the preloaded name.\nBut let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in\ntrim_messages\nutil to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \"token\" and keep only the last two messages:\nfrom\nlangchain_core\n.\nmessages\nimport\ntrim_messages\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nMessagesState\n,\nStateGraph\n# Define trimmer\n# count each message as 1 \"token\" (token_counter=len) and keep only the last two messages\ntrimmer\n=\ntrim_messages\n(\nstrategy\n=\n\"last\"\n,\nmax_tokens\n=\n2\n,\ntoken_counter\n=\nlen\n)\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define the function that calls the model\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\ntrimmed_messages\n=\ntrimmer\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\nsystem_prompt\n=\n(\n\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n)\nmessages\n=\n[\nSystemMessage\n(\ncontent\n=\nsystem_prompt\n)\n]\n+\ntrimmed_messages\nresponse\n=\nmodel\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"messages\"\n:\nresponse\n}\n# Define the node and edge\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\n# Add simple in-memory checkpointer\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nAPI Reference:\ntrim_messages\n|\nMemorySaver\n|\nStateGraph\nLet's call this new app and check the response\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ndemo_ephemeral_chat_history\n+\n[\nHumanMessage\n(\ncontent\n=\n\"What is my name?\"\n)\n]\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"3\"\n}\n}\n,\n)\n{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\nAIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\nHumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\nAIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\nHumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'),\nAIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]}\nWe can see that\ntrim_messages\nwas called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it.\nCheck out our\nhow to guide on trimming messages\nfor more.\nSummary memory\nâ€‹\nWe can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history:\ndemo_ephemeral_chat_history\n=\n[\nHumanMessage\n(\ncontent\n=\n\"Hey there! I'm Nemo.\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Hello!\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"How are you today?\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Fine thanks!\"\n)\n,\n]\nAnd now, let's update the model-calling function to distill previous interactions into a summary:\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\n,\nRemoveMessage\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nMessagesState\n,\nStateGraph\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define the function that calls the model\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\nsystem_prompt\n=\n(\n\"You are a helpful assistant. \"\n\"Answer all questions to the best of your ability. \"\n\"The provided chat history includes a summary of the earlier conversation.\"\n)\nsystem_message\n=\nSystemMessage\n(\ncontent\n=\nsystem_prompt\n)\nmessage_history\n=\nstate\n[\n\"messages\"\n]\n[\n:\n-\n1\n]\n# exclude the most recent user input\n# Summarize the messages if the chat history reaches a certain size\nif\nlen\n(\nmessage_history\n)\n>=\n4\n:\nlast_human_message\n=\nstate\n[\n\"messages\"\n]\n[\n-\n1\n]\n# Invoke the model to generate conversation summary\nsummary_prompt\n=\n(\n\"Distill the above chat messages into a single summary message. \"\n\"Include as many specific details as you can.\"\n)\nsummary_message\n=\nmodel\n.\ninvoke\n(\nmessage_history\n+\n[\nHumanMessage\n(\ncontent\n=\nsummary_prompt\n)\n]\n)\n# Delete messages that we no longer want to show up\ndelete_messages\n=\n[\nRemoveMessage\n(\nid\n=\nm\n.\nid\n)\nfor\nm\nin\nstate\n[\n\"messages\"\n]\n]\n# Re-add user message\nhuman_message\n=\nHumanMessage\n(\ncontent\n=\nlast_human_message\n.\ncontent\n)\n# Call the model with summary & response\nresponse\n=\nmodel\n.\ninvoke\n(\n[\nsystem_message\n,\nsummary_message\n,\nhuman_message\n]\n)\nmessage_updates\n=\n[\nsummary_message\n,\nhuman_message\n,\nresponse\n]\n+\ndelete_messages\nelse\n:\nmessage_updates\n=\nmodel\n.\ninvoke\n(\n[\nsystem_message\n]\n+\nstate\n[\n\"messages\"\n]\n)\nreturn\n{\n\"messages\"\n:\nmessage_updates\n}\n# Define the node and edge\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\n# Add simple in-memory checkpointer\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nAPI Reference:\nHumanMessage\n|\nRemoveMessage\n|\nMemorySaver\n|\nStateGraph\nLet's see if it remembers the name we gave it:\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ndemo_ephemeral_chat_history\n+\n[\nHumanMessage\n(\n\"What did I say my name was?\"\n)\n]\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"4\"\n}\n}\n,\n)\n{'messages': [AIMessage(content=\"Nemo greeted me, and I responded positively, indicating that I'm doing well.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}),\nHumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'),\nAIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]}\nNote that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/example_selectors/",
    "How-to guides\nHow to use example selectors\nOn this page\nHow to use example selectors\nIf you have a large number of examples, you may need to select which ones to include in the prompt. The\nExample Selector\nis the class responsible for doing so.\nThe base interface is defined as below:\nclass\nBaseExampleSelector\n(\nABC\n)\n:\n\"\"\"Interface for selecting examples to include in prompts.\"\"\"\n@abstractmethod\ndef\nselect_examples\n(\nself\n,\ninput_variables\n:\nDict\n[\nstr\n,\nstr\n]\n)\n-\n>\nList\n[\ndict\n]\n:\n\"\"\"Select which examples to use based on the inputs.\"\"\"\n@abstractmethod\ndef\nadd_example\n(\nself\n,\nexample\n:\nDict\n[\nstr\n,\nstr\n]\n)\n-\n>\nAny\n:\n\"\"\"Add new example to store.\"\"\"\nThe only method it needs to define is a\nselect_examples\nmethod. This takes in the input variables and then returns a list of examples. It is up to each specific implementation as to how those examples are selected.\nLangChain has a few different types of example selectors. For an overview of all these types, see the\nbelow table\n.\nIn this guide, we will walk through creating a custom example selector.\nExamples\nâ€‹\nIn order to use an example selector, we need to create a list of examples. These should generally be example inputs and outputs. For this demo purpose, let's imagine we are selecting examples of how to translate English to Italian.\nexamples\n=\n[\n{\n\"input\"\n:\n\"hi\"\n,\n\"output\"\n:\n\"ciao\"\n}\n,\n{\n\"input\"\n:\n\"bye\"\n,\n\"output\"\n:\n\"arrivederci\"\n}\n,\n{\n\"input\"\n:\n\"soccer\"\n,\n\"output\"\n:\n\"calcio\"\n}\n,\n]\nCustom Example Selector\nâ€‹\nLet's write an example selector that chooses what example to pick based on the length of the word.\nfrom\nlangchain_core\n.\nexample_selectors\n.\nbase\nimport\nBaseExampleSelector\nclass\nCustomExampleSelector\n(\nBaseExampleSelector\n)\n:\ndef\n__init__\n(\nself\n,\nexamples\n)\n:\nself\n.\nexamples\n=\nexamples\ndef\nadd_example\n(\nself\n,\nexample\n)\n:\nself\n.\nexamples\n.\nappend\n(\nexample\n)\ndef\nselect_examples\n(\nself\n,\ninput_variables\n)\n:\n# This assumes knowledge that part of the input will be a 'text' key\nnew_word\n=\ninput_variables\n[\n\"input\"\n]\nnew_word_length\n=\nlen\n(\nnew_word\n)\n# Initialize variables to store the best match and its length difference\nbest_match\n=\nNone\nsmallest_diff\n=\nfloat\n(\n\"inf\"\n)\n# Iterate through each example\nfor\nexample\nin\nself\n.\nexamples\n:\n# Calculate the length difference with the first word of the example\ncurrent_diff\n=\nabs\n(\nlen\n(\nexample\n[\n\"input\"\n]\n)\n-\nnew_word_length\n)\n# Update the best match if the current one is closer in length\nif\ncurrent_diff\n<\nsmallest_diff\n:\nsmallest_diff\n=\ncurrent_diff\nbest_match\n=\nexample\nreturn\n[\nbest_match\n]\nAPI Reference:\nBaseExampleSelector\nexample_selector\n=\nCustomExampleSelector\n(\nexamples\n)\nexample_selector\n.\nselect_examples\n(\n{\n\"input\"\n:\n\"okay\"\n}\n)\n[{'input': 'bye', 'output': 'arrivederci'}]\nexample_selector\n.\nadd_example\n(\n{\n\"input\"\n:\n\"hand\"\n,\n\"output\"\n:\n\"mano\"\n}\n)\nexample_selector\n.\nselect_examples\n(\n{\n\"input\"\n:\n\"okay\"\n}\n)\n[{'input': 'hand', 'output': 'mano'}]\nUse in a Prompt\nâ€‹\nWe can now use this example selector in a prompt\nfrom\nlangchain_core\n.\nprompts\n.\nfew_shot\nimport\nFewShotPromptTemplate\nfrom\nlangchain_core\n.\nprompts\n.\nprompt\nimport\nPromptTemplate\nexample_prompt\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Input: {input} -> Output: {output}\"\n)\nAPI Reference:\nFewShotPromptTemplate\n|\nPromptTemplate\nprompt\n=\nFewShotPromptTemplate\n(\nexample_selector\n=\nexample_selector\n,\nexample_prompt\n=\nexample_prompt\n,\nsuffix\n=\n\"Input: {input} -> Output:\"\n,\nprefix\n=\n\"Translate the following words from English to Italian:\"\n,\ninput_variables\n=\n[\n\"input\"\n]\n,\n)\nprint\n(\nprompt\n.\nformat\n(\ninput\n=\n\"word\"\n)\n)\nTranslate the following words from English to Italian:\nInput: hand -> Output: mano\nInput: word -> Output:\nExample Selector Types\nâ€‹\nName\nDescription\nSimilarity\nUses semantic similarity between inputs and examples to decide which examples to choose.\nMMR\nUses Max Marginal Relevance between inputs and examples to decide which examples to choose.\nLength\nSelects examples based on how many can fit within a certain length\nNgram\nUses ngram overlap between inputs and examples to decide which examples to choose.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/graph_semantic/",
    "How-to guides\nHow to add a semantic layer over graph database\nOn this page\nHow to add a semantic layer over graph database\nYou can use database queries to retrieve information from a graph database like Neo4j.\nOne option is to use LLMs to generate Cypher statements.\nWhile that option provides excellent flexibility, the solution could be brittle and not consistently generating precise Cypher statements.\nInstead of generating Cypher statements, we can implement Cypher templates as tools in a semantic layer that an LLM agent can interact with.\nSetup\nâ€‹\nFirst, get required packages and set environment variables:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain langchain\n-\nneo4j langchain\n-\nopenai\nWe default to OpenAI models in this guide, but you can swap them out for the model provider of your choice.\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\n# Uncomment the below to use LangSmith. Not required.\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\nÂ·Â·Â·Â·Â·Â·Â·Â·\nNext, we need to define Neo4j credentials.\nFollow\nthese installation steps\nto set up a Neo4j database.\nos\n.\nenviron\n[\n\"NEO4J_URI\"\n]\n=\n\"bolt://localhost:7687\"\nos\n.\nenviron\n[\n\"NEO4J_USERNAME\"\n]\n=\n\"neo4j\"\nos\n.\nenviron\n[\n\"NEO4J_PASSWORD\"\n]\n=\n\"password\"\nThe below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.\nfrom\nlangchain_neo4j\nimport\nNeo4jGraph\ngraph\n=\nNeo4jGraph\n(\nrefresh_schema\n=\nFalse\n)\n# Import movie information\nmovies_query\n=\n\"\"\"\nLOAD CSV WITH HEADERS FROM\n'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'\nAS row\nMERGE (m:Movie {id:row.movieId})\nSET m.released = date(row.released),\nm.title = row.title,\nm.imdbRating = toFloat(row.imdbRating)\nFOREACH (director in split(row.director, '|') |\nMERGE (p:Person {name:trim(director)})\nMERGE (p)-[:DIRECTED]->(m))\nFOREACH (actor in split(row.actors, '|') |\nMERGE (p:Person {name:trim(actor)})\nMERGE (p)-[:ACTED_IN]->(m))\nFOREACH (genre in split(row.genres, '|') |\nMERGE (g:Genre {name:trim(genre)})\nMERGE (m)-[:IN_GENRE]->(g))\n\"\"\"\ngraph\n.\nquery\n(\nmovies_query\n)\n[]\nCustom tools with Cypher templates\nâ€‹\nA semantic layer consists of various tools exposed to an LLM that it can use to interact with a knowledge graph.\nThey can be of various complexity. You can think of each tool in a semantic layer as a function.\nThe function we will implement is to retrieve information about movies or their cast.\ndescription_query\n=\n\"\"\"\nMATCH (m:Movie|Person)\nWHERE m.title CONTAINS $candidate OR m.name CONTAINS $candidate\nMATCH (m)-[r:ACTED_IN|IN_GENRE]-(t)\nWITH m, type(r) as type, collect(coalesce(t.name, t.title)) as names\nWITH m, type+\": \"+reduce(s=\"\", n IN names | s + n + \", \") as types\nWITH m, collect(types) as contexts\nWITH m, \"type:\" + labels(m)[0] + \"\\ntitle: \"+ coalesce(m.title, m.name)\n+ \"\\nyear: \"+coalesce(m.released,\"\") +\"\\n\" +\nreduce(s=\"\", c in contexts | s + substring(c, 0, size(c)-2) +\"\\n\") as context\nRETURN context LIMIT 1\n\"\"\"\ndef\nget_information\n(\nentity\n:\nstr\n)\n-\n>\nstr\n:\ntry\n:\ndata\n=\ngraph\n.\nquery\n(\ndescription_query\n,\nparams\n=\n{\n\"candidate\"\n:\nentity\n}\n)\nreturn\ndata\n[\n0\n]\n[\n\"context\"\n]\nexcept\nIndexError\n:\nreturn\n\"No information was found\"\nYou can observe that we have defined the Cypher statement used to retrieve information.\nTherefore, we can avoid generating Cypher statements and use the LLM agent to only populate the input parameters.\nTo provide additional information to an LLM agent about when to use the tool and their input parameters, we wrap the function as a tool.\nfrom\ntyping\nimport\nOptional\n,\nType\nfrom\nlangchain_core\n.\ntools\nimport\nBaseTool\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nInformationInput\n(\nBaseModel\n)\n:\nentity\n:\nstr\n=\nField\n(\ndescription\n=\n\"movie or a person mentioned in the question\"\n)\nclass\nInformationTool\n(\nBaseTool\n)\n:\nname\n:\nstr\n=\n\"Information\"\ndescription\n:\nstr\n=\n(\n\"useful for when you need to answer questions about various actors or movies\"\n)\nargs_schema\n:\nType\n[\nBaseModel\n]\n=\nInformationInput\ndef\n_run\n(\nself\n,\nentity\n:\nstr\n,\n)\n-\n>\nstr\n:\n\"\"\"Use the tool.\"\"\"\nreturn\nget_information\n(\nentity\n)\nasync\ndef\n_arun\n(\nself\n,\nentity\n:\nstr\n,\n)\n-\n>\nstr\n:\n\"\"\"Use the tool asynchronously.\"\"\"\nreturn\nget_information\n(\nentity\n)\nAPI Reference:\nBaseTool\nLangGraph Agent\nâ€‹\nWe will implement a straightforward ReAct agent using LangGraph.\nThe agent consists of an LLM and tools step. As we interact with the agent, we will first call the LLM to decide if we should use tools. Then we will run a loop:\nIf the agent said to take an action (i.e. call tool), weâ€™ll run the tools and pass the results back to the agent.\nIf the agent did not ask to run tools, we will finish (respond to the user).\nThe code implementation is as straightforward as it gets. First we bind the tools to the LLM and define the assistant step.\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\n,\nSystemMessage\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph\n.\ngraph\nimport\nMessagesState\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\ntools\n=\n[\nInformationTool\n(\n)\n]\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n)\n# System message\nsys_msg\n=\nSystemMessage\n(\ncontent\n=\n\"You are a helpful assistant tasked with finding and explaining relevant information about movies.\"\n)\n# Node\ndef\nassistant\n(\nstate\n:\nMessagesState\n)\n:\nreturn\n{\n\"messages\"\n:\n[\nllm_with_tools\n.\ninvoke\n(\n[\nsys_msg\n]\n+\nstate\n[\n\"messages\"\n]\n)\n]\n}\nAPI Reference:\nHumanMessage\n|\nSystemMessage\nNext we define the LangGraph flow.\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\nfrom\nlanggraph\n.\nprebuilt\nimport\nToolNode\n,\ntools_condition\n# Graph\nbuilder\n=\nStateGraph\n(\nMessagesState\n)\n# Define nodes: these do the work\nbuilder\n.\nadd_node\n(\n\"assistant\"\n,\nassistant\n)\nbuilder\n.\nadd_node\n(\n\"tools\"\n,\nToolNode\n(\ntools\n)\n)\n# Define edges: these determine how the control flow moves\nbuilder\n.\nadd_edge\n(\nSTART\n,\n\"assistant\"\n)\nbuilder\n.\nadd_conditional_edges\n(\n\"assistant\"\n,\n# If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n# If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\ntools_condition\n,\n)\nbuilder\n.\nadd_edge\n(\n\"tools\"\n,\n\"assistant\"\n)\nreact_graph\n=\nbuilder\n.\ncompile\n(\n)\n# Show\ndisplay\n(\nImage\n(\nreact_graph\n.\nget_graph\n(\nxray\n=\nTrue\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nAPI Reference:\nStateGraph\n|\nToolNode\n|\ntools_condition\nLet's test the workflow now with an example question.\ninput_messages\n=\n[\nHumanMessage\n(\ncontent\n=\n\"Who played in the Casino?\"\n)\n]\nmessages\n=\nreact_graph\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n)\nfor\nm\nin\nmessages\n[\n\"messages\"\n]\n:\nm\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWho played in the Casino?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nInformation (call_j4usgFStGtBM16fuguRaeoGc)\nCall ID: call_j4usgFStGtBM16fuguRaeoGc\nArgs:\nentity: Casino\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: Information\ntype:Movie\ntitle: Casino\nyear: 1995-11-22\nACTED_IN: Robert De Niro, Joe Pesci, Sharon Stone, James Woods\nIN_GENRE: Drama, Crime\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe movie \"Casino,\" released in 1995, features the following actors:\n- Robert De Niro\n- Joe Pesci\n- Sharon Stone\n- James Woods\nThe film is in the Drama and Crime genres.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/parallel/",
    "How-to guides\nHow to invoke runnables in parallel\nOn this page\nHow to invoke runnables in parallel\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Expression Language (LCEL)\nChaining runnables\nThe\nRunnableParallel\nprimitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the\nRunnableParallel\n. The final return value is a dict with the results of each value under its appropriate key.\nFormatting with\nRunnableParallels\nâ€‹\nRunnableParallels\nare useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence. You can use them to split or fork the chain so that multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:\nInput\n/ \\\n/   \\\nBranch1 Branch2\n\\   /\n\\ /\nCombine\nBelow, the input to prompt is expected to be a map with keys\n\"context\"\nand\n\"question\"\n. The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the\n\"question\"\nkey.\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\n,\nOpenAIEmbeddings\nvectorstore\n=\nFAISS\n.\nfrom_texts\n(\n[\n\"harrison worked at kensho\"\n]\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\ntemplate\n=\n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\n\"\"\"\n# The prompt expects input with keys for \"context\" and \"question\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nmodel\n=\nChatOpenAI\n(\n)\nretrieval_chain\n=\n(\n{\n\"context\"\n:\nretriever\n,\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n)\nretrieval_chain\n.\ninvoke\n(\n\"where did harrison work?\"\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\n'Harrison worked at Kensho.'\ntip\nNote that when composing a RunnableParallel with another Runnable we don't even need to wrap our dictionary in the RunnableParallel class â€”Â the type conversion is handled for us. In the context of a chain, these are equivalent:\n{\"context\": retriever, \"question\": RunnablePassthrough()}\nRunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\nRunnableParallel(context=retriever, question=RunnablePassthrough())\nSee the section on\ncoercion for more\n.\nUsing itemgetter as shorthand\nâ€‹\nNote that you can use Python's\nitemgetter\nas shorthand to extract data from the map when combining with\nRunnableParallel\n. You can find more information about itemgetter in the\nPython Documentation\n.\nIn the example below, we use itemgetter to extract specific keys from the map:\nfrom\noperator\nimport\nitemgetter\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\n,\nOpenAIEmbeddings\nvectorstore\n=\nFAISS\n.\nfrom_texts\n(\n[\n\"harrison worked at kensho\"\n]\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\ntemplate\n=\n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\nAnswer in the following language: {language}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nchain\n=\n(\n{\n\"context\"\n:\nitemgetter\n(\n\"question\"\n)\n|\nretriever\n,\n\"question\"\n:\nitemgetter\n(\n\"question\"\n)\n,\n\"language\"\n:\nitemgetter\n(\n\"language\"\n)\n,\n}\n|\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n)\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"where did harrison work\"\n,\n\"language\"\n:\n\"italian\"\n}\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\n'Harrison ha lavorato a Kensho.'\nParallelize steps\nâ€‹\nRunnableParallels make it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableParallel\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\n)\njoke_chain\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"tell me a joke about {topic}\"\n)\n|\nmodel\npoem_chain\n=\n(\nChatPromptTemplate\n.\nfrom_template\n(\n\"write a 2-line poem about {topic}\"\n)\n|\nmodel\n)\nmap_chain\n=\nRunnableParallel\n(\njoke\n=\njoke_chain\n,\npoem\n=\npoem_chain\n)\nmap_chain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bear\"\n}\n)\nAPI Reference:\nChatPromptTemplate\n|\nRunnableParallel\n{'joke': AIMessage(content=\"Why don't bears like fast food? Because they can't catch it!\", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe024170-c251-4b7a-bfd4-64a3737c67f2-0'),\n'poem': AIMessage(content='In the quiet of the forest, the bear roams free\\nMajestic and wild, a sight to see.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 15, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-2707913e-a743-4101-b6ec-840df4568a76-0')}\nParallelism\nâ€‹\nRunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier\njoke_chain\n,\npoem_chain\nand\nmap_chain\nall have about the same runtime, even though\nmap_chain\nexecutes both of the other two.\n%\n%\ntimeit\njoke_chain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bear\"\n}\n)\n610 ms Â± 64 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n%\n%\ntimeit\npoem_chain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bear\"\n}\n)\n599 ms Â± 73.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n%\n%\ntimeit\nmap_chain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bear\"\n}\n)\n643 ms Â± 77.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\nNext steps\nâ€‹\nYou now know some ways to format and parallelize chain steps with\nRunnableParallel\n.\nTo learn more, see the other how-to guides on runnables in this section.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/chat_streaming/",
    "How-to guides\nHow to stream chat model responses\nOn this page\nHow to stream chat model responses\nAll\nchat models\nimplement the\nRunnable interface\n, which comes with a\ndefault\nimplementations of standard runnable methods (i.e.\nainvoke\n,\nbatch\n,\nabatch\n,\nstream\n,\nastream\n,\nastream_events\n).\nThe\ndefault\nstreaming implementation provides an\nIterator\n(or\nAsyncIterator\nfor asynchronous streaming) that yields a single value: the final output from the underlying chat model provider.\ntip\nThe\ndefault\nimplementation does\nnot\nprovide support for token-by-token streaming, but it ensures that the model can be swapped in for any other model as it supports the same standard interface.\nThe ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support.\nSee which\nintegrations support token-by-token streaming here\n.\nSync streaming\nâ€‹\nBelow we use a\n|\nto help visualize the delimiter between tokens.\nfrom\nlangchain_anthropic\n.\nchat_models\nimport\nChatAnthropic\nchat\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\nfor\nchunk\nin\nchat\n.\nstream\n(\n\"Write me a 1 verse song about goldfish on the moon\"\n)\n:\nprint\n(\nchunk\n.\ncontent\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nHere| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|\nFloating| up| in| the| star|ry| night|,|\nFins| a|-|gl|im|mer| in| the| pale| moon|light|.|\nGol|dfish| swimming|,| peaceful| an|d free|,|\nSe|ren|ely| |drif|ting| across| the| lunar| sea|.|\nAsync Streaming\nâ€‹\nfrom\nlangchain_anthropic\n.\nchat_models\nimport\nChatAnthropic\nchat\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\nasync\nfor\nchunk\nin\nchat\n.\nastream\n(\n\"Write me a 1 verse song about goldfish on the moon\"\n)\n:\nprint\n(\nchunk\n.\ncontent\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nHere| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|\nFloating| up| above| the| Earth|,|\nGol|dfish| swim| in| alien| m|irth|.|\nIn| their| bowl| of| lunar| dust|,|\nGl|it|tering| scales| reflect| the| trust|\nOf| swimming| free| in| this| new| worl|d,|\nWhere| their| aqu|atic| dream|'s| unf|ur|le|d.|\nAstream events\nâ€‹\nChat models also support the standard\nastream events\nmethod.\nThis method is useful if you're streaming output from a larger LLM application that contains multiple steps (e.g., an LLM chain composed of a prompt, llm and parser).\nfrom\nlangchain_anthropic\n.\nchat_models\nimport\nChatAnthropic\nchat\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\nidx\n=\n0\nasync\nfor\nevent\nin\nchat\n.\nastream_events\n(\n\"Write me a 1 verse song about goldfish on the moon\"\n)\n:\nidx\n+=\n1\nif\nidx\n>=\n5\n:\n# Truncate the output\nprint\n(\n\"...Truncated\"\n)\nbreak\nprint\n(\nevent\n)\n{'event': 'on_chat_model_start', 'data': {'input': 'Write me a 1 verse song about goldfish on the moon'}, 'name': 'ChatAnthropic', 'tags': [], 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e', usage_metadata={'input_tokens': 21, 'output_tokens': 2, 'total_tokens': 23, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=\"Here's\", additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=' a short one-verse song', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}\n...Truncated\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/binding/",
    "How-to guides\nHow to add default invocation args to a Runnable\nOn this page\nHow to add default invocation args to a Runnable\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Expression Language (LCEL)\nChaining runnables\nTool calling\nSometimes we want to invoke a\nRunnable\nwithin a\nRunnableSequence\nwith constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use the\nRunnable.bind()\nmethod to set these arguments ahead of time.\nBinding stop sequences\nâ€‹\nSuppose we have a simple prompt + model chain:\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\"\n,\n)\n,\n(\n\"human\"\n,\n\"{equation_statement}\"\n)\n,\n]\n)\nmodel\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\nrunnable\n=\n(\n{\n\"equation_statement\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n)\nprint\n(\nrunnable\n.\ninvoke\n(\n\"x raised to the third plus seven equals 12\"\n)\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\nEQUATION: x^3 + 7 = 12\nSOLUTION:\nSubtract 7 from both sides:\nx^3 = 5\nTake the cube root of both sides:\nx = âˆ›5\nand want to call the model with certain\nstop\nwords so that we shorten the output as is useful in certain types of prompting techniques. While we can pass some arguments into the constructor, other runtime args use the\n.bind()\nmethod as follows:\nrunnable\n=\n(\n{\n\"equation_statement\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nmodel\n.\nbind\n(\nstop\n=\n\"SOLUTION\"\n)\n|\nStrOutputParser\n(\n)\n)\nprint\n(\nrunnable\n.\ninvoke\n(\n\"x raised to the third plus seven equals 12\"\n)\n)\nEQUATION: x^3 + 7 = 12\nWhat you can bind to a Runnable will depend on the extra parameters you can pass when invoking it.\nAttaching OpenAI tools\nâ€‹\nAnother common use-case is tool calling. While you should generally use the\n.bind_tools()\nmethod for tool-calling models, you can also bind provider-specific args directly if you want lower level control:\ntools\n=\n[\n{\n\"type\"\n:\n\"function\"\n,\n\"function\"\n:\n{\n\"name\"\n:\n\"get_current_weather\"\n,\n\"description\"\n:\n\"Get the current weather in a given location\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The city and state, e.g. San Francisco, CA\"\n,\n}\n,\n\"unit\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"enum\"\n:\n[\n\"celsius\"\n,\n\"fahrenheit\"\n]\n}\n,\n}\n,\n\"required\"\n:\n[\n\"location\"\n]\n,\n}\n,\n}\n,\n}\n]\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\n.\nbind\n(\ntools\n=\ntools\n)\nmodel\n.\ninvoke\n(\n\"What's the weather in SF, NYC and LA?\"\n)\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])\nNext steps\nâ€‹\nYou now know how to bind runtime arguments to a Runnable.\nTo learn more, see the other how-to guides on runnables in this section, including:\nUsing configurable fields and alternatives\nto change parameters of a step in a chain, or even swap out entire steps, at runtime\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/chatbots_retrieval/",
    "How-to guides\nHow to add retrieval to chatbots\nOn this page\nHow to add retrieval to chatbots\nRetrieval\nis a common technique chatbots use to augment their responses with data outside a chat model's training data. This section will cover how to implement retrieval in the context of chatbots, but it's worth noting that retrieval is a very subtle and deep topic - we encourage you to explore\nother parts of the documentation\nthat go into greater depth!\nSetup\nâ€‹\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named\nOPENAI_API_KEY\n:\n%\npip install\n-\nqU langchain langchain\n-\nopenai langchain\n-\nchroma beautifulsoup4\n# Set env var OPENAI_API_KEY or load from a .env file:\nimport\ndotenv\ndotenv\n.\nload_dotenv\n(\n)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\nYou should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nTrue\nLet's also set up a chat model that we'll use for the below examples.\nfrom\nlangchain_openai\nimport\nChatOpenAI\nchat\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0.2\n)\nCreating a retriever\nâ€‹\nWe'll use\nthe LangSmith documentation\nas source material and store the content in a\nvector store\nfor later retrieval. Note that this example will gloss over some of the specifics around parsing and storing a data source - you can see more\nin-depth documentation on creating retrieval systems here\n.\nLet's use a document loader to pull text from the docs:\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nloader\n=\nWebBaseLoader\n(\n\"https://docs.smith.langchain.com/overview\"\n)\ndata\n=\nloader\n.\nload\n(\n)\nNext, we split it into smaller chunks that the LLM's context window can handle and store it in a vector database:\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n500\n,\nchunk_overlap\n=\n0\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndata\n)\nThen we embed and store those chunks in a vector database:\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nvectorstore\n=\nChroma\n.\nfrom_documents\n(\ndocuments\n=\nall_splits\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\nAnd finally, let's create a retriever from our initialized vectorstore:\n# k is the number of chunks to retrieve\nretriever\n=\nvectorstore\n.\nas_retriever\n(\nk\n=\n4\n)\ndocs\n=\nretriever\n.\ninvoke\n(\n\"Can LangSmith help test my LLM applications?\"\n)\ndocs\n[Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content=\"does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})]\nWe can see that invoking the retriever above results in some parts of the LangSmith docs that contain information about testing that our chatbot can use as context when answering questions. And now we've got a retriever that can return related data from the LangSmith docs!\nDocument chains\nâ€‹\nNow that we have a retriever that can return LangChain docs, let's create a chain that can use them as context to answer questions. We'll use a\ncreate_stuff_documents_chain\nhelper function to \"stuff\" all of the input documents into the prompt. It will also handle formatting the docs as strings.\nIn addition to a chat model, the function also expects a prompt that has a\ncontext\nvariables, as well as a placeholder for chat history messages named\nmessages\n. We'll create an appropriate prompt and pass it as shown below:\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\nimport\ncreate_stuff_documents_chain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nMessagesPlaceholder\nSYSTEM_TEMPLATE\n=\n\"\"\"\nAnswer the user's questions based on the below context.\nIf the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n<context>\n{context}\n</context>\n\"\"\"\nquestion_answering_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nSYSTEM_TEMPLATE\n,\n)\n,\nMessagesPlaceholder\n(\nvariable_name\n=\n\"messages\"\n)\n,\n]\n)\ndocument_chain\n=\ncreate_stuff_documents_chain\n(\nchat\n,\nquestion_answering_prompt\n)\nAPI Reference:\nChatPromptTemplate\n|\nMessagesPlaceholder\nWe can invoke this\ndocument_chain\nby itself to answer questions. Let's use the docs we retrieved above and the same question,\nhow can langsmith help with testing?\n:\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\ndocument_chain\n.\ninvoke\n(\n{\n\"context\"\n:\ndocs\n,\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"Can LangSmith help test my LLM applications?\"\n)\n]\n,\n}\n)\nAPI Reference:\nHumanMessage\n'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'\nLooks good! For comparison, we can try it with no context docs and compare the result:\ndocument_chain\n.\ninvoke\n(\n{\n\"context\"\n:\n[\n]\n,\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"Can LangSmith help test my LLM applications?\"\n)\n]\n,\n}\n)\n\"I don't know about LangSmith's specific capabilities for testing LLM applications. It's best to reach out to LangSmith directly to inquire about their services and how they can assist with testing your LLM applications.\"\nWe can see that the LLM does not return any results.\nRetrieval chains\nâ€‹\nLet's combine this document chain with the retriever. Here's one way this can look:\nfrom\ntyping\nimport\nDict\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\ndef\nparse_retriever_input\n(\nparams\n:\nDict\n)\n:\nreturn\nparams\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\ncontent\nretrieval_chain\n=\nRunnablePassthrough\n.\nassign\n(\ncontext\n=\nparse_retriever_input\n|\nretriever\n,\n)\n.\nassign\n(\nanswer\n=\ndocument_chain\n,\n)\nAPI Reference:\nRunnablePassthrough\nGiven a list of input messages, we extract the content of the last message in the list and pass that to the retriever to fetch some documents. Then, we pass those documents as context to our document chain to generate a final response.\nInvoking this chain combines both steps outlined above:\nretrieval_chain\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"Can LangSmith help test my LLM applications?\"\n)\n]\n,\n}\n)\n{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n'context': [Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content=\"does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})],\n'answer': 'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}\nLooks good!\nQuery transformation\nâ€‹\nOur retrieval chain is capable of answering questions about LangSmith, but there's a problem - chatbots interact with users conversationally, and therefore have to deal with followup questions.\nThe chain in its current form will struggle with this. Consider a followup question to our original question like\nTell me more!\n. If we invoke our retriever with that query directly, we get documents irrelevant to LLM application testing:\nretriever\n.\ninvoke\n(\n\"Tell me more!\"\n)\n[Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})]\nThis is because the retriever has no innate concept of state, and will only pull documents most similar to the query given. To solve this, we can transform the query into a standalone query without any external references an LLM.\nHere's an example:\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nHumanMessage\nquery_transform_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\nMessagesPlaceholder\n(\nvariable_name\n=\n\"messages\"\n)\n,\n(\n\"user\"\n,\n\"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\"\n,\n)\n,\n]\n)\nquery_transformation_chain\n=\nquery_transform_prompt\n|\nchat\nquery_transformation_chain\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"Can LangSmith help test my LLM applications?\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"Tell me more!\"\n)\n,\n]\n,\n}\n)\nAPI Reference:\nAIMessage\n|\nHumanMessage\nAIMessage(content='\"LangSmith LLM application testing and evaluation\"')\nAwesome! That transformed query would pull up context documents related to LLM application testing.\nLet's add this to our retrieval chain. We can wrap our retriever as follows:\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableBranch\nquery_transforming_retriever_chain\n=\nRunnableBranch\n(\n(\nlambda\nx\n:\nlen\n(\nx\n.\nget\n(\n\"messages\"\n,\n[\n]\n)\n)\n==\n1\n,\n# If only one message, then we just pass that message's content to retriever\n(\nlambda\nx\n:\nx\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\ncontent\n)\n|\nretriever\n,\n)\n,\n# If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\nquery_transform_prompt\n|\nchat\n|\nStrOutputParser\n(\n)\n|\nretriever\n,\n)\n.\nwith_config\n(\nrun_name\n=\n\"chat_retriever_chain\"\n)\nAPI Reference:\nStrOutputParser\n|\nRunnableBranch\nThen, we can use this query transformation chain to make our retrieval chain better able to handle such followup questions:\nSYSTEM_TEMPLATE\n=\n\"\"\"\nAnswer the user's questions based on the below context.\nIf the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n<context>\n{context}\n</context>\n\"\"\"\nquestion_answering_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nSYSTEM_TEMPLATE\n,\n)\n,\nMessagesPlaceholder\n(\nvariable_name\n=\n\"messages\"\n)\n,\n]\n)\ndocument_chain\n=\ncreate_stuff_documents_chain\n(\nchat\n,\nquestion_answering_prompt\n)\nconversational_retrieval_chain\n=\nRunnablePassthrough\n.\nassign\n(\ncontext\n=\nquery_transforming_retriever_chain\n,\n)\n.\nassign\n(\nanswer\n=\ndocument_chain\n,\n)\nAwesome! Let's invoke this new chain with the same inputs as earlier:\nconversational_retrieval_chain\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"Can LangSmith help test my LLM applications?\"\n)\n,\n]\n}\n)\n{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n'context': [Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content=\"does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})],\n'answer': 'Yes, LangSmith can help test and evaluate LLM (Language Model) applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}\nconversational_retrieval_chain\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"Can LangSmith help test my LLM applications?\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"Tell me more!\"\n)\n,\n]\n,\n}\n)\n{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\nAIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'),\nHumanMessage(content='Tell me more!')],\n'context': [Document(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),\nDocument(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})],\n'answer': 'LangSmith simplifies the initial setup for building reliable LLM applications, but it acknowledges that there is still work needed to bring the performance of prompts, chains, and agents up to the level where they are reliable enough to be used in production. It also provides the capability to manually review and annotate runs through annotation queues, allowing you to select runs based on criteria like model type or automatic evaluation scores for human review. This feature is particularly useful for assessing subjective qualities that automatic evaluators struggle with.'}\nYou can check out\nthis LangSmith trace\nto see the internal query transformation step for yourself.\nStreaming\nâ€‹\nBecause this chain is constructed with LCEL, you can use familiar methods like\n.stream()\nwith it:\nstream\n=\nconversational_retrieval_chain\n.\nstream\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"Can LangSmith help test my LLM applications?\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"Tell me more!\"\n)\n,\n]\n,\n}\n)\nfor\nchunk\nin\nstream\n:\nprint\n(\nchunk\n)\n{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'), AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'), HumanMessage(content='Tell me more!')]}\n{'context': [Document(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}), Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}), Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}), Document(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})]}\n{'answer': ''}\n{'answer': 'Lang'}\n{'answer': 'Smith'}\n{'answer': ' simpl'}\n{'answer': 'ifies'}\n{'answer': ' the'}\n{'answer': ' initial'}\n{'answer': ' setup'}\n{'answer': ' for'}\n{'answer': ' building'}\n{'answer': ' reliable'}\n{'answer': ' L'}\n{'answer': 'LM'}\n{'answer': ' applications'}\n{'answer': '.'}\n{'answer': ' It'}\n{'answer': ' provides'}\n{'answer': ' features'}\n{'answer': ' for'}\n{'answer': ' manually'}\n{'answer': ' reviewing'}\n{'answer': ' and'}\n{'answer': ' annot'}\n{'answer': 'ating'}\n{'answer': ' runs'}\n{'answer': ' through'}\n{'answer': ' annotation'}\n{'answer': ' queues'}\n{'answer': ','}\n{'answer': ' allowing'}\n{'answer': ' you'}\n{'answer': ' to'}\n{'answer': ' select'}\n{'answer': ' runs'}\n{'answer': ' based'}\n{'answer': ' on'}\n{'answer': ' criteria'}\n{'answer': ' like'}\n{'answer': ' model'}\n{'answer': ' type'}\n{'answer': ' or'}\n{'answer': ' automatic'}\n{'answer': ' evaluation'}\n{'answer': ' scores'}\n{'answer': ','}\n{'answer': ' and'}\n{'answer': ' queue'}\n{'answer': ' them'}\n{'answer': ' up'}\n{'answer': ' for'}\n{'answer': ' human'}\n{'answer': ' review'}\n{'answer': '.'}\n{'answer': ' As'}\n{'answer': ' a'}\n{'answer': ' reviewer'}\n{'answer': ','}\n{'answer': ' you'}\n{'answer': ' can'}\n{'answer': ' quickly'}\n{'answer': ' step'}\n{'answer': ' through'}\n{'answer': ' the'}\n{'answer': ' runs'}\n{'answer': ','}\n{'answer': ' view'}\n{'answer': ' the'}\n{'answer': ' input'}\n{'answer': ','}\n{'answer': ' output'}\n{'answer': ','}\n{'answer': ' and'}\n{'answer': ' any'}\n{'answer': ' existing'}\n{'answer': ' tags'}\n{'answer': ' before'}\n{'answer': ' adding'}\n{'answer': ' your'}\n{'answer': ' own'}\n{'answer': ' feedback'}\n{'answer': '.'}\n{'answer': ' This'}\n{'answer': ' can'}\n{'answer': ' be'}\n{'answer': ' particularly'}\n{'answer': ' useful'}\n{'answer': ' for'}\n{'answer': ' assessing'}\n{'answer': ' subjective'}\n{'answer': ' qualities'}\n{'answer': ' that'}\n{'answer': ' automatic'}\n{'answer': ' evalu'}\n{'answer': 'ators'}\n{'answer': ' struggle'}\n{'answer': ' with'}\n{'answer': '.'}\n{'answer': ''}\nFurther reading\nâ€‹\nThis guide only scratches the surface of retrieval techniques. For more on different ways of ingesting, preparing, and retrieving the most relevant data, check out the relevant how-to guides\nhere\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/few_shot_examples_chat/",
    "How-to guides\nHow to use few shot examples in chat models\nOn this page\nHow to use few shot examples in chat models\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nPrompt templates\nExample selectors\nChat models\nVectorstores\nThis guide covers how to prompt a chat model with example inputs and outputs. Providing the model with a few such examples is called\nfew-shotting\n, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\nThere does not appear to be solid consensus on how best to do few-shot prompting, and the optimal prompt compilation will likely vary by model. Because of this, we provide few-shot prompt templates like the\nFewShotChatMessagePromptTemplate\nas a flexible starting point, and you can modify or replace them as you see fit.\nThe goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.\nNote:\nThe following code examples are for chat models only, since\nFewShotChatMessagePromptTemplates\nare designed to output formatted\nchat messages\nrather than pure strings. For similar few-shot prompt examples for pure string templates compatible with completion models (LLMs), see the\nfew-shot prompt templates\nguide.\nFixed Examples\nâ€‹\nThe most basic (and common) few-shot prompting technique is to use fixed prompt examples. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production.\nThe basic components of the template are:\nexamples\n: A list of dictionary examples to include in the final prompt.\nexample_prompt\n: converts each example into 1 or more messages through its\nformat_messages\nmethod. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.\nBelow is a simple demonstration. First, define the examples you'd like to include. Let's give the LLM an unfamiliar mathematical operator, denoted by the \"ðŸ¦œ\" emoji:\n%\npip install\n-\nqU langchain langchain\n-\nopenai langchain\n-\nchroma\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nIf we try to ask the model what the result of this expression is, it will fail:\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0.0\n)\nmodel\n.\ninvoke\n(\n\"What is 2 ðŸ¦œ 9?\"\n)\nAIMessage(content='The expression \"2 ðŸ¦œ 9\" is not a standard mathematical operation or equation. It appears to be a combination of the number 2 and the parrot emoji ðŸ¦œ followed by the number 9. It does not have a specific mathematical meaning.', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 17, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aad12dda-5c47-4a1e-9949-6fe94e03242a-0', usage_metadata={'input_tokens': 17, 'output_tokens': 54, 'total_tokens': 71})\nNow let's see what happens if we give the LLM some examples to work with. We'll define some below:\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nFewShotChatMessagePromptTemplate\nexamples\n=\n[\n{\n\"input\"\n:\n\"2 ðŸ¦œ 2\"\n,\n\"output\"\n:\n\"4\"\n}\n,\n{\n\"input\"\n:\n\"2 ðŸ¦œ 3\"\n,\n\"output\"\n:\n\"5\"\n}\n,\n]\nAPI Reference:\nChatPromptTemplate\n|\nFewShotChatMessagePromptTemplate\nNext, assemble them into the few-shot prompt template.\n# This is a prompt template used to format each individual example.\nexample_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n(\n\"ai\"\n,\n\"{output}\"\n)\n,\n]\n)\nfew_shot_prompt\n=\nFewShotChatMessagePromptTemplate\n(\nexample_prompt\n=\nexample_prompt\n,\nexamples\n=\nexamples\n,\n)\nprint\n(\nfew_shot_prompt\n.\ninvoke\n(\n{\n}\n)\n.\nto_messages\n(\n)\n)\n[HumanMessage(content='2 ðŸ¦œ 2'), AIMessage(content='4'), HumanMessage(content='2 ðŸ¦œ 3'), AIMessage(content='5')]\nFinally, we assemble the final prompt as shown below, passing\nfew_shot_prompt\ndirectly into the\nfrom_messages\nfactory method, and use it with a model:\nfinal_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a wondrous wizard of math.\"\n)\n,\nfew_shot_prompt\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\nAnd now let's ask the model the initial question and see how it does:\nfrom\nlangchain_openai\nimport\nChatOpenAI\nchain\n=\nfinal_prompt\n|\nmodel\nchain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What is 2 ðŸ¦œ 9?\"\n}\n)\nAIMessage(content='11', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ec4e051-262f-408e-ad00-3f2ebeb561c3-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})\nAnd we can see that the model has now inferred that the parrot emoji means addition from the given few-shot examples!\nDynamic few-shot prompting\nâ€‹\nSometimes you may want to select only a few examples from your overall set to show based on the input. For this, you can replace the\nexamples\npassed into\nFewShotChatMessagePromptTemplate\nwith an\nexample_selector\n. The other components remain the same as above! Our dynamic few-shot prompt template would look like:\nexample_selector\n: responsible for selecting few-shot examples (and the order in which they are returned) for a given input. These implement the\nBaseExampleSelector\ninterface. A common example is the vectorstore-backed\nSemanticSimilarityExampleSelector\nexample_prompt\n: convert each example into 1 or more messages through its\nformat_messages\nmethod. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.\nThese once again can be composed with other messages and chat templates to assemble your final prompt.\nLet's walk through an example with the\nSemanticSimilarityExampleSelector\n. Since this implementation uses a vectorstore to select examples based on semantic similarity, we will want to first populate the store. Since the basic idea here is that we want to search for and return examples most similar to the text input, we embed the\nvalues\nof our prompt examples rather than considering the keys:\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_core\n.\nexample_selectors\nimport\nSemanticSimilarityExampleSelector\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nexamples\n=\n[\n{\n\"input\"\n:\n\"2 ðŸ¦œ 2\"\n,\n\"output\"\n:\n\"4\"\n}\n,\n{\n\"input\"\n:\n\"2 ðŸ¦œ 3\"\n,\n\"output\"\n:\n\"5\"\n}\n,\n{\n\"input\"\n:\n\"2 ðŸ¦œ 4\"\n,\n\"output\"\n:\n\"6\"\n}\n,\n{\n\"input\"\n:\n\"What did the cow say to the moon?\"\n,\n\"output\"\n:\n\"nothing at all\"\n}\n,\n{\n\"input\"\n:\n\"Write me a poem about the moon\"\n,\n\"output\"\n:\n\"One for the moon, and one for me, who are we to talk about the moon?\"\n,\n}\n,\n]\nto_vectorize\n=\n[\n\" \"\n.\njoin\n(\nexample\n.\nvalues\n(\n)\n)\nfor\nexample\nin\nexamples\n]\nembeddings\n=\nOpenAIEmbeddings\n(\n)\nvectorstore\n=\nChroma\n.\nfrom_texts\n(\nto_vectorize\n,\nembeddings\n,\nmetadatas\n=\nexamples\n)\nAPI Reference:\nSemanticSimilarityExampleSelector\nCreate the\nexample_selector\nâ€‹\nWith a vectorstore created, we can create the\nexample_selector\n. Here we will call it in isolation, and set\nk\non it to only fetch the two example closest to the input.\nexample_selector\n=\nSemanticSimilarityExampleSelector\n(\nvectorstore\n=\nvectorstore\n,\nk\n=\n2\n,\n)\n# The prompt template will load examples by passing the input do the `select_examples` method\nexample_selector\n.\nselect_examples\n(\n{\n\"input\"\n:\n\"horse\"\n}\n)\n[{'input': 'What did the cow say to the moon?', 'output': 'nothing at all'},\n{'input': '2 ðŸ¦œ 4', 'output': '6'}]\nCreate prompt template\nâ€‹\nWe now assemble the prompt template, using the\nexample_selector\ncreated above.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nFewShotChatMessagePromptTemplate\n# Define the few-shot prompt.\nfew_shot_prompt\n=\nFewShotChatMessagePromptTemplate\n(\n# The input variables select the values to pass to the example_selector\ninput_variables\n=\n[\n\"input\"\n]\n,\nexample_selector\n=\nexample_selector\n,\n# Define how each example will be formatted.\n# In this case, each example will become 2 messages:\n# 1 human, and 1 AI\nexample_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n(\n\"ai\"\n,\n\"{output}\"\n)\n]\n)\n,\n)\nprint\n(\nfew_shot_prompt\n.\ninvoke\n(\ninput\n=\n\"What's 3 ðŸ¦œ 3?\"\n)\n.\nto_messages\n(\n)\n)\nAPI Reference:\nChatPromptTemplate\n|\nFewShotChatMessagePromptTemplate\n[HumanMessage(content='2 ðŸ¦œ 3'), AIMessage(content='5'), HumanMessage(content='2 ðŸ¦œ 4'), AIMessage(content='6')]\nAnd we can pass this few-shot chat message prompt template into another chat prompt template:\nfinal_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a wondrous wizard of math.\"\n)\n,\nfew_shot_prompt\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\nprint\n(\nfew_shot_prompt\n.\ninvoke\n(\ninput\n=\n\"What's 3 ðŸ¦œ 3?\"\n)\n)\nmessages=[HumanMessage(content='2 ðŸ¦œ 3'), AIMessage(content='5'), HumanMessage(content='2 ðŸ¦œ 4'), AIMessage(content='6')]\nUse with an chat model\nâ€‹\nFinally, you can connect your model to the few-shot prompt.\nchain\n=\nfinal_prompt\n|\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0.0\n)\nchain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What's 3 ðŸ¦œ 3?\"\n}\n)\nAIMessage(content='6', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d1863e5e-17cd-4e9d-bf7a-b9f118747a65-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})\nNext steps\n\u0000\u0000â€‹\nYou've now learned how to add few-shot examples to your chat prompts.\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on\nfew shotting with text completion models\n, or the other\nexample selector how-to guides\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/function_calling/",
    "How-to guides\nHow to do tool/function calling\nOn this page\nHow to do tool/function calling\ninfo\nWe use the term tool calling interchangeably with function calling. Although\nfunction calling is sometimes meant to refer to invocations of a single function,\nwe treat all models as though they can return multiple tool or function calls in\neach message.\nTool calling allows a model to respond to a given prompt by generating output that\nmatches a user-defined schema. While the name implies that the model is performing\nsome action, this is actually not the case! The model is coming up with the\narguments to a tool, and actually running the tool (or not) is up to the user -\nfor example, if you want to\nextract output matching some schema\nfrom unstructured text, you could give the model an \"extraction\" tool that takes\nparameters matching the desired schema, then treat the generated output as your final\nresult.\nA tool call includes a name, arguments dict, and an optional identifier. The\narguments dict is structured\n{argument_name: argument_value}\n.\nMany LLM providers, including\nAnthropic\n,\nCohere\n,\nGoogle\n,\nMistral\n,\nOpenAI\n, and others,\nsupport variants of a tool calling feature. These features typically allow requests\nto the LLM to include available tools and their schemas, and for responses to include\ncalls to these tools. For instance, given a search engine tool, an LLM might handle a\nquery by first issuing a call to the search engine. The system calling the LLM can\nreceive the tool call, execute it, and return the output to the LLM to inform its\nresponse. LangChain includes a suite of\nbuilt-in tools\nand supports several methods for defining your own\ncustom tools\n.\nTool-calling is extremely useful for building\ntool-using chains and agents\n,\nand for getting structured outputs from models more generally.\nProviders adopt different conventions for formatting tool schemas and tool calls.\nFor instance, Anthropic returns tool calls as parsed structures within a larger content block:\n[\n{\n\"text\"\n:\n\"<thinking>\\nI should use a tool.\\n</thinking>\"\n,\n\"type\"\n:\n\"text\"\n}\n,\n{\n\"id\"\n:\n\"id_value\"\n,\n\"input\"\n:\n{\n\"arg_name\"\n:\n\"arg_value\"\n}\n,\n\"name\"\n:\n\"tool_name\"\n,\n\"type\"\n:\n\"tool_use\"\n}\n]\nwhereas OpenAI separates tool calls into a distinct parameter, with arguments as JSON strings:\n{\n\"tool_calls\"\n:\n[\n{\n\"id\"\n:\n\"id_value\"\n,\n\"function\"\n:\n{\n\"arguments\"\n:\n'{\"arg_name\": \"arg_value\"}'\n,\n\"name\"\n:\n\"tool_name\"\n}\n,\n\"type\"\n:\n\"function\"\n}\n]\n}\nLangChain implements standard interfaces for defining tools, passing them to LLMs,\nand representing tool calls.\nPassing tools to LLMs\nâ€‹\nChat models supporting tool calling features implement a\n.bind_tools\nmethod, which\nreceives a list of LangChain\ntool objects\nand binds them to the chat model in its expected format. Subsequent invocations of the\nchat model will include tool schemas in its calls to the LLM.\nFor example, we can define the schema for custom tools using the\n@tool\ndecorator\non Python functions:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Adds a and b.\"\"\"\nreturn\na\n+\nb\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiplies a and b.\"\"\"\nreturn\na\n*\nb\ntools\n=\n[\nadd\n,\nmultiply\n]\nAPI Reference:\ntool\nOr below, we define the schema using Pydantic:\nfrom\npydantic\nimport\nBaseModel\n,\nField\n# Note that the docstrings here are crucial, as they will be passed along\n# to the model along with the class name.\nclass\nAdd\n(\nBaseModel\n)\n:\n\"\"\"Add two integers together.\"\"\"\na\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"First integer\"\n)\nb\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Second integer\"\n)\nclass\nMultiply\n(\nBaseModel\n)\n:\n\"\"\"Multiply two integers together.\"\"\"\na\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"First integer\"\n)\nb\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Second integer\"\n)\ntools\n=\n[\nAdd\n,\nMultiply\n]\nWe can bind them to chat models as follows:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nWe can use the\nbind_tools()\nmethod to handle converting\nMultiply\nto a \"tool\" and binding it to the model (i.e.,\npassing it in each time the model is invoked).\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n)\nTool calls\nâ€‹\nIf tool calls are included in a LLM response, they are attached to the corresponding\nmessage\nor\nmessage chunk\nas a list of\ntool call\nobjects in the\n.tool_calls\nattribute. A\nToolCall\nis a typed dict that includes a\ntool name, dict of argument values, and (optionally) an identifier. Messages with no\ntool calls default to an empty list for this attribute.\nExample:\nquery\n=\n\"What is 3 * 12? Also, what is 11 + 49?\"\nllm_with_tools\n.\ninvoke\n(\nquery\n)\n.\ntool_calls\n[{'name': 'Multiply',\n'args': {'a': 3, 'b': 12},\n'id': 'call_1Tdp5wUXbYQzpkBoagGXqUTo'},\n{'name': 'Add',\n'args': {'a': 11, 'b': 49},\n'id': 'call_k9v09vYioS3X0Qg35zESuUKI'}]\nThe\n.tool_calls\nattribute should contain valid tool calls. Note that on occasion,\nmodel providers may output malformed tool calls (e.g., arguments that are not\nvalid JSON). When parsing fails in these cases, instances\nof\nInvalidToolCall\nare populated in the\n.invalid_tool_calls\nattribute. An\nInvalidToolCall\ncan have\na name, string arguments, identifier, and error message.\nIf desired,\noutput parsers\ncan further\nprocess the output. For example, we can convert back to the original Pydantic class:\nfrom\nlangchain_core\n.\noutput_parsers\n.\nopenai_tools\nimport\nPydanticToolsParser\nchain\n=\nllm_with_tools\n|\nPydanticToolsParser\n(\ntools\n=\n[\nMultiply\n,\nAdd\n]\n)\nchain\n.\ninvoke\n(\nquery\n)\nAPI Reference:\nPydanticToolsParser\n[Multiply(a=3, b=12), Add(a=11, b=49)]\nStreaming\nâ€‹\nWhen tools are called in a streaming context,\nmessage chunks\nwill be populated with\ntool call chunk\nobjects in a list via the\n.tool_call_chunks\nattribute. A\nToolCallChunk\nincludes\noptional string fields for the tool\nname\n,\nargs\n, and\nid\n, and includes an optional\ninteger field\nindex\nthat can be used to join chunks together. Fields are optional\nbecause portions of a tool call may be streamed across different chunks (e.g., a chunk\nthat includes a substring of the arguments may have null values for the tool name and id).\nBecause message chunks inherit from their parent message class, an\nAIMessageChunk\nwith tool call chunks will also include\n.tool_calls\nand\n.invalid_tool_calls\nfields.\nThese fields are parsed best-effort from the message's tool call chunks.\nNote that not all providers currently support streaming for tool calls.\nExample:\nasync\nfor\nchunk\nin\nllm_with_tools\n.\nastream\n(\nquery\n)\n:\nprint\n(\nchunk\n.\ntool_call_chunks\n)\n[]\n[{'name': 'Multiply', 'args': '', 'id': 'call_d39MsxKM5cmeGJOoYKdGBgzc', 'index': 0}]\n[{'name': None, 'args': '{\"a\"', 'id': None, 'index': 0}]\n[{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}]\n[{'name': None, 'args': '\"b\": 1', 'id': None, 'index': 0}]\n[{'name': None, 'args': '2}', 'id': None, 'index': 0}]\n[{'name': 'Add', 'args': '', 'id': 'call_QJpdxD9AehKbdXzMHxgDMMhs', 'index': 1}]\n[{'name': None, 'args': '{\"a\"', 'id': None, 'index': 1}]\n[{'name': None, 'args': ': 11,', 'id': None, 'index': 1}]\n[{'name': None, 'args': ' \"b\": ', 'id': None, 'index': 1}]\n[{'name': None, 'args': '49}', 'id': None, 'index': 1}]\n[]\nNote that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various\ntool output parsers\nsupport streaming.\nFor example, below we accumulate tool call chunks:\nfirst\n=\nTrue\nasync\nfor\nchunk\nin\nllm_with_tools\n.\nastream\n(\nquery\n)\n:\nif\nfirst\n:\ngathered\n=\nchunk\nfirst\n=\nFalse\nelse\n:\ngathered\n=\ngathered\n+\nchunk\nprint\n(\ngathered\n.\ntool_call_chunks\n)\n[]\n[{'name': 'Multiply', 'args': '', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\"', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, ', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 1', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\"', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11,', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": ', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]\nprint\n(\ntype\n(\ngathered\n.\ntool_call_chunks\n[\n0\n]\n[\n\"args\"\n]\n)\n)\n<class 'str'>\nAnd below we accumulate tool calls to demonstrate partial parsing:\nfirst\n=\nTrue\nasync\nfor\nchunk\nin\nllm_with_tools\n.\nastream\n(\nquery\n)\n:\nif\nfirst\n:\ngathered\n=\nchunk\nfirst\n=\nFalse\nelse\n:\ngathered\n=\ngathered\n+\nchunk\nprint\n(\ngathered\n.\ntool_calls\n)\n[]\n[]\n[{'name': 'Multiply', 'args': {}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]\n[{'name': 'Multiply', 'args': {'a': 3}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]\nprint\n(\ntype\n(\ngathered\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\n)\n<class 'dict'>\nPassing tool outputs to model\nâ€‹\nIf we're using the model-generated tool invocations to actually call tools and want to pass the tool results back to the model, we can do so using\nToolMessage\ns.\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\n,\nToolMessage\nmessages\n=\n[\nHumanMessage\n(\nquery\n)\n]\nai_msg\n=\nllm_with_tools\n.\ninvoke\n(\nmessages\n)\nmessages\n.\nappend\n(\nai_msg\n)\nfor\ntool_call\nin\nai_msg\n.\ntool_calls\n:\nselected_tool\n=\n{\n\"add\"\n:\nadd\n,\n\"multiply\"\n:\nmultiply\n}\n[\ntool_call\n[\n\"name\"\n]\n.\nlower\n(\n)\n]\ntool_output\n=\nselected_tool\n.\ninvoke\n(\ntool_call\n[\n\"args\"\n]\n)\nmessages\n.\nappend\n(\nToolMessage\n(\ntool_output\n,\ntool_call_id\n=\ntool_call\n[\n\"id\"\n]\n)\n)\nmessages\nAPI Reference:\nHumanMessage\n|\nToolMessage\n[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'),\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_K5DsWEmgt6D08EI9AFu9NaL1', 'function': {'arguments': '{\"a\": 3, \"b\": 12}', 'name': 'Multiply'}, 'type': 'function'}, {'id': 'call_qywVrsplg0ZMv7LHYYMjyG81', 'function': {'arguments': '{\"a\": 11, \"b\": 49}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 105, 'total_tokens': 155}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1a0b8cdd-9221-4d94-b2ed-5701f67ce9fe-0', tool_calls=[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_K5DsWEmgt6D08EI9AFu9NaL1'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_qywVrsplg0ZMv7LHYYMjyG81'}]),\nToolMessage(content='36', tool_call_id='call_K5DsWEmgt6D08EI9AFu9NaL1'),\nToolMessage(content='60', tool_call_id='call_qywVrsplg0ZMv7LHYYMjyG81')]\nllm_with_tools\n.\ninvoke\n(\nmessages\n)\nAIMessage(content='3 * 12 is 36 and 11 + 49 is 60.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 171, 'total_tokens': 189}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-a6c8093c-b16a-4c92-8308-7c9ac998118c-0')\nFew-shot prompting\nâ€‹\nFor more complex tool use it's very useful to add few-shot examples to the prompt. We can do this by adding\nAIMessage\ns with\nToolCall\ns and corresponding\nToolMessage\ns to our prompt.\nFor example, even with some special instructions our model can get tripped up by order of operations:\nllm_with_tools\n.\ninvoke\n(\n\"Whats 119 times 8 minus 20. Don't do any math yourself, only use tools for math. Respect order of operations\"\n)\n.\ntool_calls\n[{'name': 'Multiply',\n'args': {'a': 119, 'b': 8},\n'id': 'call_Dl3FXRVkQCFW4sUNYOe4rFr7'},\n{'name': 'Add',\n'args': {'a': 952, 'b': -20},\n'id': 'call_n03l4hmka7VZTCiP387Wud2C'}]\nThe model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.\nBy adding a prompt with some examples we can correct this behavior:\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nexamples\n=\n[\nHumanMessage\n(\n\"What's the product of 317253 and 128472 plus four\"\n,\nname\n=\n\"example_user\"\n)\n,\nAIMessage\n(\n\"\"\n,\nname\n=\n\"example_assistant\"\n,\ntool_calls\n=\n[\n{\n\"name\"\n:\n\"Multiply\"\n,\n\"args\"\n:\n{\n\"x\"\n:\n317253\n,\n\"y\"\n:\n128472\n}\n,\n\"id\"\n:\n\"1\"\n}\n]\n,\n)\n,\nToolMessage\n(\n\"16505054784\"\n,\ntool_call_id\n=\n\"1\"\n)\n,\nAIMessage\n(\n\"\"\n,\nname\n=\n\"example_assistant\"\n,\ntool_calls\n=\n[\n{\n\"name\"\n:\n\"Add\"\n,\n\"args\"\n:\n{\n\"x\"\n:\n16505054784\n,\n\"y\"\n:\n4\n}\n,\n\"id\"\n:\n\"2\"\n}\n]\n,\n)\n,\nToolMessage\n(\n\"16505054788\"\n,\ntool_call_id\n=\n\"2\"\n)\n,\nAIMessage\n(\n\"The product of 317253 and 128472 plus four is 16505054788\"\n,\nname\n=\n\"example_assistant\"\n,\n)\n,\n]\nsystem\n=\n\"\"\"You are bad at math but are an expert at using a calculator.\nUse past tool usage as an example of how to correctly use the tools.\"\"\"\nfew_shot_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n*\nexamples\n,\n(\n\"human\"\n,\n\"{query}\"\n)\n,\n]\n)\nchain\n=\n{\n\"query\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nfew_shot_prompt\n|\nllm_with_tools\nchain\n.\ninvoke\n(\n\"Whats 119 times 8 minus 20\"\n)\n.\ntool_calls\nAPI Reference:\nAIMessage\n|\nChatPromptTemplate\n|\nRunnablePassthrough\n[{'name': 'Multiply',\n'args': {'a': 119, 'b': 8},\n'id': 'call_MoSgwzIhPxhclfygkYaKIsGZ'}]\nSeems like we get the correct output this time.\nHere's what the\nLangSmith trace\nlooks like.\nNext steps\nâ€‹\nOutput parsing\n: See\nOpenAI Tools output\nparsers\nto learn about extracting the function calling API responses into\nvarious formats.\nStructured output chains\n:\nSome models have constructors\nthat\nhandle creating a structured output chain for you.\nTool use\n: See how to construct chains and agents that\ncall the invoked tools in\nthese\nguides\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/installation/",
    "How-to guides\nHow to install LangChain packages\nOn this page\nHow to install LangChain packages\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\nOfficial release\nâ€‹\nTo install the main\nlangchain\npackage, run:\nPip\nConda\npip install langchain\nconda install langchain -c conda-forge\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\nEcosystem packages\nâ€‹\nWith the exception of the\nlangsmith\nSDK, all packages in the LangChain ecosystem depend on\nlangchain-core\n, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the different packages are related.\nA directed arrow indicates that the source package depends on the target package:\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as\nlangchain-core\n).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\nLangChain core\nâ€‹\nThe\nlangchain-core\npackage contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by\nlangchain\n, but can also be used separately. Install with:\npip install langchain-core\nIntegration packages\nâ€‹\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the\nIntegration docs\n.\nYou can see a list of all integration packages in the\nAPI reference\nunder the \"Partner libs\" dropdown.\nTo install one of these run:\npip install langchain-openai\nAny integrations that haven't been split out into their own packages will live in the\nlangchain-community\npackage. Install with:\npip install langchain-community\nLangChain experimental\nâ€‹\nThe\nlangchain-experimental\npackage holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\npip install langchain-experimental\nLangGraph\nâ€‹\nlanggraph\nis a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\npip install langgraph\nLangServe\nâ€‹\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\npip install \"langserve[all]\"\nfor both client and server dependencies. Or\npip install \"langserve[client]\"\nfor client code, and\npip install \"langserve[server]\"\nfor server code.\nLangChain CLI\nâ€‹\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\npip install langchain-cli\nLangSmith SDK\nâ€‹\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\nlangchain-core\n, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\npip install langsmith\nFrom source\nâ€‹\nIf you want to install a package from source, you can do so by cloning the\nmain LangChain repo\n, enter the directory of the package you want to install\nPATH/TO/REPO/langchain/libs/{package}\n, and run:\npip install -e .\nLangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see\nall repos here\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/query_few_shot/",
    "How-to guides\nHow to add examples to the prompt for query analysis\nOn this page\nHow to add examples to the prompt for query analysis\nAs our query analysis becomes more complex, the LLM may struggle to understand how exactly it should respond in certain scenarios. In order to improve performance here, we can\nadd examples\nto the prompt to guide the LLM.\nLet's take a look at how we can add examples for a LangChain YouTube video query analyzer.\nSetup\nâ€‹\nInstall dependencies\nâ€‹\n# %pip install -qU langchain-core langchain-openai\nSet environment variables\nâ€‹\nWe'll use OpenAI in this example:\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\n# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nQuery schema\nâ€‹\nWe'll define a query schema that we want our model to output. To make our query analysis a bit more interesting, we'll add a\nsub_queries\nfield that contains more narrow questions derived from the top level question.\nfrom\ntyping\nimport\nList\n,\nOptional\nfrom\npydantic\nimport\nBaseModel\n,\nField\nsub_queries_description\n=\n\"\"\"\\\nIf the original question contains multiple distinct sub-questions, \\\nor if there are more generic questions that would be helpful to answer in \\\norder to answer the original question, write a list of all relevant sub-questions. \\\nMake sure this list is comprehensive and covers all parts of the original question. \\\nIt's ok if there's redundancy in the sub-questions. \\\nMake sure the sub-questions are as narrowly focused as possible.\"\"\"\nclass\nSearch\n(\nBaseModel\n)\n:\n\"\"\"Search over a database of tutorial videos about a software library.\"\"\"\nquery\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Primary similarity search query applied to video transcripts.\"\n,\n)\nsub_queries\n:\nList\n[\nstr\n]\n=\nField\n(\ndefault_factory\n=\nlist\n,\ndescription\n=\nsub_queries_description\n)\npublish_year\n:\nOptional\n[\nint\n]\n=\nField\n(\nNone\n,\ndescription\n=\n\"Year video was published\"\n)\nQuery generation\nâ€‹\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nMessagesPlaceholder\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\nsystem\n=\n\"\"\"You are an expert at converting user questions into database queries. \\\nYou have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\nGiven a question, return a list of database queries optimized to retrieve the most relevant results.\nIf there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\nMessagesPlaceholder\n(\n\"examples\"\n,\noptional\n=\nTrue\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n]\n)\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nSearch\n)\nquery_analyzer\n=\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nstructured_llm\nAPI Reference:\nChatPromptTemplate\n|\nMessagesPlaceholder\n|\nRunnablePassthrough\nLet's try out our query analyzer without any examples in the prompt:\nquery_analyzer\n.\ninvoke\n(\n\"what's the difference between web voyager and reflection agents? do both use langgraph?\"\n)\nSearch(query='difference between web voyager and reflection agents', sub_queries=['what is web voyager', 'what are reflection agents', 'do both web voyager and reflection agents use langgraph?'], publish_year=None)\nAdding examples and tuning the prompt\nâ€‹\nThis works pretty well, but we probably want it to decompose the question even further to separate the queries about Web Voyager and Reflection Agents.\nTo tune our query generation results, we can add some examples of inputs questions and gold standard output queries to our prompt.\nexamples\n=\n[\n]\nquestion\n=\n\"What's chat langchain, is it a langchain template?\"\nquery\n=\nSearch\n(\nquery\n=\n\"What is chat langchain and is it a langchain template?\"\n,\nsub_queries\n=\n[\n\"What is chat langchain\"\n,\n\"What is a langchain template\"\n]\n,\n)\nexamples\n.\nappend\n(\n{\n\"input\"\n:\nquestion\n,\n\"tool_calls\"\n:\n[\nquery\n]\n}\n)\nquestion\n=\n\"How to build multi-agent system and stream intermediate steps from it\"\nquery\n=\nSearch\n(\nquery\n=\n\"How to build multi-agent system and stream intermediate steps from it\"\n,\nsub_queries\n=\n[\n\"How to build multi-agent system\"\n,\n\"How to stream intermediate steps from multi-agent system\"\n,\n\"How to stream intermediate steps\"\n,\n]\n,\n)\nexamples\n.\nappend\n(\n{\n\"input\"\n:\nquestion\n,\n\"tool_calls\"\n:\n[\nquery\n]\n}\n)\nquestion\n=\n\"LangChain agents vs LangGraph?\"\nquery\n=\nSearch\n(\nquery\n=\n\"What's the difference between LangChain agents and LangGraph? How do you deploy them?\"\n,\nsub_queries\n=\n[\n\"What are LangChain agents\"\n,\n\"What is LangGraph\"\n,\n\"How do you deploy LangChain agents\"\n,\n\"How do you deploy LangGraph\"\n,\n]\n,\n)\nexamples\n.\nappend\n(\n{\n\"input\"\n:\nquestion\n,\n\"tool_calls\"\n:\n[\nquery\n]\n}\n)\nNow we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a\ntool_example_to_messages\nhelper function to handle this for us:\nimport\nuuid\nfrom\ntyping\nimport\nDict\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nBaseMessage\n,\nHumanMessage\n,\nSystemMessage\n,\nToolMessage\n,\n)\ndef\ntool_example_to_messages\n(\nexample\n:\nDict\n)\n-\n>\nList\n[\nBaseMessage\n]\n:\nmessages\n:\nList\n[\nBaseMessage\n]\n=\n[\nHumanMessage\n(\ncontent\n=\nexample\n[\n\"input\"\n]\n)\n]\nopenai_tool_calls\n=\n[\n]\nfor\ntool_call\nin\nexample\n[\n\"tool_calls\"\n]\n:\nopenai_tool_calls\n.\nappend\n(\n{\n\"id\"\n:\nstr\n(\nuuid\n.\nuuid4\n(\n)\n)\n,\n\"type\"\n:\n\"function\"\n,\n\"function\"\n:\n{\n\"name\"\n:\ntool_call\n.\n__class__\n.\n__name__\n,\n\"arguments\"\n:\ntool_call\n.\njson\n(\n)\n,\n}\n,\n}\n)\nmessages\n.\nappend\n(\nAIMessage\n(\ncontent\n=\n\"\"\n,\nadditional_kwargs\n=\n{\n\"tool_calls\"\n:\nopenai_tool_calls\n}\n)\n)\ntool_outputs\n=\nexample\n.\nget\n(\n\"tool_outputs\"\n)\nor\n[\n\"You have correctly called this tool.\"\n]\n*\nlen\n(\nopenai_tool_calls\n)\nfor\noutput\n,\ntool_call\nin\nzip\n(\ntool_outputs\n,\nopenai_tool_calls\n)\n:\nmessages\n.\nappend\n(\nToolMessage\n(\ncontent\n=\noutput\n,\ntool_call_id\n=\ntool_call\n[\n\"id\"\n]\n)\n)\nreturn\nmessages\nexample_msgs\n=\n[\nmsg\nfor\nex\nin\nexamples\nfor\nmsg\nin\ntool_example_to_messages\n(\nex\n)\n]\nAPI Reference:\nAIMessage\n|\nBaseMessage\n|\nHumanMessage\n|\nSystemMessage\n|\nToolMessage\nfrom\nlangchain_core\n.\nprompts\nimport\nMessagesPlaceholder\nquery_analyzer_with_examples\n=\n(\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n.\npartial\n(\nexamples\n=\nexample_msgs\n)\n|\nstructured_llm\n)\nAPI Reference:\nMessagesPlaceholder\nquery_analyzer_with_examples\n.\ninvoke\n(\n\"what's the difference between web voyager and reflection agents? do both use langgraph?\"\n)\nSearch(query=\"What's the difference between web voyager and reflection agents? Do both use langgraph?\", sub_queries=['What is web voyager', 'What are reflection agents', 'Do web voyager and reflection agents use langgraph?'], publish_year=None)\nThanks to our examples we get a slightly more decomposed search query. With some more prompt engineering and tuning of our examples we could improve query generation even more.\nYou can see that the examples are passed to the model as messages in the\nLangSmith trace\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/few_shot_examples/",
    "How-to guides\nHow to use few shot examples\nOn this page\nHow to use few shot examples\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nPrompt templates\nExample selectors\nLLMs\nVectorstores\nIn this guide, we'll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called\nfew-shotting\n, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\nA few-shot prompt template can be constructed from either a set of examples, or from an\nExample Selector\nclass responsible for choosing a subset of examples from the defined set.\nThis guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see\nhere\n.\nCreate a formatter for the few-shot examples\nâ€‹\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a\nPromptTemplate\nobject.\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nexample_prompt\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Question: {question}\\n{answer}\"\n)\nAPI Reference:\nPromptTemplate\nCreating the example set\nâ€‹\nNext, we'll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\nexamples\n=\n[\n{\n\"question\"\n:\n\"Who lived longer, Muhammad Ali or Alan Turing?\"\n,\n\"answer\"\n:\n\"\"\"\nAre follow up questions needed here: Yes.\nFollow up: How old was Muhammad Ali when he died?\nIntermediate answer: Muhammad Ali was 74 years old when he died.\nFollow up: How old was Alan Turing when he died?\nIntermediate answer: Alan Turing was 41 years old when he died.\nSo the final answer is: Muhammad Ali\n\"\"\"\n,\n}\n,\n{\n\"question\"\n:\n\"When was the founder of craigslist born?\"\n,\n\"answer\"\n:\n\"\"\"\nAre follow up questions needed here: Yes.\nFollow up: Who was the founder of craigslist?\nIntermediate answer: Craigslist was founded by Craig Newmark.\nFollow up: When was Craig Newmark born?\nIntermediate answer: Craig Newmark was born on December 6, 1952.\nSo the final answer is: December 6, 1952\n\"\"\"\n,\n}\n,\n{\n\"question\"\n:\n\"Who was the maternal grandfather of George Washington?\"\n,\n\"answer\"\n:\n\"\"\"\nAre follow up questions needed here: Yes.\nFollow up: Who was the mother of George Washington?\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\nFollow up: Who was the father of Mary Ball Washington?\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\nSo the final answer is: Joseph Ball\n\"\"\"\n,\n}\n,\n{\n\"question\"\n:\n\"Are both the directors of Jaws and Casino Royale from the same country?\"\n,\n\"answer\"\n:\n\"\"\"\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of Jaws?\nIntermediate Answer: The director of Jaws is Steven Spielberg.\nFollow up: Where is Steven Spielberg from?\nIntermediate Answer: The United States.\nFollow up: Who is the director of Casino Royale?\nIntermediate Answer: The director of Casino Royale is Martin Campbell.\nFollow up: Where is Martin Campbell from?\nIntermediate Answer: New Zealand.\nSo the final answer is: No\n\"\"\"\n,\n}\n,\n]\nLet's test the formatting prompt with one of our examples:\nprint\n(\nexample_prompt\n.\ninvoke\n(\nexamples\n[\n0\n]\n)\n.\nto_string\n(\n)\n)\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?\nAre follow up questions needed here: Yes.\nFollow up: How old was Muhammad Ali when he died?\nIntermediate answer: Muhammad Ali was 74 years old when he died.\nFollow up: How old was Alan Turing when he died?\nIntermediate answer: Alan Turing was 41 years old when he died.\nSo the final answer is: Muhammad Ali\nPass the examples and formatter to\nFewShotPromptTemplate\nâ€‹\nFinally, create a\nFewShotPromptTemplate\nobject. This object takes in the few-shot examples and the formatter for the few-shot examples. When this\nFewShotPromptTemplate\nis formatted, it formats the passed examples using the\nexample_prompt\n, then and adds them to the final prompt before\nsuffix\n:\nfrom\nlangchain_core\n.\nprompts\nimport\nFewShotPromptTemplate\nprompt\n=\nFewShotPromptTemplate\n(\nexamples\n=\nexamples\n,\nexample_prompt\n=\nexample_prompt\n,\nsuffix\n=\n\"Question: {input}\"\n,\ninput_variables\n=\n[\n\"input\"\n]\n,\n)\nprint\n(\nprompt\n.\ninvoke\n(\n{\n\"input\"\n:\n\"Who was the father of Mary Ball Washington?\"\n}\n)\n.\nto_string\n(\n)\n)\nAPI Reference:\nFewShotPromptTemplate\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?\nAre follow up questions needed here: Yes.\nFollow up: How old was Muhammad Ali when he died?\nIntermediate answer: Muhammad Ali was 74 years old when he died.\nFollow up: How old was Alan Turing when he died?\nIntermediate answer: Alan Turing was 41 years old when he died.\nSo the final answer is: Muhammad Ali\nQuestion: When was the founder of craigslist born?\nAre follow up questions needed here: Yes.\nFollow up: Who was the founder of craigslist?\nIntermediate answer: Craigslist was founded by Craig Newmark.\nFollow up: When was Craig Newmark born?\nIntermediate answer: Craig Newmark was born on December 6, 1952.\nSo the final answer is: December 6, 1952\nQuestion: Who was the maternal grandfather of George Washington?\nAre follow up questions needed here: Yes.\nFollow up: Who was the mother of George Washington?\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\nFollow up: Who was the father of Mary Ball Washington?\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\nSo the final answer is: Joseph Ball\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of Jaws?\nIntermediate Answer: The director of Jaws is Steven Spielberg.\nFollow up: Where is Steven Spielberg from?\nIntermediate Answer: The United States.\nFollow up: Who is the director of Casino Royale?\nIntermediate Answer: The director of Casino Royale is Martin Campbell.\nFollow up: Where is Martin Campbell from?\nIntermediate Answer: New Zealand.\nSo the final answer is: No\nQuestion: Who was the father of Mary Ball Washington?\nBy providing the model with examples like this, we can guide the model to a better response.\nUsing an example selector\nâ€‹\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the\nFewShotPromptTemplate\nobject, we will feed them into an implementation of\nExampleSelector\ncalled\nSemanticSimilarityExampleSelector\ninstance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\nTo show what it looks like, let's initialize an instance and call it in isolation:\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_core\n.\nexample_selectors\nimport\nSemanticSimilarityExampleSelector\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nexample_selector\n=\nSemanticSimilarityExampleSelector\n.\nfrom_examples\n(\n# This is the list of examples available to select from.\nexamples\n,\n# This is the embedding class used to produce embeddings which are used to measure semantic similarity.\nOpenAIEmbeddings\n(\n)\n,\n# This is the VectorStore class that is used to store the embeddings and do a similarity search over.\nChroma\n,\n# This is the number of examples to produce.\nk\n=\n1\n,\n)\n# Select the most similar example to the input.\nquestion\n=\n\"Who was the father of Mary Ball Washington?\"\nselected_examples\n=\nexample_selector\n.\nselect_examples\n(\n{\n\"question\"\n:\nquestion\n}\n)\nprint\n(\nf\"Examples most similar to the input:\n{\nquestion\n}\n\"\n)\nfor\nexample\nin\nselected_examples\n:\nprint\n(\n\"\\n\"\n)\nfor\nk\n,\nv\nin\nexample\n.\nitems\n(\n)\n:\nprint\n(\nf\"\n{\nk\n}\n:\n{\nv\n}\n\"\n)\nAPI Reference:\nSemanticSimilarityExampleSelector\nExamples most similar to the input: Who was the father of Mary Ball Washington?\nanswer:\nAre follow up questions needed here: Yes.\nFollow up: Who was the mother of George Washington?\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\nFollow up: Who was the father of Mary Ball Washington?\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\nSo the final answer is: Joseph Ball\nquestion: Who was the maternal grandfather of George Washington?\nNow, let's create a\nFewShotPromptTemplate\nobject. This object takes in the example selector and the formatter prompt for the few-shot examples.\nprompt\n=\nFewShotPromptTemplate\n(\nexample_selector\n=\nexample_selector\n,\nexample_prompt\n=\nexample_prompt\n,\nsuffix\n=\n\"Question: {input}\"\n,\ninput_variables\n=\n[\n\"input\"\n]\n,\n)\nprint\n(\nprompt\n.\ninvoke\n(\n{\n\"input\"\n:\n\"Who was the father of Mary Ball Washington?\"\n}\n)\n.\nto_string\n(\n)\n)\nQuestion: Who was the maternal grandfather of George Washington?\nAre follow up questions needed here: Yes.\nFollow up: Who was the mother of George Washington?\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\nFollow up: Who was the father of Mary Ball Washington?\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\nSo the final answer is: Joseph Ball\nQuestion: Who was the father of Mary Ball Washington?\nNext steps\nâ€‹\nYou've now learned how to add few-shot examples to your prompts.\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on\nfew shotting with chat models\n, or the other\nexample selector how-to guides\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/functions/",
    "How-to guides\nHow to run custom functions\nOn this page\nHow to run custom functions\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Expression Language (LCEL)\nChaining runnables\nYou can use arbitrary functions as\nRunnables\n. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called\nRunnableLambdas\n.\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.\nThis guide will cover:\nHow to explicitly create a runnable from a custom function using the\nRunnableLambda\nconstructor and the convenience\n@chain\ndecorator\nCoercion of custom functions into runnables when used in chains\nHow to accept and use run metadata in your custom function\nHow to stream with custom functions by having them return generators\nUsing the constructor\nâ€‹\nBelow, we explicitly wrap our custom logic using the\nRunnableLambda\nconstructor:\n%\npip install\n-\nqU langchain langchain_openai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nfrom\noperator\nimport\nitemgetter\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\nlangchain_openai\nimport\nChatOpenAI\ndef\nlength_function\n(\ntext\n)\n:\nreturn\nlen\n(\ntext\n)\ndef\n_multiple_length_function\n(\ntext1\n,\ntext2\n)\n:\nreturn\nlen\n(\ntext1\n)\n*\nlen\n(\ntext2\n)\ndef\nmultiple_length_function\n(\n_dict\n)\n:\nreturn\n_multiple_length_function\n(\n_dict\n[\n\"text1\"\n]\n,\n_dict\n[\n\"text2\"\n]\n)\nmodel\n=\nChatOpenAI\n(\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"what is {a} + {b}\"\n)\nchain\n=\n(\n{\n\"a\"\n:\nitemgetter\n(\n\"foo\"\n)\n|\nRunnableLambda\n(\nlength_function\n)\n,\n\"b\"\n:\n{\n\"text1\"\n:\nitemgetter\n(\n\"foo\"\n)\n,\n\"text2\"\n:\nitemgetter\n(\n\"bar\"\n)\n}\n|\nRunnableLambda\n(\nmultiple_length_function\n)\n,\n}\n|\nprompt\n|\nmodel\n)\nchain\n.\ninvoke\n(\n{\n\"foo\"\n:\n\"bar\"\n,\n\"bar\"\n:\n\"gah\"\n}\n)\nAPI Reference:\nChatPromptTemplate\n|\nRunnableLambda\nAIMessage(content='3 + 9 equals 12.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-73728de3-e483-49e3-ad54-51bd9570e71a-0')\nThe convenience\n@chain\ndecorator\nâ€‹\nYou can also turn an arbitrary function into a chain by adding a\n@chain\ndecorator. This is functionally equivalent to wrapping the function in a\nRunnableLambda\nconstructor as shown above. Here's an example:\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nrunnables\nimport\nchain\nprompt1\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"Tell me a joke about {topic}\"\n)\nprompt2\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"What is the subject of this joke: {joke}\"\n)\n@chain\ndef\ncustom_chain\n(\ntext\n)\n:\nprompt_val1\n=\nprompt1\n.\ninvoke\n(\n{\n\"topic\"\n:\ntext\n}\n)\noutput1\n=\nChatOpenAI\n(\n)\n.\ninvoke\n(\nprompt_val1\n)\nparsed_output1\n=\nStrOutputParser\n(\n)\n.\ninvoke\n(\noutput1\n)\nchain2\n=\nprompt2\n|\nChatOpenAI\n(\n)\n|\nStrOutputParser\n(\n)\nreturn\nchain2\n.\ninvoke\n(\n{\n\"joke\"\n:\nparsed_output1\n}\n)\ncustom_chain\n.\ninvoke\n(\n\"bears\"\n)\nAPI Reference:\nStrOutputParser\n|\nchain\n'The subject of the joke is the bear and his girlfriend.'\nAbove, the\n@chain\ndecorator is used to convert\ncustom_chain\ninto a runnable, which we invoke with the\n.invoke()\nmethod.\nIf you are using a tracing with\nLangSmith\n, you should see a\ncustom_chain\ntrace in there, with the calls to OpenAI nested underneath.\nAutomatic coercion in chains\nâ€‹\nWhen using custom functions in chains with the pipe operator (\n|\n), you can omit the\nRunnableLambda\nor\n@chain\nconstructor and rely on coercion. Here's a simple example with a function that takes the output from the model and returns the first five letters of it:\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"tell me a story about {topic}\"\n)\nmodel\n=\nChatOpenAI\n(\n)\nchain_with_coerced_function\n=\nprompt\n|\nmodel\n|\n(\nlambda\nx\n:\nx\n.\ncontent\n[\n:\n5\n]\n)\nchain_with_coerced_function\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\n'Once '\nNote that we didn't need to wrap the custom function\n(lambda x: x.content[:5])\nin a\nRunnableLambda\nconstructor because the\nmodel\non the left of the pipe operator is already a Runnable. The custom function is\ncoerced\ninto a runnable. See\nthis section\nfor more information.\nPassing run metadata\nâ€‹\nRunnable lambdas can optionally accept a\nRunnableConfig\nparameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\nimport\njson\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\ndef\nparse_or_fix\n(\ntext\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n:\nfixing_chain\n=\n(\nChatPromptTemplate\n.\nfrom_template\n(\n\"Fix the following text:\\n\\n\\`\\`\\`text\\n{input}\\n\\`\\`\\`\\nError: {error}\"\n\" Don't narrate, just respond with the fixed data.\"\n)\n|\nmodel\n|\nStrOutputParser\n(\n)\n)\nfor\n_\nin\nrange\n(\n3\n)\n:\ntry\n:\nreturn\njson\n.\nloads\n(\ntext\n)\nexcept\nException\nas\ne\n:\ntext\n=\nfixing_chain\n.\ninvoke\n(\n{\n\"input\"\n:\ntext\n,\n\"error\"\n:\ne\n}\n,\nconfig\n)\nreturn\n\"Failed to parse\"\nfrom\nlangchain_community\n.\ncallbacks\nimport\nget_openai_callback\nwith\nget_openai_callback\n(\n)\nas\ncb\n:\noutput\n=\nRunnableLambda\n(\nparse_or_fix\n)\n.\ninvoke\n(\n\"{foo: bar}\"\n,\n{\n\"tags\"\n:\n[\n\"my-tag\"\n]\n,\n\"callbacks\"\n:\n[\ncb\n]\n}\n)\nprint\n(\noutput\n)\nprint\n(\ncb\n)\nAPI Reference:\nRunnableConfig\n{'foo': 'bar'}\nTokens Used: 62\nPrompt Tokens: 56\nCompletion Tokens: 6\nSuccessful Requests: 1\nTotal Cost (USD): $9.6e-05\nfrom\nlangchain_community\n.\ncallbacks\nimport\nget_openai_callback\nwith\nget_openai_callback\n(\n)\nas\ncb\n:\noutput\n=\nRunnableLambda\n(\nparse_or_fix\n)\n.\ninvoke\n(\n\"{foo: bar}\"\n,\n{\n\"tags\"\n:\n[\n\"my-tag\"\n]\n,\n\"callbacks\"\n:\n[\ncb\n]\n}\n)\nprint\n(\noutput\n)\nprint\n(\ncb\n)\n{'foo': 'bar'}\nTokens Used: 62\nPrompt Tokens: 56\nCompletion Tokens: 6\nSuccessful Requests: 1\nTotal Cost (USD): $9.6e-05\nStreaming\nâ€‹\nnote\nRunnableLambda\nis best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use\nRunnableGenerator\ninstead as in the example below.\nYou can use generator functions (ie. functions that use the\nyield\nkeyword, and behave like iterators) in a chain.\nThe signature of these generators should be\nIterator[Input] -> Iterator[Output]\n. Or for async generators:\nAsyncIterator[Input] -> AsyncIterator[Output]\n.\nThese are useful for:\nimplementing a custom output parser\nmodifying the output of a previous step, while preserving streaming capabilities\nHere's an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\nfrom\ntyping\nimport\nIterator\n,\nList\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\"\n)\nstr_chain\n=\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\nfor\nchunk\nin\nstr_chain\n.\nstream\n(\n{\n\"animal\"\n:\n\"bear\"\n}\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"\"\n,\nflush\n=\nTrue\n)\nlion, tiger, wolf, gorilla, panda\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\n# This is a custom parser that splits an iterator of llm tokens\n# into a list of strings separated by commas\ndef\nsplit_into_list\n(\ninput\n:\nIterator\n[\nstr\n]\n)\n-\n>\nIterator\n[\nList\n[\nstr\n]\n]\n:\n# hold partial input until we get a comma\nbuffer\n=\n\"\"\nfor\nchunk\nin\ninput\n:\n# add current chunk to buffer\nbuffer\n+=\nchunk\n# while there are commas in the buffer\nwhile\n\",\"\nin\nbuffer\n:\n# split buffer on comma\ncomma_index\n=\nbuffer\n.\nindex\n(\n\",\"\n)\n# yield everything before the comma\nyield\n[\nbuffer\n[\n:\ncomma_index\n]\n.\nstrip\n(\n)\n]\n# save the rest for the next iteration\nbuffer\n=\nbuffer\n[\ncomma_index\n+\n1\n:\n]\n# yield the last chunk\nyield\n[\nbuffer\n.\nstrip\n(\n)\n]\nlist_chain\n=\nstr_chain\n|\nsplit_into_list\nfor\nchunk\nin\nlist_chain\n.\nstream\n(\n{\n\"animal\"\n:\n\"bear\"\n}\n)\n:\nprint\n(\nchunk\n,\nflush\n=\nTrue\n)\n['lion']\n['tiger']\n['wolf']\n['gorilla']\n['raccoon']\nInvoking it gives a full array of values:\nlist_chain\n.\ninvoke\n(\n{\n\"animal\"\n:\n\"bear\"\n}\n)\n['lion', 'tiger', 'wolf', 'gorilla', 'raccoon']\nAsync version\nâ€‹\nIf you are working in an\nasync\nenvironment, here is an\nasync\nversion of the above example:\nfrom\ntyping\nimport\nAsyncIterator\nasync\ndef\nasplit_into_list\n(\ninput\n:\nAsyncIterator\n[\nstr\n]\n,\n)\n-\n>\nAsyncIterator\n[\nList\n[\nstr\n]\n]\n:\n# async def\nbuffer\n=\n\"\"\nasync\nfor\n(\nchunk\n)\nin\ninput\n:\n# `input` is a `async_generator` object, so use `async for`\nbuffer\n+=\nchunk\nwhile\n\",\"\nin\nbuffer\n:\ncomma_index\n=\nbuffer\n.\nindex\n(\n\",\"\n)\nyield\n[\nbuffer\n[\n:\ncomma_index\n]\n.\nstrip\n(\n)\n]\nbuffer\n=\nbuffer\n[\ncomma_index\n+\n1\n:\n]\nyield\n[\nbuffer\n.\nstrip\n(\n)\n]\nlist_chain\n=\nstr_chain\n|\nasplit_into_list\nasync\nfor\nchunk\nin\nlist_chain\n.\nastream\n(\n{\n\"animal\"\n:\n\"bear\"\n}\n)\n:\nprint\n(\nchunk\n,\nflush\n=\nTrue\n)\n['lion']\n['tiger']\n['wolf']\n['gorilla']\n['panda']\nawait\nlist_chain\n.\nainvoke\n(\n{\n\"animal\"\n:\n\"bear\"\n}\n)\n['lion', 'tiger', 'wolf', 'gorilla', 'panda']\nNext steps\nâ€‹\nNow you've learned a few different ways to use custom logic within your chains, and how to implement streaming.\nTo learn more, see the other how-to guides on runnables in this section.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/output_parser_structured/",
    "How-to guides\nHow to use output parsers to parse an LLM response into structured format\nOn this page\nHow to use output parsers to parse an LLM response into structured format\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support\nbuilt-in ways to return structured output\n, not all do.\nOutput parsers\nare classes that help structure language model responses. There are two main methods an output parser must implement:\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\nAnd then one optional one:\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\nGet started\nâ€‹\nBelow we go over the main type of output parser, the\nPydanticOutputParser\n.\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nPydanticOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nOpenAI\nfrom\npydantic\nimport\nBaseModel\n,\nField\n,\nmodel_validator\nmodel\n=\nOpenAI\n(\nmodel_name\n=\n\"gpt-3.5-turbo-instruct\"\n,\ntemperature\n=\n0.0\n)\n# Define your desired data structure.\nclass\nJoke\n(\nBaseModel\n)\n:\nsetup\n:\nstr\n=\nField\n(\ndescription\n=\n\"question to set up a joke\"\n)\npunchline\n:\nstr\n=\nField\n(\ndescription\n=\n\"answer to resolve the joke\"\n)\n# You can add custom validation logic easily with Pydantic.\n@model_validator\n(\nmode\n=\n\"before\"\n)\n@classmethod\ndef\nquestion_ends_with_question_mark\n(\ncls\n,\nvalues\n:\ndict\n)\n-\n>\ndict\n:\nsetup\n=\nvalues\n.\nget\n(\n\"setup\"\n)\nif\nsetup\nand\nsetup\n[\n-\n1\n]\n!=\n\"?\"\n:\nraise\nValueError\n(\n\"Badly formed question!\"\n)\nreturn\nvalues\n# Set up a parser + inject instructions into the prompt template.\nparser\n=\nPydanticOutputParser\n(\npydantic_object\n=\nJoke\n)\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n,\ninput_variables\n=\n[\n\"query\"\n]\n,\npartial_variables\n=\n{\n\"format_instructions\"\n:\nparser\n.\nget_format_instructions\n(\n)\n}\n,\n)\n# And a query intended to prompt a language model to populate the data structure.\nprompt_and_model\n=\nprompt\n|\nmodel\noutput\n=\nprompt_and_model\n.\ninvoke\n(\n{\n\"query\"\n:\n\"Tell me a joke.\"\n}\n)\nparser\n.\ninvoke\n(\noutput\n)\nAPI Reference:\nPydanticOutputParser\n|\nPromptTemplate\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')\nLCEL\nâ€‹\nOutput parsers implement the\nRunnable interface\n, the basic building block of the\nLangChain Expression Language (LCEL)\n. This means they support\ninvoke\n,\nainvoke\n,\nstream\n,\nastream\n,\nbatch\n,\nabatch\n,\nastream_log\ncalls.\nOutput parsers accept a string or\nBaseMessage\nas input and can return an arbitrary type.\nparser\n.\ninvoke\n(\noutput\n)\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')\nInstead of manually invoking the parser, we also could've just added it to our\nRunnable\nsequence:\nchain\n=\nprompt\n|\nmodel\n|\nparser\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\n\"Tell me a joke.\"\n}\n)\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\nThe\nSimpleJsonOutputParser\nfor example can stream through partial outputs:\nfrom\nlangchain\n.\noutput_parsers\n.\njson\nimport\nSimpleJsonOutputParser\njson_prompt\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Return a JSON object with an `answer` key that answers the following question: {question}\"\n)\njson_parser\n=\nSimpleJsonOutputParser\n(\n)\njson_chain\n=\njson_prompt\n|\nmodel\n|\njson_parser\nlist\n(\njson_chain\n.\nstream\n(\n{\n\"question\"\n:\n\"Who invented the microscope?\"\n}\n)\n)\n[{},\n{'answer': ''},\n{'answer': 'Ant'},\n{'answer': 'Anton'},\n{'answer': 'Antonie'},\n{'answer': 'Antonie van'},\n{'answer': 'Antonie van Lee'},\n{'answer': 'Antonie van Leeu'},\n{'answer': 'Antonie van Leeuwen'},\n{'answer': 'Antonie van Leeuwenho'},\n{'answer': 'Antonie van Leeuwenhoek'}]\nSimilarly,for\nPydanticOutputParser\n:\nlist\n(\nchain\n.\nstream\n(\n{\n\"query\"\n:\n\"Tell me a joke.\"\n}\n)\n)\n[Joke(setup='Why did the tomato turn red?', punchline=''),\nJoke(setup='Why did the tomato turn red?', punchline='Because'),\nJoke(setup='Why did the tomato turn red?', punchline='Because it'),\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw'),\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the'),\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the salad'),\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing'),\nJoke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/query_no_queries/",
    "How-to guides\nHow to handle cases where no queries are generated\nOn this page\nHow to handle cases where no queries are generated\nSometimes, a query analysis technique may allow for any number of queries to be generated - including no queries! In this case, our overall chain will need to inspect the result of the query analysis before deciding whether to call the retriever or not.\nWe will use mock data for this example.\nSetup\nâ€‹\nInstall dependencies\nâ€‹\n%\npip install\n-\nqU langchain langchain\n-\ncommunity langchain\n-\nopenai langchain\n-\nchroma\nNote: you may need to restart the kernel to use updated packages.\nSet environment variables\nâ€‹\nWe'll use OpenAI in this example:\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\n# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nCreate Index\nâ€‹\nWe will create a vectorstore over fake information.\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntexts\n=\n[\n\"Harrison worked at Kensho\"\n]\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-small\"\n)\nvectorstore\n=\nChroma\n.\nfrom_texts\n(\ntexts\n,\nembeddings\n,\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\nQuery analysis\nâ€‹\nWe will use function calling to structure the output. However, we will configure the LLM such that is doesn't NEED to call the function representing a search query (should it decide not to). We will also then use a prompt to do query analysis that explicitly lays when it should and shouldn't make a search.\nfrom\ntyping\nimport\nOptional\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nSearch\n(\nBaseModel\n)\n:\n\"\"\"Search over a database of job records.\"\"\"\nquery\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Similarity search query applied to job record.\"\n,\n)\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\nsystem\n=\n\"\"\"You have the ability to issue search queries to get information to help answer user information.\nYou do not NEED to look things up. If you don't need to, then just respond normally.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n]\n)\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\nstructured_llm\n=\nllm\n.\nbind_tools\n(\n[\nSearch\n]\n)\nquery_analyzer\n=\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nstructured_llm\nAPI Reference:\nChatPromptTemplate\n|\nRunnablePassthrough\nWe can see that by invoking this we get an message that sometimes - but not always - returns a tool call.\nquery_analyzer\n.\ninvoke\n(\n\"where did Harrison Work\"\n)\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_korLZrh08PTRL94f4L7rFqdj', 'function': {'arguments': '{\"query\":\"Harrison\"}', 'name': 'Search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 95, 'total_tokens': 109}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ea94d376-37bf-4f80-abe6-e3b42b767ea0-0', tool_calls=[{'name': 'Search', 'args': {'query': 'Harrison'}, 'id': 'call_korLZrh08PTRL94f4L7rFqdj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 95, 'output_tokens': 14, 'total_tokens': 109})\nquery_analyzer\n.\ninvoke\n(\n\"hi!\"\n)\nAIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 93, 'total_tokens': 103}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-ebdfc44a-455a-4ca6-be85-84559886b1e1-0', usage_metadata={'input_tokens': 93, 'output_tokens': 10, 'total_tokens': 103})\nRetrieval with query analysis\nâ€‹\nSo how would we include this in a chain? Let's look at an example below.\nfrom\nlangchain_core\n.\noutput_parsers\n.\nopenai_tools\nimport\nPydanticToolsParser\nfrom\nlangchain_core\n.\nrunnables\nimport\nchain\noutput_parser\n=\nPydanticToolsParser\n(\ntools\n=\n[\nSearch\n]\n)\nAPI Reference:\nPydanticToolsParser\n|\nchain\n@chain\ndef\ncustom_chain\n(\nquestion\n)\n:\nresponse\n=\nquery_analyzer\n.\ninvoke\n(\nquestion\n)\nif\n\"tool_calls\"\nin\nresponse\n.\nadditional_kwargs\n:\nquery\n=\noutput_parser\n.\ninvoke\n(\nresponse\n)\ndocs\n=\nretriever\n.\ninvoke\n(\nquery\n[\n0\n]\n.\nquery\n)\n# Could add more logic - like another LLM call - here\nreturn\ndocs\nelse\n:\nreturn\nresponse\ncustom_chain\n.\ninvoke\n(\n\"where did Harrison Work\"\n)\nNumber of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n[Document(page_content='Harrison worked at Kensho')]\ncustom_chain\n.\ninvoke\n(\n\"hi!\"\n)\nAIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 93, 'total_tokens': 103}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-e87f058d-30c0-4075-8a89-a01b982d557e-0', usage_metadata={'input_tokens': 93, 'output_tokens': 10, 'total_tokens': 103})\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/routing/",
    "How-to guides\nHow to route between sub-chains\nOn this page\nHow to route between sub-chains\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Expression Language (LCEL)\nChaining runnables\nConfiguring chain parameters at runtime\nPrompt templates\nChat Messages\nRouting allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing can help provide structure and consistency around interactions with models by allowing you to define states and use information related to those states as context to model calls.\nThere are two ways to perform routing:\nConditionally return runnables from a\nRunnableLambda\n(recommended)\nUsing a\nRunnableBranch\n(legacy)\nWe'll illustrate both methods using a two step sequence where the first step classifies an input question as being about\nLangChain\n,\nAnthropic\n, or\nOther\n, then routes to a corresponding prompt chain.\nExample Setup\nâ€‹\nFirst, let's create a chain that will identify incoming questions as being about\nLangChain\n,\nAnthropic\n, or\nOther\n:\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nchain\n=\n(\nPromptTemplate\n.\nfrom_template\n(\n\"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.\nDo not respond with more than one word.\n<question>\n{question}\n</question>\nClassification:\"\"\"\n)\n|\nChatAnthropic\n(\nmodel_name\n=\n\"claude-3-haiku-20240307\"\n)\n|\nStrOutputParser\n(\n)\n)\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"how do I call Anthropic?\"\n}\n)\nAPI Reference:\nStrOutputParser\n|\nPromptTemplate\n'Anthropic'\nNow, let's create three sub chains:\nlangchain_chain\n=\nPromptTemplate\n.\nfrom_template\n(\n\"\"\"You are an expert in langchain. \\\nAlways answer questions starting with \"As Harrison Chase told me\". \\\nRespond to the following question:\nQuestion: {question}\nAnswer:\"\"\"\n)\n|\nChatAnthropic\n(\nmodel_name\n=\n\"claude-3-haiku-20240307\"\n)\nanthropic_chain\n=\nPromptTemplate\n.\nfrom_template\n(\n\"\"\"You are an expert in anthropic. \\\nAlways answer questions starting with \"As Dario Amodei told me\". \\\nRespond to the following question:\nQuestion: {question}\nAnswer:\"\"\"\n)\n|\nChatAnthropic\n(\nmodel_name\n=\n\"claude-3-haiku-20240307\"\n)\ngeneral_chain\n=\nPromptTemplate\n.\nfrom_template\n(\n\"\"\"Respond to the following question:\nQuestion: {question}\nAnswer:\"\"\"\n)\n|\nChatAnthropic\n(\nmodel_name\n=\n\"claude-3-haiku-20240307\"\n)\nUsing a custom function (Recommended)\nâ€‹\nYou can also use a custom function to route between different outputs. Here's an example:\ndef\nroute\n(\ninfo\n)\n:\nif\n\"anthropic\"\nin\ninfo\n[\n\"topic\"\n]\n.\nlower\n(\n)\n:\nreturn\nanthropic_chain\nelif\n\"langchain\"\nin\ninfo\n[\n\"topic\"\n]\n.\nlower\n(\n)\n:\nreturn\nlangchain_chain\nelse\n:\nreturn\ngeneral_chain\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfull_chain\n=\n{\n\"topic\"\n:\nchain\n,\n\"question\"\n:\nlambda\nx\n:\nx\n[\n\"question\"\n]\n}\n|\nRunnableLambda\n(\nroute\n)\nAPI Reference:\nRunnableLambda\nfull_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"how do I use Anthropic?\"\n}\n)\nAIMessage(content=\"As Dario Amodei told me, to use Anthropic, you can start by exploring the company's website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \\n\\nDepending on your specific needs, you can look into Anthropic's AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\\n\\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic's approach to AI development.\\n\\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I'd encourage you to reach out to them directly to discuss how they can best support your specific requirements.\", response_metadata={'id': 'msg_01CtLFgFSwvTaJomrihE87Ra', 'content': [ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you can start by exploring the company's website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \\n\\nDepending on your specific needs, you can look into Anthropic's AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\\n\\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic's approach to AI development.\\n\\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I'd encourage you to reach out to them directly to discuss how they can best support your specific requirements.\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, output_tokens=219)})\nfull_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"how do I use LangChain?\"\n}\n)\nAIMessage(content=\"As Harrison Chase told me, using LangChain involves a few key steps:\\n\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\n\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\n\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\n\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\n\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\\n\\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\n\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\n\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", response_metadata={'id': 'msg_01H3UXAAHG4TwxJLpxwuuVU7', 'content': [ContentBlock(text=\"As Harrison Chase told me, using LangChain involves a few key steps:\\n\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\n\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\n\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\n\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\n\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\\n\\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\n\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\n\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, output_tokens=400)})\nfull_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"whats 2 + 2\"\n}\n)\nAIMessage(content='4', response_metadata={'id': 'msg_01UAKP81jTZu9fyiyFYhsbHc', 'content': [ContentBlock(text='4', type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=28, output_tokens=5)})\nUsing a RunnableBranch\nâ€‹\nA\nRunnableBranch\nis a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does\nnot\noffer anything that you can't achieve in a custom function as described above, so we recommend using a custom function instead.\nA\nRunnableBranch\nis initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\nIf no provided conditions match, it runs the default runnable.\nHere's an example of what it looks like in action:\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableBranch\nbranch\n=\nRunnableBranch\n(\n(\nlambda\nx\n:\n\"anthropic\"\nin\nx\n[\n\"topic\"\n]\n.\nlower\n(\n)\n,\nanthropic_chain\n)\n,\n(\nlambda\nx\n:\n\"langchain\"\nin\nx\n[\n\"topic\"\n]\n.\nlower\n(\n)\n,\nlangchain_chain\n)\n,\ngeneral_chain\n,\n)\nfull_chain\n=\n{\n\"topic\"\n:\nchain\n,\n\"question\"\n:\nlambda\nx\n:\nx\n[\n\"question\"\n]\n}\n|\nbranch\nfull_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"how do I use Anthropic?\"\n}\n)\nAPI Reference:\nRunnableBranch\nAIMessage(content=\"As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \\n\\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic's technology and services can support your specific needs.\\n\\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We're here to collaborate and help you harness the power of advanced AI responsibly.\", response_metadata={'id': 'msg_0187BVnpniPDJnVvwf3M1LdY', 'content': [ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \\n\\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic's technology and services can support your specific needs.\\n\\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We're here to collaborate and help you harness the power of advanced AI responsibly.\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, output_tokens=160)})\nfull_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"how do I use LangChain?\"\n}\n)\nAIMessage(content=\"As Harrison Chase told me, using LangChain involves several key steps. First, you'll need to install the LangChain library and import the necessary modules. Then, you'll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \\n\\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \\n\\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I'd recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I'm always happy to share more insights from my conversations with Harrison.\", response_metadata={'id': 'msg_01T1naS99wGPkEAP4LME8iAv', 'content': [ContentBlock(text=\"As Harrison Chase told me, using LangChain involves several key steps. First, you'll need to install the LangChain library and import the necessary modules. Then, you'll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \\n\\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \\n\\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I'd recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I'm always happy to share more insights from my conversations with Harrison.\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, output_tokens=205)})\nfull_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"whats 2 + 2\"\n}\n)\nAIMessage(content='4', response_metadata={'id': 'msg_01T6T3TS6hRCtU8JayN93QEi', 'content': [ContentBlock(text='4', type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=28, output_tokens=5)})\nRouting by semantic similarity\nâ€‹\nOne especially useful technique is to use embeddings to route a query to the most relevant prompt. Here's an example.\nfrom\nlangchain_community\n.\nutils\n.\nmath\nimport\ncosine_similarity\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nphysics_template\n=\n\"\"\"You are a very smart physics professor. \\\nYou are great at answering questions about physics in a concise and easy to understand manner. \\\nWhen you don't know the answer to a question you admit that you don't know.\nHere is a question:\n{query}\"\"\"\nmath_template\n=\n\"\"\"You are a very good mathematician. You are great at answering math questions. \\\nYou are so good because you are able to break down hard problems into their component parts, \\\nanswer the component parts, and then put them together to answer the broader question.\nHere is a question:\n{query}\"\"\"\nembeddings\n=\nOpenAIEmbeddings\n(\n)\nprompt_templates\n=\n[\nphysics_template\n,\nmath_template\n]\nprompt_embeddings\n=\nembeddings\n.\nembed_documents\n(\nprompt_templates\n)\ndef\nprompt_router\n(\ninput\n)\n:\nquery_embedding\n=\nembeddings\n.\nembed_query\n(\ninput\n[\n\"query\"\n]\n)\nsimilarity\n=\ncosine_similarity\n(\n[\nquery_embedding\n]\n,\nprompt_embeddings\n)\n[\n0\n]\nmost_similar\n=\nprompt_templates\n[\nsimilarity\n.\nargmax\n(\n)\n]\nprint\n(\n\"Using MATH\"\nif\nmost_similar\n==\nmath_template\nelse\n\"Using PHYSICS\"\n)\nreturn\nPromptTemplate\n.\nfrom_template\n(\nmost_similar\n)\nchain\n=\n(\n{\n\"query\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nRunnableLambda\n(\nprompt_router\n)\n|\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\n|\nStrOutputParser\n(\n)\n)\nAPI Reference:\nStrOutputParser\n|\nPromptTemplate\n|\nRunnableLambda\n|\nRunnablePassthrough\nprint\n(\nchain\n.\ninvoke\n(\n\"What's a black hole\"\n)\n)\nUsing PHYSICS\nAs a physics professor, I would be happy to provide a concise and easy-to-understand explanation of what a black hole is.\nA black hole is an incredibly dense region of space-time where the gravitational pull is so strong that nothing, not even light, can escape from it. This means that if you were to get too close to a black hole, you would be pulled in and crushed by the intense gravitational forces.\nThe formation of a black hole occurs when a massive star, much larger than our Sun, reaches the end of its life and collapses in on itself. This collapse causes the matter to become extremely dense, and the gravitational force becomes so strong that it creates a point of no return, known as the event horizon.\nBeyond the event horizon, the laws of physics as we know them break down, and the intense gravitational forces create a singularity, which is a point of infinite density and curvature in space-time.\nBlack holes are fascinating and mysterious objects, and there is still much to be learned about their properties and behavior. If I were unsure about any specific details or aspects of black holes, I would readily admit that I do not have a complete understanding and would encourage further research and investigation.\nprint\n(\nchain\n.\ninvoke\n(\n\"What's a path integral\"\n)\n)\nUsing MATH\nA path integral is a powerful mathematical concept in physics, particularly in the field of quantum mechanics. It was developed by the renowned physicist Richard Feynman as an alternative formulation of quantum mechanics.\nIn a path integral, instead of considering a single, definite path that a particle might take from one point to another, as in classical mechanics, the particle is considered to take all possible paths simultaneously. Each path is assigned a complex-valued weight, and the total probability amplitude for the particle to go from one point to another is calculated by summing (integrating) over all possible paths.\nThe key ideas behind the path integral formulation are:\n1. Superposition principle: In quantum mechanics, particles can exist in a superposition of multiple states or paths simultaneously.\n2. Probability amplitude: The probability amplitude for a particle to go from one point to another is calculated by summing the complex-valued weights of all possible paths.\n3. Weighting of paths: Each path is assigned a weight based on the action (the time integral of the Lagrangian) along that path. Paths with lower action have a greater weight.\n4. Feynman's approach: Feynman developed the path integral formulation as an alternative to the traditional wave function approach in quantum mechanics, providing a more intuitive and conceptual understanding of quantum phenomena.\nThe path integral approach is particularly useful in quantum field theory, where it provides a powerful framework for calculating transition probabilities and understanding the behavior of quantum systems. It has also found applications in various areas of physics, such as condensed matter, statistical mechanics, and even in finance (the path integral approach to option pricing).\nThe mathematical construction of the path integral involves the use of advanced concepts from functional analysis and measure theory, making it a powerful and sophisticated tool in the physicist's arsenal.\nNext steps\nâ€‹\nYou've now learned how to add routing to your composed LCEL chains.\nNext, check out the other how-to guides on runnables in this section.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/structured_output/",
    "How-to guides\nHow to return structured data from a model\nOn this page\nHow to return structured data from a model\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nFunction/tool calling\nIt is often useful to have a model return output that matches a specific\nschema\n. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.\nThe\n.with_structured_output()\nmethod\nâ€‹\nSupported models\nYou can find a\nlist of models that support this method here\n.\nThis is the easiest and most reliable way to get structured outputs.\nwith_structured_output()\nis implemented for\nmodels that provide native APIs for structuring outputs\n, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\nThis method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or\nmessages\nit outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class,\nJSON Schema\nor a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\nAs an example, let's get a model to generate a joke and separate the setup from the punchline:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nPydantic class\nâ€‹\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\nfrom\ntyping\nimport\nOptional\nfrom\npydantic\nimport\nBaseModel\n,\nField\n# Pydantic\nclass\nJoke\n(\nBaseModel\n)\n:\n\"\"\"Joke to tell user.\"\"\"\nsetup\n:\nstr\n=\nField\n(\ndescription\n=\n\"The setup of the joke\"\n)\npunchline\n:\nstr\n=\nField\n(\ndescription\n=\n\"The punchline to the joke\"\n)\nrating\n:\nOptional\n[\nint\n]\n=\nField\n(\ndefault\n=\nNone\n,\ndescription\n=\n\"How funny the joke is, from 1 to 10\"\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nJoke\n)\nstructured_llm\n.\ninvoke\n(\n\"Tell me a joke about cats\"\n)\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\ntip\nBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time\nwith_structured_output\nis using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\nTypedDict or JSON Schema\nâ€‹\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special\nAnnotated\nsyntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is\nnot\nfilled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\nRequirements\nCore:\nlangchain-core>=0.2.26\nTyping extensions: It is highly recommended to import\nAnnotated\nand\nTypedDict\nfrom\ntyping_extensions\ninstead of\ntyping\nto ensure consistent behavior across Python versions.\nfrom\ntyping\nimport\nOptional\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\n# TypedDict\nclass\nJoke\n(\nTypedDict\n)\n:\n\"\"\"Joke to tell user.\"\"\"\nsetup\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"The setup of the joke\"\n]\n# Alternatively, we could have specified setup as:\n# setup: str                    # no default, no description\n# setup: Annotated[str, ...]    # no default, no description\n# setup: Annotated[str, \"foo\"]  # default, no description\npunchline\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"The punchline of the joke\"\n]\nrating\n:\nAnnotated\n[\nOptional\n[\nint\n]\n,\nNone\n,\n\"How funny the joke is, from 1 to 10\"\n]\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nJoke\n)\nstructured_llm\n.\ninvoke\n(\n\"Tell me a joke about cats\"\n)\n{'setup': 'Why was the cat sitting on the computer?',\n'punchline': 'Because it wanted to keep an eye on the mouse!',\n'rating': 7}\nEquivalently, we can pass in a\nJSON Schema\ndict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\njson_schema\n=\n{\n\"title\"\n:\n\"joke\"\n,\n\"description\"\n:\n\"Joke to tell user.\"\n,\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"setup\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The setup of the joke\"\n,\n}\n,\n\"punchline\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The punchline to the joke\"\n,\n}\n,\n\"rating\"\n:\n{\n\"type\"\n:\n\"integer\"\n,\n\"description\"\n:\n\"How funny the joke is, from 1 to 10\"\n,\n\"default\"\n:\nNone\n,\n}\n,\n}\n,\n\"required\"\n:\n[\n\"setup\"\n,\n\"punchline\"\n]\n,\n}\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\njson_schema\n)\nstructured_llm\n.\ninvoke\n(\n\"Tell me a joke about cats\"\n)\n{'setup': 'Why was the cat sitting on the computer?',\n'punchline': 'Because it wanted to keep an eye on the mouse!',\n'rating': 7}\nChoosing between multiple schemas\nâ€‹\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\nUsing Pydantic\nâ€‹\nfrom\ntyping\nimport\nUnion\nclass\nJoke\n(\nBaseModel\n)\n:\n\"\"\"Joke to tell user.\"\"\"\nsetup\n:\nstr\n=\nField\n(\ndescription\n=\n\"The setup of the joke\"\n)\npunchline\n:\nstr\n=\nField\n(\ndescription\n=\n\"The punchline to the joke\"\n)\nrating\n:\nOptional\n[\nint\n]\n=\nField\n(\ndefault\n=\nNone\n,\ndescription\n=\n\"How funny the joke is, from 1 to 10\"\n)\nclass\nConversationalResponse\n(\nBaseModel\n)\n:\n\"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\nresponse\n:\nstr\n=\nField\n(\ndescription\n=\n\"A conversational response to the user's query\"\n)\nclass\nFinalResponse\n(\nBaseModel\n)\n:\nfinal_output\n:\nUnion\n[\nJoke\n,\nConversationalResponse\n]\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nFinalResponse\n)\nstructured_llm\n.\ninvoke\n(\n\"Tell me a joke about cats\"\n)\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\nstructured_llm\n.\ninvoke\n(\n\"How are you today?\"\n)\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\nUsing TypedDict\nâ€‹\nfrom\ntyping\nimport\nOptional\n,\nUnion\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\nclass\nJoke\n(\nTypedDict\n)\n:\n\"\"\"Joke to tell user.\"\"\"\nsetup\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"The setup of the joke\"\n]\npunchline\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"The punchline of the joke\"\n]\nrating\n:\nAnnotated\n[\nOptional\n[\nint\n]\n,\nNone\n,\n\"How funny the joke is, from 1 to 10\"\n]\nclass\nConversationalResponse\n(\nTypedDict\n)\n:\n\"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\nresponse\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"A conversational response to the user's query\"\n]\nclass\nFinalResponse\n(\nTypedDict\n)\n:\nfinal_output\n:\nUnion\n[\nJoke\n,\nConversationalResponse\n]\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nFinalResponse\n)\nstructured_llm\n.\ninvoke\n(\n\"Tell me a joke about cats\"\n)\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n'punchline': 'Because it wanted to keep an eye on the mouse!',\n'rating': 7}}\nstructured_llm\n.\ninvoke\n(\n\"How are you today?\"\n)\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\nResponses shall be identical to the ones shown in the Pydantic example.\nAlternatively, you can use tool calling directly to allow the model to choose between options, if your\nchosen model supports it\n. This involves a bit more parsing and setup but in some instances leads to better performance because you don't have to use nested schemas. See\nthis how-to guide\nfor more details.\nStreaming\nâ€‹\nWe can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).\ninfo\nNote that what's yielded is already aggregated chunks, not deltas.\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\n# TypedDict\nclass\nJoke\n(\nTypedDict\n)\n:\n\"\"\"Joke to tell user.\"\"\"\nsetup\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"The setup of the joke\"\n]\npunchline\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"The punchline of the joke\"\n]\nrating\n:\nAnnotated\n[\nOptional\n[\nint\n]\n,\nNone\n,\n\"How funny the joke is, from 1 to 10\"\n]\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nJoke\n)\nfor\nchunk\nin\nstructured_llm\n.\nstream\n(\n\"Tell me a joke about cats\"\n)\n:\nprint\n(\nchunk\n)\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why was'}\n{'setup': 'Why was the'}\n{'setup': 'Why was the cat'}\n{'setup': 'Why was the cat sitting'}\n{'setup': 'Why was the cat sitting on'}\n{'setup': 'Why was the cat sitting on the'}\n{'setup': 'Why was the cat sitting on the computer'}\n{'setup': 'Why was the cat sitting on the computer?'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': ''}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\nFew-shot prompting\nâ€‹\nFor more complex schemas it's very useful to add few-shot examples to the prompt. This can be done in a few ways.\nThe simplest and most universal way is to add examples to a system message in the prompt:\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nsystem\n=\n\"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\nReturn a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").\nHere are some examples of jokes:\nexample_user: Tell me a joke about planes\nexample_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}\nexample_user: Tell me another joke about planes\nexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}\nexample_user: Now about caterpillars\nexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n]\n)\nfew_shot_structured_llm\n=\nprompt\n|\nstructured_llm\nfew_shot_structured_llm\n.\ninvoke\n(\n\"what's something funny about woodpeckers\"\n)\nAPI Reference:\nChatPromptTemplate\n{'setup': 'Woodpecker',\n'punchline': \"Woodpecker you a joke, but I'm afraid it might be too 'hole-some'!\",\n'rating': 7}\nWhen the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you're using makes use of tool calling in its API reference.\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nHumanMessage\n,\nToolMessage\nexamples\n=\n[\nHumanMessage\n(\n\"Tell me a joke about planes\"\n,\nname\n=\n\"example_user\"\n)\n,\nAIMessage\n(\n\"\"\n,\nname\n=\n\"example_assistant\"\n,\ntool_calls\n=\n[\n{\n\"name\"\n:\n\"joke\"\n,\n\"args\"\n:\n{\n\"setup\"\n:\n\"Why don't planes ever get tired?\"\n,\n\"punchline\"\n:\n\"Because they have rest wings!\"\n,\n\"rating\"\n:\n2\n,\n}\n,\n\"id\"\n:\n\"1\"\n,\n}\n]\n,\n)\n,\n# Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.\nToolMessage\n(\n\"\"\n,\ntool_call_id\n=\n\"1\"\n)\n,\n# Some models also expect an AIMessage to follow any ToolMessages,\n# so you may need to add an AIMessage here.\nHumanMessage\n(\n\"Tell me another joke about planes\"\n,\nname\n=\n\"example_user\"\n)\n,\nAIMessage\n(\n\"\"\n,\nname\n=\n\"example_assistant\"\n,\ntool_calls\n=\n[\n{\n\"name\"\n:\n\"joke\"\n,\n\"args\"\n:\n{\n\"setup\"\n:\n\"Cargo\"\n,\n\"punchline\"\n:\n\"Cargo 'vroom vroom', but planes go 'zoom zoom'!\"\n,\n\"rating\"\n:\n10\n,\n}\n,\n\"id\"\n:\n\"2\"\n,\n}\n]\n,\n)\n,\nToolMessage\n(\n\"\"\n,\ntool_call_id\n=\n\"2\"\n)\n,\nHumanMessage\n(\n\"Now about caterpillars\"\n,\nname\n=\n\"example_user\"\n)\n,\nAIMessage\n(\n\"\"\n,\nname\n=\n\"example_assistant\"\n,\ntool_calls\n=\n[\n{\n\"name\"\n:\n\"joke\"\n,\n\"args\"\n:\n{\n\"setup\"\n:\n\"Caterpillar\"\n,\n\"punchline\"\n:\n\"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\"\n,\n\"rating\"\n:\n5\n,\n}\n,\n\"id\"\n:\n\"3\"\n,\n}\n]\n,\n)\n,\nToolMessage\n(\n\"\"\n,\ntool_call_id\n=\n\"3\"\n)\n,\n]\nsystem\n=\n\"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\nReturn a joke which has the setup (the response to \"Who's there?\") \\\nand the final punchline (the response to \"<setup> who?\").\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"placeholder\"\n,\n\"{examples}\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n]\n)\nfew_shot_structured_llm\n=\nprompt\n|\nstructured_llm\nfew_shot_structured_llm\n.\ninvoke\n(\n{\n\"input\"\n:\n\"crocodiles\"\n,\n\"examples\"\n:\nexamples\n}\n)\nAPI Reference:\nAIMessage\n|\nHumanMessage\n|\nToolMessage\n{'setup': 'Crocodile',\n'punchline': 'Crocodile be seeing you later, alligator!',\n'rating': 6}\nFor more on few shot prompting when using tool calling, see\nhere\n.\n(Advanced) Specifying the method for structuring outputs\nâ€‹\nFor models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the\nmethod=\nargument.\nJSON mode\nIf using JSON mode you'll have to still specify the desired schema in the model prompt. The schema you pass to\nwith_structured_output\nwill only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.\nTo see if the model you're using supports JSON mode, check its entry in the\nAPI reference\n.\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nNone\n,\nmethod\n=\n\"json_mode\"\n)\nstructured_llm\n.\ninvoke\n(\n\"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n)\n{'setup': 'Why was the cat sitting on the computer?',\n'punchline': 'Because it wanted to keep an eye on the mouse!'}\n(Advanced) Raw outputs\nâ€‹\nLLMs aren't perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing\ninclude_raw=True\n. This changes the output format to contain the raw message output, the\nparsed\nvalue (if successful), and any resulting errors:\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nJoke\n,\ninclude_raw\n=\nTrue\n)\nstructured_llm\n.\ninvoke\n(\n\"Tell me a joke about cats\"\n)\n{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'function': {'arguments': '{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 93, 'total_tokens': 126}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}, 'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 33, 'total_tokens': 126}),\n'parsed': {'setup': 'Why was the cat sitting on the computer?',\n'punchline': 'Because it wanted to keep an eye on the mouse!',\n'rating': 7},\n'parsing_error': None}\nPrompting and parsing model outputs directly\nâ€‹\nNot all models support\n.with_structured_output()\n, since not all models have tool calling or JSON mode support. For such models you'll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\nUsing\nPydanticOutputParser\nâ€‹\nThe following example uses the built-in\nPydanticOutputParser\nto parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding\nformat_instructions\ndirectly to the prompt from a method on the parser:\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nPydanticOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nPerson\n(\nBaseModel\n)\n:\n\"\"\"Information about a person.\"\"\"\nname\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The name of the person\"\n)\nheight_in_meters\n:\nfloat\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The height of the person expressed in meters.\"\n)\nclass\nPeople\n(\nBaseModel\n)\n:\n\"\"\"Identifying information about all people in a text.\"\"\"\npeople\n:\nList\n[\nPerson\n]\n# Set up a parser\nparser\n=\nPydanticOutputParser\n(\npydantic_object\n=\nPeople\n)\n# Prompt\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\"\n,\n)\n,\n(\n\"human\"\n,\n\"{query}\"\n)\n,\n]\n)\n.\npartial\n(\nformat_instructions\n=\nparser\n.\nget_format_instructions\n(\n)\n)\nAPI Reference:\nPydanticOutputParser\n|\nChatPromptTemplate\nLetâ€™s take a look at what information is sent to the model:\nquery\n=\n\"Anna is 23 years old and she is 6 feet tall\"\nprint\n(\nprompt\n.\ninvoke\n(\n{\n\"query\"\n:\nquery\n}\n)\n.\nto_string\n(\n)\n)\nSystem: Answer the user query. Wrap the output in `json` tags\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\nHere is the output schema:\n\\`\\`\\`\n{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\n\\`\\`\\`\nHuman: Anna is 23 years old and she is 6 feet tall\nAnd now let's invoke it:\nchain\n=\nprompt\n|\nllm\n|\nparser\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\nquery\n}\n)\nPeople(people=[Person(name='Anna', height_in_meters=1.8288)])\nFor a deeper dive into using output parsers with prompting techniques for structured output, see\nthis guide\n.\nCustom Parsing\nâ€‹\nYou can also create a custom prompt and parser with\nLangChain Expression Language (LCEL)\n, using a plain function to parse the output from the model:\nimport\njson\nimport\nre\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nPerson\n(\nBaseModel\n)\n:\n\"\"\"Information about a person.\"\"\"\nname\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The name of the person\"\n)\nheight_in_meters\n:\nfloat\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The height of the person expressed in meters.\"\n)\nclass\nPeople\n(\nBaseModel\n)\n:\n\"\"\"Identifying information about all people in a text.\"\"\"\npeople\n:\nList\n[\nPerson\n]\n# Prompt\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"Answer the user query. Output your answer as JSON that  \"\n\"matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \"\n\"Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\"\n,\n)\n,\n(\n\"human\"\n,\n\"{query}\"\n)\n,\n]\n)\n.\npartial\n(\nschema\n=\nPeople\n.\nmodel_json_schema\n(\n)\n)\n# Custom parser\ndef\nextract_json\n(\nmessage\n:\nAIMessage\n)\n-\n>\nList\n[\ndict\n]\n:\n\"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.\nParameters:\ntext (str): The text containing the JSON content.\nReturns:\nlist: A list of extracted JSON strings.\n\"\"\"\ntext\n=\nmessage\n.\ncontent\n# Define the regular expression pattern to match JSON blocks\npattern\n=\nr\"\\`\\`\\`json(.*?)\\`\\`\\`\"\n# Find all non-overlapping matches of the pattern in the string\nmatches\n=\nre\n.\nfindall\n(\npattern\n,\ntext\n,\nre\n.\nDOTALL\n)\n# Return the list of matched JSON strings, stripping any leading or trailing whitespace\ntry\n:\nreturn\n[\njson\n.\nloads\n(\nmatch\n.\nstrip\n(\n)\n)\nfor\nmatch\nin\nmatches\n]\nexcept\nException\n:\nraise\nValueError\n(\nf\"Failed to parse:\n{\nmessage\n}\n\"\n)\nAPI Reference:\nAIMessage\n|\nChatPromptTemplate\nHere is the prompt sent to the model:\nquery\n=\n\"Anna is 23 years old and she is 6 feet tall\"\nprint\n(\nprompt\n.\nformat_prompt\n(\nquery\n=\nquery\n)\n.\nto_string\n(\n)\n)\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\`\\`\\`json\n{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}\n\\`\\`\\`. Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\nHuman: Anna is 23 years old and she is 6 feet tall\nAnd here's what it looks like when we invoke it:\nchain\n=\nprompt\n|\nllm\n|\nextract_json\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\nquery\n}\n)\n[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]\nCombining with Additional Tools\nâ€‹\nWhen you need to use both structured output and additional tools (like web search), note the order of operations:\nCorrect Order\n:\n# 1. Bind tools first\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nweb_search_tool\n,\ncalculator_tool\n]\n)\n# 2. Apply structured output\nstructured_llm\n=\nllm_with_tools\n.\nwith_structured_output\n(\nMySchema\n)\nIncorrect Order\n:\n# This will fail with \"Tool 'MySchema' not found\" error\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nMySchema\n)\nbroken_llm\n=\nstructured_llm\n.\nbind_tools\n(\n[\nweb_search_tool\n]\n)\nWhy Order Matters:\nwith_structured_output()\ninternally uses tool calling to enforce the schema. When you bind additional tools afterward, it creates a conflict in the tool resolution system.\nComplete Example:\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nclass\nSearchResult\n(\nBaseModel\n)\n:\n\"\"\"Structured search result.\"\"\"\nquery\n:\nstr\n=\nField\n(\ndescription\n=\n\"The search query\"\n)\nfindings\n:\nstr\n=\nField\n(\ndescription\n=\n\"Summary of findings\"\n)\n# Define tools\nsearch_tool\n=\n{\n\"type\"\n:\n\"function\"\n,\n\"function\"\n:\n{\n\"name\"\n:\n\"web_search\"\n,\n\"description\"\n:\n\"Search the web for information\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"query\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"Search query\"\n}\n}\n,\n\"required\"\n:\n[\n\"query\"\n]\n,\n}\n,\n}\n,\n}\n# Correct approach\nllm\n=\nChatOpenAI\n(\n)\nllm_with_search\n=\nllm\n.\nbind_tools\n(\n[\nsearch_tool\n]\n)\nstructured_search_llm\n=\nllm_with_search\n.\nwith_structured_output\n(\nSearchResult\n)\n# Now you can use both search and get structured output\nresult\n=\nstructured_search_llm\n.\ninvoke\n(\n\"Search for latest AI research and summarize\"\n)\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/summarize_map_reduce/",
    "How-to guides\nHow to summarize text through parallelization\nOn this page\nHow to summarize text through parallelization\nLLMs can summarize and otherwise distill desired information from text, including large volumes of text. In many cases, especially when the amount of text is large compared to the size of the model's context window, it can be helpful (or necessary) to break up the summarization task into smaller components.\nMap-reduce represents one class of strategies for accomplishing this. The idea is to break the text into \"sub-documents\", and first map each sub-document to an individual summary using an LLM. Then, we reduce or consolidate those summaries into a single global summary.\nNote that the map step is typically parallelized over the input documents. This strategy is especially effective when understanding of a sub-document does not rely on preceeding context. For example, when summarizing a corpus of many, shorter documents.\nLangGraph\n, built on top of\nlangchain-core\n, supports\nmap-reduce\nworkflows and is well-suited to this problem:\nLangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;\nLangGraph's\ncheckpointing\nsupports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.\nThe LangGraph implementation is straightforward to modify and extend.\nBelow, we demonstrate how to summarize text via a map-reduce strategy.\nLoad chat model\nâ€‹\nLet's first load a chat model:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nLoad documents\nâ€‹\nFirst we load in our documents. We will use\nWebBaseLoader\nto load a blog post, and split the documents into smaller sub-documents.\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\ntext_splitter\n=\nCharacterTextSplitter\n.\nfrom_tiktoken_encoder\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n0\n)\nloader\n=\nWebBaseLoader\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n)\ndocs\n=\nloader\n.\nload\n(\n)\nsplit_docs\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\nprint\n(\nf\"Generated\n{\nlen\n(\nsplit_docs\n)\n}\ndocuments.\"\n)\nCreated a chunk of size 1003, which is longer than the specified 1000\n``````output\nGenerated 14 documents.\nCreate graph\nâ€‹\nMap step\nâ€‹\nLet's first define the prompt associated with the map step, and associated it with the LLM via a\nchain\n:\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nmap_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"human\"\n,\n\"Write a concise summary of the following:\\\\n\\\\n{context}\"\n)\n]\n)\nmap_chain\n=\nmap_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\nReduce step\nâ€‹\nWe also define a chain that takes the document mapping results and reduces them into a single output.\nreduce_template\n=\n\"\"\"\nThe following is a set of summaries:\n{docs}\nTake these and distill it into a final, consolidated summary\nof the main themes.\n\"\"\"\nreduce_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\nreduce_template\n)\n]\n)\nreduce_chain\n=\nreduce_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\nOrchestration via LangGraph\nâ€‹\nBelow we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.\nMap-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive \"collapsing\" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.\nWe will need to install\nlanggraph\n:\npip install\n-\nqU langgraph\nimport\noperator\nfrom\ntyping\nimport\nAnnotated\n,\nList\n,\nLiteral\n,\nTypedDict\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\n.\nreduce\nimport\n(\nacollapse_docs\n,\nsplit_list_of_docs\n,\n)\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlanggraph\n.\nconstants\nimport\nSend\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\ntoken_max\n=\n1000\ndef\nlength_function\n(\ndocuments\n:\nList\n[\nDocument\n]\n)\n-\n>\nint\n:\n\"\"\"Get number of tokens for input contents.\"\"\"\nreturn\nsum\n(\nllm\n.\nget_num_tokens\n(\ndoc\n.\npage_content\n)\nfor\ndoc\nin\ndocuments\n)\n# This will be the overall state of the main graph.\n# It will contain the input document contents, corresponding\n# summaries, and a final summary.\nclass\nOverallState\n(\nTypedDict\n)\n:\n# Notice here we use the operator.add\n# This is because we want combine all the summaries we generate\n# from individual nodes back into one list - this is essentially\n# the \"reduce\" part\ncontents\n:\nList\n[\nstr\n]\nsummaries\n:\nAnnotated\n[\nlist\n,\noperator\n.\nadd\n]\ncollapsed_summaries\n:\nList\n[\nDocument\n]\nfinal_summary\n:\nstr\n# This will be the state of the node that we will \"map\" all\n# documents to in order to generate summaries\nclass\nSummaryState\n(\nTypedDict\n)\n:\ncontent\n:\nstr\n# Here we generate a summary, given a document\nasync\ndef\ngenerate_summary\n(\nstate\n:\nSummaryState\n)\n:\nresponse\n=\nawait\nmap_chain\n.\nainvoke\n(\nstate\n[\n\"content\"\n]\n)\nreturn\n{\n\"summaries\"\n:\n[\nresponse\n]\n}\n# Here we define the logic to map out over the documents\n# We will use this an edge in the graph\ndef\nmap_summaries\n(\nstate\n:\nOverallState\n)\n:\n# We will return a list of `Send` objects\n# Each `Send` object consists of the name of a node in the graph\n# as well as the state to send to that node\nreturn\n[\nSend\n(\n\"generate_summary\"\n,\n{\n\"content\"\n:\ncontent\n}\n)\nfor\ncontent\nin\nstate\n[\n\"contents\"\n]\n]\ndef\ncollect_summaries\n(\nstate\n:\nOverallState\n)\n:\nreturn\n{\n\"collapsed_summaries\"\n:\n[\nDocument\n(\nsummary\n)\nfor\nsummary\nin\nstate\n[\n\"summaries\"\n]\n]\n}\n# Add node to collapse summaries\nasync\ndef\ncollapse_summaries\n(\nstate\n:\nOverallState\n)\n:\ndoc_lists\n=\nsplit_list_of_docs\n(\nstate\n[\n\"collapsed_summaries\"\n]\n,\nlength_function\n,\ntoken_max\n)\nresults\n=\n[\n]\nfor\ndoc_list\nin\ndoc_lists\n:\nresults\n.\nappend\n(\nawait\nacollapse_docs\n(\ndoc_list\n,\nreduce_chain\n.\nainvoke\n)\n)\nreturn\n{\n\"collapsed_summaries\"\n:\nresults\n}\n# This represents a conditional edge in the graph that determines\n# if we should collapse the summaries or not\ndef\nshould_collapse\n(\nstate\n:\nOverallState\n,\n)\n-\n>\nLiteral\n[\n\"collapse_summaries\"\n,\n\"generate_final_summary\"\n]\n:\nnum_tokens\n=\nlength_function\n(\nstate\n[\n\"collapsed_summaries\"\n]\n)\nif\nnum_tokens\n>\ntoken_max\n:\nreturn\n\"collapse_summaries\"\nelse\n:\nreturn\n\"generate_final_summary\"\n# Here we will generate the final summary\nasync\ndef\ngenerate_final_summary\n(\nstate\n:\nOverallState\n)\n:\nresponse\n=\nawait\nreduce_chain\n.\nainvoke\n(\nstate\n[\n\"collapsed_summaries\"\n]\n)\nreturn\n{\n\"final_summary\"\n:\nresponse\n}\n# Construct the graph\n# Nodes:\ngraph\n=\nStateGraph\n(\nOverallState\n)\ngraph\n.\nadd_node\n(\n\"generate_summary\"\n,\ngenerate_summary\n)\n# same as before\ngraph\n.\nadd_node\n(\n\"collect_summaries\"\n,\ncollect_summaries\n)\ngraph\n.\nadd_node\n(\n\"collapse_summaries\"\n,\ncollapse_summaries\n)\ngraph\n.\nadd_node\n(\n\"generate_final_summary\"\n,\ngenerate_final_summary\n)\n# Edges:\ngraph\n.\nadd_conditional_edges\n(\nSTART\n,\nmap_summaries\n,\n[\n\"generate_summary\"\n]\n)\ngraph\n.\nadd_edge\n(\n\"generate_summary\"\n,\n\"collect_summaries\"\n)\ngraph\n.\nadd_conditional_edges\n(\n\"collect_summaries\"\n,\nshould_collapse\n)\ngraph\n.\nadd_conditional_edges\n(\n\"collapse_summaries\"\n,\nshould_collapse\n)\ngraph\n.\nadd_edge\n(\n\"generate_final_summary\"\n,\nEND\n)\napp\n=\ngraph\n.\ncompile\n(\n)\nAPI Reference:\nDocument\n|\nSend\n|\nStateGraph\nLangGraph allows the graph structure to be plotted to help visualize its function:\nfrom\nIPython\n.\ndisplay\nimport\nImage\nImage\n(\napp\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\nInvoke graph\nâ€‹\nWhen running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.\nNote that because we have a loop in the graph, it can be helpful to specify a\nrecursion_limit\non its execution. This will raise a specific error when the specified limit is exceeded.\nasync\nfor\nstep\nin\napp\n.\nastream\n(\n{\n\"contents\"\n:\n[\ndoc\n.\npage_content\nfor\ndoc\nin\nsplit_docs\n]\n}\n,\n{\n\"recursion_limit\"\n:\n10\n}\n,\n)\n:\nprint\n(\nlist\n(\nstep\n.\nkeys\n(\n)\n)\n)\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['collect_summaries']\n['collapse_summaries']\n['collapse_summaries']\n['generate_final_summary']\nprint\n(\nstep\n)\n{'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the provided documents highlights the advancements and applications of large language models (LLMs) in artificial intelligence, particularly in autonomous agents and software development. Key themes include:\\n\\n1. **Integration of LLMs**: LLMs play a crucial role in enabling autonomous agents to perform complex tasks through advanced reasoning and decision-making techniques, such as Chain of Thought (CoT) and Tree of Thoughts.\\n\\n2. **Memory Management**: The categorization of memory into sensory, short-term, and long-term types parallels machine learning concepts, with short-term memory facilitating in-context learning and long-term memory enhanced by external storage solutions.\\n\\n3. **Tool Use and APIs**: Autonomous agents utilize external APIs to expand their capabilities, demonstrating adaptability and improved problem-solving skills.\\n\\n4. **Search Algorithms**: Various approximate nearest neighbor search algorithms, including Locality-Sensitive Hashing (LSH) and FAISS, are discussed for enhancing search efficiency in high-dimensional spaces.\\n\\n5. **Neuro-Symbolic Architectures**: The integration of neuro-symbolic systems, such as the MRKL framework, combines expert modules with LLMs to improve problem-solving, particularly in complex tasks.\\n\\n6. **Challenges and Innovations**: The documents address challenges like hallucination and inefficient planning in LLMs, alongside innovative methods such as Chain of Hindsight (CoH) and Algorithm Distillation (AD) for performance enhancement.\\n\\n7. **Software Development Practices**: The use of LLMs in software development is explored, particularly in creating structured applications like a Super Mario game using the model-view-controller (MVC) architecture, emphasizing task management, component organization, and documentation.\\n\\n8. **Limitations of LLMs**: Constraints such as finite context length and challenges in long-term planning are acknowledged, along with concerns regarding the reliability of natural language as an interface.\\n\\nOverall, the integration of LLMs and neuro-symbolic architectures signifies a significant evolution in AI, with ongoing research focused on enhancing planning, memory management, and problem-solving capabilities across various applications.'}}\nNext steps\nâ€‹\nCheck out the\nLangGraph documentation\nfor detail on building with LangGraph, including\nthis guide\non the details of map-reduce in LangGraph.\nSee the summarization\nhow-to guides\nfor additional summarization strategies, including those designed for larger volumes of text.\nSee also\nthis tutorial\nfor more detail on summarization.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/summarize_refine/",
    "How-to guides\nHow to summarize text through iterative refinement\nOn this page\nHow to summarize text through iterative refinement\nLLMs can summarize and otherwise distill desired information from text, including large volumes of text. In many cases, especially when the amount of text is large compared to the size of the model's context window, it can be helpful (or necessary) to break up the summarization task into smaller components.\nIterative refinement represents one strategy for summarizing long texts. The strategy is as follows:\nSplit a text into smaller documents;\nSummarize the first document;\nRefine or update the result based on the next document;\nRepeat through the sequence of documents until finished.\nNote that this strategy is not parallelized. It is especially effective when understanding of a sub-document depends on prior context-- for instance, when summarizing a novel or body of text with an inherent sequence.\nLangGraph\n, built on top of\nlangchain-core\n, is well-suited to this problem:\nLangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;\nLangGraph's\ncheckpointing\nsupports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.\nBecause it is assembled from modular components, it is also simple to extend or modify (e.g., to incorporate\ntool calling\nor other behavior).\nBelow, we demonstrate how to summarize text via iterative refinement.\nLoad chat model\nâ€‹\nLet's first load a chat model:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nLoad documents\nâ€‹\nNext, we need some documents to summarize. Below, we generate some toy documents for illustrative purposes. See the document loader\nhow-to guides\nand\nintegration pages\nfor additional sources of data. The\nsummarization tutorial\nalso includes an example summarizing a blog post.\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\ndocuments\n=\n[\nDocument\n(\npage_content\n=\n\"Apples are red\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"apple_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Blueberries are blue\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"blueberry_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Bananas are yelow\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"banana_book\"\n}\n)\n,\n]\nAPI Reference:\nDocument\nCreate graph\nâ€‹\nBelow we show a LangGraph implementation of this process:\nWe generate a simple chain for the initial summary that plucks out the first document, formats it into a prompt and runs inference with our LLM.\nWe generate a second\nrefine_summary_chain\nthat operates on each successive document, refining the initial summary.\nWe will need to install\nlanggraph\n:\npip install\n-\nqU langgraph\nimport\noperator\nfrom\ntyping\nimport\nList\n,\nLiteral\n,\nTypedDict\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nfrom\nlanggraph\n.\nconstants\nimport\nSend\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\n# Initial summary\nsummarize_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\n\"Write a concise summary of the following: {context}\"\n)\n,\n]\n)\ninitial_summary_chain\n=\nsummarize_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\n# Refining the summary with new docs\nrefine_template\n=\n\"\"\"\nProduce a final summary.\nExisting summary up to this point:\n{existing_answer}\nNew context:\n------------\n{context}\n------------\nGiven the new context, refine the original summary.\n\"\"\"\nrefine_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\nrefine_template\n)\n]\n)\nrefine_summary_chain\n=\nrefine_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\n# We will define the state of the graph to hold the document\n# contents and summary. We also include an index to keep track\n# of our position in the sequence of documents.\nclass\nState\n(\nTypedDict\n)\n:\ncontents\n:\nList\n[\nstr\n]\nindex\n:\nint\nsummary\n:\nstr\n# We define functions for each node, including a node that generates\n# the initial summary:\nasync\ndef\ngenerate_initial_summary\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n:\nsummary\n=\nawait\ninitial_summary_chain\n.\nainvoke\n(\nstate\n[\n\"contents\"\n]\n[\n0\n]\n,\nconfig\n,\n)\nreturn\n{\n\"summary\"\n:\nsummary\n,\n\"index\"\n:\n1\n}\n# And a node that refines the summary based on the next document\nasync\ndef\nrefine_summary\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n:\ncontent\n=\nstate\n[\n\"contents\"\n]\n[\nstate\n[\n\"index\"\n]\n]\nsummary\n=\nawait\nrefine_summary_chain\n.\nainvoke\n(\n{\n\"existing_answer\"\n:\nstate\n[\n\"summary\"\n]\n,\n\"context\"\n:\ncontent\n}\n,\nconfig\n,\n)\nreturn\n{\n\"summary\"\n:\nsummary\n,\n\"index\"\n:\nstate\n[\n\"index\"\n]\n+\n1\n}\n# Here we implement logic to either exit the application or refine\n# the summary.\ndef\nshould_refine\n(\nstate\n:\nState\n)\n-\n>\nLiteral\n[\n\"refine_summary\"\n,\nEND\n]\n:\nif\nstate\n[\n\"index\"\n]\n>=\nlen\n(\nstate\n[\n\"contents\"\n]\n)\n:\nreturn\nEND\nelse\n:\nreturn\n\"refine_summary\"\ngraph\n=\nStateGraph\n(\nState\n)\ngraph\n.\nadd_node\n(\n\"generate_initial_summary\"\n,\ngenerate_initial_summary\n)\ngraph\n.\nadd_node\n(\n\"refine_summary\"\n,\nrefine_summary\n)\ngraph\n.\nadd_edge\n(\nSTART\n,\n\"generate_initial_summary\"\n)\ngraph\n.\nadd_conditional_edges\n(\n\"generate_initial_summary\"\n,\nshould_refine\n)\ngraph\n.\nadd_conditional_edges\n(\n\"refine_summary\"\n,\nshould_refine\n)\napp\n=\ngraph\n.\ncompile\n(\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnableConfig\n|\nSend\n|\nStateGraph\nLangGraph allows the graph structure to be plotted to help visualize its function:\nfrom\nIPython\n.\ndisplay\nimport\nImage\nImage\n(\napp\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\nInvoke graph\nâ€‹\nWe can step through the execution as follows, printing out the summary as it is refined:\nasync\nfor\nstep\nin\napp\n.\nastream\n(\n{\n\"contents\"\n:\n[\ndoc\n.\npage_content\nfor\ndoc\nin\ndocuments\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nif\nsummary\n:=\nstep\n.\nget\n(\n\"summary\"\n)\n:\nprint\n(\nsummary\n)\nApples are characterized by their red color.\nApples are characterized by their red color, while blueberries are known for their blue hue.\nApples are characterized by their red color, blueberries are known for their blue hue, and bananas are recognized for their yellow color.\nThe final\nstep\ncontains the summary as synthesized from the entire set of documents.\nNext steps\nâ€‹\nCheck out the summarization\nhow-to guides\nfor additional summarization strategies, including those designed for larger volumes of text.\nSee\nthis tutorial\nfor more detail on summarization.\nSee also the\nLangGraph documentation\nfor detail on building with LangGraph.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/summarize_stuff/",
    "How-to guides\nHow to summarize text in a single LLM call\nOn this page\nHow to summarize text in a single LLM call\nLLMs can summarize and otherwise distill desired information from text, including large volumes of text. In many cases, especially for models with larger context windows, this can be adequately achieved via a single LLM call.\nLangChain implements a simple\npre-built chain\nthat \"stuffs\" a prompt with the desired context for summarization and other purposes. In this guide we demonstrate how to use the chain.\nLoad chat model\nâ€‹\nLet's first load a\nchat model\n:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nLoad documents\nâ€‹\nNext, we need some documents to summarize. Below, we generate some toy documents for illustrative purposes. See the document loader\nhow-to guides\nand\nintegration pages\nfor additional sources of data. The\nsummarization tutorial\nalso includes an example summarizing a blog post.\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\ndocuments\n=\n[\nDocument\n(\npage_content\n=\n\"Apples are red\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"apple_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Blueberries are blue\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"blueberry_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Bananas are yelow\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"banana_book\"\n}\n)\n,\n]\nAPI Reference:\nDocument\nLoad chain\nâ€‹\nBelow, we define a simple prompt and instantiate the chain with our chat model and documents:\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\nimport\ncreate_stuff_documents_chain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"Summarize this content: {context}\"\n)\nchain\n=\ncreate_stuff_documents_chain\n(\nllm\n,\nprompt\n)\nAPI Reference:\nChatPromptTemplate\nInvoke chain\nâ€‹\nBecause the chain is a\nRunnable\n, it implements the usual methods for invocation:\nresult\n=\nchain\n.\ninvoke\n(\n{\n\"context\"\n:\ndocuments\n}\n)\nresult\n'The content describes the colors of three fruits: apples are red, blueberries are blue, and bananas are yellow.'\nStreaming\nâ€‹\nNote that the chain also supports streaming of individual output tokens:\nfor\nchunk\nin\nchain\n.\nstream\n(\n{\n\"context\"\n:\ndocuments\n}\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"|\"\n)\n|The| content| describes| the| colors| of| three| fruits|:| apples| are| red|,| blueberries| are| blue|,| and| bananas| are| yellow|.||\nNext steps\nâ€‹\nSee the summarization\nhow-to guides\nfor additional summarization strategies, including those designed for larger volumes of text.\nSee also\nthis tutorial\nfor more detail on summarization.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/toolkits/",
    "How-to guides\nHow to use toolkits\nHow to use toolkits\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\nAll Toolkits expose a\nget_tools\nmethod which returns a list of tools.\nYou can therefore do:\n# Initialize a toolkit\ntoolkit\n=\nExampleTookit\n(\n.\n.\n.\n)\n# Get list of tools\ntools\n=\ntoolkit\n.\nget_tools\n(\n)\n# Create agent\nagent\n=\ncreate_agent_method\n(\nllm\n,\ntools\n,\nprompt\n)\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tools_prompting/",
    "How-to guides\nHow to add ad-hoc tool calling capability to LLMs and Chat Models\nOn this page\nHow to add ad-hoc tool calling capability to LLMs and Chat Models\ncaution\nSome models have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the\nhow to use a chat model to call tools\nguide for more information.\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Tools\nFunction/tool calling\nChat models\nLLMs\nIn this guide, we'll see how to add\nad-hoc\ntool calling support to a chat model. This is an alternative method to invoke tools if you're using a model that does not natively support\ntool calling\n.\nWe'll do this by simply writing a prompt that will get the model to invoke the appropriate tools. Here's a diagram of the logic:\nSetup\nâ€‹\nWe'll need to install the following packages:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain langchain\n-\ncommunity\nIf you'd like to use LangSmith, uncomment the below:\nimport\ngetpass\nimport\nos\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nYou can select any of the given models for this how-to guide. Keep in mind that most of these models already\nsupport native tool calling\n, so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the\nhow to use a chat model to call tools\nguide.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nTo illustrate the idea, we'll use\nphi3\nvia Ollama, which does\nNOT\nhave native support for tool calling. If you'd like to use\nOllama\nas well follow\nthese instructions\n.\nfrom\nlangchain_community\n.\nllms\nimport\nOllama\nmodel\n=\nOllama\n(\nmodel\n=\n\"phi3\"\n)\nCreate a tool\nâ€‹\nFirst, let's create an\nadd\nand\nmultiply\ntools. For more information on creating custom tools, please see\nthis guide\n.\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nmultiply\n(\nx\n:\nfloat\n,\ny\n:\nfloat\n)\n-\n>\nfloat\n:\n\"\"\"Multiply two numbers together.\"\"\"\nreturn\nx\n*\ny\n@tool\ndef\nadd\n(\nx\n:\nint\n,\ny\n:\nint\n)\n-\n>\nint\n:\n\"Add two numbers.\"\nreturn\nx\n+\ny\ntools\n=\n[\nmultiply\n,\nadd\n]\n# Let's inspect the tools\nfor\nt\nin\ntools\n:\nprint\n(\n\"--\"\n)\nprint\n(\nt\n.\nname\n)\nprint\n(\nt\n.\ndescription\n)\nprint\n(\nt\n.\nargs\n)\nAPI Reference:\ntool\n--\nmultiply\nMultiply two numbers together.\n{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}\n--\nadd\nAdd two numbers.\n{'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}\nmultiply\n.\ninvoke\n(\n{\n\"x\"\n:\n4\n,\n\"y\"\n:\n5\n}\n)\n20.0\nCreating our prompt\nâ€‹\nWe'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form\n{\"name\": \"...\", \"arguments\": {...}}\n.\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nJsonOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\ntools\nimport\nrender_text_description\nrendered_tools\n=\nrender_text_description\n(\ntools\n)\nprint\n(\nrendered_tools\n)\nAPI Reference:\nJsonOutputParser\n|\nChatPromptTemplate\n|\nrender_text_description\nmultiply(x: float, y: float) -> float - Multiply two numbers together.\nadd(x: int, y: int) -> int - Add two numbers.\nsystem_prompt\n=\nf\"\"\"\\\nYou are an assistant that has access to the following set of tools.\nHere are the names and descriptions for each tool:\n{\nrendered_tools\n}\nGiven the user input, return the name and input of the tool to use.\nReturn your response as a JSON blob with 'name' and 'arguments' keys.\nThe `arguments` should be a dictionary, with keys corresponding\nto the argument names and the values corresponding to the requested values.\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem_prompt\n)\n,\n(\n\"user\"\n,\n\"{input}\"\n)\n]\n)\nchain\n=\nprompt\n|\nmodel\nmessage\n=\nchain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"what's 3 plus 1132\"\n}\n)\n# Let's take a look at the output from the model\n# if the model is an LLM (not a chat model), the output will be a string.\nif\nisinstance\n(\nmessage\n,\nstr\n)\n:\nprint\n(\nmessage\n)\nelse\n:\n# Otherwise it's a chat model\nprint\n(\nmessage\n.\ncontent\n)\n{\n\"name\": \"add\",\n\"arguments\": {\n\"x\": 3,\n\"y\": 1132\n}\n}\nAdding an output parser\nâ€‹\nWe'll use the\nJsonOutputParser\nfor parsing our models output to JSON.\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nJsonOutputParser\nchain\n=\nprompt\n|\nmodel\n|\nJsonOutputParser\n(\n)\nchain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"what's thirteen times 4\"\n}\n)\nAPI Reference:\nJsonOutputParser\n{'name': 'multiply', 'arguments': {'x': 13.0, 'y': 4.0}}\nimportant\nðŸŽ‰ Amazing! ðŸŽ‰ We now instructed our model on how to\nrequest\nthat a tool be invoked.\nNow, let's create some logic to actually run the tool!\nInvoking the tool ðŸƒ\nâ€‹\nNow that the model can request that a tool be invoked, we need to write a function that can actually invoke\nthe tool.\nThe function will select the appropriate tool by name, and pass to it the arguments chosen by the model.\nfrom\ntyping\nimport\nAny\n,\nDict\n,\nOptional\n,\nTypedDict\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nclass\nToolCallRequest\n(\nTypedDict\n)\n:\n\"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\nname\n:\nstr\narguments\n:\nDict\n[\nstr\n,\nAny\n]\ndef\ninvoke_tool\n(\ntool_call_request\n:\nToolCallRequest\n,\nconfig\n:\nOptional\n[\nRunnableConfig\n]\n=\nNone\n)\n:\n\"\"\"A function that we can use the perform a tool invocation.\nArgs:\ntool_call_request: a dict that contains the keys name and arguments.\nThe name must match the name of a tool that exists.\nThe arguments are the arguments to that tool.\nconfig: This is configuration information that LangChain uses that contains\nthings like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\nReturns:\noutput from the requested tool\n\"\"\"\ntool_name_to_tool\n=\n{\ntool\n.\nname\n:\ntool\nfor\ntool\nin\ntools\n}\nname\n=\ntool_call_request\n[\n\"name\"\n]\nrequested_tool\n=\ntool_name_to_tool\n[\nname\n]\nreturn\nrequested_tool\n.\ninvoke\n(\ntool_call_request\n[\n\"arguments\"\n]\n,\nconfig\n=\nconfig\n)\nAPI Reference:\nRunnableConfig\nLet's test this out ðŸ§ª!\ninvoke_tool\n(\n{\n\"name\"\n:\n\"multiply\"\n,\n\"arguments\"\n:\n{\n\"x\"\n:\n3\n,\n\"y\"\n:\n5\n}\n}\n)\n15.0\nLet's put it together\nâ€‹\nLet's put it together into a chain that creates a calculator with add and multiplication capabilities.\nchain\n=\nprompt\n|\nmodel\n|\nJsonOutputParser\n(\n)\n|\ninvoke_tool\nchain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"what's thirteen times 4.14137281\"\n}\n)\n53.83784653\nReturning tool inputs\nâ€‹\nIt can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by\nRunnablePassthrough.assign\n-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nchain\n=\n(\nprompt\n|\nmodel\n|\nJsonOutputParser\n(\n)\n|\nRunnablePassthrough\n.\nassign\n(\noutput\n=\ninvoke_tool\n)\n)\nchain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"what's thirteen times 4.14137281\"\n}\n)\nAPI Reference:\nRunnablePassthrough\n{'name': 'multiply',\n'arguments': {'x': 13, 'y': 4.14137281},\n'output': 53.83784653}\nWhat's next?\nâ€‹\nThis how-to guide shows the \"happy path\" when the model correctly outputs all the required tool information.\nIn reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.\nYou will need to be prepared to add strategies to improve the output from the model; e.g.,\nProvide few shot examples.\nAdd error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output).\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/agent_executor/",
    "How-to guides\nBuild an Agent with AgentExecutor (Legacy)\nOn this page\nBuild an Agent with AgentExecutor (Legacy)\nimportant\nThis section will cover building with the legacy LangChain AgentExecutor. These are fine for getting started, but past a certain point, you will likely want flexibility and control that they do not offer. For working with more advanced agents, we'd recommend checking out\nLangGraph Agents\nor the\nmigration guide\nBy themselves, language models can't take actions - they just output text.\nA big use case for LangChain is creating\nagents\n.\nAgents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be.\nThe results of those actions can then be fed back into the agent and it determines whether more actions are needed, or whether it is okay to finish.\nIn this tutorial, we will build an agent that can interact with multiple different tools: one being a local database, the other being a search engine. You will be able to ask this agent questions, watch it call tools, and have conversations with it.\nConcepts\nâ€‹\nConcepts we will cover are:\nUsing\nlanguage models\n, in particular their tool calling ability\nCreating a\nRetriever\nto expose specific information to our agent\nUsing a Search\nTool\nto look up things online\nChat History\n, which allows a chatbot to \"remember\" past interactions and take them into account when responding to follow-up questions.\nDebugging and tracing your application using\nLangSmith\nSetup\nâ€‹\nJupyter Notebook\nâ€‹\nThis guide (and most of the other guides in the documentation) uses\nJupyter notebooks\nand assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See\nhere\nfor instructions on how to install.\nInstallation\nâ€‹\nTo install LangChain run:\nPip\nConda\npip install langchain\nconda install langchain -c conda-forge\nFor more details, see our\nInstallation guide\n.\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nOr, if in a notebook, you can set them with:\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nDefine tools\nâ€‹\nWe first need to create the tools we want to use. We will use two tools:\nTavily\n(to search online) and then a retriever over a local index we will create\nTavily\nâ€‹\nWe have a built-in tool in LangChain to easily use Tavily search engine as tool.\nNote that this requires an API key - they have a free tier, but if you don't have one or don't want to create one, you can always ignore this step.\nOnce you create your API key, you will need to export that as:\nexport TAVILY_API_KEY=\"...\"\nfrom\nlangchain_community\n.\ntools\n.\ntavily_search\nimport\nTavilySearchResults\nsearch\n=\nTavilySearchResults\n(\nmax_results\n=\n2\n)\nsearch\n.\ninvoke\n(\n\"what is the weather in SF\"\n)\n[{'url': 'https://www.weatherapi.com/',\n'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1714000492, 'localtime': '2024-04-24 16:14'}, 'current': {'last_updated_epoch': 1713999600, 'last_updated': '2024-04-24 16:00', 'temp_c': 15.6, 'temp_f': 60.1, 'is_day': 1, 'condition': {'text': 'Overcast', 'icon': '//cdn.weatherapi.com/weather/64x64/day/122.png', 'code': 1009}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 330, 'wind_dir': 'NNW', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 72, 'cloud': 100, 'feelslike_c': 15.6, 'feelslike_f': 60.1, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 14.8, 'gust_kph': 23.8}}\"},\n{'url': 'https://www.weathertab.com/en/c/e/04/united-states/california/san-francisco/',\n'content': 'San Francisco Weather Forecast for Apr 2024 - Risk of Rain Graph. Rain Risk Graph: Monthly Overview. Bar heights indicate rain risk percentages. Yellow bars mark low-risk days, while black and grey bars signal higher risks. Grey-yellow bars act as buffers, advising to keep at least one day clear from the riskier grey and black days, guiding ...'}]\nRetriever\nâ€‹\nWe will also create a retriever over some data of our own. For a deeper explanation of each step here, see\nthis tutorial\n.\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nloader\n=\nWebBaseLoader\n(\n\"https://docs.smith.langchain.com/overview\"\n)\ndocs\n=\nloader\n.\nload\n(\n)\ndocuments\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\n.\nsplit_documents\n(\ndocs\n)\nvector\n=\nFAISS\n.\nfrom_documents\n(\ndocuments\n,\nOpenAIEmbeddings\n(\n)\n)\nretriever\n=\nvector\n.\nas_retriever\n(\n)\nretriever\n.\ninvoke\n(\n\"how to upload a dataset\"\n)\n[\n0\n]\nDocument(page_content='# The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \\'langsmith\\';import { runOnDataset } from \\'langchain/smith\\';import { EvaluationResult } from \\'langsmith/evaluation\\';const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {    description: \"A sample dataset in LangSmith.\"});await client.createExamples({    inputs: [        { postfix: \"to LangSmith\" },        { postfix: \"to Evaluations in LangSmith\" },    ],    outputs: [        { output: \"Welcome to LangSmith\" },        { output: \"Welcome to Evaluations in LangSmith\" },    ],    datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async ({ run, example }: { run: Run; example?:', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | \\uf8ffÃ¼Â¶ÃºÃ”âˆÃ¨\\uf8ffÃ¼Ãµâ€ Ã”âˆÃ¨ LangSmith', 'description': 'Introduction', 'language': 'en'})\nNow that we have populated our index that we will do doing retrieval over, we can easily turn it into a tool (the format needed for an agent to properly use it)\nfrom\nlangchain\n.\ntools\n.\nretriever\nimport\ncreate_retriever_tool\nretriever_tool\n=\ncreate_retriever_tool\n(\nretriever\n,\n\"langsmith_search\"\n,\n\"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\"\n,\n)\nTools\nâ€‹\nNow that we have created both, we can create a list of tools that we will use downstream.\ntools\n=\n[\nsearch\n,\nretriever_tool\n]\nUsing Language Models\nâ€‹\nNext, let's learn how to use a language model by to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nYou can call the language model by passing in a list of messages. By default, the response is a\ncontent\nstring.\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nresponse\n=\nmodel\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"hi!\"\n)\n]\n)\nresponse\n.\ncontent\nAPI Reference:\nHumanMessage\n'Hello! How can I assist you today?'\nWe can now see what it is like to enable this model to do tool calling. In order to enable that we use\n.bind_tools\nto give the language model knowledge of these tools\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\ntools\n)\nWe can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the\ncontent\nfield as well as the\ntool_calls\nfield.\nresponse\n=\nmodel_with_tools\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"Hi!\"\n)\n]\n)\nprint\n(\nf\"ContentString:\n{\nresponse\n.\ncontent\n}\n\"\n)\nprint\n(\nf\"ToolCalls:\n{\nresponse\n.\ntool_calls\n}\n\"\n)\nContentString: Hello! How can I assist you today?\nToolCalls: []\nNow, let's try calling it with some input that would expect a tool to be called.\nresponse\n=\nmodel_with_tools\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"What's the weather in SF?\"\n)\n]\n)\nprint\n(\nf\"ContentString:\n{\nresponse\n.\ncontent\n}\n\"\n)\nprint\n(\nf\"ToolCalls:\n{\nresponse\n.\ntool_calls\n}\n\"\n)\nContentString:\nToolCalls: [{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_4HteVahXkRAkWjp6dGXryKZX'}]\nWe can see that there's now no content, but there is a tool call! It wants us to call the Tavily Search tool.\nThis isn't calling that tool yet - it's just telling us to. In order to actually calll it, we'll want to create our agent.\nCreate the agent\nâ€‹\nNow that we have defined the tools and the LLM, we can create the agent. We will be using a tool calling agent - for more information on this type of agent, as well as other options, see\nthis guide\n.\nWe can first choose the prompt we want to use to guide the agent.\nIf you want to see the contents of this prompt and have access to LangSmith, you can go to:\nhttps://smith.langchain.com/hub/hwchase17/openai-functions-agent\nfrom\nlangchain\nimport\nhub\n# Get the prompt to use - you can modify this!\nprompt\n=\nhub\n.\npull\n(\n\"hwchase17/openai-functions-agent\"\n)\nprompt\n.\nmessages\n[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\nMessagesPlaceholder(variable_name='chat_history', optional=True),\nHumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\nMessagesPlaceholder(variable_name='agent_scratchpad')]\nNow, we can initialize the agent with the LLM, the prompt, and the tools. The agent is responsible for taking in input and deciding what actions to take. Crucially, the Agent does not execute those actions - that is done by the AgentExecutor (next step). For more information about how to think about these components, see our\nconceptual guide\n.\nNote that we are passing in the\nmodel\n, not\nmodel_with_tools\n. That is because\ncreate_tool_calling_agent\nwill call\n.bind_tools\nfor us under the hood.\nfrom\nlangchain\n.\nagents\nimport\ncreate_tool_calling_agent\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n)\nFinally, we combine the agent (the brains) with the tools inside the AgentExecutor (which will repeatedly call the agent and execute tools).\nfrom\nlangchain\n.\nagents\nimport\nAgentExecutor\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n)\nRun the agent\nâ€‹\nWe can now run the agent on a few queries! Note that for now, these are all\nstateless\nqueries (it won't remember previous interactions).\nFirst up, let's how it responds when there's no need to call a tool:\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"hi!\"\n}\n)\n{'input': 'hi!', 'output': 'Hello! How can I assist you today?'}\nIn order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the\nLangSmith trace\nLet's now try it out on an example where it should be invoking the retriever\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"how can langsmith help with testing?\"\n}\n)\n{'input': 'how can langsmith help with testing?',\n'output': 'LangSmith is a platform that aids in building production-grade Language Learning Model (LLM) applications. It can assist with testing in several ways:\\n\\n1. **Monitoring and Evaluation**: LangSmith allows close monitoring and evaluation of your application. This helps you to ensure the quality of your application and deploy it with confidence.\\n\\n2. **Tracing**: LangSmith has tracing capabilities that can be beneficial for debugging and understanding the behavior of your application.\\n\\n3. **Evaluation Capabilities**: LangSmith has built-in tools for evaluating the performance of your LLM. \\n\\n4. **Prompt Hub**: This is a prompt management tool built into LangSmith that can help in testing different prompts and their responses.\\n\\nPlease note that to use LangSmith, you would need to install it and create an API key. The platform offers Python and Typescript SDKs for utilization. It works independently and does not require the use of LangChain.'}\nLet's take a look at the\nLangSmith trace\nto make sure it's actually calling that.\nNow let's try one where it needs to call the search tool:\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"whats the weather in sf?\"\n}\n)\n{'input': 'whats the weather in sf?',\n'output': 'The current weather in San Francisco is partly cloudy with a temperature of 16.1Â°C (61.0Â°F). The wind is coming from the WNW at a speed of 10.5 mph. The humidity is at 67%. [source](https://www.weatherapi.com/)'}\nWe can check out the\nLangSmith trace\nto make sure it's calling the search tool effectively.\nAdding in memory\nâ€‹\nAs mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in previous\nchat_history\n. Note: it needs to be called\nchat_history\nbecause of the prompt we are using. If we use a different prompt, we could change the variable name\n# Here we pass in an empty list of messages for chat_history because it is the first message in the chat\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"hi! my name is bob\"\n,\n\"chat_history\"\n:\n[\n]\n}\n)\n{'input': 'hi! my name is bob',\n'chat_history': [],\n'output': 'Hello Bob! How can I assist you today?'}\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nHumanMessage\nAPI Reference:\nAIMessage\n|\nHumanMessage\nagent_executor\n.\ninvoke\n(\n{\n\"chat_history\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"hi! my name is bob\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Hello Bob! How can I assist you today?\"\n)\n,\n]\n,\n\"input\"\n:\n\"what's my name?\"\n,\n}\n)\n{'chat_history': [HumanMessage(content='hi! my name is bob'),\nAIMessage(content='Hello Bob! How can I assist you today?')],\n'input': \"what's my name?\",\n'output': 'Your name is Bob. How can I assist you further?'}\nIf we want to keep track of these messages automatically, we can wrap this in a RunnableWithMessageHistory. For more information on how to use this, see\nthis guide\n.\nfrom\nlangchain_community\n.\nchat_message_histories\nimport\nChatMessageHistory\nfrom\nlangchain_core\n.\nchat_history\nimport\nBaseChatMessageHistory\nfrom\nlangchain_core\n.\nrunnables\n.\nhistory\nimport\nRunnableWithMessageHistory\nstore\n=\n{\n}\ndef\nget_session_history\n(\nsession_id\n:\nstr\n)\n-\n>\nBaseChatMessageHistory\n:\nif\nsession_id\nnot\nin\nstore\n:\nstore\n[\nsession_id\n]\n=\nChatMessageHistory\n(\n)\nreturn\nstore\n[\nsession_id\n]\nAPI Reference:\nBaseChatMessageHistory\n|\nRunnableWithMessageHistory\nBecause we have multiple inputs, we need to specify two things:\ninput_messages_key\n: The input key to use to add to the conversation history.\nhistory_messages_key\n: The key to add the loaded messages into.\nagent_with_chat_history\n=\nRunnableWithMessageHistory\n(\nagent_executor\n,\nget_session_history\n,\ninput_messages_key\n=\n\"input\"\n,\nhistory_messages_key\n=\n\"chat_history\"\n,\n)\nagent_with_chat_history\n.\ninvoke\n(\n{\n\"input\"\n:\n\"hi! I'm bob\"\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"session_id\"\n:\n\"<foo>\"\n}\n}\n,\n)\n{'input': \"hi! I'm bob\",\n'chat_history': [],\n'output': 'Hello Bob! How can I assist you today?'}\nagent_with_chat_history\n.\ninvoke\n(\n{\n\"input\"\n:\n\"what's my name?\"\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"session_id\"\n:\n\"<foo>\"\n}\n}\n,\n)\n{'input': \"what's my name?\",\n'chat_history': [HumanMessage(content=\"hi! I'm bob\"),\nAIMessage(content='Hello Bob! How can I assist you today?')],\n'output': 'Your name is Bob.'}\nExample LangSmith trace:\nhttps://smith.langchain.com/public/98c8d162-60ae-4493-aa9f-992d87bd0429/r\nConclusion\nâ€‹\nThat's a wrap! In this quick start we covered how to create a simple agent. Agents are a complex topic, and there's lot to learn!\nimportant\nThis section covered building with LangChain Agents. They are fine for getting started, but past a certain point you will likely want flexibility and control which they do not offer. To develop more advanced agents, we recommend checking out\nLangGraph\nIf you want to continue using LangChain agents, some good advanced guides are:\nHow to use LangGraph's built-in versions of\nAgentExecutor\nHow to create a custom agent\nHow to stream responses from an agent\nHow to return structured output from an agent\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/graph_constructing/",
    "How-to guides\nHow to construct knowledge graphs\nOn this page\nHow to construct knowledge graphs\nIn this guide we'll go over the basic ways of constructing a knowledge graph based on unstructured text. The constructured graph can then be used as knowledge base in a\nRAG\napplication.\nâš ï¸ Security note âš ï¸\nâ€‹\nConstructing knowledge graphs requires executing write access to the database. There are inherent risks in doing this. Make sure that you verify and validate data before importing it. For more on general security best practices,\nsee here\n.\nArchitecture\nâ€‹\nAt a high-level, the steps of constructing a knowledge graph from text are:\nExtracting structured information from text\n: Model is used to extract structured graph information from text.\nStoring into graph database\n: Storing the extracted structured graph information into a graph database enables downstream RAG applications\nSetup\nâ€‹\nFirst, get required packages and set environment variables.\nIn this example, we will be using Neo4j graph database.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain langchain\n-\nneo4j langchain\n-\nopenai langchain\n-\nexperimental neo4j\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nWe default to OpenAI models in this guide.\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\n# Uncomment the below to use LangSmith. Not required.\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\nÂ·Â·Â·Â·Â·Â·Â·Â·\nNext, we need to define Neo4j credentials and connection.\nFollow\nthese installation steps\nto set up a Neo4j database.\nimport\nos\nfrom\nlangchain_neo4j\nimport\nNeo4jGraph\nos\n.\nenviron\n[\n\"NEO4J_URI\"\n]\n=\n\"bolt://localhost:7687\"\nos\n.\nenviron\n[\n\"NEO4J_USERNAME\"\n]\n=\n\"neo4j\"\nos\n.\nenviron\n[\n\"NEO4J_PASSWORD\"\n]\n=\n\"password\"\ngraph\n=\nNeo4jGraph\n(\nrefresh_schema\n=\nFalse\n)\nLLM Graph Transformer\nâ€‹\nExtracting graph data from text enables the transformation of unstructured information into structured formats, facilitating deeper insights and more efficient navigation through complex relationships and patterns. The\nLLMGraphTransformer\nconverts text documents into structured graph documents by leveraging a LLM to parse and categorize entities and their relationships. The selection of the LLM model significantly influences the output by determining the accuracy and nuance of the extracted graph data.\nimport\nos\nfrom\nlangchain_experimental\n.\ngraph_transformers\nimport\nLLMGraphTransformer\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\ntemperature\n=\n0\n,\nmodel_name\n=\n\"gpt-4-turbo\"\n)\nllm_transformer\n=\nLLMGraphTransformer\n(\nllm\n=\nllm\n)\nNow we can pass in example text and examine the results.\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\ntext\n=\n\"\"\"\nMarie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\nShe was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\nHer husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\nShe was, in 1906, the first woman to become a professor at the University of Paris.\n\"\"\"\ndocuments\n=\n[\nDocument\n(\npage_content\n=\ntext\n)\n]\ngraph_documents\n=\nawait\nllm_transformer\n.\naconvert_to_graph_documents\n(\ndocuments\n)\nprint\n(\nf\"Nodes:\n{\ngraph_documents\n[\n0\n]\n.\nnodes\n}\n\"\n)\nprint\n(\nf\"Relationships:\n{\ngraph_documents\n[\n0\n]\n.\nrelationships\n}\n\"\n)\nAPI Reference:\nDocument\nNodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]\nRelationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='MARRIED', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='PROFESSOR', properties={})]\nExamine the following image to better grasp the structure of the generated knowledge graph.\nNote that the graph construction process is non-deterministic since we are using LLM. Therefore, you might get slightly different results on each execution.\nAdditionally, you have the flexibility to define specific types of nodes and relationships for extraction according to your requirements.\nllm_transformer_filtered\n=\nLLMGraphTransformer\n(\nllm\n=\nllm\n,\nallowed_nodes\n=\n[\n\"Person\"\n,\n\"Country\"\n,\n\"Organization\"\n]\n,\nallowed_relationships\n=\n[\n\"NATIONALITY\"\n,\n\"LOCATED_IN\"\n,\n\"WORKED_AT\"\n,\n\"SPOUSE\"\n]\n,\n)\ngraph_documents_filtered\n=\nawait\nllm_transformer_filtered\n.\naconvert_to_graph_documents\n(\ndocuments\n)\nprint\n(\nf\"Nodes:\n{\ngraph_documents_filtered\n[\n0\n]\n.\nnodes\n}\n\"\n)\nprint\n(\nf\"Relationships:\n{\ngraph_documents_filtered\n[\n0\n]\n.\nrelationships\n}\n\"\n)\nNodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]\nRelationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]\nTo define the graph schema more precisely, consider using a three-tuple approach for relationships. In this approach, each tuple consists of three elements: the source node, the relationship type, and the target node.\nallowed_relationships\n=\n[\n(\n\"Person\"\n,\n\"SPOUSE\"\n,\n\"Person\"\n)\n,\n(\n\"Person\"\n,\n\"NATIONALITY\"\n,\n\"Country\"\n)\n,\n(\n\"Person\"\n,\n\"WORKED_AT\"\n,\n\"Organization\"\n)\n,\n]\nllm_transformer_tuple\n=\nLLMGraphTransformer\n(\nllm\n=\nllm\n,\nallowed_nodes\n=\n[\n\"Person\"\n,\n\"Country\"\n,\n\"Organization\"\n]\n,\nallowed_relationships\n=\nallowed_relationships\n,\n)\ngraph_documents_filtered\n=\nawait\nllm_transformer_tuple\n.\naconvert_to_graph_documents\n(\ndocuments\n)\nprint\n(\nf\"Nodes:\n{\ngraph_documents_filtered\n[\n0\n]\n.\nnodes\n}\n\"\n)\nprint\n(\nf\"Relationships:\n{\ngraph_documents_filtered\n[\n0\n]\n.\nrelationships\n}\n\"\n)\nNodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]\nRelationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]\nFor a better understanding of the generated graph, we can again visualize it.\nThe\nnode_properties\nparameter enables the extraction of node properties, allowing the creation of a more detailed graph.\nWhen set to\nTrue\n, LLM autonomously identifies and extracts relevant node properties.\nConversely, if\nnode_properties\nis defined as a list of strings, the LLM selectively retrieves only the specified properties from the text.\nllm_transformer_props\n=\nLLMGraphTransformer\n(\nllm\n=\nllm\n,\nallowed_nodes\n=\n[\n\"Person\"\n,\n\"Country\"\n,\n\"Organization\"\n]\n,\nallowed_relationships\n=\n[\n\"NATIONALITY\"\n,\n\"LOCATED_IN\"\n,\n\"WORKED_AT\"\n,\n\"SPOUSE\"\n]\n,\nnode_properties\n=\n[\n\"born_year\"\n]\n,\n)\ngraph_documents_props\n=\nawait\nllm_transformer_props\n.\naconvert_to_graph_documents\n(\ndocuments\n)\nprint\n(\nf\"Nodes:\n{\ngraph_documents_props\n[\n0\n]\n.\nnodes\n}\n\"\n)\nprint\n(\nf\"Relationships:\n{\ngraph_documents_props\n[\n0\n]\n.\nrelationships\n}\n\"\n)\nNodes:[Node(id='Marie Curie', type='Person', properties={'born_year': '1867'}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={}), Node(id='Poland', type='Country', properties={}), Node(id='France', type='Country', properties={})]\nRelationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Poland', type='Country', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='France', type='Country', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]\nStoring to graph database\nâ€‹\nThe generated graph documents can be stored to a graph database using the\nadd_graph_documents\nmethod.\ngraph\n.\nadd_graph_documents\n(\ngraph_documents_props\n)\nMost graph databases support indexes to optimize data import and retrieval. Since we might not know all the node labels in advance, we can handle this by adding a secondary base label to each node using the\nbaseEntityLabel\nparameter.\ngraph\n.\nadd_graph_documents\n(\ngraph_documents\n,\nbaseEntityLabel\n=\nTrue\n)\nResults will look like:\nThe final option is to also import the source documents for the extracted nodes and relationships. This approach lets us track which documents each entity appeared in.\ngraph\n.\nadd_graph_documents\n(\ngraph_documents\n,\ninclude_source\n=\nTrue\n)\nGraph will have the following structure:\nIn this visualization, the source document is highlighted in blue, with all entities extracted from it connected by\nMENTIONS\nrelationships.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/prompts_partial/",
    "How-to guides\nHow to partially format prompt templates\nOn this page\nHow to partially format prompt templates\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nPrompt templates\nLike partially binding arguments to a function, it can make sense to \"partial\" a\nprompt template\n- e.g. pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.\nLangChain supports this in two ways:\nPartial formatting with string values.\nPartial formatting with functions that return string values.\nIn the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.\nPartial with strings\nâ€‹\nOne common use case for wanting to partial a prompt template is if you get access to some of the variables in a prompt before others. For example, suppose you have a prompt template that requires two variables,\nfoo\nand\nbaz\n. If you get the\nfoo\nvalue early on in your chain, but the\nbaz\nvalue later, it can be inconvenient to pass both variables all the way through the chain. Instead, you can partial the prompt template with the\nfoo\nvalue, and then pass the partialed prompt template along and just use that. Below is an example of doing this:\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nprompt\n=\nPromptTemplate\n.\nfrom_template\n(\n\"{foo}{bar}\"\n)\npartial_prompt\n=\nprompt\n.\npartial\n(\nfoo\n=\n\"foo\"\n)\nprint\n(\npartial_prompt\n.\nformat\n(\nbar\n=\n\"baz\"\n)\n)\nAPI Reference:\nPromptTemplate\nfoobaz\nYou can also just initialize the prompt with the partialed variables.\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"{foo}{bar}\"\n,\ninput_variables\n=\n[\n\"bar\"\n]\n,\npartial_variables\n=\n{\n\"foo\"\n:\n\"foo\"\n}\n)\nprint\n(\nprompt\n.\nformat\n(\nbar\n=\n\"baz\"\n)\n)\nfoobaz\nPartial with functions\nâ€‹\nThe other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables is inconvenient. In this case, it's handy to be able to partial the prompt with a function that always returns the current date.\nfrom\ndatetime\nimport\ndatetime\ndef\n_get_datetime\n(\n)\n:\nnow\n=\ndatetime\n.\nnow\n(\n)\nreturn\nnow\n.\nstrftime\n(\n\"%m/%d/%Y, %H:%M:%S\"\n)\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"Tell me a {adjective} joke about the day {date}\"\n,\ninput_variables\n=\n[\n\"adjective\"\n,\n\"date\"\n]\n,\n)\npartial_prompt\n=\nprompt\n.\npartial\n(\ndate\n=\n_get_datetime\n)\nprint\n(\npartial_prompt\n.\nformat\n(\nadjective\n=\n\"funny\"\n)\n)\nTell me a funny joke about the day 04/21/2024, 19:43:57\nYou can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow.\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"Tell me a {adjective} joke about the day {date}\"\n,\ninput_variables\n=\n[\n\"adjective\"\n]\n,\npartial_variables\n=\n{\n\"date\"\n:\n_get_datetime\n}\n,\n)\nprint\n(\nprompt\n.\nformat\n(\nadjective\n=\n\"funny\"\n)\n)\nTell me a funny joke about the day 04/21/2024, 19:43:57\nNext steps\nâ€‹\nYou've now learned how to partially apply variables to your prompt templates.\nNext, check out the other how-to guides on prompt templates in this section, like\nadding few-shot examples to your prompt templates\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/query_multiple_queries/",
    "How-to guides\nHow to handle multiple queries when doing query analysis\nOn this page\nHow to handle multiple queries when doing query analysis\nSometimes, a query analysis technique may allow for multiple queries to be generated. In these cases, we need to remember to run all queries and then to combine the results. We will show a simple example (using mock data) of how to do that.\nSetup\nâ€‹\nInstall dependencies\nâ€‹\n%\npip install\n-\nqU langchain langchain\n-\ncommunity langchain\n-\nopenai langchain\n-\nchroma\nNote: you may need to restart the kernel to use updated packages.\nSet environment variables\nâ€‹\nWe'll use OpenAI in this example:\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\n# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nCreate Index\nâ€‹\nWe will create a vectorstore over fake information.\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntexts\n=\n[\n\"Harrison worked at Kensho\"\n,\n\"Ankush worked at Facebook\"\n]\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-small\"\n)\nvectorstore\n=\nChroma\n.\nfrom_texts\n(\ntexts\n,\nembeddings\n,\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n}\n)\nQuery analysis\nâ€‹\nWe will use function calling to structure the output. We will let it return multiple queries.\nfrom\ntyping\nimport\nList\n,\nOptional\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nSearch\n(\nBaseModel\n)\n:\n\"\"\"Search over a database of job records.\"\"\"\nqueries\n:\nList\n[\nstr\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Distinct queries to search for\"\n,\n)\nfrom\nlangchain_core\n.\noutput_parsers\n.\nopenai_tools\nimport\nPydanticToolsParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\noutput_parser\n=\nPydanticToolsParser\n(\ntools\n=\n[\nSearch\n]\n)\nsystem\n=\n\"\"\"You have the ability to issue search queries to get information to help answer user information.\nIf you need to look up two distinct pieces of information, you are allowed to do that!\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n]\n)\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nSearch\n)\nquery_analyzer\n=\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nstructured_llm\nAPI Reference:\nPydanticToolsParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\nWe can see that this allows for creating multiple queries\nquery_analyzer\n.\ninvoke\n(\n\"where did Harrison Work\"\n)\nSearch(queries=['Harrison Work', 'Harrison employment history'])\nquery_analyzer\n.\ninvoke\n(\n\"where did Harrison and ankush Work\"\n)\nSearch(queries=['Harrison work history', 'Ankush work history'])\nRetrieval with query analysis\nâ€‹\nSo how would we include this in a chain? One thing that will make this a lot easier is if we call our retriever asynchronously - this will let us loop over the queries and not get blocked on the response time.\nfrom\nlangchain_core\n.\nrunnables\nimport\nchain\nAPI Reference:\nchain\n@chain\nasync\ndef\ncustom_chain\n(\nquestion\n)\n:\nresponse\n=\nawait\nquery_analyzer\n.\nainvoke\n(\nquestion\n)\ndocs\n=\n[\n]\nfor\nquery\nin\nresponse\n.\nqueries\n:\nnew_docs\n=\nawait\nretriever\n.\nainvoke\n(\nquery\n)\ndocs\n.\nextend\n(\nnew_docs\n)\n# You probably want to think about reranking or deduplicating documents here\n# But that is a separate topic\nreturn\ndocs\nawait\ncustom_chain\n.\nainvoke\n(\n\"where did Harrison Work\"\n)\n[Document(page_content='Harrison worked at Kensho'),\nDocument(page_content='Harrison worked at Kensho')]\nawait\ncustom_chain\n.\nainvoke\n(\n\"where did Harrison and ankush Work\"\n)\n[Document(page_content='Harrison worked at Kensho'),\nDocument(page_content='Ankush worked at Facebook')]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tools_builtin/",
    "How-to guides\nHow to use built-in tools and toolkits\nOn this page\nHow to use built-in tools and toolkits\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Tools\nLangChain Toolkits\nTools\nâ€‹\nLangChain has a large collection of 3rd party tools. Please visit\nTool Integrations\nfor a list of the available tools.\nimportant\nWhen using 3rd party tools, make sure that you understand how the tool works, what permissions\nit has. Read over its documentation and check if anything is required from you\nfrom a security point of view. Please see our\nsecurity\nguidelines for more information.\nLet's try out the\nWikipedia integration\n.\n!pip install\n-\nqU langchain\n-\ncommunity wikipedia\nfrom\nlangchain_community\n.\ntools\nimport\nWikipediaQueryRun\nfrom\nlangchain_community\n.\nutilities\nimport\nWikipediaAPIWrapper\napi_wrapper\n=\nWikipediaAPIWrapper\n(\ntop_k_results\n=\n1\n,\ndoc_content_chars_max\n=\n100\n)\ntool\n=\nWikipediaQueryRun\n(\napi_wrapper\n=\napi_wrapper\n)\nprint\n(\ntool\n.\ninvoke\n(\n{\n\"query\"\n:\n\"langchain\"\n}\n)\n)\nPage: LangChain\nSummary: LangChain is a framework designed to simplify the creation of applications\nThe tool has the following defaults associated with it:\nprint\n(\nf\"Name:\n{\ntool\n.\nname\n}\n\"\n)\nprint\n(\nf\"Description:\n{\ntool\n.\ndescription\n}\n\"\n)\nprint\n(\nf\"args schema:\n{\ntool\n.\nargs\n}\n\"\n)\nprint\n(\nf\"returns directly?:\n{\ntool\n.\nreturn_direct\n}\n\"\n)\nName: wikipedia\nDescription: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\nargs schema: {'query': {'description': 'query to look up on wikipedia', 'title': 'Query', 'type': 'string'}}\nreturns directly?: False\nCustomizing Default Tools\nâ€‹\nWe can also modify the built in name, description, and JSON schema of the arguments.\nWhen defining the JSON schema of the arguments, it is important that the inputs remain the same as the function, so you shouldn't change that. But you can define custom descriptions for each input easily.\nfrom\nlangchain_community\n.\ntools\nimport\nWikipediaQueryRun\nfrom\nlangchain_community\n.\nutilities\nimport\nWikipediaAPIWrapper\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nWikiInputs\n(\nBaseModel\n)\n:\n\"\"\"Inputs to the wikipedia tool.\"\"\"\nquery\n:\nstr\n=\nField\n(\ndescription\n=\n\"query to look up in Wikipedia, should be 3 or less words\"\n)\ntool\n=\nWikipediaQueryRun\n(\nname\n=\n\"wiki-tool\"\n,\ndescription\n=\n\"look up things in wikipedia\"\n,\nargs_schema\n=\nWikiInputs\n,\napi_wrapper\n=\napi_wrapper\n,\nreturn_direct\n=\nTrue\n,\n)\nprint\n(\ntool\n.\nrun\n(\n\"langchain\"\n)\n)\nPage: LangChain\nSummary: LangChain is a framework designed to simplify the creation of applications\nprint\n(\nf\"Name:\n{\ntool\n.\nname\n}\n\"\n)\nprint\n(\nf\"Description:\n{\ntool\n.\ndescription\n}\n\"\n)\nprint\n(\nf\"args schema:\n{\ntool\n.\nargs\n}\n\"\n)\nprint\n(\nf\"returns directly?:\n{\ntool\n.\nreturn_direct\n}\n\"\n)\nName: wiki-tool\nDescription: look up things in wikipedia\nargs schema: {'query': {'description': 'query to look up in Wikipedia, should be 3 or less words', 'title': 'Query', 'type': 'string'}}\nreturns directly?: True\nHow to use built-in toolkits\nâ€‹\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\nAll Toolkits expose a\nget_tools\nmethod which returns a list of tools.\nYou're usually meant to use them this way:\n# Initialize a toolkit\ntoolkit\n=\nExampleTookit\n(\n.\n.\n.\n)\n# Get list of tools\ntools\n=\ntoolkit\n.\nget_tools\n(\n)\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/passthrough/",
    "How-to guides\nHow to pass through arguments from one step to the next\nOn this page\nHow to pass through arguments from one step to the next\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Expression Language (LCEL)\nChaining runnables\nCalling runnables in parallel\nCustom functions\nWhen composing chains with several steps, sometimes you will want to pass data from previous steps unchanged for use as input to a later step. The\nRunnablePassthrough\nclass allows you to do just this, and is typically is used in conjunction with a\nRunnableParallel\nto pass data through to a later step in your constructed chains.\nSee the example below:\n%\npip install\n-\nqU langchain langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableParallel\n,\nRunnablePassthrough\nrunnable\n=\nRunnableParallel\n(\npassed\n=\nRunnablePassthrough\n(\n)\n,\nmodified\n=\nlambda\nx\n:\nx\n[\n\"num\"\n]\n+\n1\n,\n)\nrunnable\n.\ninvoke\n(\n{\n\"num\"\n:\n1\n}\n)\nAPI Reference:\nRunnableParallel\n|\nRunnablePassthrough\n{'passed': {'num': 1}, 'modified': 2}\nAs seen above,\npassed\nkey was called with\nRunnablePassthrough()\nand so it simply passed on\n{'num': 1}\n.\nWe also set a second key in the map with\nmodified\n. This uses a lambda to set a single value adding 1 to the num, which resulted in\nmodified\nkey with the value of\n2\n.\nRetrieval Example\nâ€‹\nIn the example below, we see a more real-world use case where we use\nRunnablePassthrough\nalong with\nRunnableParallel\nin a chain to properly format inputs to a prompt:\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\n,\nOpenAIEmbeddings\nvectorstore\n=\nFAISS\n.\nfrom_texts\n(\n[\n\"harrison worked at kensho\"\n]\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\ntemplate\n=\n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nmodel\n=\nChatOpenAI\n(\n)\nretrieval_chain\n=\n(\n{\n\"context\"\n:\nretriever\n,\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n)\nretrieval_chain\n.\ninvoke\n(\n\"where did harrison work?\"\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\n'Harrison worked at Kensho.'\nHere the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key. The\nRunnablePassthrough\nallows us to pass on the user's question to the prompt and model.\nNext steps\nâ€‹\nNow you've learned how to pass data through your chains to help format the data flowing through your chains.\nTo learn more, see the other how-to guides on runnables in this section.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/prompts_composition/",
    "How-to guides\nHow to compose prompts together\nOn this page\nHow to compose prompts together\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nPrompt templates\nLangChain provides a user friendly interface for composing different parts of\nprompts\ntogether. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse of components.\nString prompt composition\nâ€‹\nWhen working with string prompts, each template is joined together. You can work with either prompts directly or strings (the first element in the list needs to be a prompt).\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nprompt\n=\n(\nPromptTemplate\n.\nfrom_template\n(\n\"Tell me a joke about {topic}\"\n)\n+\n\", make it funny\"\n+\n\"\\n\\nand in {language}\"\n)\nprompt\nAPI Reference:\nPromptTemplate\nPromptTemplate(input_variables=['language', 'topic'], template='Tell me a joke about {topic}, make it funny\\n\\nand in {language}')\nprompt\n.\nformat\n(\ntopic\n=\n\"sports\"\n,\nlanguage\n=\n\"spanish\"\n)\n'Tell me a joke about sports, make it funny\\n\\nand in spanish'\nChat prompt composition\nâ€‹\nA chat prompt is made up of a list of messages. Similarly to the above example, we can concatenate chat prompt templates. Each new element is a new message in the final prompt.\nFirst, let's initialize the a\nChatPromptTemplate\nwith a\nSystemMessage\n.\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nHumanMessage\n,\nSystemMessage\nprompt\n=\nSystemMessage\n(\ncontent\n=\n\"You are a nice pirate\"\n)\nAPI Reference:\nAIMessage\n|\nHumanMessage\n|\nSystemMessage\nYou can then easily create a pipeline combining it with other messages\nor\nmessage templates.\nUse a\nMessage\nwhen there is no variables to be formatted, use a\nMessageTemplate\nwhen there are variables to be formatted. You can also use just a string (note: this will automatically get inferred as a\nHumanMessagePromptTemplate\n.)\nnew_prompt\n=\n(\nprompt\n+\nHumanMessage\n(\ncontent\n=\n\"hi\"\n)\n+\nAIMessage\n(\ncontent\n=\n\"what?\"\n)\n+\n\"{input}\"\n)\nUnder the hood, this creates an instance of the ChatPromptTemplate class, so you can use it just as you did before!\nnew_prompt\n.\nformat_messages\n(\ninput\n=\n\"i said hi\"\n)\n[SystemMessage(content='You are a nice pirate'),\nHumanMessage(content='hi'),\nAIMessage(content='what?'),\nHumanMessage(content='i said hi')]\nUsing PipelinePrompt\nâ€‹\nDeprecated\nPipelinePromptTemplate is deprecated; for more information, please refer to\nPipelinePromptTemplate\n.\nLangChain includes a class called\nPipelinePromptTemplate\n, which can be useful when you want to reuse parts of prompts. A PipelinePrompt consists of two main parts:\nFinal prompt: The final prompt that is returned\nPipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name.\nfrom\nlangchain_core\n.\nprompts\nimport\nPipelinePromptTemplate\n,\nPromptTemplate\nfull_template\n=\n\"\"\"{introduction}\n{example}\n{start}\"\"\"\nfull_prompt\n=\nPromptTemplate\n.\nfrom_template\n(\nfull_template\n)\nintroduction_template\n=\n\"\"\"You are impersonating {person}.\"\"\"\nintroduction_prompt\n=\nPromptTemplate\n.\nfrom_template\n(\nintroduction_template\n)\nexample_template\n=\n\"\"\"Here's an example of an interaction:\nQ: {example_q}\nA: {example_a}\"\"\"\nexample_prompt\n=\nPromptTemplate\n.\nfrom_template\n(\nexample_template\n)\nstart_template\n=\n\"\"\"Now, do this for real!\nQ: {input}\nA:\"\"\"\nstart_prompt\n=\nPromptTemplate\n.\nfrom_template\n(\nstart_template\n)\ninput_prompts\n=\n[\n(\n\"introduction\"\n,\nintroduction_prompt\n)\n,\n(\n\"example\"\n,\nexample_prompt\n)\n,\n(\n\"start\"\n,\nstart_prompt\n)\n,\n]\npipeline_prompt\n=\nPipelinePromptTemplate\n(\nfinal_prompt\n=\nfull_prompt\n,\npipeline_prompts\n=\ninput_prompts\n)\npipeline_prompt\n.\ninput_variables\nAPI Reference:\nPipelinePromptTemplate\n|\nPromptTemplate\n['person', 'example_a', 'example_q', 'input']\nprint\n(\npipeline_prompt\n.\nformat\n(\nperson\n=\n\"Elon Musk\"\n,\nexample_q\n=\n\"What's your favorite car?\"\n,\nexample_a\n=\n\"Tesla\"\n,\ninput\n=\n\"What's your favorite social media site?\"\n,\n)\n)\nYou are impersonating Elon Musk.\nHere's an example of an interaction:\nQ: What's your favorite car?\nA: Tesla\nNow, do this for real!\nQ: What's your favorite social media site?\nA:\nNext steps\nâ€‹\nYou've now learned how to compose prompts together.\nNext, check out the other how-to guides on prompt templates in this section, like\nadding few-shot examples to your prompt templates\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/query_multiple_retrievers/",
    "How-to guides\nHow to handle multiple retrievers when doing query analysis\nOn this page\nHow to handle multiple retrievers when doing query analysis\nSometimes, a query analysis technique may allow for selection of which\nretriever\nto use. To use this, you will need to add some logic to select the retriever to do. We will show a simple example (using mock data) of how to do that.\nSetup\nâ€‹\nInstall dependencies\nâ€‹\n%\npip install\n-\nqU langchain langchain\n-\ncommunity langchain\n-\nopenai langchain\n-\nchroma\nNote: you may need to restart the kernel to use updated packages.\nSet environment variables\nâ€‹\nWe'll use OpenAI in this example:\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\n# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nCreate Index\nâ€‹\nWe will create a vectorstore over fake information.\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntexts\n=\n[\n\"Harrison worked at Kensho\"\n]\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-small\"\n)\nvectorstore\n=\nChroma\n.\nfrom_texts\n(\ntexts\n,\nembeddings\n,\ncollection_name\n=\n\"harrison\"\n)\nretriever_harrison\n=\nvectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n}\n)\ntexts\n=\n[\n\"Ankush worked at Facebook\"\n]\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-small\"\n)\nvectorstore\n=\nChroma\n.\nfrom_texts\n(\ntexts\n,\nembeddings\n,\ncollection_name\n=\n\"ankush\"\n)\nretriever_ankush\n=\nvectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n}\n)\nQuery analysis\nâ€‹\nWe will use function calling to structure the output. We will let it return multiple queries.\nfrom\ntyping\nimport\nList\n,\nOptional\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nSearch\n(\nBaseModel\n)\n:\n\"\"\"Search for information about a person.\"\"\"\nquery\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Query to look up\"\n,\n)\nperson\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Person to look things up for. Should be `HARRISON` or `ANKUSH`.\"\n,\n)\nfrom\nlangchain_core\n.\noutput_parsers\n.\nopenai_tools\nimport\nPydanticToolsParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\noutput_parser\n=\nPydanticToolsParser\n(\ntools\n=\n[\nSearch\n]\n)\nsystem\n=\n\"\"\"You have the ability to issue search queries to get information to help answer user information.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n]\n)\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nSearch\n)\nquery_analyzer\n=\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nstructured_llm\nAPI Reference:\nPydanticToolsParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\nWe can see that this allows for routing between retrievers\nquery_analyzer\n.\ninvoke\n(\n\"where did Harrison Work\"\n)\nSearch(query='work history', person='HARRISON')\nquery_analyzer\n.\ninvoke\n(\n\"where did ankush Work\"\n)\nSearch(query='work history', person='ANKUSH')\nRetrieval with query analysis\nâ€‹\nSo how would we include this in a chain? We just need some simple logic to select the retriever and pass in the search query\nfrom\nlangchain_core\n.\nrunnables\nimport\nchain\nAPI Reference:\nchain\nretrievers\n=\n{\n\"HARRISON\"\n:\nretriever_harrison\n,\n\"ANKUSH\"\n:\nretriever_ankush\n,\n}\n@chain\ndef\ncustom_chain\n(\nquestion\n)\n:\nresponse\n=\nquery_analyzer\n.\ninvoke\n(\nquestion\n)\nretriever\n=\nretrievers\n[\nresponse\n.\nperson\n]\nreturn\nretriever\n.\ninvoke\n(\nresponse\n.\nquery\n)\ncustom_chain\n.\ninvoke\n(\n\"where did Harrison Work\"\n)\n[Document(page_content='Harrison worked at Kensho')]\ncustom_chain\n.\ninvoke\n(\n\"where did ankush Work\"\n)\n[Document(page_content='Ankush worked at Facebook')]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/assign/",
    "How-to guides\nHow to add values to a chain's state\nOn this page\nHow to add values to a chain's state\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Expression Language (LCEL)\nChaining runnables\nCalling runnables in parallel\nCustom functions\nPassing data through\nAn alternate way of\npassing data through\nsteps of a chain is to leave the current values of the chain state unchanged while assigning a new value under a given key. The\nRunnablePassthrough.assign()\nstatic method takes an input value and adds the extra arguments passed to the assign function.\nThis is useful in the common\nLangChain Expression Language\npattern of additively creating a dictionary to use as input to a later step.\nHere's an example:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableParallel\n,\nRunnablePassthrough\nrunnable\n=\nRunnableParallel\n(\nextra\n=\nRunnablePassthrough\n.\nassign\n(\nmult\n=\nlambda\nx\n:\nx\n[\n\"num\"\n]\n*\n3\n)\n,\nmodified\n=\nlambda\nx\n:\nx\n[\n\"num\"\n]\n+\n1\n,\n)\nrunnable\n.\ninvoke\n(\n{\n\"num\"\n:\n1\n}\n)\nAPI Reference:\nRunnableParallel\n|\nRunnablePassthrough\n{'extra': {'num': 1, 'mult': 3}, 'modified': 2}\nLet's break down what's happening here.\nThe input to the chain is\n{\"num\": 1}\n. This is passed into a\nRunnableParallel\n, which invokes the runnables it is passed in parallel with that input.\nThe value under the\nextra\nkey is invoked.\nRunnablePassthrough.assign()\nkeeps the original keys in the input dict (\n{\"num\": 1}\n), and assigns a new key called\nmult\n. The value is\nlambda x: x[\"num\"] * 3)\n, which is\n3\n. Thus, the result is\n{\"num\": 1, \"mult\": 3}\n.\n{\"num\": 1, \"mult\": 3}\nis returned to the\nRunnableParallel\ncall, and is set as the value to the key\nextra\n.\nAt the same time, the\nmodified\nkey is called. The result is\n2\n, since the lambda extracts a key called\n\"num\"\nfrom its input and adds one.\nThus, the result is\n{'extra': {'num': 1, 'mult': 3}, 'modified': 2}\n.\nStreaming\nâ€‹\nOne convenient feature of this method is that it allows values to pass through as soon as they are available. To show this off, we'll use\nRunnablePassthrough.assign()\nto immediately return source docs in a retrieval chain:\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\n,\nOpenAIEmbeddings\nvectorstore\n=\nFAISS\n.\nfrom_texts\n(\n[\n\"harrison worked at kensho\"\n]\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\ntemplate\n=\n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nmodel\n=\nChatOpenAI\n(\n)\ngeneration_chain\n=\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\nretrieval_chain\n=\n{\n\"context\"\n:\nretriever\n,\n\"question\"\n:\nRunnablePassthrough\n(\n)\n,\n}\n|\nRunnablePassthrough\n.\nassign\n(\noutput\n=\ngeneration_chain\n)\nstream\n=\nretrieval_chain\n.\nstream\n(\n\"where did harrison work?\"\n)\nfor\nchunk\nin\nstream\n:\nprint\n(\nchunk\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\n{'question': 'where did harrison work?'}\n{'context': [Document(page_content='harrison worked at kensho')]}\n{'output': ''}\n{'output': 'H'}\n{'output': 'arrison'}\n{'output': ' worked'}\n{'output': ' at'}\n{'output': ' Kens'}\n{'output': 'ho'}\n{'output': '.'}\n{'output': ''}\nWe can see that the first chunk contains the original\n\"question\"\nsince that is immediately available. The second chunk contains\n\"context\"\nsince the retriever finishes second. Finally, the output from the\ngeneration_chain\nstreams in chunks as soon as it is available.\nNext steps\nâ€‹\nNow you've learned how to pass data through your chains to help format the data flowing through your chains.\nTo learn more, see the other how-to guides on runnables in this section.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/query_constructing_filters/",
    "How-to guides\nHow to construct filters for query analysis\nHow to construct filters for query analysis\nWe may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.\nThis can be done manually, but LangChain also provides some \"Translators\" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.\nfrom\ntyping\nimport\nOptional\nfrom\nlangchain\n.\nchains\n.\nquery_constructor\n.\nir\nimport\n(\nComparator\n,\nComparison\n,\nOperation\n,\nOperator\n,\nStructuredQuery\n,\n)\nfrom\nlangchain_community\n.\nquery_constructors\n.\nchroma\nimport\nChromaTranslator\nfrom\nlangchain_community\n.\nquery_constructors\n.\nelasticsearch\nimport\nElasticsearchTranslator\nfrom\npydantic\nimport\nBaseModel\nIn this example,\nyear\nand\nauthor\nare both attributes to filter on.\nclass\nSearch\n(\nBaseModel\n)\n:\nquery\n:\nstr\nstart_year\n:\nOptional\n[\nint\n]\nauthor\n:\nOptional\n[\nstr\n]\nsearch_query\n=\nSearch\n(\nquery\n=\n\"RAG\"\n,\nstart_year\n=\n2022\n,\nauthor\n=\n\"LangChain\"\n)\ndef\nconstruct_comparisons\n(\nquery\n:\nSearch\n)\n:\ncomparisons\n=\n[\n]\nif\nquery\n.\nstart_year\nis\nnot\nNone\n:\ncomparisons\n.\nappend\n(\nComparison\n(\ncomparator\n=\nComparator\n.\nGT\n,\nattribute\n=\n\"start_year\"\n,\nvalue\n=\nquery\n.\nstart_year\n,\n)\n)\nif\nquery\n.\nauthor\nis\nnot\nNone\n:\ncomparisons\n.\nappend\n(\nComparison\n(\ncomparator\n=\nComparator\n.\nEQ\n,\nattribute\n=\n\"author\"\n,\nvalue\n=\nquery\n.\nauthor\n,\n)\n)\nreturn\ncomparisons\ncomparisons\n=\nconstruct_comparisons\n(\nsearch_query\n)\n_filter\n=\nOperation\n(\noperator\n=\nOperator\n.\nAND\n,\narguments\n=\ncomparisons\n)\nElasticsearchTranslator\n(\n)\n.\nvisit_operation\n(\n_filter\n)\n{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},\n{'term': {'metadata.author.keyword': 'LangChain'}}]}}\nChromaTranslator\n(\n)\n.\nvisit_operation\n(\n_filter\n)\n{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/configure/",
    "How-to guides\nHow to configure runtime chain internals\nOn this page\nHow to configure runtime chain internals\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nThe Runnable interface\nChaining runnables\nBinding runtime arguments\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things within your chains.\nThis can include tweaking parameters such as temperature or even swapping out one model for another.\nIn order to make this experience as easy as possible, we have defined two methods.\nA\nconfigurable_fields\nmethod. This lets you configure particular fields of a runnable.\nThis is related to the\n.bind\nmethod on runnables, but allows you to specify parameters for a given step in a chain at runtime rather than specifying them beforehand.\nA\nconfigurable_alternatives\nmethod. With this method, you can list out alternatives for any particular runnable that can be set during runtime, and swap them for those specified alternatives.\nConfigurable Fields\nâ€‹\nLet's walk through an example that configures chat model fields like temperature at runtime:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nConfiguring fields on a chat model\nâ€‹\nIf using\ninit_chat_model\nto create a chat model, you can specify configurable fields in the constructor:\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"openai:gpt-4o-mini\"\n,\nconfigurable_fields\n=\n(\n\"temperature\"\n,\n)\n,\n)\nYou can then set the parameter at runtime using\n.with_config\n:\nresponse\n=\nllm\n.\nwith_config\n(\n{\n\"temperature\"\n:\n0\n}\n)\n.\ninvoke\n(\n\"Hello\"\n)\nprint\n(\nresponse\n.\ncontent\n)\nHello! How can I assist you today?\ntip\nIn addition to invocation parameters like temperature, configuring fields this way extends to clients and other attributes.\nUse with tools\nâ€‹\nThis method is applicable when\nbinding tools\nas well:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nget_weather\n(\nlocation\n:\nstr\n)\n:\n\"\"\"Get the weather.\"\"\"\nreturn\n\"It's sunny.\"\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nget_weather\n]\n)\nresponse\n=\nllm_with_tools\n.\nwith_config\n(\n{\n\"temperature\"\n:\n0\n}\n)\n.\ninvoke\n(\n\"What's the weather in SF?\"\n)\nresponse\n.\ntool_calls\nAPI Reference:\ntool\n[{'name': 'get_weather',\n'args': {'location': 'San Francisco'},\n'id': 'call_B93EttzlGyYUhzbIIiMcl3bE',\n'type': 'tool_call'}]\nIn addition to\n.with_config\n, we can now include the parameter when passing a configuration directly. See example below, where we allow the underlying model temperature to be configurable inside of a\nlanggraph agent\n:\n! pip install\n-\n-\nupgrade langgraph\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nagent\n=\ncreate_react_agent\n(\nllm\n,\n[\nget_weather\n]\n)\nresponse\n=\nagent\n.\ninvoke\n(\n{\n\"messages\"\n:\n\"What's the weather in Boston?\"\n}\n,\n{\n\"configurable\"\n:\n{\n\"temperature\"\n:\n0\n}\n}\n,\n)\nAPI Reference:\ncreate_react_agent\nConfiguring fields on arbitrary Runnables\nâ€‹\nYou can also use the\n.configurable_fields\nmethod on arbitrary\nRunnables\n, as shown below:\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\n.\nconfigurable_fields\n(\ntemperature\n=\nConfigurableField\n(\nid\n=\n\"llm_temperature\"\n,\nname\n=\n\"LLM Temperature\"\n,\ndescription\n=\n\"The temperature of the LLM\"\n,\n)\n)\nmodel\n.\ninvoke\n(\n\"pick a random number\"\n)\nAPI Reference:\nPromptTemplate\n|\nConfigurableField\nAIMessage(content='17', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba26a0da-0a69-4533-ab7f-21178a73d303-0')\nAbove, we defined\ntemperature\nas a\nConfigurableField\nthat we can set at runtime. To do so, we use the\nwith_config\nmethod like this:\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm_temperature\"\n:\n0.9\n}\n)\n.\ninvoke\n(\n\"pick a random number\"\n)\nAIMessage(content='12', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba8422ad-be77-4cb1-ac45-ad0aae74e3d9-0')\nNote that the passed\nllm_temperature\nentry in the dict has the same key as the\nid\nof the\nConfigurableField\n.\nWe can also do this to affect just one step that's part of a chain:\nprompt\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Pick a random number above {x}\"\n)\nchain\n=\nprompt\n|\nmodel\nchain\n.\ninvoke\n(\n{\n\"x\"\n:\n0\n}\n)\nAIMessage(content='27', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ecd4cadd-1b72-4f92-b9a0-15e08091f537-0')\nchain\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm_temperature\"\n:\n0.9\n}\n)\n.\ninvoke\n(\n{\n\"x\"\n:\n0\n}\n)\nAIMessage(content='35', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-a916602b-3460-46d3-a4a8-7c926ec747c0-0')\nConfigurable Alternatives\nâ€‹\nThe\nconfigurable_alternatives()\nmethod allows us to swap out steps in a chain with an alternative. Below, we swap out one chat model for another:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\nanthropic\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"ANTHROPIC_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"ANTHROPIC_API_KEY\"\n]\n=\ngetpass\n(\n)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\nYou should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n,\ntemperature\n=\n0\n)\n.\nconfigurable_alternatives\n(\n# This gives this field an id\n# When configuring the end runnable, we can then use this id to configure this field\nConfigurableField\n(\nid\n=\n\"llm\"\n)\n,\n# This sets a default_key.\n# If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\ndefault_key\n=\n\"anthropic\"\n,\n# This adds a new option, with name `openai` that is equal to `ChatOpenAI()`\nopenai\n=\nChatOpenAI\n(\n)\n,\n# This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`\ngpt4\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4\"\n)\n,\n# You can add more configuration options here\n)\nprompt\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Tell me a joke about {topic}\"\n)\nchain\n=\nprompt\n|\nllm\n# By default it will call Anthropic\nchain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\nAPI Reference:\nPromptTemplate\n|\nConfigurableField\nAIMessage(content=\"Here's a bear joke for you:\\n\\nWhy don't bears wear socks? \\nBecause they have bear feet!\\n\\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!\", response_metadata={'id': 'msg_018edUHh5fUbWdiimhrC3dZD', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-775bc58c-28d7-4e6b-a268-48fa6661f02f-0')\n# We can use `.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to use\nchain\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"openai\"\n}\n)\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\nAIMessage(content=\"Why don't bears like fast food?\\n\\nBecause they can't catch it!\", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-7bdaa992-19c9-4f0d-9a0c-1f326bc992d4-0')\n# If we use the `default_key` then it uses the default\nchain\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"anthropic\"\n}\n)\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\nAIMessage(content=\"Here's a bear joke for you:\\n\\nWhy don't bears wear socks? \\nBecause they have bear feet!\\n\\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!\", response_metadata={'id': 'msg_01BZvbmnEPGBtcxRWETCHkct', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-59b6ee44-a1cd-41b8-a026-28ee67cdd718-0')\nWith Prompts\nâ€‹\nWe can do a similar thing, but alternate between prompts\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n,\ntemperature\n=\n0\n)\nprompt\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Tell me a joke about {topic}\"\n)\n.\nconfigurable_alternatives\n(\n# This gives this field an id\n# When configuring the end runnable, we can then use this id to configure this field\nConfigurableField\n(\nid\n=\n\"prompt\"\n)\n,\n# This sets a default_key.\n# If we specify this key, the default prompt (asking for a joke, as initialized above) will be used\ndefault_key\n=\n\"joke\"\n,\n# This adds a new option, with name `poem`\npoem\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Write a short poem about {topic}\"\n)\n,\n# You can add more configuration options here\n)\nchain\n=\nprompt\n|\nllm\n# By default it will write a joke\nchain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\nAIMessage(content=\"Here's a bear joke for you:\\n\\nWhy don't bears wear socks? \\nBecause they have bear feet!\", response_metadata={'id': 'msg_01DtM1cssjNFZYgeS3gMZ49H', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 28}}, id='run-8199af7d-ea31-443d-b064-483693f2e0a1-0')\n# We can configure it write a poem\nchain\n.\nwith_config\n(\nconfigurable\n=\n{\n\"prompt\"\n:\n\"poem\"\n}\n)\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\nAIMessage(content=\"Here is a short poem about bears:\\n\\nMajestic bears, strong and true,\\nRoaming the forests, wild and free.\\nPowerful paws, fur soft and brown,\\nCommanding respect, nature's crown.\\n\\nForaging for berries, fishing streams,\\nProtecting their young, fierce and keen.\\nMighty bears, a sight to behold,\\nGuardians of the wilderness, untold.\\n\\nIn the wild they reign supreme,\\nEmbodying nature's grand theme.\\nBears, a symbol of strength and grace,\\nCaptivating all who see their face.\", response_metadata={'id': 'msg_01Wck3qPxrjURtutvtodaJFn', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 134}}, id='run-69414a1e-51d7-4bec-a307-b34b7d61025e-0')\nWith Prompts and LLMs\nâ€‹\nWe can also have multiple things configurable!\nHere's an example doing that with both prompts and LLMs.\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n,\ntemperature\n=\n0\n)\n.\nconfigurable_alternatives\n(\n# This gives this field an id\n# When configuring the end runnable, we can then use this id to configure this field\nConfigurableField\n(\nid\n=\n\"llm\"\n)\n,\n# This sets a default_key.\n# If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\ndefault_key\n=\n\"anthropic\"\n,\n# This adds a new option, with name `openai` that is equal to `ChatOpenAI()`\nopenai\n=\nChatOpenAI\n(\n)\n,\n# This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`\ngpt4\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4\"\n)\n,\n# You can add more configuration options here\n)\nprompt\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Tell me a joke about {topic}\"\n)\n.\nconfigurable_alternatives\n(\n# This gives this field an id\n# When configuring the end runnable, we can then use this id to configure this field\nConfigurableField\n(\nid\n=\n\"prompt\"\n)\n,\n# This sets a default_key.\n# If we specify this key, the default prompt (asking for a joke, as initialized above) will be used\ndefault_key\n=\n\"joke\"\n,\n# This adds a new option, with name `poem`\npoem\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Write a short poem about {topic}\"\n)\n,\n# You can add more configuration options here\n)\nchain\n=\nprompt\n|\nllm\n# We can configure it write a poem with OpenAI\nchain\n.\nwith_config\n(\nconfigurable\n=\n{\n\"prompt\"\n:\n\"poem\"\n,\n\"llm\"\n:\n\"openai\"\n}\n)\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\nAIMessage(content=\"In the forest deep and wide,\\nBears roam with grace and pride.\\nWith fur as dark as night,\\nThey rule the land with all their might.\\n\\nIn winter's chill, they hibernate,\\nIn spring they emerge, hungry and great.\\nWith claws sharp and eyes so keen,\\nThey hunt for food, fierce and lean.\\n\\nBut beneath their tough exterior,\\nLies a gentle heart, warm and superior.\\nThey love their cubs with all their might,\\nProtecting them through day and night.\\n\\nSo let us admire these majestic creatures,\\nIn awe of their strength and features.\\nFor in the wild, they reign supreme,\\nThe mighty bears, a timeless dream.\", response_metadata={'token_usage': {'completion_tokens': 133, 'prompt_tokens': 13, 'total_tokens': 146}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-5eec0b96-d580-49fd-ac4e-e32a0803b49b-0')\n# We can always just configure only one if we want\nchain\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"openai\"\n}\n)\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\nAIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-c1b14c9c-4988-49b8-9363-15bfd479973a-0')\nSaving configurations\nâ€‹\nWe can also easily save configured chains as their own objects\nopenai_joke\n=\nchain\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"openai\"\n}\n)\nopenai_joke\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\nAIMessage(content=\"Why did the bear break up with his girlfriend? \\nBecause he couldn't bear the relationship anymore!\", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-391ebd55-9137-458b-9a11-97acaff6a892-0')\nNext steps\nâ€‹\nYou now know how to configure a chain's internal steps at runtime.\nTo learn more, see the other how-to guides on runnables in this section, including:\nUsing\n.bind()\nas a simpler way to set a runnable's runtime parameters\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/query_high_cardinality/",
    "How-to guides\nHow to deal with high-cardinality categoricals when doing query analysis\nOn this page\nHow to deal with high-cardinality categoricals when doing query analysis\nYou may want to do query analysis to create a filter on a categorical column. One of the difficulties here is that you usually need to specify the EXACT categorical value. The issue is you need to make sure the LLM generates that categorical value exactly. This can be done relatively easy with prompting when there are only a few values that are valid. When there are a high number of valid values then it becomes more difficult, as those values may not fit in the LLM context, or (if they do) there may be too many for the LLM to properly attend to.\nIn this notebook we take a look at how to approach this.\nSetup\nâ€‹\nInstall dependencies\nâ€‹\n%\npip install\n-\nqU langchain langchain\n-\ncommunity langchain\n-\nopenai faker langchain\n-\nchroma\nNote: you may need to restart the kernel to use updated packages.\nSet environment variables\nâ€‹\nWe'll use OpenAI in this example:\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\n# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nSet up data\nâ€‹\nWe will generate a bunch of fake names\nfrom\nfaker\nimport\nFaker\nfake\n=\nFaker\n(\n)\nnames\n=\n[\nfake\n.\nname\n(\n)\nfor\n_\nin\nrange\n(\n10000\n)\n]\nLet's look at some of the names\nnames\n[\n0\n]\n'Jacob Adams'\nnames\n[\n567\n]\n'Eric Acevedo'\nQuery Analysis\nâ€‹\nWe can now set up a baseline query analysis\nfrom\npydantic\nimport\nBaseModel\n,\nField\n,\nmodel_validator\nclass\nSearch\n(\nBaseModel\n)\n:\nquery\n:\nstr\nauthor\n:\nstr\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\nsystem\n=\n\"\"\"Generate a relevant search query for a library system\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n]\n)\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nSearch\n)\nquery_analyzer\n=\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nstructured_llm\nAPI Reference:\nChatPromptTemplate\n|\nRunnablePassthrough\nWe can see that if we spell the name exactly correctly, it knows how to handle it\nquery_analyzer\n.\ninvoke\n(\n\"what are books about aliens by Jesse Knight\"\n)\nSearch(query='aliens', author='Jesse Knight')\nThe issue is that the values you want to filter on may NOT be spelled exactly correctly\nquery_analyzer\n.\ninvoke\n(\n\"what are books about aliens by jess knight\"\n)\nSearch(query='aliens', author='Jess Knight')\nAdd in all values\nâ€‹\nOne way around this is to add ALL possible values to the prompt. That will generally guide the query in the right direction\nsystem\n=\n\"\"\"Generate a relevant search query for a library system.\n`author` attribute MUST be one of:\n{authors}\nDo NOT hallucinate author name!\"\"\"\nbase_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n]\n)\nprompt\n=\nbase_prompt\n.\npartial\n(\nauthors\n=\n\", \"\n.\njoin\n(\nnames\n)\n)\nquery_analyzer_all\n=\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nstructured_llm\nHowever... if the list of categoricals is long enough, it may error!\ntry\n:\nres\n=\nquery_analyzer_all\n.\ninvoke\n(\n\"what are books about aliens by jess knight\"\n)\nexcept\nException\nas\ne\n:\nprint\n(\ne\n)\nWe can try to use a longer context window... but with so much information in there, it is not garunteed to pick it up reliably\nllm_long\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4-turbo-preview\"\n,\ntemperature\n=\n0\n)\nstructured_llm_long\n=\nllm_long\n.\nwith_structured_output\n(\nSearch\n)\nquery_analyzer_all\n=\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nstructured_llm_long\nquery_analyzer_all\n.\ninvoke\n(\n\"what are books about aliens by jess knight\"\n)\nSearch(query='aliens', author='jess knight')\nFind and all relevant values\nâ€‹\nInstead, what we can do is create an index over the relevant values and then query that for the N most relevant values,\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-small\"\n)\nvectorstore\n=\nChroma\n.\nfrom_texts\n(\nnames\n,\nembeddings\n,\ncollection_name\n=\n\"author_names\"\n)\ndef\nselect_names\n(\nquestion\n)\n:\n_docs\n=\nvectorstore\n.\nsimilarity_search\n(\nquestion\n,\nk\n=\n10\n)\n_names\n=\n[\nd\n.\npage_content\nfor\nd\nin\n_docs\n]\nreturn\n\", \"\n.\njoin\n(\n_names\n)\ncreate_prompt\n=\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n,\n\"authors\"\n:\nselect_names\n,\n}\n|\nbase_prompt\nquery_analyzer_select\n=\ncreate_prompt\n|\nstructured_llm\ncreate_prompt\n.\ninvoke\n(\n\"what are books by jess knight\"\n)\nChatPromptValue(messages=[SystemMessage(content='Generate a relevant search query for a library system.\\n\\n`author` attribute MUST be one of:\\n\\nJennifer Knight, Jill Knight, John Knight, Dr. Jeffrey Knight, Christopher Knight, Andrea Knight, Brandy Knight, Jennifer Keller, Becky Chambers, Sarah Knapp\\n\\nDo NOT hallucinate author name!'), HumanMessage(content='what are books by jess knight')])\nquery_analyzer_select\n.\ninvoke\n(\n\"what are books about aliens by jess knight\"\n)\nSearch(query='books about aliens', author='Jennifer Knight')\nReplace after selection\nâ€‹\nAnother method is to let the LLM fill in whatever value, but then convert that value to a valid value.\nThis can actually be done with the Pydantic class itself!\nclass\nSearch\n(\nBaseModel\n)\n:\nquery\n:\nstr\nauthor\n:\nstr\n@model_validator\n(\nmode\n=\n\"before\"\n)\n@classmethod\ndef\ndouble\n(\ncls\n,\nvalues\n:\ndict\n)\n-\n>\ndict\n:\nauthor\n=\nvalues\n[\n\"author\"\n]\nclosest_valid_author\n=\nvectorstore\n.\nsimilarity_search\n(\nauthor\n,\nk\n=\n1\n)\n[\n0\n]\n.\npage_content\nvalues\n[\n\"author\"\n]\n=\nclosest_valid_author\nreturn\nvalues\nsystem\n=\n\"\"\"Generate a relevant search query for a library system\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n]\n)\ncorrective_structure_llm\n=\nllm\n.\nwith_structured_output\n(\nSearch\n)\ncorrective_query_analyzer\n=\n(\n{\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\ncorrective_structure_llm\n)\ncorrective_query_analyzer\n.\ninvoke\n(\n\"what are books about aliens by jes knight\"\n)\nSearch(query='aliens', author='John Knight')\n# TODO: show trigram similarity\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/document_loader_custom/",
    "How-to guides\nCustom Document Loader\nOn this page\nHow to create a custom Document Loader\nOverview\nâ€‹\nApplications based on LLMs frequently entail extracting data from databases or files, like PDFs, and converting it into a format that LLMs can utilize. In LangChain, this usually involves creating Document objects, which encapsulate the extracted text (\npage_content\n) along with metadataâ€”a dictionary containing details about the document, such as the author's name or the date of publication.\nDocument\nobjects are often formatted into prompts that are fed into an LLM, allowing the LLM to use the information in the\nDocument\nto generate a desired response (e.g., summarizing the document).\nDocuments\ncan be either used immediately or indexed into a vectorstore for future retrieval and use.\nThe main abstractions for\nDocument Loading\nare:\nComponent\nDescription\nDocument\nContains\ntext\nand\nmetadata\nBaseLoader\nUse to convert raw data into\nDocuments\nBlob\nA representation of binary data that's located either in a file or in memory\nBaseBlobParser\nLogic to parse a\nBlob\nto yield\nDocument\nobjects\nThis guide will demonstrate how to write custom document loading and file parsing logic; specifically, we'll see how to:\nCreate a standard document Loader by sub-classing from\nBaseLoader\n.\nCreate a parser using\nBaseBlobParser\nand use it in conjunction with\nBlob\nand\nBlobLoaders\n. This is useful primarily when working with files.\nStandard Document Loader\nâ€‹\nA document loader can be implemented by sub-classing from a\nBaseLoader\nwhich provides a standard interface for loading documents.\nInterface\nâ€‹\nMethod Name\nExplanation\nlazy_load\nUsed to load documents one by one\nlazily\n. Use for production code.\nalazy_load\nAsync variant of\nlazy_load\nload\nUsed to load all the documents into memory\neagerly\n. Use for prototyping or interactive work.\naload\nUsed to load all the documents into memory\neagerly\n. Use for prototyping or interactive work.\nAdded in 2024-04 to LangChain.\nThe\nload\nmethods is a convenience method meant solely for prototyping work -- it just invokes\nlist(self.lazy_load())\n.\nThe\nalazy_load\nhas a default implementation that will delegate to\nlazy_load\n. If you're using async, we recommend overriding the default implementation and providing a native async implementation.\nimportant\nWhen implementing a document loader do\nNOT\nprovide parameters via the\nlazy_load\nor\nalazy_load\nmethods.\nAll configuration is expected to be passed through the initializer (\ninit\n). This was a design choice made by LangChain to make sure that once a document loader has been instantiated it has all the information needed to load documents.\nInstallation\nâ€‹\nInstall\nlangchain-core\nand\nlangchain_community\n.\n%\npip install\n-\nqU langchain_core langchain_community\nImplementation\nâ€‹\nLet's create an example of a standard document loader that loads a file and creates a document from each line in the file.\nfrom\ntyping\nimport\nAsyncIterator\n,\nIterator\nfrom\nlangchain_core\n.\ndocument_loaders\nimport\nBaseLoader\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nclass\nCustomDocumentLoader\n(\nBaseLoader\n)\n:\n\"\"\"An example document loader that reads a file line by line.\"\"\"\ndef\n__init__\n(\nself\n,\nfile_path\n:\nstr\n)\n-\n>\nNone\n:\n\"\"\"Initialize the loader with a file path.\nArgs:\nfile_path: The path to the file to load.\n\"\"\"\nself\n.\nfile_path\n=\nfile_path\ndef\nlazy_load\n(\nself\n)\n-\n>\nIterator\n[\nDocument\n]\n:\n# <-- Does not take any arguments\n\"\"\"A lazy loader that reads a file line by line.\nWhen you're implementing lazy load methods, you should use a generator\nto yield documents one by one.\n\"\"\"\nwith\nopen\n(\nself\n.\nfile_path\n,\nencoding\n=\n\"utf-8\"\n)\nas\nf\n:\nline_number\n=\n0\nfor\nline\nin\nf\n:\nyield\nDocument\n(\npage_content\n=\nline\n,\nmetadata\n=\n{\n\"line_number\"\n:\nline_number\n,\n\"source\"\n:\nself\n.\nfile_path\n}\n,\n)\nline_number\n+=\n1\n# alazy_load is OPTIONAL.\n# If you leave out the implementation, a default implementation which delegates to lazy_load will be used!\nasync\ndef\nalazy_load\n(\nself\n,\n)\n-\n>\nAsyncIterator\n[\nDocument\n]\n:\n# <-- Does not take any arguments\n\"\"\"An async lazy loader that reads a file line by line.\"\"\"\n# Requires aiofiles\n# https://github.com/Tinche/aiofiles\nimport\naiofiles\nasync\nwith\naiofiles\n.\nopen\n(\nself\n.\nfile_path\n,\nencoding\n=\n\"utf-8\"\n)\nas\nf\n:\nline_number\n=\n0\nasync\nfor\nline\nin\nf\n:\nyield\nDocument\n(\npage_content\n=\nline\n,\nmetadata\n=\n{\n\"line_number\"\n:\nline_number\n,\n\"source\"\n:\nself\n.\nfile_path\n}\n,\n)\nline_number\n+=\n1\nAPI Reference:\nBaseLoader\n|\nDocument\nTest ðŸ§ª\nâ€‹\nTo test out the document loader, we need a file with some quality content.\nwith\nopen\n(\n\"./meow.txt\"\n,\n\"w\"\n,\nencoding\n=\n\"utf-8\"\n)\nas\nf\n:\nquality_content\n=\n\"meow meowðŸ± \\n meow meowðŸ± \\n meowðŸ˜»ðŸ˜»\"\nf\n.\nwrite\n(\nquality_content\n)\nloader\n=\nCustomDocumentLoader\n(\n\"./meow.txt\"\n)\n%\npip install\n-\nq aiofiles\n## Test out the lazy load interface\nfor\ndoc\nin\nloader\n.\nlazy_load\n(\n)\n:\nprint\n(\n)\nprint\n(\ntype\n(\ndoc\n)\n)\nprint\n(\ndoc\n)\n<class 'langchain_core.documents.base.Document'>\npage_content='meow meowðŸ±\n' metadata={'line_number': 0, 'source': './meow.txt'}\n<class 'langchain_core.documents.base.Document'>\npage_content=' meow meowðŸ±\n' metadata={'line_number': 1, 'source': './meow.txt'}\n<class 'langchain_core.documents.base.Document'>\npage_content=' meowðŸ˜»ðŸ˜»' metadata={'line_number': 2, 'source': './meow.txt'}\n## Test out the async implementation\nasync\nfor\ndoc\nin\nloader\n.\nalazy_load\n(\n)\n:\nprint\n(\n)\nprint\n(\ntype\n(\ndoc\n)\n)\nprint\n(\ndoc\n)\n<class 'langchain_core.documents.base.Document'>\npage_content='meow meowðŸ±\n' metadata={'line_number': 0, 'source': './meow.txt'}\n<class 'langchain_core.documents.base.Document'>\npage_content=' meow meowðŸ±\n' metadata={'line_number': 1, 'source': './meow.txt'}\n<class 'langchain_core.documents.base.Document'>\npage_content=' meowðŸ˜»ðŸ˜»' metadata={'line_number': 2, 'source': './meow.txt'}\ntip\nload()\ncan be helpful in an interactive environment such as a jupyter notebook.\nAvoid using it for production code since eager loading assumes that all the content\ncan fit into memory, which is not always the case, especially for enterprise data.\nloader\n.\nload\n(\n)\n[Document(metadata={'line_number': 0, 'source': './meow.txt'}, page_content='meow meowðŸ± \\n'),\nDocument(metadata={'line_number': 1, 'source': './meow.txt'}, page_content=' meow meowðŸ± \\n'),\nDocument(metadata={'line_number': 2, 'source': './meow.txt'}, page_content=' meowðŸ˜»ðŸ˜»')]\nWorking with Files\nâ€‹\nMany document loaders involve parsing files. The difference between such loaders usually stems from how the file is parsed, rather than how the file is loaded. For example, you can use\nopen\nto read the binary content of either a PDF or a markdown file, but you need different parsing logic to convert that binary data into text.\nAs a result, it can be helpful to decouple the parsing logic from the loading logic, which makes it easier to re-use a given parser regardless of how the data was loaded.\nBaseBlobParser\nâ€‹\nA\nBaseBlobParser\nis an interface that accepts a\nblob\nand outputs a list of\nDocument\nobjects. A\nblob\nis a representation of data that lives either in memory or in a file. LangChain python has a\nBlob\nprimitive which is inspired by the\nBlob WebAPI spec\n.\nfrom\nlangchain_core\n.\ndocument_loaders\nimport\nBaseBlobParser\n,\nBlob\nclass\nMyParser\n(\nBaseBlobParser\n)\n:\n\"\"\"A simple parser that creates a document from each line.\"\"\"\ndef\nlazy_parse\n(\nself\n,\nblob\n:\nBlob\n)\n-\n>\nIterator\n[\nDocument\n]\n:\n\"\"\"Parse a blob into a document line by line.\"\"\"\nline_number\n=\n0\nwith\nblob\n.\nas_bytes_io\n(\n)\nas\nf\n:\nfor\nline\nin\nf\n:\nline_number\n+=\n1\nyield\nDocument\n(\npage_content\n=\nline\n,\nmetadata\n=\n{\n\"line_number\"\n:\nline_number\n,\n\"source\"\n:\nblob\n.\nsource\n}\n,\n)\nAPI Reference:\nBaseBlobParser\n|\nBlob\nblob\n=\nBlob\n.\nfrom_path\n(\n\"./meow.txt\"\n)\nparser\n=\nMyParser\n(\n)\nlist\n(\nparser\n.\nlazy_parse\n(\nblob\n)\n)\n[Document(metadata={'line_number': 1, 'source': './meow.txt'}, page_content='meow meowðŸ± \\n'),\nDocument(metadata={'line_number': 2, 'source': './meow.txt'}, page_content=' meow meowðŸ± \\n'),\nDocument(metadata={'line_number': 3, 'source': './meow.txt'}, page_content=' meowðŸ˜»ðŸ˜»')]\nUsing the\nblob\nAPI also allows one to load content directly from memory without having to read it from a file!\nblob\n=\nBlob\n(\ndata\n=\nb\"some data from memory\\nmeow\"\n)\nlist\n(\nparser\n.\nlazy_parse\n(\nblob\n)\n)\n[Document(metadata={'line_number': 1, 'source': None}, page_content='some data from memory\\n'),\nDocument(metadata={'line_number': 2, 'source': None}, page_content='meow')]\nBlob\nâ€‹\nLet's take a quick look through some of the Blob API.\nblob\n=\nBlob\n.\nfrom_path\n(\n\"./meow.txt\"\n,\nmetadata\n=\n{\n\"foo\"\n:\n\"bar\"\n}\n)\nblob\n.\nencoding\n'utf-8'\nblob\n.\nas_bytes\n(\n)\nb'meow meow\\xf0\\x9f\\x90\\xb1 \\n meow meow\\xf0\\x9f\\x90\\xb1 \\n meow\\xf0\\x9f\\x98\\xbb\\xf0\\x9f\\x98\\xbb'\nblob\n.\nas_string\n(\n)\n'meow meowðŸ± \\n meow meowðŸ± \\n meowðŸ˜»ðŸ˜»'\nblob\n.\nas_bytes_io\n(\n)\n<contextlib._GeneratorContextManager at 0x74b8d42e9940>\nblob\n.\nmetadata\n{'foo': 'bar'}\nblob\n.\nsource\n'./meow.txt'\nBlob Loaders\nâ€‹\nWhile a parser encapsulates the logic needed to parse binary data into documents,\nblob loaders\nencapsulate the logic that's necessary to load blobs from a given storage location.\nAt the moment,\nLangChain\nsupports\nFileSystemBlobLoader\nand\nCloudBlobLoader\n.\nYou can use the\nFileSystemBlobLoader\nto load blobs and then use the parser to parse them.\nfrom\nlangchain_community\n.\ndocument_loaders\n.\nblob_loaders\nimport\nFileSystemBlobLoader\nfilesystem_blob_loader\n=\nFileSystemBlobLoader\n(\npath\n=\n\".\"\n,\nglob\n=\n\"*.mdx\"\n,\nshow_progress\n=\nTrue\n)\n%\npip install\n-\nq tqdm\nparser\n=\nMyParser\n(\n)\nfor\nblob\nin\nfilesystem_blob_loader\n.\nyield_blobs\n(\n)\n:\nfor\ndoc\nin\nparser\n.\nlazy_parse\n(\nblob\n)\n:\nprint\n(\ndoc\n)\nbreak\nOr, you can use\nCloudBlobLoader\nto load blobs from a cloud storage location (Supports s3://, az://, gs://, file:// schemes).\n%\npip install\n-\nq\n'cloudpathlib[s3]'\nfrom\ncloudpathlib\nimport\nS3Client\n,\nS3Path\nfrom\nlangchain_community\n.\ndocument_loaders\n.\nblob_loaders\nimport\nCloudBlobLoader\nclient\n=\nS3Client\n(\nno_sign_request\n=\nTrue\n)\nclient\n.\nset_as_default_client\n(\n)\npath\n=\nS3Path\n(\n\"s3://bucket-01\"\n,\nclient\n=\nclient\n)\n# Supports s3://, az://, gs://, file:// schemes.\ncloud_loader\n=\nCloudBlobLoader\n(\npath\n,\nglob\n=\n\"**/*.pdf\"\n,\nshow_progress\n=\nTrue\n)\nfor\nblob\nin\ncloud_loader\n.\nyield_blobs\n(\n)\n:\nprint\n(\nblob\n)\nGeneric Loader\n\u0000â€‹\nLangChain has a\nGenericLoader\nabstraction which composes a\nBlobLoader\nwith a\nBaseBlobParser\n.\nGenericLoader\nis meant to provide standardized classmethods that make it easy to use existing\nBlobLoader\nimplementations. At the moment, the\nFileSystemBlobLoader\nand\nCloudBlobLoader\nare supported. See example below:\nfrom\nlangchain_community\n.\ndocument_loaders\n.\ngeneric\nimport\nGenericLoader\ngeneric_loader_filesystem\n=\nGenericLoader\n(\nblob_loader\n=\nfilesystem_blob_loader\n,\nblob_parser\n=\nparser\n)\nfor\nidx\n,\ndoc\nin\nenumerate\n(\ngeneric_loader_filesystem\n.\nlazy_load\n(\n)\n)\n:\nif\nidx\n<\n5\n:\nprint\n(\ndoc\n)\nprint\n(\n\"... output truncated for demo purposes\"\n)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 1224.82it/s]\n``````output\npage_content='# Text embedding models\n' metadata={'line_number': 1, 'source': 'embed_text.mdx'}\npage_content='\n' metadata={'line_number': 2, 'source': 'embed_text.mdx'}\npage_content=':::info\n' metadata={'line_number': 3, 'source': 'embed_text.mdx'}\npage_content='Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.\n' metadata={'line_number': 4, 'source': 'embed_text.mdx'}\npage_content=':::\n' metadata={'line_number': 5, 'source': 'embed_text.mdx'}\n... output truncated for demo purposes\nCustom Generic Loader\nâ€‹\nIf you really like creating classes, you can sub-class and create a class to encapsulate the logic together.\nYou can sub-class from this class to load content using an existing loader.\nfrom\ntyping\nimport\nAny\nclass\nMyCustomLoader\n(\nGenericLoader\n)\n:\n@staticmethod\ndef\nget_parser\n(\n**\nkwargs\n:\nAny\n)\n-\n>\nBaseBlobParser\n:\n\"\"\"Override this method to associate a default parser with the class.\"\"\"\nreturn\nMyParser\n(\n)\nloader\n=\nMyCustomLoader\n.\nfrom_filesystem\n(\npath\n=\n\".\"\n,\nglob\n=\n\"*.mdx\"\n,\nshow_progress\n=\nTrue\n)\nfor\nidx\n,\ndoc\nin\nenumerate\n(\nloader\n.\nlazy_load\n(\n)\n)\n:\nif\nidx\n<\n5\n:\nprint\n(\ndoc\n)\nprint\n(\n\"... output truncated for demo purposes\"\n)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 814.86it/s]\n``````output\npage_content='# Text embedding models\n' metadata={'line_number': 1, 'source': 'embed_text.mdx'}\npage_content='\n' metadata={'line_number': 2, 'source': 'embed_text.mdx'}\npage_content=':::info\n' metadata={'line_number': 3, 'source': 'embed_text.mdx'}\npage_content='Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.\n' metadata={'line_number': 4, 'source': 'embed_text.mdx'}\npage_content=':::\n' metadata={'line_number': 5, 'source': 'embed_text.mdx'}\n... output truncated for demo purposes\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/MultiQueryRetriever/",
    "How-to guides\nHow to use the MultiQueryRetriever\nOn this page\nHow to use the MultiQueryRetriever\nDistance-based\nvector database\nretrieval\nembeds\n(represents) queries in high-dimensional space and finds similar embedded documents based on a distance metric. But, retrieval may produce different results with subtle changes in query wording, or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\nThe\nMultiQueryRetriever\nautomates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the\nMultiQueryRetriever\ncan mitigate some of the limitations of the distance-based retrieval and get a richer set of results.\nLet's build a vectorstore using the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng from the\nRAG tutorial\n:\n# Build a sample vectorDB\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load blog post\nloader\n=\nWebBaseLoader\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n)\ndata\n=\nloader\n.\nload\n(\n)\n# Split\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n500\n,\nchunk_overlap\n=\n0\n)\nsplits\n=\ntext_splitter\n.\nsplit_documents\n(\ndata\n)\n# VectorDB\nembedding\n=\nOpenAIEmbeddings\n(\n)\nvectordb\n=\nChroma\n.\nfrom_documents\n(\ndocuments\n=\nsplits\n,\nembedding\n=\nembedding\n)\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\nSimple usage\nâ€‹\nSpecify the LLM to use for query generation, and the retriever will do the rest.\nfrom\nlangchain\n.\nretrievers\n.\nmulti_query\nimport\nMultiQueryRetriever\nfrom\nlangchain_openai\nimport\nChatOpenAI\nquestion\n=\n\"What are the approaches to Task Decomposition?\"\nllm\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\nretriever_from_llm\n=\nMultiQueryRetriever\n.\nfrom_llm\n(\nretriever\n=\nvectordb\n.\nas_retriever\n(\n)\n,\nllm\n=\nllm\n)\n# Set logging for the queries\nimport\nlogging\nlogging\n.\nbasicConfig\n(\n)\nlogging\n.\ngetLogger\n(\n\"langchain.retrievers.multi_query\"\n)\n.\nsetLevel\n(\nlogging\n.\nINFO\n)\nunique_docs\n=\nretriever_from_llm\n.\ninvoke\n(\nquestion\n)\nlen\n(\nunique_docs\n)\nINFO:langchain.retrievers.multi_query:Generated queries: ['1. How can Task Decomposition be achieved through different methods?', '2. What strategies are commonly used for Task Decomposition?', '3. What are the various ways to break down tasks in Task Decomposition?']\n5\nNote that the underlying queries generated by the\nretriever\nare logged at the\nINFO\nlevel.\nSupplying your own prompt\nâ€‹\nUnder the hood,\nMultiQueryRetriever\ngenerates queries using a specific\nprompt\n. To customize this prompt:\nMake a\nPromptTemplate\nwith an input variable for the question;\nImplement an\noutput parser\nlike the one below to split the result into a list of queries.\nThe prompt and output parser together must support the generation of a list of queries.\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nBaseOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\npydantic\nimport\nBaseModel\n,\nField\n# Output parser will split the LLM result into a list of queries\nclass\nLineListOutputParser\n(\nBaseOutputParser\n[\nList\n[\nstr\n]\n]\n)\n:\n\"\"\"Output parser for a list of lines.\"\"\"\ndef\nparse\n(\nself\n,\ntext\n:\nstr\n)\n-\n>\nList\n[\nstr\n]\n:\nlines\n=\ntext\n.\nstrip\n(\n)\n.\nsplit\n(\n\"\\n\"\n)\nreturn\nlist\n(\nfilter\n(\nNone\n,\nlines\n)\n)\n# Remove empty lines\noutput_parser\n=\nLineListOutputParser\n(\n)\nQUERY_PROMPT\n=\nPromptTemplate\n(\ninput_variables\n=\n[\n\"question\"\n]\n,\ntemplate\n=\n\"\"\"You are an AI language model assistant. Your task is to generate five\ndifferent versions of the given user question to retrieve relevant documents from a vector\ndatabase. By generating multiple perspectives on the user question, your goal is to help\nthe user overcome some of the limitations of the distance-based similarity search.\nProvide these alternative questions separated by newlines.\nOriginal question: {question}\"\"\"\n,\n)\nllm\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\n# Chain\nllm_chain\n=\nQUERY_PROMPT\n|\nllm\n|\noutput_parser\n# Other inputs\nquestion\n=\n\"What are the approaches to Task Decomposition?\"\nAPI Reference:\nBaseOutputParser\n|\nPromptTemplate\n# Run\nretriever\n=\nMultiQueryRetriever\n(\nretriever\n=\nvectordb\n.\nas_retriever\n(\n)\n,\nllm_chain\n=\nllm_chain\n,\nparser_key\n=\n\"lines\"\n)\n# \"lines\" is the key (attribute name) of the parsed output\n# Results\nunique_docs\n=\nretriever\n.\ninvoke\n(\n\"What does the course say about regression?\"\n)\nlen\n(\nunique_docs\n)\nINFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide insights on regression from the course material?', '2. How is regression discussed in the course content?', '3. What information does the course offer regarding regression?', '4. In what way is regression covered in the course?', \"5. What are the course's teachings on regression?\"]\n9\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/add_scores_retriever/",
    "How-to guides\nHow to add scores to retriever results\nOn this page\nHow to add scores to retriever results\nRetrievers\nwill return sequences of\nDocument\nobjects, which by default include no information about the process that retrieved them (e.g., a similarity score against a query). Here we demonstrate how to add retrieval scores to the\n.metadata\nof documents:\nFrom\nvectorstore retrievers\n;\nFrom higher-order LangChain retrievers, such as\nSelfQueryRetriever\nor\nMultiVectorRetriever\n.\nFor (1), we will implement a short wrapper function around the corresponding\nvector store\n. For (2), we will update a method of the corresponding class.\nCreate vector store\nâ€‹\nFirst we populate a vector store with some data. We will use a\nPineconeVectorStore\n, but this guide is compatible with any LangChain vector store that implements a\n.similarity_search_with_score\nmethod.\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\ndocs\n=\n[\nDocument\n(\npage_content\n=\n\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\"\n,\nmetadata\n=\n{\n\"year\"\n:\n1993\n,\n\"rating\"\n:\n7.7\n,\n\"genre\"\n:\n\"science fiction\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\"\n,\nmetadata\n=\n{\n\"year\"\n:\n2010\n,\n\"director\"\n:\n\"Christopher Nolan\"\n,\n\"rating\"\n:\n8.2\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\"\n,\nmetadata\n=\n{\n\"year\"\n:\n2006\n,\n\"director\"\n:\n\"Satoshi Kon\"\n,\n\"rating\"\n:\n8.6\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"A bunch of normal-sized women are supremely wholesome and some men pine after them\"\n,\nmetadata\n=\n{\n\"year\"\n:\n2019\n,\n\"director\"\n:\n\"Greta Gerwig\"\n,\n\"rating\"\n:\n8.3\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Toys come alive and have a blast doing so\"\n,\nmetadata\n=\n{\n\"year\"\n:\n1995\n,\n\"genre\"\n:\n\"animated\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Three men walk into the Zone, three men walk out of the Zone\"\n,\nmetadata\n=\n{\n\"year\"\n:\n1979\n,\n\"director\"\n:\n\"Andrei Tarkovsky\"\n,\n\"genre\"\n:\n\"thriller\"\n,\n\"rating\"\n:\n9.9\n,\n}\n,\n)\n,\n]\nvectorstore\n=\nPineconeVectorStore\n.\nfrom_documents\n(\ndocs\n,\nindex_name\n=\n\"sample\"\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\nAPI Reference:\nDocument\nRetriever\nâ€‹\nTo obtain scores from a vector store retriever, we wrap the underlying vector store's\n.similarity_search_with_score\nmethod in a short function that packages scores into the associated document's metadata.\nWe add a\n@chain\ndecorator to the function to create a\nRunnable\nthat can be used similarly to a typical retriever.\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_core\n.\nrunnables\nimport\nchain\n@chain\ndef\nretriever\n(\nquery\n:\nstr\n)\n-\n>\nList\n[\nDocument\n]\n:\ndocs\n,\nscores\n=\nzip\n(\n*\nvectorstore\n.\nsimilarity_search_with_score\n(\nquery\n)\n)\nfor\ndoc\n,\nscore\nin\nzip\n(\ndocs\n,\nscores\n)\n:\ndoc\n.\nmetadata\n[\n\"score\"\n]\n=\nscore\nreturn\ndocs\nAPI Reference:\nDocument\n|\nchain\nresult\n=\nretriever\n.\ninvoke\n(\n\"dinosaur\"\n)\nresult\n(Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993.0, 'score': 0.84429127}),\nDocument(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995.0, 'score': 0.792038262}),\nDocument(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979.0, 'score': 0.751571238}),\nDocument(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006.0, 'score': 0.747471571}))\nNote that similarity scores from the retrieval step are included in the metadata of the above documents.\nSelfQueryRetriever\nâ€‹\nSelfQueryRetriever\nwill use a LLM to generate a query that is potentially structured-- for example, it can construct filters for the retrieval on top of the usual semantic-similarity driven selection. See\nthis guide\nfor more detail.\nSelfQueryRetriever\nincludes a short (1 - 2 line) method\n_get_docs_with_query\nthat executes the\nvectorstore\nsearch. We can subclass\nSelfQueryRetriever\nand override this method to propagate similarity scores.\nFirst, following the\nhow-to guide\n, we will need to establish some metadata on which to filter:\nfrom\nlangchain\n.\nchains\n.\nquery_constructor\n.\nbase\nimport\nAttributeInfo\nfrom\nlangchain\n.\nretrievers\n.\nself_query\n.\nbase\nimport\nSelfQueryRetriever\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmetadata_field_info\n=\n[\nAttributeInfo\n(\nname\n=\n\"genre\"\n,\ndescription\n=\n\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\"\n,\ntype\n=\n\"string\"\n,\n)\n,\nAttributeInfo\n(\nname\n=\n\"year\"\n,\ndescription\n=\n\"The year the movie was released\"\n,\ntype\n=\n\"integer\"\n,\n)\n,\nAttributeInfo\n(\nname\n=\n\"director\"\n,\ndescription\n=\n\"The name of the movie director\"\n,\ntype\n=\n\"string\"\n,\n)\n,\nAttributeInfo\n(\nname\n=\n\"rating\"\n,\ndescription\n=\n\"A 1-10 rating for the movie\"\n,\ntype\n=\n\"float\"\n)\n,\n]\ndocument_content_description\n=\n\"Brief summary of a movie\"\nllm\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\nWe then override the\n_get_docs_with_query\nto use the\nsimilarity_search_with_score\nmethod of the underlying vector store:\nfrom\ntyping\nimport\nAny\n,\nDict\nclass\nCustomSelfQueryRetriever\n(\nSelfQueryRetriever\n)\n:\ndef\n_get_docs_with_query\n(\nself\n,\nquery\n:\nstr\n,\nsearch_kwargs\n:\nDict\n[\nstr\n,\nAny\n]\n)\n-\n>\nList\n[\nDocument\n]\n:\n\"\"\"Get docs, adding score information.\"\"\"\ndocs\n,\nscores\n=\nzip\n(\n*\nself\n.\nvectorstore\n.\nsimilarity_search_with_score\n(\nquery\n,\n**\nsearch_kwargs\n)\n)\nfor\ndoc\n,\nscore\nin\nzip\n(\ndocs\n,\nscores\n)\n:\ndoc\n.\nmetadata\n[\n\"score\"\n]\n=\nscore\nreturn\ndocs\nInvoking this retriever will now include similarity scores in the document metadata. Note that the underlying structured-query capabilities of\nSelfQueryRetriever\nare retained.\nretriever\n=\nCustomSelfQueryRetriever\n.\nfrom_llm\n(\nllm\n,\nvectorstore\n,\ndocument_content_description\n,\nmetadata_field_info\n,\n)\nresult\n=\nretriever\n.\ninvoke\n(\n\"dinosaur movie with rating less than 8\"\n)\nresult\n(Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993.0, 'score': 0.84429127}),)\nMultiVectorRetriever\nâ€‹\nMultiVectorRetriever\nallows you to associate multiple vectors with a single document. This can be useful in a number of applications. For example, we can index small chunks of a larger document and run the retrieval on the chunks, but return the larger \"parent\" document when invoking the retriever.\nParentDocumentRetriever\n, a subclass of\nMultiVectorRetriever\n, includes convenience methods for populating a vector store to support this. Further applications are detailed in this\nhow-to guide\n.\nTo propagate similarity scores through this retriever, we can again subclass\nMultiVectorRetriever\nand override a method. This time we will override\n_get_relevant_documents\n.\nFirst, we prepare some fake data. We generate fake \"whole documents\" and store them in a document store; here we will use a simple\nInMemoryStore\n.\nfrom\nlangchain\n.\nstorage\nimport\nInMemoryStore\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# The storage layer for the parent documents\ndocstore\n=\nInMemoryStore\n(\n)\nfake_whole_documents\n=\n[\n(\n\"fake_id_1\"\n,\nDocument\n(\npage_content\n=\n\"fake whole document 1\"\n)\n)\n,\n(\n\"fake_id_2\"\n,\nDocument\n(\npage_content\n=\n\"fake whole document 2\"\n)\n)\n,\n]\ndocstore\n.\nmset\n(\nfake_whole_documents\n)\nNext we will add some fake \"sub-documents\" to our vector store. We can link these sub-documents to the parent documents by populating the\n\"doc_id\"\nkey in its metadata.\ndocs\n=\n[\nDocument\n(\npage_content\n=\n\"A snippet from a larger document discussing cats.\"\n,\nmetadata\n=\n{\n\"doc_id\"\n:\n\"fake_id_1\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"A snippet from a larger document discussing discourse.\"\n,\nmetadata\n=\n{\n\"doc_id\"\n:\n\"fake_id_1\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"A snippet from a larger document discussing chocolate.\"\n,\nmetadata\n=\n{\n\"doc_id\"\n:\n\"fake_id_2\"\n}\n,\n)\n,\n]\nvectorstore\n.\nadd_documents\n(\ndocs\n)\n['62a85353-41ff-4346-bff7-be6c8ec2ed89',\n'5d4a0e83-4cc5-40f1-bc73-ed9cbad0ee15',\n'8c1d9a56-120f-45e4-ba70-a19cd19a38f4']\nTo propagate the scores, we subclass\nMultiVectorRetriever\nand override its\n_get_relevant_documents\nmethod. Here we will make two changes:\nWe will add similarity scores to the metadata of the corresponding \"sub-documents\" using the\nsimilarity_search_with_score\nmethod of the underlying vector store as above;\nWe will include a list of these sub-documents in the metadata of the retrieved parent document. This surfaces what snippets of text were identified by the retrieval, together with their corresponding similarity scores.\nfrom\ncollections\nimport\ndefaultdict\nfrom\nlangchain\n.\nretrievers\nimport\nMultiVectorRetriever\nfrom\nlangchain_core\n.\ncallbacks\nimport\nCallbackManagerForRetrieverRun\nclass\nCustomMultiVectorRetriever\n(\nMultiVectorRetriever\n)\n:\ndef\n_get_relevant_documents\n(\nself\n,\nquery\n:\nstr\n,\n*\n,\nrun_manager\n:\nCallbackManagerForRetrieverRun\n)\n-\n>\nList\n[\nDocument\n]\n:\n\"\"\"Get documents relevant to a query.\nArgs:\nquery: String to find relevant documents for\nrun_manager: The callbacks handler to use\nReturns:\nList of relevant documents\n\"\"\"\nresults\n=\nself\n.\nvectorstore\n.\nsimilarity_search_with_score\n(\nquery\n,\n**\nself\n.\nsearch_kwargs\n)\n# Map doc_ids to list of sub-documents, adding scores to metadata\nid_to_doc\n=\ndefaultdict\n(\nlist\n)\nfor\ndoc\n,\nscore\nin\nresults\n:\ndoc_id\n=\ndoc\n.\nmetadata\n.\nget\n(\n\"doc_id\"\n)\nif\ndoc_id\n:\ndoc\n.\nmetadata\n[\n\"score\"\n]\n=\nscore\nid_to_doc\n[\ndoc_id\n]\n.\nappend\n(\ndoc\n)\n# Fetch documents corresponding to doc_ids, retaining sub_docs in metadata\ndocs\n=\n[\n]\nfor\n_id\n,\nsub_docs\nin\nid_to_doc\n.\nitems\n(\n)\n:\ndocstore_docs\n=\nself\n.\ndocstore\n.\nmget\n(\n[\n_id\n]\n)\nif\ndocstore_docs\n:\nif\ndoc\n:=\ndocstore_docs\n[\n0\n]\n:\ndoc\n.\nmetadata\n[\n\"sub_docs\"\n]\n=\nsub_docs\ndocs\n.\nappend\n(\ndoc\n)\nreturn\ndocs\nAPI Reference:\nCallbackManagerForRetrieverRun\nInvoking this retriever, we can see that it identifies the correct parent document, including the relevant snippet from the sub-document with similarity score.\nretriever\n=\nCustomMultiVectorRetriever\n(\nvectorstore\n=\nvectorstore\n,\ndocstore\n=\ndocstore\n)\nretriever\n.\ninvoke\n(\n\"cat\"\n)\n[Document(page_content='fake whole document 1', metadata={'sub_docs': [Document(page_content='A snippet from a larger document discussing cats.', metadata={'doc_id': 'fake_id_1', 'score': 0.831276655})]})]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/caching_embeddings/",
    "How-to guides\nCaching\nOn this page\nCaching\nEmbeddings\ncan be stored or temporarily cached to avoid needing to recompute them.\nCaching embeddings can be done using a\nCacheBackedEmbeddings\n. The cache backed embedder is a wrapper around an embedder that caches\nembeddings in a key-value store. The text is hashed and the hash is used as the key in the cache.\nThe main supported way to initialize a\nCacheBackedEmbeddings\nis\nfrom_bytes_store\n. It takes the following parameters:\nunderlying_embedder: The embedder to use for embedding.\ndocument_embedding_cache: Any\nByteStore\nfor caching document embeddings.\nbatch_size: (optional, defaults to\nNone\n) The number of documents to embed between store updates.\nnamespace: (optional, defaults to\n\"\"\n) The namespace to use for document cache. This namespace is used to avoid collisions with other caches. For example, set it to the name of the embedding model used.\nquery_embedding_cache: (optional, defaults to\nNone\nor not caching) A\nByteStore\nfor caching query embeddings, or\nTrue\nto use the same store as\ndocument_embedding_cache\n.\nAttention\n:\nBe sure to set the\nnamespace\nparameter to avoid collisions of the same text embedded using different embeddings models.\nCacheBackedEmbeddings\ndoes not cache query embeddings by default. To enable query caching, one needs to specify a\nquery_embedding_cache\n.\nfrom\nlangchain\n.\nembeddings\nimport\nCacheBackedEmbeddings\nUsing with a Vector Store\nâ€‹\nFirst, let's see an example that uses the local file system for storing embeddings and uses FAISS vector store for retrieval.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain\n-\nopenai faiss\n-\ncpu\nfrom\nlangchain\n.\nstorage\nimport\nLocalFileStore\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nTextLoader\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\nunderlying_embeddings\n=\nOpenAIEmbeddings\n(\n)\nstore\n=\nLocalFileStore\n(\n\"./cache/\"\n)\ncached_embedder\n=\nCacheBackedEmbeddings\n.\nfrom_bytes_store\n(\nunderlying_embeddings\n,\nstore\n,\nnamespace\n=\nunderlying_embeddings\n.\nmodel\n)\nThe cache is empty prior to embedding:\nlist\n(\nstore\n.\nyield_keys\n(\n)\n)\n[]\nLoad the document, split it into chunks, embed each chunk and load it into the vector store.\nraw_documents\n=\nTextLoader\n(\n\"state_of_the_union.txt\"\n)\n.\nload\n(\n)\ntext_splitter\n=\nCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n0\n)\ndocuments\n=\ntext_splitter\n.\nsplit_documents\n(\nraw_documents\n)\nCreate the vector store:\n%\n%\ntime\ndb\n=\nFAISS\n.\nfrom_documents\n(\ndocuments\n,\ncached_embedder\n)\nCPU times: user 218 ms, sys: 29.7 ms, total: 248 ms\nWall time: 1.02 s\nIf we try to create the vector store again, it'll be much faster since it does not need to re-compute any embeddings.\n%\n%\ntime\ndb2\n=\nFAISS\n.\nfrom_documents\n(\ndocuments\n,\ncached_embedder\n)\nCPU times: user 15.7 ms, sys: 2.22 ms, total: 18 ms\nWall time: 17.2 ms\nAnd here are some of the embeddings that got created:\nlist\n(\nstore\n.\nyield_keys\n(\n)\n)\n[\n:\n5\n]\n['text-embedding-ada-00217a6727d-8916-54eb-b196-ec9c9d6ca472',\n'text-embedding-ada-0025fc0d904-bd80-52da-95c9-441015bfb438',\n'text-embedding-ada-002e4ad20ef-dfaa-5916-9459-f90c6d8e8159',\n'text-embedding-ada-002ed199159-c1cd-5597-9757-f80498e8f17b',\n'text-embedding-ada-0021297d37a-2bc1-5e19-bf13-6c950f075062']\nSwapping the\nByteStore\nIn order to use a different\nByteStore\n, just use it when creating your\nCacheBackedEmbeddings\n. Below, we create an equivalent cached embeddings object, except using the non-persistent\nInMemoryByteStore\ninstead:\nfrom\nlangchain\n.\nembeddings\nimport\nCacheBackedEmbeddings\nfrom\nlangchain\n.\nstorage\nimport\nInMemoryByteStore\nstore\n=\nInMemoryByteStore\n(\n)\ncached_embedder\n=\nCacheBackedEmbeddings\n.\nfrom_bytes_store\n(\nunderlying_embeddings\n,\nstore\n,\nnamespace\n=\nunderlying_embeddings\n.\nmodel\n)\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/callbacks_async/",
    "How-to guides\nHow to use callbacks in async environments\nOn this page\nHow to use callbacks in async environments\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nCallbacks\nCustom callback handlers\nIf you are planning to use the async APIs, it is recommended to use and extend\nAsyncCallbackHandler\nto avoid blocking the event.\nwarning\nIf you use a sync\nCallbackHandler\nwhile using an async method to run your LLM / Chain / Tool / Agent, it will still work. However, under the hood, it will be called with\nrun_in_executor\nwhich can cause issues if your\nCallbackHandler\nis not thread-safe.\ndanger\nIf you're on\npython<=3.10\n, you need to remember to propagate\nconfig\nor\ncallbacks\nwhen invoking other\nrunnable\nfrom within a\nRunnableLambda\n,\nRunnableGenerator\nor\n@tool\n. If you do not do this,\nthe callbacks will not be propagated to the child runnables being invoked.\nimport\nasyncio\nfrom\ntyping\nimport\nAny\n,\nDict\n,\nList\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\ncallbacks\nimport\nAsyncCallbackHandler\n,\nBaseCallbackHandler\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nfrom\nlangchain_core\n.\noutputs\nimport\nLLMResult\nclass\nMyCustomSyncHandler\n(\nBaseCallbackHandler\n)\n:\ndef\non_llm_new_token\n(\nself\n,\ntoken\n:\nstr\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Sync handler being called in a `thread_pool_executor`: token:\n{\ntoken\n}\n\"\n)\nclass\nMyCustomAsyncHandler\n(\nAsyncCallbackHandler\n)\n:\n\"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\nasync\ndef\non_llm_start\n(\nself\n,\nserialized\n:\nDict\n[\nstr\n,\nAny\n]\n,\nprompts\n:\nList\n[\nstr\n]\n,\n**\nkwargs\n:\nAny\n)\n-\n>\nNone\n:\n\"\"\"Run when chain starts running.\"\"\"\nprint\n(\n\"zzzz....\"\n)\nawait\nasyncio\n.\nsleep\n(\n0.3\n)\nclass_name\n=\nserialized\n[\n\"name\"\n]\nprint\n(\n\"Hi! I just woke up. Your llm is starting\"\n)\nasync\ndef\non_llm_end\n(\nself\n,\nresponse\n:\nLLMResult\n,\n**\nkwargs\n:\nAny\n)\n-\n>\nNone\n:\n\"\"\"Run when chain ends running.\"\"\"\nprint\n(\n\"zzzz....\"\n)\nawait\nasyncio\n.\nsleep\n(\n0.3\n)\nprint\n(\n\"Hi! I just woke up. Your llm is ending\"\n)\n# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n# Additionally, we pass in a list with our custom handler\nchat\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n,\nmax_tokens\n=\n25\n,\nstreaming\n=\nTrue\n,\ncallbacks\n=\n[\nMyCustomSyncHandler\n(\n)\n,\nMyCustomAsyncHandler\n(\n)\n]\n,\n)\nawait\nchat\n.\nagenerate\n(\n[\n[\nHumanMessage\n(\ncontent\n=\n\"Tell me a joke\"\n)\n]\n]\n)\nAPI Reference:\nAsyncCallbackHandler\n|\nBaseCallbackHandler\n|\nHumanMessage\n|\nLLMResult\nzzzz....\nHi! I just woke up. Your llm is starting\nSync handler being called in a `thread_pool_executor`: token:\nSync handler being called in a `thread_pool_executor`: token: Why\nSync handler being called in a `thread_pool_executor`: token:  don't scientists trust atoms?\nBecause they make up\nSync handler being called in a `thread_pool_executor`: token:  everything!\nSync handler being called in a `thread_pool_executor`: token:\nzzzz....\nHi! I just woke up. Your llm is ending\nLLMResult(generations=[[ChatGeneration(text=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", message=AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--a596349d-8a7c-45fe-8691-bb1f9cfd6c08-0', usage_metadata={'input_tokens': 11, 'output_tokens': 17, 'total_tokens': 28, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}))]], llm_output={}, run=[RunInfo(run_id=UUID('a596349d-8a7c-45fe-8691-bb1f9cfd6c08'))], type='LLMResult')\nNext steps\nâ€‹\nYou've now learned how to create your own custom callback handlers.\nNext, check out the other how-to guides in this section, such as\nhow to attach callbacks to a runnable\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/callbacks_attach/",
    "How-to guides\nHow to attach callbacks to a runnable\nOn this page\nHow to attach callbacks to a runnable\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nCallbacks\nCustom callback handlers\nChaining runnables\nAttach runtime arguments to a Runnable\nIf you are composing a chain of runnables and want to reuse callbacks across multiple executions, you can attach callbacks with the\n.with_config()\nmethod. This saves you the need to pass callbacks in each time you invoke the chain.\nimportant\nwith_config()\nbinds a configuration which will be interpreted as\nruntime\nconfiguration. So these callbacks will propagate to all child components.\nHere's an example:\nfrom\ntyping\nimport\nAny\n,\nDict\n,\nList\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\ncallbacks\nimport\nBaseCallbackHandler\nfrom\nlangchain_core\n.\nmessages\nimport\nBaseMessage\nfrom\nlangchain_core\n.\noutputs\nimport\nLLMResult\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nclass\nLoggingHandler\n(\nBaseCallbackHandler\n)\n:\ndef\non_chat_model_start\n(\nself\n,\nserialized\n:\nDict\n[\nstr\n,\nAny\n]\n,\nmessages\n:\nList\n[\nList\n[\nBaseMessage\n]\n]\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\n\"Chat model started\"\n)\ndef\non_llm_end\n(\nself\n,\nresponse\n:\nLLMResult\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Chat model ended, response:\n{\nresponse\n}\n\"\n)\ndef\non_chain_start\n(\nself\n,\nserialized\n:\nDict\n[\nstr\n,\nAny\n]\n,\ninputs\n:\nDict\n[\nstr\n,\nAny\n]\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Chain\n{\nserialized\n.\nget\n(\n'name'\n)\n}\nstarted\"\n)\ndef\non_chain_end\n(\nself\n,\noutputs\n:\nDict\n[\nstr\n,\nAny\n]\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Chain ended, outputs:\n{\noutputs\n}\n\"\n)\ncallbacks\n=\n[\nLoggingHandler\n(\n)\n]\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"What is 1 + {number}?\"\n)\nchain\n=\nprompt\n|\nllm\nchain_with_callbacks\n=\nchain\n.\nwith_config\n(\ncallbacks\n=\ncallbacks\n)\nchain_with_callbacks\n.\ninvoke\n(\n{\n\"number\"\n:\n\"2\"\n}\n)\nAPI Reference:\nBaseCallbackHandler\n|\nBaseMessage\n|\nLLMResult\n|\nChatPromptTemplate\nError in LoggingHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n``````output\nChain ChatPromptTemplate started\nChain ended, outputs: messages=[HumanMessage(content='What is 1 + 2?', additional_kwargs={}, response_metadata={})]\nChat model started\nChat model ended, response: generations=[[ChatGeneration(text='The sum of 1 + 2 is 3.', message=AIMessage(content='The sum of 1 + 2 is 3.', additional_kwargs={}, response_metadata={'id': 'msg_01F1qPrmBD9igfzHdqVipmKX', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 17, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--71edddf3-2474-42dc-ad43-fadb4882c3c8-0', usage_metadata={'input_tokens': 16, 'output_tokens': 17, 'total_tokens': 33, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}))]] llm_output={'id': 'msg_01F1qPrmBD9igfzHdqVipmKX', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 17, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'} run=None type='LLMResult'\nChain ended, outputs: content='The sum of 1 + 2 is 3.' additional_kwargs={} response_metadata={'id': 'msg_01F1qPrmBD9igfzHdqVipmKX', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 17, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'} id='run--71edddf3-2474-42dc-ad43-fadb4882c3c8-0' usage_metadata={'input_tokens': 16, 'output_tokens': 17, 'total_tokens': 33, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\nAIMessage(content='The sum of 1 + 2 is 3.', additional_kwargs={}, response_metadata={'id': 'msg_01F1qPrmBD9igfzHdqVipmKX', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 17, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--71edddf3-2474-42dc-ad43-fadb4882c3c8-0', usage_metadata={'input_tokens': 16, 'output_tokens': 17, 'total_tokens': 33, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\nThe bound callbacks will run for all nested module runs.\nNext steps\nâ€‹\nYou've now learned how to attach callbacks to a chain.\nNext, check out the other how-to guides in this section, such as how to\npass callbacks in at runtime\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/callbacks_constructor/",
    "How-to guides\nHow to propagate callbacks  constructor\nOn this page\nHow to propagate callbacks  constructor\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nCallbacks\nCustom callback handlers\nMost LangChain modules allow you to pass\ncallbacks\ndirectly into the constructor (i.e., initializer). In this case, the callbacks will only be called for that instance (and any nested runs).\nwarning\nConstructor callbacks are scoped only to the object they are defined on. They are\nnot\ninherited by children of the object. This can lead to confusing behavior,\nand it's generally better to pass callbacks as a run time argument.\nHere's an example:\nfrom\ntyping\nimport\nAny\n,\nDict\n,\nList\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\ncallbacks\nimport\nBaseCallbackHandler\nfrom\nlangchain_core\n.\nmessages\nimport\nBaseMessage\nfrom\nlangchain_core\n.\noutputs\nimport\nLLMResult\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nclass\nLoggingHandler\n(\nBaseCallbackHandler\n)\n:\ndef\non_chat_model_start\n(\nself\n,\nserialized\n:\nDict\n[\nstr\n,\nAny\n]\n,\nmessages\n:\nList\n[\nList\n[\nBaseMessage\n]\n]\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\n\"Chat model started\"\n)\ndef\non_llm_end\n(\nself\n,\nresponse\n:\nLLMResult\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Chat model ended, response:\n{\nresponse\n}\n\"\n)\ndef\non_chain_start\n(\nself\n,\nserialized\n:\nDict\n[\nstr\n,\nAny\n]\n,\ninputs\n:\nDict\n[\nstr\n,\nAny\n]\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Chain\n{\nserialized\n.\nget\n(\n'name'\n)\n}\nstarted\"\n)\ndef\non_chain_end\n(\nself\n,\noutputs\n:\nDict\n[\nstr\n,\nAny\n]\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Chain ended, outputs:\n{\noutputs\n}\n\"\n)\ncallbacks\n=\n[\nLoggingHandler\n(\n)\n]\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n,\ncallbacks\n=\ncallbacks\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"What is 1 + {number}?\"\n)\nchain\n=\nprompt\n|\nllm\nchain\n.\ninvoke\n(\n{\n\"number\"\n:\n\"2\"\n}\n)\nAPI Reference:\nBaseCallbackHandler\n|\nBaseMessage\n|\nLLMResult\n|\nChatPromptTemplate\nChat model started\nChat model ended, response: generations=[[ChatGeneration(text='1 + 2 = 3', message=AIMessage(content='1 + 2 = 3', additional_kwargs={}, response_metadata={'id': 'msg_01DQMbSk263KpY2vouHM5gsC', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 13, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--ab896e4e-c3fd-48b1-a41a-b6b525cbc041-0', usage_metadata={'input_tokens': 16, 'output_tokens': 13, 'total_tokens': 29, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}))]] llm_output={'id': 'msg_01DQMbSk263KpY2vouHM5gsC', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 13, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'} run=None type='LLMResult'\nAIMessage(content='1 + 2 = 3', additional_kwargs={}, response_metadata={'id': 'msg_01DQMbSk263KpY2vouHM5gsC', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 13, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--ab896e4e-c3fd-48b1-a41a-b6b525cbc041-0', usage_metadata={'input_tokens': 16, 'output_tokens': 13, 'total_tokens': 29, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\nYou can see that we only see events from the chat model run - no chain events from the prompt or broader chain.\nNext steps\nâ€‹\nYou've now learned how to pass callbacks into a constructor.\nNext, check out the other how-to guides in this section, such as how to\npass callbacks at runtime\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/callbacks_custom_events/",
    "How-to guides\nHow to dispatch custom callback events\nOn this page\nHow to dispatch custom callback events\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nCallbacks\nCustom callback handlers\nAstream Events API\nthe\nastream_events\nmethod will surface custom callback events.\nIn some situations, you may want to dispatch a custom callback event from within a\nRunnable\nso it can be surfaced\nin a custom callback handler or via the\nAstream Events API\n.\nFor example, if you have a long running tool with multiple steps, you can dispatch custom events between the steps and use these custom events to monitor progress.\nYou could also surface these custom events to an end user of your application to show them how the current task is progressing.\nTo dispatch a custom event you need to decide on two attributes for the event: the\nname\nand the\ndata\n.\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nimportant\nDispatching custom callback events requires\nlangchain-core>=0.2.15\n.\nCustom callback events can only be dispatched from within an existing\nRunnable\n.\nIf using\nastream_events\n, you must use\nversion='v2'\nto see custom events.\nSending or rendering custom callbacks events in LangSmith is not yet supported.\nCOMPATIBILITY\nLangChain cannot automatically propagate configuration, including callbacks necessary for astream_events(), to child runnables if you are running async code in\npython<=3.10\n. This is a common reason why you may fail to see events being emitted from custom runnables or tools.\nIf you are running\npython<=3.10\n, you will need to manually propagate the\nRunnableConfig\nobject to the child runnable in async environments. For an example of how to manually propagate the config, see the implementation of the\nbar\nRunnableLambda below.\nIf you are running\npython>=3.11\n, the\nRunnableConfig\nwill automatically propagate to child runnables in async environment. However, it is still a good idea to propagate the\nRunnableConfig\nmanually if your code may run in other Python versions.\nAstream Events API\nâ€‹\nThe most useful way to consume custom events is via the\nAstream Events API\n.\nWe can use the\nasync\nadispatch_custom_event\nAPI to emit custom events in an async setting.\nimportant\nTo see custom events via the astream events API, you need to use the newer\nv2\nAPI of\nastream_events\n.\nfrom\nlangchain_core\n.\ncallbacks\n.\nmanager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\nlangchain_core\n.\nrunnables\n.\nconfig\nimport\nRunnableConfig\n@RunnableLambda\nasync\ndef\nfoo\n(\nx\n:\nstr\n)\n-\n>\nstr\n:\nawait\nadispatch_custom_event\n(\n\"event1\"\n,\n{\n\"x\"\n:\nx\n}\n)\nawait\nadispatch_custom_event\n(\n\"event2\"\n,\n5\n)\nreturn\nx\nasync\nfor\nevent\nin\nfoo\n.\nastream_events\n(\n\"hello world\"\n,\nversion\n=\n\"v2\"\n)\n:\nprint\n(\nevent\n)\nAPI Reference:\nadispatch_custom_event\n|\nRunnableLambda\n|\nRunnableConfig\n{'event': 'on_chain_start', 'data': {'input': 'hello world'}, 'name': 'foo', 'tags': [], 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'metadata': {}, 'parent_ids': []}\n{'event': 'on_custom_event', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'event1', 'tags': [], 'metadata': {}, 'data': {'x': 'hello world'}, 'parent_ids': []}\n{'event': 'on_custom_event', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'event2', 'tags': [], 'metadata': {}, 'data': 5, 'parent_ids': []}\n{'event': 'on_chain_stream', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'foo', 'tags': [], 'metadata': {}, 'data': {'chunk': 'hello world'}, 'parent_ids': []}\n{'event': 'on_chain_end', 'data': {'output': 'hello world'}, 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'foo', 'tags': [], 'metadata': {}, 'parent_ids': []}\nIn python <= 3.10, you must propagate the config manually!\nfrom\nlangchain_core\n.\ncallbacks\n.\nmanager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\nlangchain_core\n.\nrunnables\n.\nconfig\nimport\nRunnableConfig\n@RunnableLambda\nasync\ndef\nbar\n(\nx\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nstr\n:\n\"\"\"An example that shows how to manually propagate config.\nYou must do this if you're running python<=3.10.\n\"\"\"\nawait\nadispatch_custom_event\n(\n\"event1\"\n,\n{\n\"x\"\n:\nx\n}\n,\nconfig\n=\nconfig\n)\nawait\nadispatch_custom_event\n(\n\"event2\"\n,\n5\n,\nconfig\n=\nconfig\n)\nreturn\nx\nasync\nfor\nevent\nin\nbar\n.\nastream_events\n(\n\"hello world\"\n,\nversion\n=\n\"v2\"\n)\n:\nprint\n(\nevent\n)\nAPI Reference:\nadispatch_custom_event\n|\nRunnableLambda\n|\nRunnableConfig\n{'event': 'on_chain_start', 'data': {'input': 'hello world'}, 'name': 'bar', 'tags': [], 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'metadata': {}, 'parent_ids': []}\n{'event': 'on_custom_event', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'event1', 'tags': [], 'metadata': {}, 'data': {'x': 'hello world'}, 'parent_ids': []}\n{'event': 'on_custom_event', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'event2', 'tags': [], 'metadata': {}, 'data': 5, 'parent_ids': []}\n{'event': 'on_chain_stream', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'bar', 'tags': [], 'metadata': {}, 'data': {'chunk': 'hello world'}, 'parent_ids': []}\n{'event': 'on_chain_end', 'data': {'output': 'hello world'}, 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'bar', 'tags': [], 'metadata': {}, 'parent_ids': []}\nAsync Callback Handler\nâ€‹\nYou can also consume the dispatched event via an async callback handler.\nfrom\ntyping\nimport\nAny\n,\nDict\n,\nList\n,\nOptional\nfrom\nuuid\nimport\nUUID\nfrom\nlangchain_core\n.\ncallbacks\nimport\nAsyncCallbackHandler\nfrom\nlangchain_core\n.\ncallbacks\n.\nmanager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\nlangchain_core\n.\nrunnables\n.\nconfig\nimport\nRunnableConfig\nclass\nAsyncCustomCallbackHandler\n(\nAsyncCallbackHandler\n)\n:\nasync\ndef\non_custom_event\n(\nself\n,\nname\n:\nstr\n,\ndata\n:\nAny\n,\n*\n,\nrun_id\n:\nUUID\n,\ntags\n:\nOptional\n[\nList\n[\nstr\n]\n]\n=\nNone\n,\nmetadata\n:\nOptional\n[\nDict\n[\nstr\n,\nAny\n]\n]\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n-\n>\nNone\n:\nprint\n(\nf\"Received event\n{\nname\n}\nwith data:\n{\ndata\n}\n, with tags:\n{\ntags\n}\n, with metadata:\n{\nmetadata\n}\nand run_id:\n{\nrun_id\n}\n\"\n)\n@RunnableLambda\nasync\ndef\nbar\n(\nx\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nstr\n:\n\"\"\"An example that shows how to manually propagate config.\nYou must do this if you're running python<=3.10.\n\"\"\"\nawait\nadispatch_custom_event\n(\n\"event1\"\n,\n{\n\"x\"\n:\nx\n}\n,\nconfig\n=\nconfig\n)\nawait\nadispatch_custom_event\n(\n\"event2\"\n,\n5\n,\nconfig\n=\nconfig\n)\nreturn\nx\nasync_handler\n=\nAsyncCustomCallbackHandler\n(\n)\nawait\nfoo\n.\nainvoke\n(\n1\n,\n{\n\"callbacks\"\n:\n[\nasync_handler\n]\n,\n\"tags\"\n:\n[\n\"foo\"\n,\n\"bar\"\n]\n}\n)\nAPI Reference:\nAsyncCallbackHandler\n|\nadispatch_custom_event\n|\nRunnableLambda\n|\nRunnableConfig\nReceived event event1 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: a62b84be-7afd-4829-9947-7165df1f37d9\nReceived event event2 with data: 5, with tags: ['foo', 'bar'], with metadata: {} and run_id: a62b84be-7afd-4829-9947-7165df1f37d9\n1\nSync Callback Handler\nâ€‹\nLet's see how to emit custom events in a sync environment using\ndispatch_custom_event\n.\nYou\nmust\ncall\ndispatch_custom_event\nfrom within an existing\nRunnable\n.\nfrom\ntyping\nimport\nAny\n,\nDict\n,\nList\n,\nOptional\nfrom\nuuid\nimport\nUUID\nfrom\nlangchain_core\n.\ncallbacks\nimport\nBaseCallbackHandler\nfrom\nlangchain_core\n.\ncallbacks\n.\nmanager\nimport\n(\ndispatch_custom_event\n,\n)\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\nlangchain_core\n.\nrunnables\n.\nconfig\nimport\nRunnableConfig\nclass\nCustomHandler\n(\nBaseCallbackHandler\n)\n:\ndef\non_custom_event\n(\nself\n,\nname\n:\nstr\n,\ndata\n:\nAny\n,\n*\n,\nrun_id\n:\nUUID\n,\ntags\n:\nOptional\n[\nList\n[\nstr\n]\n]\n=\nNone\n,\nmetadata\n:\nOptional\n[\nDict\n[\nstr\n,\nAny\n]\n]\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n-\n>\nNone\n:\nprint\n(\nf\"Received event\n{\nname\n}\nwith data:\n{\ndata\n}\n, with tags:\n{\ntags\n}\n, with metadata:\n{\nmetadata\n}\nand run_id:\n{\nrun_id\n}\n\"\n)\n@RunnableLambda\ndef\nfoo\n(\nx\n:\nint\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nint\n:\ndispatch_custom_event\n(\n\"event1\"\n,\n{\n\"x\"\n:\nx\n}\n)\ndispatch_custom_event\n(\n\"event2\"\n,\n{\n\"x\"\n:\nx\n}\n)\nreturn\nx\nhandler\n=\nCustomHandler\n(\n)\nfoo\n.\ninvoke\n(\n1\n,\n{\n\"callbacks\"\n:\n[\nhandler\n]\n,\n\"tags\"\n:\n[\n\"foo\"\n,\n\"bar\"\n]\n}\n)\nAPI Reference:\nBaseCallbackHandler\n|\ndispatch_custom_event\n|\nRunnableLambda\n|\nRunnableConfig\nReceived event event1 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: 27b5ce33-dc26-4b34-92dd-08a89cb22268\nReceived event event2 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: 27b5ce33-dc26-4b34-92dd-08a89cb22268\n1\nNext steps\nâ€‹\nYou've seen how to emit custom events, you can check out the more in depth guide for\nastream events\nwhich is the easiest way to leverage custom events.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/callbacks_runtime/",
    "How-to guides\nHow to pass callbacks in at runtime\nOn this page\nHow to pass callbacks in at runtime\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nCallbacks\nCustom callback handlers\nIn many cases, it is advantageous to pass in handlers instead when running the object. When we pass through\nCallbackHandlers\nusing the\ncallbacks\nkeyword arg when executing a run, those callbacks will be issued by all nested objects involved in the execution. For example, when a handler is passed through to an Agent, it will be used for all callbacks related to the agent and all the objects involved in the agent's execution, in this case, the Tools and LLM.\nThis prevents us from having to manually attach the handlers to each individual nested object. Here's an example:\nfrom\ntyping\nimport\nAny\n,\nDict\n,\nList\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\ncallbacks\nimport\nBaseCallbackHandler\nfrom\nlangchain_core\n.\nmessages\nimport\nBaseMessage\nfrom\nlangchain_core\n.\noutputs\nimport\nLLMResult\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nclass\nLoggingHandler\n(\nBaseCallbackHandler\n)\n:\ndef\non_chat_model_start\n(\nself\n,\nserialized\n:\nDict\n[\nstr\n,\nAny\n]\n,\nmessages\n:\nList\n[\nList\n[\nBaseMessage\n]\n]\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\n\"Chat model started\"\n)\ndef\non_llm_end\n(\nself\n,\nresponse\n:\nLLMResult\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Chat model ended, response:\n{\nresponse\n}\n\"\n)\ndef\non_chain_start\n(\nself\n,\nserialized\n:\nDict\n[\nstr\n,\nAny\n]\n,\ninputs\n:\nDict\n[\nstr\n,\nAny\n]\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Chain\n{\nserialized\n.\nget\n(\n'name'\n)\n}\nstarted\"\n)\ndef\non_chain_end\n(\nself\n,\noutputs\n:\nDict\n[\nstr\n,\nAny\n]\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"Chain ended, outputs:\n{\noutputs\n}\n\"\n)\ncallbacks\n=\n[\nLoggingHandler\n(\n)\n]\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"What is 1 + {number}?\"\n)\nchain\n=\nprompt\n|\nllm\nchain\n.\ninvoke\n(\n{\n\"number\"\n:\n\"2\"\n}\n,\nconfig\n=\n{\n\"callbacks\"\n:\ncallbacks\n}\n)\nAPI Reference:\nBaseCallbackHandler\n|\nBaseMessage\n|\nLLMResult\n|\nChatPromptTemplate\nError in LoggingHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n``````output\nChain ChatPromptTemplate started\nChain ended, outputs: messages=[HumanMessage(content='What is 1 + 2?', additional_kwargs={}, response_metadata={})]\nChat model started\nChat model ended, response: generations=[[ChatGeneration(text='1 + 2 = 3', message=AIMessage(content='1 + 2 = 3', additional_kwargs={}, response_metadata={'id': 'msg_019ieJt8K32iC77qBbQmSa9m', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 13, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--2f596356-99c9-45ef-94ff-fb170072abdf-0', usage_metadata={'input_tokens': 16, 'output_tokens': 13, 'total_tokens': 29, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}))]] llm_output={'id': 'msg_019ieJt8K32iC77qBbQmSa9m', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 13, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'} run=None type='LLMResult'\nChain ended, outputs: content='1 + 2 = 3' additional_kwargs={} response_metadata={'id': 'msg_019ieJt8K32iC77qBbQmSa9m', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 13, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'} id='run--2f596356-99c9-45ef-94ff-fb170072abdf-0' usage_metadata={'input_tokens': 16, 'output_tokens': 13, 'total_tokens': 29, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\nAIMessage(content='1 + 2 = 3', additional_kwargs={}, response_metadata={'id': 'msg_019ieJt8K32iC77qBbQmSa9m', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 13, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--2f596356-99c9-45ef-94ff-fb170072abdf-0', usage_metadata={'input_tokens': 16, 'output_tokens': 13, 'total_tokens': 29, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\nIf there are already existing callbacks associated with a module, these will run in addition to any passed in at runtime.\nNext steps\nâ€‹\nYou've now learned how to pass callbacks at runtime.\nNext, check out the other how-to guides in this section, such as how to\npass callbacks into a module constructor\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/character_text_splitter/",
    "How-to guides\nHow to split by character\nHow to split by character\nThis is the simplest method. This\nsplits\nbased on a given character sequence, which defaults to\n\"\\n\\n\"\n. Chunk length is measured by number of characters.\nHow the text is split: by single character separator.\nHow the chunk size is measured: by number of characters.\nTo obtain the string content directly, use\n.split_text\n.\nTo create LangChain\nDocument\nobjects (e.g., for use in downstream tasks), use\n.create_documents\n.\n%\npip install\n-\nqU langchain\n-\ntext\n-\nsplitters\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\n# Load an example document\nwith\nopen\n(\n\"state_of_the_union.txt\"\n)\nas\nf\n:\nstate_of_the_union\n=\nf\n.\nread\n(\n)\ntext_splitter\n=\nCharacterTextSplitter\n(\nseparator\n=\n\"\\n\\n\"\n,\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n,\nlength_function\n=\nlen\n,\nis_separator_regex\n=\nFalse\n,\n)\ntexts\n=\ntext_splitter\n.\ncreate_documents\n(\n[\nstate_of_the_union\n]\n)\nprint\n(\ntexts\n[\n0\n]\n)\npage_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'\nUse\n.create_documents\nto propagate metadata associated with each document to the output chunks:\nmetadatas\n=\n[\n{\n\"document\"\n:\n1\n}\n,\n{\n\"document\"\n:\n2\n}\n]\ndocuments\n=\ntext_splitter\n.\ncreate_documents\n(\n[\nstate_of_the_union\n,\nstate_of_the_union\n]\n,\nmetadatas\n=\nmetadatas\n)\nprint\n(\ndocuments\n[\n0\n]\n)\npage_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.' metadata={'document': 1}\nUse\n.split_text\nto obtain the string content directly:\ntext_splitter\n.\nsplit_text\n(\nstate_of_the_union\n)\n[\n0\n]\n'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/chat_model_caching/",
    "How-to guides\nHow to cache chat model responses\nOn this page\nHow to cache chat model responses\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nLLMs\nLangChain provides an optional caching layer for\nchat models\n. This is useful for two main reasons:\nIt can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. This is especially useful during app development.\nIt can speed up your application by reducing the number of API calls you make to the LLM provider.\nThis guide will walk you through how to enable this in your apps.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\n# <!-- ruff: noqa: F821 -->\nfrom\nlangchain_core\n.\nglobals\nimport\nset_llm_cache\nAPI Reference:\nset_llm_cache\nIn Memory Cache\nâ€‹\nThis is an ephemeral cache that stores model calls in memory. It will be wiped when your environment restarts, and is not shared across processes.\n%\n%\ntime\nfrom\nlangchain_core\n.\ncaches\nimport\nInMemoryCache\nset_llm_cache\n(\nInMemoryCache\n(\n)\n)\n# The first time, it is not yet in cache, so it should take longer\nllm\n.\ninvoke\n(\n\"Tell me a joke\"\n)\nAPI Reference:\nInMemoryCache\nCPU times: user 645 ms, sys: 214 ms, total: 859 ms\nWall time: 829 ms\nAIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')\n%\n%\ntime\n# The second time it is, so it goes faster\nllm\n.\ninvoke\n(\n\"Tell me a joke\"\n)\nCPU times: user 822 Âµs, sys: 288 Âµs, total: 1.11 ms\nWall time: 1.06 ms\nAIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')\nSQLite Cache\nâ€‹\nThis cache implementation uses a\nSQLite\ndatabase to store responses, and will last across process restarts.\n!rm\n.\nlangchain\n.\ndb\n# We can do the same thing with a SQLite cache\nfrom\nlangchain_community\n.\ncache\nimport\nSQLiteCache\nset_llm_cache\n(\nSQLiteCache\n(\ndatabase_path\n=\n\".langchain.db\"\n)\n)\n%\n%\ntime\n# The first time, it is not yet in cache, so it should take longer\nllm\n.\ninvoke\n(\n\"Tell me a joke\"\n)\nCPU times: user 9.91 ms, sys: 7.68 ms, total: 17.6 ms\nWall time: 657 ms\nAIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')\n%\n%\ntime\n# The second time it is, so it goes faster\nllm\n.\ninvoke\n(\n\"Tell me a joke\"\n)\nCPU times: user 52.2 ms, sys: 60.5 ms, total: 113 ms\nWall time: 127 ms\nAIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')\nNext steps\nâ€‹\nYou've now learned how to cache model responses to save time and money.\nNext, check out the other how-to guides chat models in this section, like\nhow to get a model to return structured output\nor\nhow to create your own custom chat model\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/chat_model_rate_limiting/",
    "How-to guides\nHow to handle rate limits\nOn this page\nHow to handle rate limits\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nLLMs\nYou may find yourself in a situation where you are getting rate limited by the model provider API because you're making too many requests.\nFor example, this might happen if you are running many parallel queries to benchmark the chat model on a test dataset.\nIf you are facing such a situation, you can use a rate limiter to help match the rate at which you're making request to the rate allowed\nby the API.\nRequires\nlangchain-core >= 0.2.24\nThis functionality was added in\nlangchain-core == 0.2.24\n. Please make sure your package is up to date.\nInitialize a rate limiter\nâ€‹\nLangchain comes with a built-in in memory rate limiter. This rate limiter is thread safe and can be shared by multiple threads in the same process.\nThe provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size\nof the requests.\nfrom\nlangchain_core\n.\nrate_limiters\nimport\nInMemoryRateLimiter\nrate_limiter\n=\nInMemoryRateLimiter\n(\nrequests_per_second\n=\n0.1\n,\n# <-- Super slow! We can only make a request once every 10 seconds!!\ncheck_every_n_seconds\n=\n0.1\n,\n# Wake up every 100 ms to check whether allowed to make a request,\nmax_bucket_size\n=\n10\n,\n# Controls the maximum burst size.\n)\nAPI Reference:\nInMemoryRateLimiter\nChoose a model\nâ€‹\nChoose any model and pass to it the rate_limiter via the\nrate_limiter\nattribute.\nimport\nos\nimport\ntime\nfrom\ngetpass\nimport\ngetpass\nif\n\"ANTHROPIC_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"ANTHROPIC_API_KEY\"\n]\n=\ngetpass\n(\n)\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nmodel\n=\nChatAnthropic\n(\nmodel_name\n=\n\"claude-3-opus-20240229\"\n,\nrate_limiter\n=\nrate_limiter\n)\nLet's confirm that the rate limiter works. We should only be able to invoke the model once per 10 seconds.\nfor\n_\nin\nrange\n(\n5\n)\n:\ntic\n=\ntime\n.\ntime\n(\n)\nmodel\n.\ninvoke\n(\n\"hello\"\n)\ntoc\n=\ntime\n.\ntime\n(\n)\nprint\n(\ntoc\n-\ntic\n)\n11.599073648452759\n10.7502121925354\n10.244257926940918\n8.83088755607605\n11.645203590393066\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/chat_models_universal_init/",
    "How-to guides\nHow to init any model in one line\nOn this page\nHow to init any model in one line\nMany LLM applications let end users specify what model provider and model they want the application to be powered by. This requires writing some logic to initialize different\nchat models\nbased on some user configuration. The\ninit_chat_model()\nhelper method makes it easy to initialize a number of different model integrations without having to worry about import paths and class names.\nSupported models\nSee the\ninit_chat_model()\nAPI reference for a full list of supported integrations.\nMake sure you have the\nintegration packages\ninstalled for any model providers you want to support. E.g. you should have\nlangchain-openai\ninstalled to init an OpenAI model.\n%\npip install\n-\nqU langchain langchain\n-\nopenai langchain\n-\nanthropic langchain\n-\ngoogle\n-\ngenai\nBasic usage\nâ€‹\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\n# Don't forget to set your environment variables for the API keys of the respective providers!\n# For example, you can set them in your terminal or in a .env file:\n# export OPENAI_API_KEY=\"your_openai_api_key\"\n# Returns a langchain_openai.ChatOpenAI instance.\ngpt_4o\n=\ninit_chat_model\n(\n\"gpt-4o\"\n,\nmodel_provider\n=\n\"openai\"\n,\ntemperature\n=\n0\n)\n# Returns a langchain_anthropic.ChatAnthropic instance.\nclaude_opus\n=\ninit_chat_model\n(\n\"claude-3-opus-20240229\"\n,\nmodel_provider\n=\n\"anthropic\"\n,\ntemperature\n=\n0\n)\n# Returns a langchain_google_vertexai.ChatVertexAI instance.\ngemini_15\n=\ninit_chat_model\n(\n\"gemini-2.5-pro\"\n,\nmodel_provider\n=\n\"google_genai\"\n,\ntemperature\n=\n0\n)\n# Since all model integrations implement the ChatModel interface, you can use them in the same way.\nprint\n(\n\"GPT-4o: \"\n+\ngpt_4o\n.\ninvoke\n(\n\"what's your name\"\n)\n.\ncontent\n+\n\"\\n\"\n)\nprint\n(\n\"Claude Opus: \"\n+\nclaude_opus\n.\ninvoke\n(\n\"what's your name\"\n)\n.\ncontent\n+\n\"\\n\"\n)\nprint\n(\n\"Gemini 2.5: \"\n+\ngemini_15\n.\ninvoke\n(\n\"what's your name\"\n)\n.\ncontent\n+\n\"\\n\"\n)\nGPT-4o: Iâ€™m called ChatGPT. How can I assist you today?\nClaude Opus: My name is Claude. It's nice to meet you!\nGemini 2.5: I do not have a name. I am a large language model, trained by Google.\nInferring model provider\nâ€‹\nFor common and distinct model names\ninit_chat_model()\nwill attempt to infer the model provider. See the\nAPI reference\nfor a full list of inference behavior. E.g. any model that starts with\ngpt-3...\nor\ngpt-4...\nwill be inferred as using model provider\nopenai\n.\ngpt_4o\n=\ninit_chat_model\n(\n\"gpt-4o\"\n,\ntemperature\n=\n0\n)\nclaude_opus\n=\ninit_chat_model\n(\n\"claude-3-opus-20240229\"\n,\ntemperature\n=\n0\n)\ngemini_15\n=\ninit_chat_model\n(\n\"gemini-2.5-pro\"\n,\ntemperature\n=\n0\n)\nCreating a configurable model\nâ€‹\nYou can also create a runtime-configurable model by specifying\nconfigurable_fields\n. If you don't specify a\nmodel\nvalue, then \"model\" and \"model_provider\" be configurable by default.\nconfigurable_model\n=\ninit_chat_model\n(\ntemperature\n=\n0\n)\nconfigurable_model\n.\ninvoke\n(\n\"what's your name\"\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"model\"\n:\n\"gpt-4o\"\n}\n}\n)\nAIMessage(content='Iâ€™m called ChatGPT. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BwCyyBpMqn96KED6zPhLm4k9SQMiQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fada10c3-4128-406c-b83d-a850d16b365f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 13, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\nconfigurable_model\n.\ninvoke\n(\n\"what's your name\"\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"model\"\n:\n\"claude-3-5-sonnet-latest\"\n}\n}\n)\nAIMessage(content=\"My name is Claude. It's nice to meet you!\", additional_kwargs={}, response_metadata={'id': 'msg_01VDGrG9D6yefanbBG9zPJrc', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 11, 'output_tokens': 15, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-sonnet-20240620'}, id='run--f0156087-debf-4b4b-9aaa-f3328a81ef92-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\nConfigurable model with default values\nâ€‹\nWe can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:\nfirst_llm\n=\ninit_chat_model\n(\nmodel\n=\n\"gpt-4o\"\n,\ntemperature\n=\n0\n,\nconfigurable_fields\n=\n(\n\"model\"\n,\n\"model_provider\"\n,\n\"temperature\"\n,\n\"max_tokens\"\n)\n,\nconfig_prefix\n=\n\"first\"\n,\n# useful when you have a chain with multiple models\n)\nfirst_llm\n.\ninvoke\n(\n\"what's your name\"\n)\nAIMessage(content=\"I'm an AI created by OpenAI, and I don't have a personal name. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-3380f977-4b89-4f44-bc02-b64043b3166f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})\nfirst_llm\n.\ninvoke\n(\n\"what's your name\"\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"first_model\"\n:\n\"claude-3-5-sonnet-latest\"\n,\n\"first_temperature\"\n:\n0.5\n,\n\"first_max_tokens\"\n:\n100\n,\n}\n}\n,\n)\nAIMessage(content=\"My name is Claude. It's nice to meet you!\", additional_kwargs={}, response_metadata={'id': 'msg_01EFKSWpmsn2PSYPQa4cNHWb', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 11, 'output_tokens': 15}}, id='run-3c58f47c-41b9-4e56-92e7-fb9602e3787c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26})\nUsing a configurable model declaratively\nâ€‹\nWe can call declarative operations like\nbind_tools\n,\nwith_structured_output\n,\nwith_configurable\n, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nGetWeather\n(\nBaseModel\n)\n:\n\"\"\"Get the current weather in a given location\"\"\"\nlocation\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The city and state, e.g. San Francisco, CA\"\n)\nclass\nGetPopulation\n(\nBaseModel\n)\n:\n\"\"\"Get the current population in a given location\"\"\"\nlocation\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The city and state, e.g. San Francisco, CA\"\n)\nllm\n=\ninit_chat_model\n(\ntemperature\n=\n0\n)\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nGetWeather\n,\nGetPopulation\n]\n)\nllm_with_tools\n.\ninvoke\n(\n\"what's bigger in 2024 LA or NYC\"\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"model\"\n:\n\"gpt-4o\"\n}\n}\n)\n.\ntool_calls\n[{'name': 'GetPopulation',\n'args': {'location': 'Los Angeles, CA'},\n'id': 'call_Ga9m8FAArIyEjItHmztPYA22',\n'type': 'tool_call'},\n{'name': 'GetPopulation',\n'args': {'location': 'New York, NY'},\n'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',\n'type': 'tool_call'}]\nllm_with_tools\n.\ninvoke\n(\n\"what's bigger in 2024 LA or NYC\"\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"model\"\n:\n\"claude-3-5-sonnet-latest\"\n}\n}\n,\n)\n.\ntool_calls\n[{'name': 'GetPopulation',\n'args': {'location': 'Los Angeles, CA'},\n'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',\n'type': 'tool_call'},\n{'name': 'GetPopulation',\n'args': {'location': 'New York City, NY'},\n'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',\n'type': 'tool_call'}]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/chat_token_usage_tracking/",
    "How-to guides\nHow to track token usage in ChatModels\nOn this page\nHow to track token usage in ChatModels\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nTracking\ntoken\nusage to calculate cost is an important part of putting your app in production. This guide goes over how to obtain this information from your LangChain model calls.\nThis guide requires\nlangchain-anthropic\nand\nlangchain-openai >= 0.3.11\n.\n%\npip install\n-\nqU langchain\n-\nanthropic langchain\n-\nopenai\nA note on streaming with OpenAI\nOpenAI's Chat Completions API does not stream token usage statistics by default (see API reference\nhere\n).\nTo recover token counts when streaming with\nChatOpenAI\nor\nAzureChatOpenAI\n, set\nstream_usage=True\nas\ndemonstrated in this guide.\nUsing LangSmith\nâ€‹\nYou can use\nLangSmith\nto help track token usage in your LLM application. See the\nLangSmith quick start guide\n.\nUsing AIMessage.usage_metadata\nâ€‹\nA number of model providers return token usage information as part of the chat generation response. When available, this information will be included on the\nAIMessage\nobjects produced by the corresponding model.\nLangChain\nAIMessage\nobjects include a\nusage_metadata\nattribute. When populated, this attribute will be a\nUsageMetadata\ndictionary with standard keys (e.g.,\n\"input_tokens\"\nand\n\"output_tokens\"\n). They will also include information on cached token usage and tokens from multi-modal data.\nExamples:\nOpenAI\n:\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nopenai_response\n=\nllm\n.\ninvoke\n(\n\"hello\"\n)\nopenai_response\n.\nusage_metadata\n{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\nAnthropic\n:\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\nanthropic_response\n=\nllm\n.\ninvoke\n(\n\"hello\"\n)\nanthropic_response\n.\nusage_metadata\n{'input_tokens': 8, 'output_tokens': 12, 'total_tokens': 20}\nStreaming\nâ€‹\nSome providers support token count metadata in a streaming context.\nOpenAI\nâ€‹\nFor example, OpenAI will return a message\nchunk\nat the end of a stream with token usage information. This behavior is supported by\nlangchain-openai >= 0.1.9\nand can be enabled by setting\nstream_usage=True\n. This attribute can also be set when\nChatOpenAI\nis instantiated.\nnote\nBy default, the last message chunk in a stream will include a\n\"finish_reason\"\nin the message's\nresponse_metadata\nattribute. If we include token usage in streaming mode, an additional chunk containing usage metadata will be added to the end of the stream, such that\n\"finish_reason\"\nappears on the second to last message chunk.\nllm\n=\ninit_chat_model\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\naggregate\n=\nNone\nfor\nchunk\nin\nllm\n.\nstream\n(\n\"hello\"\n,\nstream_usage\n=\nTrue\n)\n:\nprint\n(\nchunk\n)\naggregate\n=\nchunk\nif\naggregate\nis\nNone\nelse\naggregate\n+\nchunk\ncontent='' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent='Hello' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent='!' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent=' How' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent=' can' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent=' I' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent=' assist' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent=' you' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent=' today' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent='?' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent='' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini'} id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'\ncontent='' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\nNote that the usage metadata will be included in the sum of the individual message chunks:\nprint\n(\naggregate\n.\ncontent\n)\nprint\n(\naggregate\n.\nusage_metadata\n)\nHello! How can I assist you today?\n{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}\nTo disable streaming token counts for OpenAI, set\nstream_usage\nto False, or omit it from the parameters:\naggregate\n=\nNone\nfor\nchunk\nin\nllm\n.\nstream\n(\n\"hello\"\n)\n:\nprint\n(\nchunk\n)\ncontent='' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent='Hello' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent='!' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent=' How' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent=' can' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent=' I' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent=' assist' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent=' you' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent=' today' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent='?' id='run-8e758550-94b0-4cca-a298-57482793c25d'\ncontent='' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini'} id='run-8e758550-94b0-4cca-a298-57482793c25d'\nYou can also enable streaming token usage by setting\nstream_usage\nwhen instantiating the chat model. This can be useful when incorporating chat models into LangChain\nchains\n: usage metadata can be monitored when\nstreaming intermediate steps\nor using tracing software such as\nLangSmith\n.\nSee the below example, where we return output structured to a desired schema, but can still observe token usage streamed from intermediate steps.\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nJoke\n(\nBaseModel\n)\n:\n\"\"\"Joke to tell user.\"\"\"\nsetup\n:\nstr\n=\nField\n(\ndescription\n=\n\"question to set up a joke\"\n)\npunchline\n:\nstr\n=\nField\n(\ndescription\n=\n\"answer to resolve the joke\"\n)\nllm\n=\ninit_chat_model\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\nstream_usage\n=\nTrue\n,\n)\n# Under the hood, .with_structured_output binds tools to the\n# chat model and appends a parser.\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nJoke\n)\nasync\nfor\nevent\nin\nstructured_llm\n.\nastream_events\n(\n\"Tell me a joke\"\n)\n:\nif\nevent\n[\n\"event\"\n]\n==\n\"on_chat_model_end\"\n:\nprint\n(\nf\"Token usage:\n{\nevent\n[\n'data'\n]\n[\n'output'\n]\n.\nusage_metadata\n}\n\\n\"\n)\nelif\nevent\n[\n\"event\"\n]\n==\n\"on_chain_end\"\nand\nevent\n[\n\"name\"\n]\n==\n\"RunnableSequence\"\n:\nprint\n(\nevent\n[\n\"data\"\n]\n[\n\"output\"\n]\n)\nelse\n:\npass\nToken usage: {'input_tokens': 79, 'output_tokens': 23, 'total_tokens': 102}\nsetup='Why was the math book sad?' punchline='Because it had too many problems.'\nToken usage is also visible in the corresponding\nLangSmith trace\nin the payload from the chat model.\nUsing callbacks\nâ€‹\nRequires\nlangchain-core>=0.3.49\nLangChain implements a callback handler and context manager that will track token usage across calls of any chat model that returns\nusage_metadata\n.\nThere are also some API-specific callback context managers that maintain pricing for different models, allowing for cost estimation in real time. They are currently only implemented for the OpenAI API and Bedrock Anthropic API, and are available in\nlangchain-community\n:\nget_openai_callback\nget_bedrock_anthropic_callback\nBelow, we demonstrate the general-purpose usage metadata callback manager. We can track token usage through configuration or as a context manager.\nTracking token usage through configuration\nâ€‹\nTo track token usage through configuration, instantiate a\nUsageMetadataCallbackHandler\nand pass it into the config:\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nfrom\nlangchain_core\n.\ncallbacks\nimport\nUsageMetadataCallbackHandler\nllm_1\n=\ninit_chat_model\n(\nmodel\n=\n\"openai:gpt-4o-mini\"\n)\nllm_2\n=\ninit_chat_model\n(\nmodel\n=\n\"anthropic:claude-3-5-haiku-latest\"\n)\ncallback\n=\nUsageMetadataCallbackHandler\n(\n)\nresult_1\n=\nllm_1\n.\ninvoke\n(\n\"Hello\"\n,\nconfig\n=\n{\n\"callbacks\"\n:\n[\ncallback\n]\n}\n)\nresult_2\n=\nllm_2\n.\ninvoke\n(\n\"Hello\"\n,\nconfig\n=\n{\n\"callbacks\"\n:\n[\ncallback\n]\n}\n)\ncallback\n.\nusage_metadata\nAPI Reference:\nUsageMetadataCallbackHandler\n{'gpt-4o-mini-2024-07-18': {'input_tokens': 8,\n'output_tokens': 10,\n'total_tokens': 18,\n'input_token_details': {'audio': 0, 'cache_read': 0},\n'output_token_details': {'audio': 0, 'reasoning': 0}},\n'claude-3-5-haiku-20241022': {'input_tokens': 8,\n'output_tokens': 21,\n'total_tokens': 29,\n'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}\nTracking token usage using a context manager\nâ€‹\nYou can also use\nget_usage_metadata_callback\nto create a context manager and aggregate usage metadata there:\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nfrom\nlangchain_core\n.\ncallbacks\nimport\nget_usage_metadata_callback\nllm_1\n=\ninit_chat_model\n(\nmodel\n=\n\"openai:gpt-4o-mini\"\n)\nllm_2\n=\ninit_chat_model\n(\nmodel\n=\n\"anthropic:claude-3-5-haiku-latest\"\n)\nwith\nget_usage_metadata_callback\n(\n)\nas\ncb\n:\nllm_1\n.\ninvoke\n(\n\"Hello\"\n)\nllm_2\n.\ninvoke\n(\n\"Hello\"\n)\nprint\n(\ncb\n.\nusage_metadata\n)\nAPI Reference:\nget_usage_metadata_callback\n{'gpt-4o-mini-2024-07-18': {'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'claude-3-5-haiku-20241022': {'input_tokens': 8, 'output_tokens': 21, 'total_tokens': 29, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}\nEither of these methods will aggregate token usage across multiple calls to each model. For example, you can use it in an\nagent\nto track token usage across repeated calls to one model:\n%\npip install\n-\nqU langgraph\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\n# Create a tool\ndef\nget_weather\n(\nlocation\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Get the weather at a location.\"\"\"\nreturn\n\"It's sunny.\"\ncallback\n=\nUsageMetadataCallbackHandler\n(\n)\ntools\n=\n[\nget_weather\n]\nagent\n=\ncreate_react_agent\n(\n\"openai:gpt-4o-mini\"\n,\ntools\n)\nfor\nstep\nin\nagent\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the weather in Boston?\"\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\nconfig\n=\n{\n\"callbacks\"\n:\n[\ncallback\n]\n}\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nprint\n(\nf\"\\nTotal usage:\n{\ncallback\n.\nusage_metadata\n}\n\"\n)\nAPI Reference:\ncreate_react_agent\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat's the weather in Boston?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nget_weather (call_izMdhUYpp9Vhx7DTNAiybzGa)\nCall ID: call_izMdhUYpp9Vhx7DTNAiybzGa\nArgs:\nlocation: Boston\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: get_weather\nIt's sunny.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe weather in Boston is sunny.\nTotal usage: {'gpt-4o-mini-2024-07-18': {'input_token_details': {'audio': 0, 'cache_read': 0}, 'input_tokens': 125, 'total_tokens': 149, 'output_tokens': 24, 'output_token_details': {'audio': 0, 'reasoning': 0}}}\nNext steps\nâ€‹\nYou've now seen a few examples of how to track token usage for supported providers.\nNext, check out the other how-to guides chat models in this section, like\nhow to get a model to return structured output\nor\nhow to add caching to your chat models\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/chatbots_tools/",
    "How-to guides\nHow to add tools to chatbots\nOn this page\nHow to add tools to chatbots\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChatbots\nAgents\nChat history\nThis section will cover how to create conversational agents: chatbots that can interact with other systems and APIs using tools.\nnote\nThis how-to guide previously built a chatbot using\nRunnableWithMessageHistory\n. You can access this version of the guide in the\nv0.2 docs\n.\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of\nLangGraph persistence\nto incorporate\nmemory\ninto new LangChain applications.\nIf your code is already relying on\nRunnableWithMessageHistory\nor\nBaseChatMessageHistory\n, you do\nnot\nneed to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses\nRunnableWithMessageHistory\nwill continue to work as expected.\nPlease see\nHow to migrate to LangGraph Memory\nfor more details.\nSetup\nâ€‹\nFor this guide, we'll be using a\ntool calling agent\nwith a single tool for searching the web. The default will be powered by\nTavily\n, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\nYou'll need to\nsign up for an account\non the Tavily website, and install the following packages:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\nopenai tavily\n-\npython langgraph\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"OpenAI API Key:\"\n)\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"TAVILY_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"TAVILY_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Tavily API Key:\"\n)\nYou will also need your OpenAI key set as\nOPENAI_API_KEY\nand your Tavily API key set as\nTAVILY_API_KEY\n.\nCreating an agent\nâ€‹\nOur end goal is to create an agent that can respond conversationally to user questions while looking up information as needed.\nFirst, let's initialize Tavily and an OpenAI\nchat model\ncapable of tool calling:\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlangchain_tavily\nimport\nTavilySearch\ntools\n=\n[\nTavilySearch\n(\nmax_results\n=\n10\n,\ntopic\n=\n\"general\"\n)\n]\n# Choose the LLM that will drive the agent\n# Only certain models support this\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nTo make our agent conversational, we can also specify a prompt. Here's an example:\nprompt\n=\n(\n\"You are a helpful assistant. \"\n\"You may not need to use tools for every query - the user may just want to chat!\"\n)\nGreat! Now let's assemble our agent using LangGraph's prebuilt\ncreate_react_agent\n, which allows you to create a\ntool-calling agent\n:\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\n# prompt allows you to preprocess the inputs to the model inside ReAct agent\n# in this case, since we're passing a prompt string, we'll just always add a SystemMessage\n# with this prompt string before any other messages sent to the model\nagent\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\nprompt\n)\nAPI Reference:\ncreate_react_agent\nRunning the agent\nâ€‹\nNow that we've set up our agent, let's try interacting with it! It can handle both trivial queries that require no lookup:\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nagent\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"I'm Nemo!\"\n)\n]\n}\n)\nAPI Reference:\nHumanMessage\n{'messages': [HumanMessage(content=\"I'm Nemo!\", additional_kwargs={}, response_metadata={}, id='40b60204-1af1-40d4-b6a7-b845a2281dd6'),\nAIMessage(content='Hi Nemo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 795, 'total_tokens': 806, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsUwqprT2mVdjqu1aaSm1jVVWYVz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--08282ec6-6d3e-4495-b004-b3b08f3879c3-0', usage_metadata={'input_tokens': 795, 'output_tokens': 11, 'total_tokens': 806, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\nOr, it can use of the passed search tool to get up to date information if needed:\nagent\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"What is the current conservation status of the Great Barrier Reef?\"\n)\n]\n,\n}\n)\n{'messages': [HumanMessage(content='What is the current conservation status of the Great Barrier Reef?', additional_kwargs={}, response_metadata={}, id='5240955c-d842-408d-af3d-4ee74db29dbd'),\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_W37BFkNuZlJu9US1Tl71xpiX', 'function': {'arguments': '{\"query\":\"current conservation status of the Great Barrier Reef\",\"time_range\":\"year\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 804, 'total_tokens': 836, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsV6EJ7F1vDipoG4dpEiBRZvuTLo', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5f5b32d7-fb80-4913-a7ec-ca9c5acaa101-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'current conservation status of the Great Barrier Reef', 'time_range': 'year', 'topic': 'general'}, 'id': 'call_W37BFkNuZlJu9US1Tl71xpiX', 'type': 'tool_call'}], usage_metadata={'input_tokens': 804, 'output_tokens': 32, 'total_tokens': 836, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\nToolMessage(content='{\"query\": \"current conservation status of the Great Barrier Reef\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"The Great Barrier Reef: Current Conservation Efforts and Future Outlook\", \"url\": \"https://discoverwildscience.com/the-great-barrier-reef-current-conservation-efforts-and-future-outlook-1-279446/\", \"content\": \"The Great Barrier Reef, a mesmerizing marvel of nature, stretches over 2,300 kilometers along the northeast coast of Australia. As the largest coral reef system in the world, it is home to an incredible diversity of marine life, including more than 1,500 species of fish and 411 types of hard coral.\", \"score\": 0.6353361, \"raw_content\": null}, {\"title\": \"Monitoring progress - Protecting the Great Barrier Reef\", \"url\": \"https://www.detsi.qld.gov.au/great-barrier-reef/monitoring-progress\", \"content\": \"Stay informed about the current state of the Great Barrier Reef through comprehensive monitoring reports and reef report cards. Delve into the scientific research and advancements contributing to reef conservation. Learn about ongoing efforts to track progress and ensure the reef\\'s long-term health.\", \"score\": 0.6347929, \"raw_content\": null}, {\"title\": \"Great Barrier Reef Outlook Report shows that the reef is in serious ...\", \"url\": \"https://biodiversitycouncil.org.au/news/great-barrier-reef-outlook-report-shows-that-the-reef-is-in-serious-trouble\", \"content\": \"The Great Barrier Reef is in very serious trouble. Climate change is the biggest threat to the reef. Catchment restoration activities that reduce sediment flowing to the reef will aid the health of the reef but cannot match the scale of destruction occurring due to marine heatwaves caused by climate change.\", \"score\": 0.5183761, \"raw_content\": null}, {\"title\": \"Water pollution threatens Great Barrier Reef\\'s survival: new report ...\", \"url\": \"https://www.marineconservation.org.au/water-pollution-threatens-great-barrier-reefs-survival-new-report-highlights-funding-need/\", \"content\": \"While this investment has supported critical work across the Great Barrier Reef catchments, more funding is needed. At current rates, the target to cut fine sediment by 25% on 2009 levels will not be met until 2047, while the target to reduce dissolved inorganic nitrogen by 60% is not expected to be achieved until 2114.\", \"score\": 0.51383984, \"raw_content\": null}, {\"title\": \"What is the state of the Great Barrier Reef? - Tangaroa Blue\", \"url\": \"https://tangaroablue.org/the-state-of-the-great-barrier-reef/\", \"content\": \"The Great Barrier Reef Outlook Report 2024, prepared every five years by the Great Barrier Reef Marine Park Authority, summarises the Reef\\'s long-term outlook based on its use, management, and risks.This year\\'s report uses data from the Australian Marine Debris Initiative Database to analyse the risks and impacts of marine debris on the Great Barrier Reef and help identify areas for\", \"score\": 0.47489962, \"raw_content\": null}, {\"title\": \"New report on Great Barrier Reef shows coral cover increases before ...\", \"url\": \"https://www.aims.gov.au/information-centre/news-and-stories/new-report-great-barrier-reef-shows-coral-cover-increases-onset-serious-bleaching-cyclones\", \"content\": \"Coral cover has increased in all three regions on the Great Barrier Reef and is at regional highs in two of the three regions. But the results come with a note of caution. ... trained scientists during manta tow surveys and is a metric which allows AIMS scientists to provide an overview of the Great Barrier Reef\\'s status and keep policy\", \"score\": 0.40330887, \"raw_content\": null}, {\"title\": \"Cycle of coral bleaching on the Great Barrier Reef now at \\'catastrophic ...\", \"url\": \"https://www.sydney.edu.au/news-opinion/news/2025/01/21/coral-bleaching-2024-great-barrier-reef-one-tree-island.html\", \"content\": \"As the Great Barrier Reef faces increasing threats from climate change, the study calls for a collaborative approach to conservation that involves local communities, scientists and policymakers. Dr Shawna Foo , a Sydney Horizon Fellow and co-author of the study, said: \\\\\"Seeing the impacts on a reef that has largely avoided mass bleaching until\", \"score\": 0.3759361, \"raw_content\": null}, {\"title\": \"Great Barrier Reef Outlook Report 2024: An ecosystem under pressure\", \"url\": \"https://icriforum.org/gbr-outlook-report-2024/\", \"content\": \"The 2024 Great Barrier Reef Outlook Report is the fourth in a series of comprehensive five-yearly reports on the Reef\\'s health, pressures, management, and potential future. It found climate-driven threats such as warming oceans and severe cyclones have been compounding other impacts from crown-of-thorns starfish outbreaks, poor water quality\", \"score\": 0.34634283, \"raw_content\": null}, {\"title\": \"UNESCO expresses \\'utmost concern\\' at the state of the Great Barrier Reef\", \"url\": \"https://theconversation.com/unesco-expresses-utmost-concern-at-the-state-of-the-great-barrier-reef-257638\", \"content\": \"This 2017 photo from Ribbon Reef, near Cairns, shows what a healthy reef looks like. J Summerling/AP Poor water quality persists. Poor water quality is a major issue on the Great Barrier Reef.\", \"score\": 0.31069487, \"raw_content\": null}, {\"title\": \"Reef health updates | Reef Authority - gbrmpa\", \"url\": \"https://www2.gbrmpa.gov.au/learn/reef-health/reef-health-updates\", \"content\": \"As the lead managers of the Great Barrier Reef, the Reef Authority keeps an eye on the Reef year-round â€” with efforts stepped up over summer, a typically high-risk period from extreme weather. The Reef Authority releases updates on the health of Reef which includes; sea surface temperatures, rainfall and floods, cyclones, crown-of-thorns\", \"score\": 0.18051112, \"raw_content\": null}], \"response_time\": 2.07}', name='tavily_search', id='cbf7ae84-1df7-4ead-b00d-f8fba2152720', tool_call_id='call_W37BFkNuZlJu9US1Tl71xpiX'),\nAIMessage(content='The current conservation status of the Great Barrier Reef is concerning. The reef is facing significant threats primarily due to climate change, which is causing marine heatwaves and coral bleaching. A report highlights that while there have been some local efforts in conservation, such as catchment restoration to reduce sediment flow, these cannot keep pace with the destruction caused by climate impacts. Recent findings from the 2024 Great Barrier Reef Outlook Report indicate that climate-driven phenomena like warming oceans and severe cyclones are exacerbating other pressures, such as crown-of-thorns starfish outbreaks and poor water quality.\\n\\nSome reports have indicated that coral cover has increased in certain regions of the reef, but overall, the health of the reef remains in serious decline. Thereâ€™s an urgent call for more funding and collaborative efforts between local communities, scientists, and policymakers to enhance conservation measures.\\n\\nFor more detailed information, you can refer to these articles:\\n- [The Great Barrier Reef: Current Conservation Efforts and Future Outlook](https://discoverwildscience.com/the-great-barrier-reef-current-conservation-efforts-and-future-outlook-1-279446/)\\n- [Great Barrier Reef Outlook Report 2024: An ecosystem under pressure](https://icriforum.org/gbr-outlook-report-2024/)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 255, 'prompt_tokens': 2208, 'total_tokens': 2463, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVAxeGL7PKGVkb2DieFPE0ZPgor', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--71441b27-81a0-427f-8784-b2ea674bebd4-0', usage_metadata={'input_tokens': 2208, 'output_tokens': 255, 'total_tokens': 2463, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\nConversational responses\nâ€‹\nBecause our prompt contains a placeholder for chat history messages, our agent can also take previous interactions into account and respond conversationally like a standard chatbot:\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nHumanMessage\nagent\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"I'm Nemo!\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Hello Nemo! How can I assist you today?\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"What is my name?\"\n)\n,\n]\n,\n}\n)\nAPI Reference:\nAIMessage\n|\nHumanMessage\n{'messages': [HumanMessage(content=\"I'm Nemo!\", additional_kwargs={}, response_metadata={}, id='8a67dea0-acd8-40f9-8c28-292c5f81c05f'),\nAIMessage(content='Hello Nemo! How can I assist you today?', additional_kwargs={}, response_metadata={}, id='92a2533e-5c62-4cbe-80f1-302f5f1caf28'),\nHumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='efa8c3d3-86d7-428f-985e-a3aadd6504bc'),\nAIMessage(content='Your name is Nemo!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 818, 'total_tokens': 824, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVIf5MX5jXUEjYCorT5bWYzc7iu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a1a32c7d-8066-4954-86f9-3a8f43fcb48d-0', usage_metadata={'input_tokens': 818, 'output_tokens': 6, 'total_tokens': 824, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\nIf preferred, you can also add memory to the LangGraph agent to manage the history of messages. Let's redeclare it this way:\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nmemory\n=\nMemorySaver\n(\n)\nagent\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\nprompt\n,\ncheckpointer\n=\nmemory\n)\nAPI Reference:\nMemorySaver\nagent\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\n\"I'm Nemo!\"\n)\n]\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}\n}\n,\n)\n{'messages': [HumanMessage(content=\"I'm Nemo!\", additional_kwargs={}, response_metadata={}, id='31c2249a-13eb-4040-b56d-0c8746fa158e'),\nAIMessage(content='Hello, Nemo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 795, 'total_tokens': 807, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVRB0FItvtPawTTIAjNwgmlQFFw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a9703ca1-de4c-4f76-b622-9683d86ca777-0', usage_metadata={'input_tokens': 795, 'output_tokens': 12, 'total_tokens': 807, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\nAnd then if we rerun our wrapped agent executor:\nagent\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\n\"What is my name?\"\n)\n]\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}\n}\n,\n)\n{'messages': [HumanMessage(content=\"I'm Nemo!\", additional_kwargs={}, response_metadata={}, id='31c2249a-13eb-4040-b56d-0c8746fa158e'),\nAIMessage(content='Hello, Nemo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 795, 'total_tokens': 807, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVRB0FItvtPawTTIAjNwgmlQFFw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a9703ca1-de4c-4f76-b622-9683d86ca777-0', usage_metadata={'input_tokens': 795, 'output_tokens': 12, 'total_tokens': 807, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\nHumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='0cde6457-8d4d-45d5-b175-ad846018c4d2'),\nAIMessage(content='Your name is Nemo! How can I help you today, Nemo?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 819, 'total_tokens': 834, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BjsVTa1plxGPNitbOcw7YVTFdmz1e', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1d742bc1-5839-4837-b6f4-9a6b92fa6897-0', usage_metadata={'input_tokens': 819, 'output_tokens': 15, 'total_tokens': 834, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\nThis\nLangSmith trace\nshows what's going on under the hood.\nFurther reading\nâ€‹\nFor more on how to build agents, check these\nLangGraph\nguides:\nagents conceptual guide\nagents tutorials\ncreate_react_agent\nFor more on tool usage, you can also check out\nthis use case section\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/code_splitter/",
    "How-to guides\nHow to split code\nOn this page\nHow to split code\nRecursiveCharacterTextSplitter\nincludes pre-built lists of separators that are useful for\nsplitting text\nin a specific programming language.\nSupported languages are stored in the\nlangchain_text_splitters.Language\nenum. They include:\n\"cpp\",\n\"go\",\n\"java\",\n\"kotlin\",\n\"js\",\n\"ts\",\n\"php\",\n\"proto\",\n\"python\",\n\"rst\",\n\"ruby\",\n\"rust\",\n\"scala\",\n\"swift\",\n\"markdown\",\n\"latex\",\n\"html\",\n\"sol\",\n\"csharp\",\n\"cobol\",\n\"c\",\n\"lua\",\n\"perl\",\n\"haskell\"\nTo view the list of separators for a given language, pass a value from this enum into\nRecursiveCharacterTextSplitter\n.\nget_separators_for_language\nTo instantiate a splitter that is tailored for a specific language, pass a value from the enum into\nRecursiveCharacterTextSplitter\n.\nfrom_language\nBelow we demonstrate examples for the various languages.\n%\npip install\n-\nqU langchain\n-\ntext\n-\nsplitters\nfrom\nlangchain_text_splitters\nimport\n(\nLanguage\n,\nRecursiveCharacterTextSplitter\n,\n)\nTo view the full list of supported languages:\n[\ne\n.\nvalue\nfor\ne\nin\nLanguage\n]\n['cpp',\n'go',\n'java',\n'kotlin',\n'js',\n'ts',\n'php',\n'proto',\n'python',\n'rst',\n'ruby',\n'rust',\n'scala',\n'swift',\n'markdown',\n'latex',\n'html',\n'sol',\n'csharp',\n'cobol',\n'c',\n'lua',\n'perl',\n'haskell',\n'elixir',\n'powershell',\n'visualbasic6']\nYou can also see the separators used for a given language:\nRecursiveCharacterTextSplitter\n.\nget_separators_for_language\n(\nLanguage\n.\nPYTHON\n)\n['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']\nPython\nâ€‹\nHere's an example using the PythonTextSplitter:\nPYTHON_CODE\n=\n\"\"\"\ndef hello_world():\nprint(\"Hello, World!\")\n# Call the function\nhello_world()\n\"\"\"\npython_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nPYTHON\n,\nchunk_size\n=\n50\n,\nchunk_overlap\n=\n0\n)\npython_docs\n=\npython_splitter\n.\ncreate_documents\n(\n[\nPYTHON_CODE\n]\n)\npython_docs\n[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\nDocument(metadata={}, page_content='# Call the function\\nhello_world()')]\nJS\nâ€‹\nHere's an example using the JS text splitter:\nJS_CODE\n=\n\"\"\"\nfunction helloWorld() {\nconsole.log(\"Hello, World!\");\n}\n// Call the function\nhelloWorld();\n\"\"\"\njs_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nJS\n,\nchunk_size\n=\n60\n,\nchunk_overlap\n=\n0\n)\njs_docs\n=\njs_splitter\n.\ncreate_documents\n(\n[\nJS_CODE\n]\n)\njs_docs\n[Document(metadata={}, page_content='function helloWorld() {\\n  console.log(\"Hello, World!\");\\n}'),\nDocument(metadata={}, page_content='// Call the function\\nhelloWorld();')]\nTS\nâ€‹\nHere's an example using the TS text splitter:\nTS_CODE\n=\n\"\"\"\nfunction helloWorld(): void {\nconsole.log(\"Hello, World!\");\n}\n// Call the function\nhelloWorld();\n\"\"\"\nts_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nTS\n,\nchunk_size\n=\n60\n,\nchunk_overlap\n=\n0\n)\nts_docs\n=\nts_splitter\n.\ncreate_documents\n(\n[\nTS_CODE\n]\n)\nts_docs\n[Document(metadata={}, page_content='function helloWorld(): void {'),\nDocument(metadata={}, page_content='console.log(\"Hello, World!\");\\n}'),\nDocument(metadata={}, page_content='// Call the function\\nhelloWorld();')]\nMarkdown\nâ€‹\nHere's an example using the Markdown text splitter:\nmarkdown_text\n=\n\"\"\"\n# ðŸ¦œï¸ðŸ”— LangChain\nâš¡ Building applications with LLMs through composability âš¡\n## What is LangChain?\n# Hopefully this code block isn't split\nLangChain is a framework for...\nAs an open-source project in a rapidly developing field, we are extremely open to contributions.\n\"\"\"\nmd_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nMARKDOWN\n,\nchunk_size\n=\n60\n,\nchunk_overlap\n=\n0\n)\nmd_docs\n=\nmd_splitter\n.\ncreate_documents\n(\n[\nmarkdown_text\n]\n)\nmd_docs\n[Document(metadata={}, page_content='# ðŸ¦œï¸ðŸ”— LangChain'),\nDocument(metadata={}, page_content='âš¡ Building applications with LLMs through composability âš¡'),\nDocument(metadata={}, page_content='## What is LangChain?'),\nDocument(metadata={}, page_content=\"# Hopefully this code block isn't split\"),\nDocument(metadata={}, page_content='LangChain is a framework for...'),\nDocument(metadata={}, page_content='As an open-source project in a rapidly developing field, we'),\nDocument(metadata={}, page_content='are extremely open to contributions.')]\nLatex\nâ€‹\nHere's an example on Latex text:\nlatex_text\n=\n\"\"\"\n\\documentclass{article}\n\\begin{document}\n\\maketitle\n\\section{Introduction}\nLarge language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.\n\\subsection{History of LLMs}\nThe earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n\\subsection{Applications of LLMs}\nLLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n\\end{document}\n\"\"\"\nlatex_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nMARKDOWN\n,\nchunk_size\n=\n60\n,\nchunk_overlap\n=\n0\n)\nlatex_docs\n=\nlatex_splitter\n.\ncreate_documents\n(\n[\nlatex_text\n]\n)\nlatex_docs\n[Document(metadata={}, page_content='\\\\documentclass{article}\\n\\n\\x08egin{document}\\n\\n\\\\maketitle'),\nDocument(metadata={}, page_content='\\\\section{Introduction}'),\nDocument(metadata={}, page_content='Large language models (LLMs) are a type of machine learning'),\nDocument(metadata={}, page_content='model that can be trained on vast amounts of text data to'),\nDocument(metadata={}, page_content='generate human-like language. In recent years, LLMs have'),\nDocument(metadata={}, page_content='made significant advances in a variety of natural language'),\nDocument(metadata={}, page_content='processing tasks, including language translation, text'),\nDocument(metadata={}, page_content='generation, and sentiment analysis.'),\nDocument(metadata={}, page_content='\\\\subsection{History of LLMs}'),\nDocument(metadata={}, page_content='The earliest LLMs were developed in the 1980s and 1990s,'),\nDocument(metadata={}, page_content='but they were limited by the amount of data that could be'),\nDocument(metadata={}, page_content='processed and the computational power available at the'),\nDocument(metadata={}, page_content='time. In the past decade, however, advances in hardware and'),\nDocument(metadata={}, page_content='software have made it possible to train LLMs on massive'),\nDocument(metadata={}, page_content='datasets, leading to significant improvements in'),\nDocument(metadata={}, page_content='performance.'),\nDocument(metadata={}, page_content='\\\\subsection{Applications of LLMs}'),\nDocument(metadata={}, page_content='LLMs have many applications in industry, including'),\nDocument(metadata={}, page_content='chatbots, content creation, and virtual assistants. They'),\nDocument(metadata={}, page_content='can also be used in academia for research in linguistics,'),\nDocument(metadata={}, page_content='psychology, and computational linguistics.'),\nDocument(metadata={}, page_content='\\\\end{document}')]\nHTML\nâ€‹\nHere's an example using an HTML text splitter:\nhtml_text\n=\n\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n<title>ðŸ¦œï¸ðŸ”— LangChain</title>\n<style>\nbody {\nfont-family: Arial, sans-serif;\n}\nh1 {\ncolor: darkblue;\n}\n</style>\n</head>\n<body>\n<div>\n<h1>ðŸ¦œï¸ðŸ”— LangChain</h1>\n<p>âš¡ Building applications with LLMs through composability âš¡</p>\n</div>\n<div>\nAs an open-source project in a rapidly developing field, we are extremely open to contributions.\n</div>\n</body>\n</html>\n\"\"\"\nhtml_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nHTML\n,\nchunk_size\n=\n60\n,\nchunk_overlap\n=\n0\n)\nhtml_docs\n=\nhtml_splitter\n.\ncreate_documents\n(\n[\nhtml_text\n]\n)\nhtml_docs\n[Document(metadata={}, page_content='<!DOCTYPE html>\\n<html>'),\nDocument(metadata={}, page_content='<head>\\n        <title>ðŸ¦œï¸ðŸ”— LangChain</title>'),\nDocument(metadata={}, page_content='<style>\\n            body {\\n                font-family: Aria'),\nDocument(metadata={}, page_content='l, sans-serif;\\n            }\\n            h1 {'),\nDocument(metadata={}, page_content='color: darkblue;\\n            }\\n        </style>\\n    </head'),\nDocument(metadata={}, page_content='>'),\nDocument(metadata={}, page_content='<body>'),\nDocument(metadata={}, page_content='<div>\\n            <h1>ðŸ¦œï¸ðŸ”— LangChain</h1>'),\nDocument(metadata={}, page_content='<p>âš¡ Building applications with LLMs through composability âš¡'),\nDocument(metadata={}, page_content='</p>\\n        </div>'),\nDocument(metadata={}, page_content='<div>\\n            As an open-source project in a rapidly dev'),\nDocument(metadata={}, page_content='eloping field, we are extremely open to contributions.'),\nDocument(metadata={}, page_content='</div>\\n    </body>\\n</html>')]\nSolidity\nâ€‹\nHere's an example using the Solidity text splitter:\nSOL_CODE\n=\n\"\"\"\npragma solidity ^0.8.20;\ncontract HelloWorld {\nfunction add(uint a, uint b) pure public returns(uint) {\nreturn a + b;\n}\n}\n\"\"\"\nsol_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nSOL\n,\nchunk_size\n=\n128\n,\nchunk_overlap\n=\n0\n)\nsol_docs\n=\nsol_splitter\n.\ncreate_documents\n(\n[\nSOL_CODE\n]\n)\nsol_docs\n[Document(metadata={}, page_content='pragma solidity ^0.8.20;'),\nDocument(metadata={}, page_content='contract HelloWorld {\\n   function add(uint a, uint b) pure public returns(uint) {\\n       return a + b;\\n   }\\n}')]\nC#\nâ€‹\nHere's an example using the C# text splitter:\nC_CODE\n=\n\"\"\"\nusing System;\nclass Program\n{\nstatic void Main()\n{\nint age = 30; // Change the age value as needed\n// Categorize the age without any console output\nif (age < 18)\n{\n// Age is under 18\n}\nelse if (age >= 18 && age < 65)\n{\n// Age is an adult\n}\nelse\n{\n// Age is a senior citizen\n}\n}\n}\n\"\"\"\nc_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nCSHARP\n,\nchunk_size\n=\n128\n,\nchunk_overlap\n=\n0\n)\nc_docs\n=\nc_splitter\n.\ncreate_documents\n(\n[\nC_CODE\n]\n)\nc_docs\n[Document(metadata={}, page_content='using System;'),\nDocument(metadata={}, page_content='class Program\\n{\\n    static void Main()\\n    {\\n        int age = 30; // Change the age value as needed'),\nDocument(metadata={}, page_content='// Categorize the age without any console output\\n        if (age < 18)\\n        {\\n            // Age is under 18'),\nDocument(metadata={}, page_content='}\\n        else if (age >= 18 && age < 65)\\n        {\\n            // Age is an adult\\n        }\\n        else\\n        {'),\nDocument(metadata={}, page_content='// Age is a senior citizen\\n        }\\n    }\\n}')]\nHaskell\nâ€‹\nHere's an example using the Haskell text splitter:\nHASKELL_CODE\n=\n\"\"\"\nmain :: IO ()\nmain = do\nputStrLn \"Hello, World!\"\n-- Some sample functions\nadd :: Int -> Int -> Int\nadd x y = x + y\n\"\"\"\nhaskell_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nHASKELL\n,\nchunk_size\n=\n50\n,\nchunk_overlap\n=\n0\n)\nhaskell_docs\n=\nhaskell_splitter\n.\ncreate_documents\n(\n[\nHASKELL_CODE\n]\n)\nhaskell_docs\n[Document(metadata={}, page_content='main :: IO ()'),\nDocument(metadata={}, page_content='main = do\\n    putStrLn \"Hello, World!\"\\n-- Some'),\nDocument(metadata={}, page_content='sample functions\\nadd :: Int -> Int -> Int\\nadd x y'),\nDocument(metadata={}, page_content='= x + y')]\nPHP\nâ€‹\nHere's an example using the PHP text splitter:\nPHP_CODE\n=\n\"\"\"<?php\nnamespace foo;\nclass Hello {\npublic function __construct() { }\n}\nfunction hello() {\necho \"Hello World!\";\n}\ninterface Human {\npublic function breath();\n}\ntrait Foo { }\nenum Color\n{\ncase Red;\ncase Blue;\n}\"\"\"\nphp_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nPHP\n,\nchunk_size\n=\n50\n,\nchunk_overlap\n=\n0\n)\nphp_docs\n=\nphp_splitter\n.\ncreate_documents\n(\n[\nPHP_CODE\n]\n)\nphp_docs\n[Document(metadata={}, page_content='<?php\\nnamespace foo;'),\nDocument(metadata={}, page_content='class Hello {'),\nDocument(metadata={}, page_content='public function __construct() { }\\n}'),\nDocument(metadata={}, page_content='function hello() {\\n    echo \"Hello World!\";\\n}'),\nDocument(metadata={}, page_content='interface Human {\\n    public function breath();\\n}'),\nDocument(metadata={}, page_content='trait Foo { }\\nenum Color\\n{\\n    case Red;'),\nDocument(metadata={}, page_content='case Blue;\\n}')]\nPowerShell\nâ€‹\nHere's an example using the PowerShell text splitter:\nPOWERSHELL_CODE\n=\n\"\"\"\n$directoryPath = Get-Location\n$items = Get-ChildItem -Path $directoryPath\n$files = $items | Where-Object { -not $_.PSIsContainer }\n$sortedFiles = $files | Sort-Object LastWriteTime\nforeach ($file in $sortedFiles) {\nWrite-Output (\"Name: \" + $file.Name + \" | Last Write Time: \" + $file.LastWriteTime)\n}\n\"\"\"\npowershell_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nlanguage\n=\nLanguage\n.\nPOWERSHELL\n,\nchunk_size\n=\n100\n,\nchunk_overlap\n=\n0\n)\npowershell_docs\n=\npowershell_splitter\n.\ncreate_documents\n(\n[\nPOWERSHELL_CODE\n]\n)\npowershell_docs\n[Document(metadata={}, page_content='$directoryPath = Get-Location\\n\\n$items = Get-ChildItem -Path $directoryPath'),\nDocument(metadata={}, page_content='$files = $items | Where-Object { -not $_.PSIsContainer }'),\nDocument(metadata={}, page_content='$sortedFiles = $files | Sort-Object LastWriteTime'),\nDocument(metadata={}, page_content='foreach ($file in $sortedFiles) {'),\nDocument(metadata={}, page_content='Write-Output (\"Name: \" + $file.Name + \" | Last Write Time: \" + $file.LastWriteTime)\\n}')]\nVisual Basic 6\nâ€‹\nVISUALBASIC6_CODE\n=\n\"\"\"Option Explicit\nPublic Sub HelloWorld()\nMsgBox \"Hello, World!\"\nEnd Sub\nPrivate Function Add(a As Integer, b As Integer) As Integer\nAdd = a + b\nEnd Function\n\"\"\"\nvisualbasic6_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_language\n(\nLanguage\n.\nVISUALBASIC6\n,\nchunk_size\n=\n128\n,\nchunk_overlap\n=\n0\n,\n)\nvisualbasic6_docs\n=\nvisualbasic6_splitter\n.\ncreate_documents\n(\n[\nVISUALBASIC6_CODE\n]\n)\nvisualbasic6_docs\n[Document(metadata={}, page_content='Option Explicit'),\nDocument(metadata={}, page_content='Public Sub HelloWorld()\\n    MsgBox \"Hello, World!\"\\nEnd Sub'),\nDocument(metadata={}, page_content='Private Function Add(a As Integer, b As Integer) As Integer\\n    Add = a + b\\nEnd Function')]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/contextual_compression/",
    "How-to guides\nHow to do retrieval with contextual compression\nOn this page\nHow to do retrieval with contextual compression\nOne challenge with\nretrieval\nis that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\nContextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. â€œCompressingâ€ here refers to both compressing the contents of an individual document and filtering out documents wholesale.\nTo use the Contextual Compression Retriever, you'll need:\na base\nretriever\na Document Compressor\nThe Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether.\nGet started\nâ€‹\n# Helper function for printing docs\ndef\npretty_print_docs\n(\ndocs\n)\n:\nprint\n(\nf\"\\n\n{\n'-'\n*\n100\n}\n\\n\"\n.\njoin\n(\n[\nf\"Document\n{\ni\n+\n1\n}\n:\\n\\n\"\n+\nd\n.\npage_content\nfor\ni\n,\nd\nin\nenumerate\n(\ndocs\n)\n]\n)\n)\nUsing a vanilla vector store retriever\nâ€‹\nLet's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can see that given an example question our retriever returns one or two relevant docs and a few irrelevant docs. And even the relevant docs have a lot of irrelevant information in them.\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nTextLoader\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\ndocuments\n=\nTextLoader\n(\n\"state_of_the_union.txt\"\n)\n.\nload\n(\n)\ntext_splitter\n=\nCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n0\n)\ntexts\n=\ntext_splitter\n.\nsplit_documents\n(\ndocuments\n)\nretriever\n=\nFAISS\n.\nfrom_documents\n(\ntexts\n,\nOpenAIEmbeddings\n(\n)\n)\n.\nas_retriever\n(\n)\ndocs\n=\nretriever\n.\ninvoke\n(\n\"What did the president say about Ketanji Brown Jackson\"\n)\npretty_print_docs\n(\ndocs\n)\nDocument 1:\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.\n----------------------------------------------------------------------------------------------------\nDocument 2:\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.\nWe can do both. At our border, weâ€™ve installed new technology like cutting-edge scanners to better detect drug smuggling.\nWeâ€™ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\nWeâ€™re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.\nWeâ€™re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n----------------------------------------------------------------------------------------------------\nDocument 3:\nAnd for our LGBTQ+ Americans, letâ€™s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong.\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.\nWhile it often appears that we never agree, that isnâ€™t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.\nAnd soon, weâ€™ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things.\nSo tonight Iâ€™m offering a Unity Agenda for the Nation. Four big things we can do together.\nFirst, beat the opioid epidemic.\n----------------------------------------------------------------------------------------------------\nDocument 4:\nTonight, Iâ€™m announcing a crackdown on these companies overcharging American businesses and consumers.\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.\nThat ends on my watch.\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect.\nWeâ€™ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees.\nLetâ€™s pass the Paycheck Fairness Act and paid leave.\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty.\nLetâ€™s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jillâ€”our First Lady who teaches full-timeâ€”calls Americaâ€™s best-kept secret: community colleges.\nAdding contextual compression with an\nLLMChainExtractor\nâ€‹\nNow let's wrap our base retriever with a\nContextualCompressionRetriever\n. We'll add an\nLLMChainExtractor\n, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.\nfrom\nlangchain\n.\nretrievers\nimport\nContextualCompressionRetriever\nfrom\nlangchain\n.\nretrievers\n.\ndocument_compressors\nimport\nLLMChainExtractor\nfrom\nlangchain_openai\nimport\nOpenAI\nllm\n=\nOpenAI\n(\ntemperature\n=\n0\n)\ncompressor\n=\nLLMChainExtractor\n.\nfrom_llm\n(\nllm\n)\ncompression_retriever\n=\nContextualCompressionRetriever\n(\nbase_compressor\n=\ncompressor\n,\nbase_retriever\n=\nretriever\n)\ncompressed_docs\n=\ncompression_retriever\n.\ninvoke\n(\n\"What did the president say about Ketanji Jackson Brown\"\n)\npretty_print_docs\n(\ncompressed_docs\n)\nDocument 1:\nI did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson.\nMore built-in compressors: filters\nâ€‹\nLLMChainFilter\nâ€‹\nThe\nLLMChainFilter\nis slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents.\nfrom\nlangchain\n.\nretrievers\n.\ndocument_compressors\nimport\nLLMChainFilter\n_filter\n=\nLLMChainFilter\n.\nfrom_llm\n(\nllm\n)\ncompression_retriever\n=\nContextualCompressionRetriever\n(\nbase_compressor\n=\n_filter\n,\nbase_retriever\n=\nretriever\n)\ncompressed_docs\n=\ncompression_retriever\n.\ninvoke\n(\n\"What did the president say about Ketanji Jackson Brown\"\n)\npretty_print_docs\n(\ncompressed_docs\n)\nDocument 1:\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.\nLLMListwiseRerank\nâ€‹\nLLMListwiseRerank\nuses\nzero-shot listwise document reranking\nand functions similarly to\nLLMChainFilter\nas a robust but more expensive option. It is recommended to use a more powerful LLM.\nNote that\nLLMListwiseRerank\nrequires a model with the\nwith_structured_output\nmethod implemented.\nfrom\nlangchain\n.\nretrievers\n.\ndocument_compressors\nimport\nLLMListwiseRerank\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\n_filter\n=\nLLMListwiseRerank\n.\nfrom_llm\n(\nllm\n,\ntop_n\n=\n1\n)\ncompression_retriever\n=\nContextualCompressionRetriever\n(\nbase_compressor\n=\n_filter\n,\nbase_retriever\n=\nretriever\n)\ncompressed_docs\n=\ncompression_retriever\n.\ninvoke\n(\n\"What did the president say about Ketanji Jackson Brown\"\n)\npretty_print_docs\n(\ncompressed_docs\n)\nDocument 1:\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.\nEmbeddingsFilter\nâ€‹\nMaking an extra LLM call over each retrieved document is expensive and slow. The\nEmbeddingsFilter\nprovides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.\nfrom\nlangchain\n.\nretrievers\n.\ndocument_compressors\nimport\nEmbeddingsFilter\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\n)\nembeddings_filter\n=\nEmbeddingsFilter\n(\nembeddings\n=\nembeddings\n,\nsimilarity_threshold\n=\n0.76\n)\ncompression_retriever\n=\nContextualCompressionRetriever\n(\nbase_compressor\n=\nembeddings_filter\n,\nbase_retriever\n=\nretriever\n)\ncompressed_docs\n=\ncompression_retriever\n.\ninvoke\n(\n\"What did the president say about Ketanji Jackson Brown\"\n)\npretty_print_docs\n(\ncompressed_docs\n)\nDocument 1:\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.\n----------------------------------------------------------------------------------------------------\nDocument 2:\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.\nWe can do both. At our border, weâ€™ve installed new technology like cutting-edge scanners to better detect drug smuggling.\nWeâ€™ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\nWeâ€™re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.\nWeâ€™re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\nStringing compressors and document transformers together\nâ€‹\nUsing the\nDocumentCompressorPipeline\nwe can also easily combine multiple compressors in sequence. Along with compressors we can add\nBaseDocumentTransformer\ns to our pipeline, which don't perform any contextual compression but simply perform some transformation on a set of documents. For example\nTextSplitter\ns can be used as document transformers to split documents into smaller pieces, and the\nEmbeddingsRedundantFilter\ncan be used to filter out redundant documents based on embedding similarity between documents.\nBelow we create a compressor pipeline by first splitting our docs into smaller chunks, then removing redundant documents, and then filtering based on relevance to the query.\nfrom\nlangchain\n.\nretrievers\n.\ndocument_compressors\nimport\nDocumentCompressorPipeline\nfrom\nlangchain_community\n.\ndocument_transformers\nimport\nEmbeddingsRedundantFilter\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\nsplitter\n=\nCharacterTextSplitter\n(\nchunk_size\n=\n300\n,\nchunk_overlap\n=\n0\n,\nseparator\n=\n\". \"\n)\nredundant_filter\n=\nEmbeddingsRedundantFilter\n(\nembeddings\n=\nembeddings\n)\nrelevant_filter\n=\nEmbeddingsFilter\n(\nembeddings\n=\nembeddings\n,\nsimilarity_threshold\n=\n0.76\n)\npipeline_compressor\n=\nDocumentCompressorPipeline\n(\ntransformers\n=\n[\nsplitter\n,\nredundant_filter\n,\nrelevant_filter\n]\n)\ncompression_retriever\n=\nContextualCompressionRetriever\n(\nbase_compressor\n=\npipeline_compressor\n,\nbase_retriever\n=\nretriever\n)\ncompressed_docs\n=\ncompression_retriever\n.\ninvoke\n(\n\"What did the president say about Ketanji Jackson Brown\"\n)\npretty_print_docs\n(\ncompressed_docs\n)\nDocument 1:\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson\n----------------------------------------------------------------------------------------------------\nDocument 2:\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.\nWhile it often appears that we never agree, that isnâ€™t true. I signed 80 bipartisan bills into law last year\n----------------------------------------------------------------------------------------------------\nDocument 3:\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder\n----------------------------------------------------------------------------------------------------\nDocument 4:\nSince sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.\nWe can do both\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/convert_runnable_to_tool/",
    "How-to guides\nHow to convert Runnables to Tools\nOn this page\nHow to convert Runnables to Tools\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nRunnables\nTools\nAgents\nHere we will demonstrate how to convert a LangChain\nRunnable\ninto a tool that can be used by agents, chains, or chat models.\nDependencies\nâ€‹\nNote\n: this guide requires\nlangchain-core\n>= 0.2.13. We will also use\nOpenAI\nfor embeddings, but any LangChain embeddings should suffice. We will use a simple\nLangGraph\nagent for demonstration purposes.\n%\n%\ncapture\n-\n-\nno\n-\nstderr\n%\npip install\n-\nU langchain\n-\ncore langchain\n-\nopenai langgraph\nLangChain\ntools\nare interfaces that an agent, chain, or chat model can use to interact with the world. See\nhere\nfor how-to guides covering tool-calling, built-in tools, custom tools, and more information.\nLangChain tools-- instances of\nBaseTool\n-- are\nRunnables\nwith additional constraints that enable them to be invoked effectively by language models:\nTheir inputs are constrained to be serializable, specifically strings and Python\ndict\nobjects;\nThey contain names and descriptions indicating how and when they should be used;\nThey may contain a detailed\nargs_schema\nfor their arguments. That is, while a tool (as a\nRunnable\n) might accept a single\ndict\ninput, the specific keys and type information needed to populate a dict should be specified in the\nargs_schema\n.\nRunnables that accept string or\ndict\ninput can be converted to tools using the\nas_tool\nmethod, which allows for the specification of names, descriptions, and additional schema information for arguments.\nBasic usage\nâ€‹\nWith typed\ndict\ninput:\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\ntyping_extensions\nimport\nTypedDict\nclass\nArgs\n(\nTypedDict\n)\n:\na\n:\nint\nb\n:\nList\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n-\n>\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]\n)\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nname\n=\n\"My tool\"\n,\ndescription\n=\n\"Explanation of when to use tool.\"\n,\n)\nAPI Reference:\nRunnableLambda\nprint\n(\nas_tool\n.\ndescription\n)\nas_tool\n.\nargs_schema\n.\nmodel_json_schema\n(\n)\nExplanation of when to use tool.\n{'properties': {'a': {'title': 'A', 'type': 'integer'},\n'b': {'items': {'type': 'integer'}, 'title': 'B', 'type': 'array'}},\n'required': ['a', 'b'],\n'title': 'My tool',\n'type': 'object'}\nas_tool\n.\ninvoke\n(\n{\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]\n}\n)\n'6'\nWithout typing information, arg types can be specified via\narg_types\n:\nfrom\ntyping\nimport\nAny\n,\nDict\ndef\ng\n(\nx\n:\nDict\n[\nstr\n,\nAny\n]\n)\n-\n>\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]\n)\n)\nrunnable\n=\nRunnableLambda\n(\ng\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nname\n=\n\"My tool\"\n,\ndescription\n=\n\"Explanation of when to use tool.\"\n,\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nList\n[\nint\n]\n}\n,\n)\nAlternatively, the schema can be fully specified by directly passing the desired\nargs_schema\nfor the tool:\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nGSchema\n(\nBaseModel\n)\n:\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nList\n[\nint\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\ng\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nGSchema\n)\nString input is also supported:\ndef\nf\n(\nx\n:\nstr\n)\n-\n>\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n-\n>\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n(\n)\nas_tool\n.\ninvoke\n(\n\"b\"\n)\n'baz'\nIn agents\nâ€‹\nBelow we will incorporate LangChain Runnables as tools in an\nagent\napplication. We will demonstrate with:\na document\nretriever\n;\na simple\nRAG\nchain, allowing an agent to delegate relevant queries to it.\nWe first instantiate a chat model that supports\ntool calling\n:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nFollowing the\nRAG tutorial\n, let's first construct a retriever:\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\ndocuments\n=\n[\nDocument\n(\npage_content\n=\n\"Dogs are great companions, known for their loyalty and friendliness.\"\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Cats are independent pets that often enjoy their own space.\"\n,\n)\n,\n]\nvectorstore\n=\nInMemoryVectorStore\n.\nfrom_documents\n(\ndocuments\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\nsearch_type\n=\n\"similarity\"\n,\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n}\n,\n)\nAPI Reference:\nDocument\n|\nInMemoryVectorStore\nWe next create use a simple pre-built\nLangGraph agent\nand provide it the tool:\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\ntools\n=\n[\nretriever\n.\nas_tool\n(\nname\n=\n\"pet_info_retriever\"\n,\ndescription\n=\n\"Get information about pets.\"\n,\n)\n]\nagent\n=\ncreate_react_agent\n(\nllm\n,\ntools\n)\nAPI Reference:\ncreate_react_agent\nfor\nchunk\nin\nagent\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\n\"What are dogs known for?\"\n)\n]\n}\n)\n:\nprint\n(\nchunk\n)\nprint\n(\n\"----\"\n)\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_W8cnfOjwqEn4cFcg19LN9mYD', 'function': {'arguments': '{\"__arg1\":\"dogs\"}', 'name': 'pet_info_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 60, 'total_tokens': 79}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d7f81de9-1fb7-4caf-81ed-16dcdb0b2ab4-0', tool_calls=[{'name': 'pet_info_retriever', 'args': {'__arg1': 'dogs'}, 'id': 'call_W8cnfOjwqEn4cFcg19LN9mYD'}], usage_metadata={'input_tokens': 60, 'output_tokens': 19, 'total_tokens': 79})]}}\n----\n{'tools': {'messages': [ToolMessage(content=\"[Document(id='86f835fe-4bbe-4ec6-aeb4-489a8b541707', page_content='Dogs are great companions, known for their loyalty and friendliness.')]\", name='pet_info_retriever', tool_call_id='call_W8cnfOjwqEn4cFcg19LN9mYD')]}}\n----\n{'agent': {'messages': [AIMessage(content='Dogs are known for being great companions, known for their loyalty and friendliness.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 134, 'total_tokens': 152}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9ca5847a-a5eb-44c0-a774-84cc2c5bbc5b-0', usage_metadata={'input_tokens': 134, 'output_tokens': 18, 'total_tokens': 152})]}}\n----\nSee\nLangSmith trace\nfor the above run.\nGoing further, we can create a simple\nRAG\nchain that takes an additional parameter-- here, the \"style\" of the answer.\nfrom\noperator\nimport\nitemgetter\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nsystem_prompt\n=\n\"\"\"\nYou are an assistant for question-answering tasks.\nUse the below context to answer the question. If\nyou don't know the answer, say you don't know.\nUse three sentences maximum and keep the answer\nconcise.\nAnswer in the style of {answer_style}.\nQuestion: {question}\nContext: {context}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem_prompt\n)\n]\n)\nrag_chain\n=\n(\n{\n\"context\"\n:\nitemgetter\n(\n\"question\"\n)\n|\nretriever\n,\n\"question\"\n:\nitemgetter\n(\n\"question\"\n)\n,\n\"answer_style\"\n:\nitemgetter\n(\n\"answer_style\"\n)\n,\n}\n|\nprompt\n|\nllm\n|\nStrOutputParser\n(\n)\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\nNote that the input schema for our chain contains the required arguments, so it converts to a tool without further specification:\nrag_chain\n.\ninput_schema\n.\nmodel_json_schema\n(\n)\n{'properties': {'question': {'title': 'Question'},\n'answer_style': {'title': 'Answer Style'}},\n'required': ['question', 'answer_style'],\n'title': 'RunnableParallel<context,question,answer_style>Input',\n'type': 'object'}\nrag_tool\n=\nrag_chain\n.\nas_tool\n(\nname\n=\n\"pet_expert\"\n,\ndescription\n=\n\"Get information about pets.\"\n,\n)\nBelow we again invoke the agent. Note that the agent populates the required parameters in its\ntool_calls\n:\nagent\n=\ncreate_react_agent\n(\nllm\n,\n[\nrag_tool\n]\n)\nfor\nchunk\nin\nagent\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\n\"What would a pirate say dogs are known for?\"\n)\n]\n}\n)\n:\nprint\n(\nchunk\n)\nprint\n(\n\"----\"\n)\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_17iLPWvOD23zqwd1QVQ00Y63', 'function': {'arguments': '{\"question\":\"What are dogs known for according to pirates?\",\"answer_style\":\"quote\"}', 'name': 'pet_expert'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 59, 'total_tokens': 87}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7fef44f3-7bba-4e63-8c51-2ad9c5e65e2e-0', tool_calls=[{'name': 'pet_expert', 'args': {'question': 'What are dogs known for according to pirates?', 'answer_style': 'quote'}, 'id': 'call_17iLPWvOD23zqwd1QVQ00Y63'}], usage_metadata={'input_tokens': 59, 'output_tokens': 28, 'total_tokens': 87})]}}\n----\n{'tools': {'messages': [ToolMessage(content='\"Dogs are known for their loyalty and friendliness, making them great companions for pirates on long sea voyages.\"', name='pet_expert', tool_call_id='call_17iLPWvOD23zqwd1QVQ00Y63')]}}\n----\n{'agent': {'messages': [AIMessage(content='According to pirates, dogs are known for their loyalty and friendliness, making them great companions for pirates on long sea voyages.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 119, 'total_tokens': 146}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a30edc3-7be0-4743-b980-ca2f8cad9b8d-0', usage_metadata={'input_tokens': 119, 'output_tokens': 27, 'total_tokens': 146})]}}\n----\nSee\nLangSmith trace\nfor the above run.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/custom_callbacks/",
    "How-to guides\nHow to create custom callback handlers\nOn this page\nHow to create custom callback handlers\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nCallbacks\nLangChain has some built-in callback handlers, but you will often want to create your own handlers with custom logic.\nTo create a custom callback handler, we need to determine the\nevent(s)\nwe want our callback handler to handle as well as what we want our callback handler to do when the event is triggered. Then all we need to do is attach the callback handler to the object, for example via\nthe constructor\nor\nat runtime\n.\nIn the example below, we'll implement streaming with a custom handler.\nIn our custom callback handler\nMyCustomHandler\n, we implement the\non_llm_new_token\nhandler to print the token we have just received. We then attach our custom handler to the model object as a constructor callback.\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\ncallbacks\nimport\nBaseCallbackHandler\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nclass\nMyCustomHandler\n(\nBaseCallbackHandler\n)\n:\ndef\non_llm_new_token\n(\nself\n,\ntoken\n:\nstr\n,\n**\nkwargs\n)\n-\n>\nNone\n:\nprint\n(\nf\"My custom handler, token:\n{\ntoken\n}\n\"\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n\"Tell me a joke about {animal}\"\n]\n)\n# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n# Additionally, we pass in our custom handler as a list to the callbacks parameter\nmodel\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n,\nstreaming\n=\nTrue\n,\ncallbacks\n=\n[\nMyCustomHandler\n(\n)\n]\n)\nchain\n=\nprompt\n|\nmodel\nresponse\n=\nchain\n.\ninvoke\n(\n{\n\"animal\"\n:\n\"bears\"\n}\n)\nAPI Reference:\nBaseCallbackHandler\n|\nChatPromptTemplate\nMy custom handler, token:\nMy custom handler, token: Why\nMy custom handler, token:  don't bears wear shoes?\nBecause they\nMy custom handler, token:  prefer to go bear-foot!\nMy custom handler, token:\nYou can see\nthis reference page\nfor a list of events you can handle. Note that the\nhandle_chain_*\nevents run for most LCEL runnables.\nNext steps\nâ€‹\nYou've now learned how to create your own custom callback handlers.\nNext, check out the other how-to guides in this section, such as\nhow to attach callbacks to a runnable\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/custom_chat_model/",
    "How-to guides\nHow to create a custom chat model class\nOn this page\nHow to create a custom chat model class\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nIn this guide, we'll learn how to create a custom\nchat model\nusing LangChain abstractions.\nWrapping your LLM with the standard\nBaseChatModel\ninterface allow you to use your LLM in existing LangChain programs with minimal code modifications!\nAs a bonus, your LLM will automatically become a LangChain\nRunnable\nand will benefit from some optimizations out of the box (e.g., batch via a threadpool), async support, the\nastream_events\nAPI, etc.\nInputs and outputs\nâ€‹\nFirst, we need to talk about\nmessages\n, which are the inputs and outputs of chat models.\nMessages\nâ€‹\nChat models take messages as inputs and return a message as output.\nLangChain has a few\nbuilt-in message types\n:\nMessage Type\nDescription\nSystemMessage\nUsed for priming AI behavior, usually passed in as the first of a sequence of input messages.\nHumanMessage\nRepresents a message from a person interacting with the chat model.\nAIMessage\nRepresents a message from the chat model. This can be either text or a request to invoke a tool.\nFunctionMessage\n/\nToolMessage\nMessage for passing the results of tool invocation back to the model.\nAIMessageChunk\n/\nHumanMessageChunk\n/ ...\nChunk variant of each type of message.\nnote\nToolMessage\nand\nFunctionMessage\nclosely follow OpenAI's\nfunction\nand\ntool\nroles.\nThis is a rapidly developing field and as more models add function calling capabilities. Expect that there will be additions to this schema.\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nBaseMessage\n,\nFunctionMessage\n,\nHumanMessage\n,\nSystemMessage\n,\nToolMessage\n,\n)\nAPI Reference:\nAIMessage\n|\nBaseMessage\n|\nFunctionMessage\n|\nHumanMessage\n|\nSystemMessage\n|\nToolMessage\nStreaming Variant\nâ€‹\nAll the chat messages have a streaming variant that contains\nChunk\nin the name.\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessageChunk\n,\nFunctionMessageChunk\n,\nHumanMessageChunk\n,\nSystemMessageChunk\n,\nToolMessageChunk\n,\n)\nAPI Reference:\nAIMessageChunk\n|\nFunctionMessageChunk\n|\nHumanMessageChunk\n|\nSystemMessageChunk\n|\nToolMessageChunk\nThese chunks are used when streaming output from chat models, and they all define an additive property!\nAIMessageChunk\n(\ncontent\n=\n\"Hello\"\n)\n+\nAIMessageChunk\n(\ncontent\n=\n\" World!\"\n)\nAIMessageChunk(content='Hello World!')\nBase Chat Model\nâ€‹\nLet's implement a chat model that echoes back the first\nn\ncharacters of the last message in the prompt!\nTo do so, we will inherit from\nBaseChatModel\nand we'll need to implement the following:\nMethod/Property\nDescription\nRequired/Optional\n_generate\nUse to generate a chat result from a prompt\nRequired\n_llm_type\n(property)\nUsed to uniquely identify the type of the model. Used for logging.\nRequired\n_identifying_params\n(property)\nRepresent model parameterization for tracing purposes.\nOptional\n_stream\nUse to implement streaming.\nOptional\n_agenerate\nUse to implement a native async method.\nOptional\n_astream\nUse to implement async version of\n_stream\n.\nOptional\ntip\nThe\n_astream\nimplementation uses\nrun_in_executor\nto launch the sync\n_stream\nin a separate thread if\n_stream\nis implemented, otherwise it fallsback to use\n_agenerate\n.\nYou can use this trick if you want to reuse the\n_stream\nimplementation, but if you're able to implement code that's natively async that's a better solution since that code will run with less overhead.\nImplementation\nâ€‹\nfrom\ntyping\nimport\nAny\n,\nDict\n,\nIterator\n,\nList\n,\nOptional\nfrom\nlangchain_core\n.\ncallbacks\nimport\n(\nCallbackManagerForLLMRun\n,\n)\nfrom\nlangchain_core\n.\nlanguage_models\nimport\nBaseChatModel\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nAIMessageChunk\n,\nBaseMessage\n,\n)\nfrom\nlangchain_core\n.\nmessages\n.\nai\nimport\nUsageMetadata\nfrom\nlangchain_core\n.\noutputs\nimport\nChatGeneration\n,\nChatGenerationChunk\n,\nChatResult\nfrom\npydantic\nimport\nField\nclass\nChatParrotLink\n(\nBaseChatModel\n)\n:\n\"\"\"A custom chat model that echoes the first `parrot_buffer_length` characters\nof the input.\nWhen contributing an implementation to LangChain, carefully document\nthe model including the initialization parameters, include\nan example of how to initialize the model and include any relevant\nlinks to the underlying models documentation or API.\nExample:\n.. code-block:: python\nmodel = ChatParrotLink(parrot_buffer_length=2, model=\"bird-brain-001\")\nresult = model.invoke([HumanMessage(content=\"hello\")])\nresult = model.batch([[HumanMessage(content=\"hello\")],\n[HumanMessage(content=\"world\")]])\n\"\"\"\nmodel_name\n:\nstr\n=\nField\n(\nalias\n=\n\"model\"\n)\n\"\"\"The name of the model\"\"\"\nparrot_buffer_length\n:\nint\n\"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\ntemperature\n:\nOptional\n[\nfloat\n]\n=\nNone\nmax_tokens\n:\nOptional\n[\nint\n]\n=\nNone\ntimeout\n:\nOptional\n[\nint\n]\n=\nNone\nstop\n:\nOptional\n[\nList\n[\nstr\n]\n]\n=\nNone\nmax_retries\n:\nint\n=\n2\ndef\n_generate\n(\nself\n,\nmessages\n:\nList\n[\nBaseMessage\n]\n,\nstop\n:\nOptional\n[\nList\n[\nstr\n]\n]\n=\nNone\n,\nrun_manager\n:\nOptional\n[\nCallbackManagerForLLMRun\n]\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n-\n>\nChatResult\n:\n\"\"\"Override the _generate method to implement the chat model logic.\nThis can be a call to an API, a call to a local model, or any other\nimplementation that generates a response to the input prompt.\nArgs:\nmessages: the prompt composed of a list of messages.\nstop: a list of strings on which the model should stop generating.\nIf generation stops due to a stop token, the stop token itself\nSHOULD BE INCLUDED as part of the output. This is not enforced\nacross models right now, but it's a good practice to follow since\nit makes it much easier to parse the output of the model\ndownstream and understand why generation stopped.\nrun_manager: A run manager with callbacks for the LLM.\n\"\"\"\n# Replace this with actual logic to generate a response from a list\n# of messages.\nlast_message\n=\nmessages\n[\n-\n1\n]\ntokens\n=\nlast_message\n.\ncontent\n[\n:\nself\n.\nparrot_buffer_length\n]\nct_input_tokens\n=\nsum\n(\nlen\n(\nmessage\n.\ncontent\n)\nfor\nmessage\nin\nmessages\n)\nct_output_tokens\n=\nlen\n(\ntokens\n)\nmessage\n=\nAIMessage\n(\ncontent\n=\ntokens\n,\nadditional_kwargs\n=\n{\n}\n,\n# Used to add additional payload to the message\nresponse_metadata\n=\n{\n# Use for response metadata\n\"time_in_seconds\"\n:\n3\n,\n\"model_name\"\n:\nself\n.\nmodel_name\n,\n}\n,\nusage_metadata\n=\n{\n\"input_tokens\"\n:\nct_input_tokens\n,\n\"output_tokens\"\n:\nct_output_tokens\n,\n\"total_tokens\"\n:\nct_input_tokens\n+\nct_output_tokens\n,\n}\n,\n)\n##\ngeneration\n=\nChatGeneration\n(\nmessage\n=\nmessage\n)\nreturn\nChatResult\n(\ngenerations\n=\n[\ngeneration\n]\n)\ndef\n_stream\n(\nself\n,\nmessages\n:\nList\n[\nBaseMessage\n]\n,\nstop\n:\nOptional\n[\nList\n[\nstr\n]\n]\n=\nNone\n,\nrun_manager\n:\nOptional\n[\nCallbackManagerForLLMRun\n]\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n-\n>\nIterator\n[\nChatGenerationChunk\n]\n:\n\"\"\"Stream the output of the model.\nThis method should be implemented if the model can generate output\nin a streaming fashion. If the model does not support streaming,\ndo not implement it. In that case streaming requests will be automatically\nhandled by the _generate method.\nArgs:\nmessages: the prompt composed of a list of messages.\nstop: a list of strings on which the model should stop generating.\nIf generation stops due to a stop token, the stop token itself\nSHOULD BE INCLUDED as part of the output. This is not enforced\nacross models right now, but it's a good practice to follow since\nit makes it much easier to parse the output of the model\ndownstream and understand why generation stopped.\nrun_manager: A run manager with callbacks for the LLM.\n\"\"\"\nlast_message\n=\nmessages\n[\n-\n1\n]\ntokens\n=\nstr\n(\nlast_message\n.\ncontent\n[\n:\nself\n.\nparrot_buffer_length\n]\n)\nct_input_tokens\n=\nsum\n(\nlen\n(\nmessage\n.\ncontent\n)\nfor\nmessage\nin\nmessages\n)\nfor\ntoken\nin\ntokens\n:\nusage_metadata\n=\nUsageMetadata\n(\n{\n\"input_tokens\"\n:\nct_input_tokens\n,\n\"output_tokens\"\n:\n1\n,\n\"total_tokens\"\n:\nct_input_tokens\n+\n1\n,\n}\n)\nct_input_tokens\n=\n0\nchunk\n=\nChatGenerationChunk\n(\nmessage\n=\nAIMessageChunk\n(\ncontent\n=\ntoken\n,\nusage_metadata\n=\nusage_metadata\n)\n)\nif\nrun_manager\n:\n# This is optional in newer versions of LangChain\n# The on_llm_new_token will be called automatically\nrun_manager\n.\non_llm_new_token\n(\ntoken\n,\nchunk\n=\nchunk\n)\nyield\nchunk\n# Let's add some other information (e.g., response metadata)\nchunk\n=\nChatGenerationChunk\n(\nmessage\n=\nAIMessageChunk\n(\ncontent\n=\n\"\"\n,\nresponse_metadata\n=\n{\n\"time_in_sec\"\n:\n3\n,\n\"model_name\"\n:\nself\n.\nmodel_name\n}\n,\n)\n)\nif\nrun_manager\n:\n# This is optional in newer versions of LangChain\n# The on_llm_new_token will be called automatically\nrun_manager\n.\non_llm_new_token\n(\ntoken\n,\nchunk\n=\nchunk\n)\nyield\nchunk\n@property\ndef\n_llm_type\n(\nself\n)\n-\n>\nstr\n:\n\"\"\"Get the type of language model used by this chat model.\"\"\"\nreturn\n\"echoing-chat-model-advanced\"\n@property\ndef\n_identifying_params\n(\nself\n)\n-\n>\nDict\n[\nstr\n,\nAny\n]\n:\n\"\"\"Return a dictionary of identifying parameters.\nThis information is used by the LangChain callback system, which\nis used for tracing purposes make it possible to monitor LLMs.\n\"\"\"\nreturn\n{\n# The model name allows users to specify custom token counting\n# rules in LLM monitoring applications (e.g., in LangSmith users\n# can provide per token pricing for their model and monitor\n# costs for the given LLM.)\n\"model_name\"\n:\nself\n.\nmodel_name\n,\n}\nAPI Reference:\nCallbackManagerForLLMRun\n|\nBaseChatModel\n|\nAIMessage\n|\nAIMessageChunk\n|\nBaseMessage\n|\nUsageMetadata\n|\nChatGeneration\n|\nChatGenerationChunk\n|\nChatResult\nLet's test it ðŸ§ª\nâ€‹\nThe chat model will implement the standard\nRunnable\ninterface of LangChain which many of the LangChain abstractions support!\nmodel\n=\nChatParrotLink\n(\nparrot_buffer_length\n=\n3\n,\nmodel\n=\n\"my_custom_model\"\n)\nmodel\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"hello!\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"Hi there human!\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"Meow!\"\n)\n,\n]\n)\nAIMessage(content='Meo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-cf11aeb6-8ab6-43d7-8c68-c1ef89b6d78e-0', usage_metadata={'input_tokens': 26, 'output_tokens': 3, 'total_tokens': 29})\nmodel\n.\ninvoke\n(\n\"hello\"\n)\nAIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-618e5ed4-d611-4083-8cf1-c270726be8d9-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})\nmodel\n.\nbatch\n(\n[\n\"hello\"\n,\n\"goodbye\"\n]\n)\n[AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-eea4ed7d-d750-48dc-90c0-7acca1ff388f-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8}),\nAIMessage(content='goo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-07cfc5c1-3c62-485f-b1e0-3d46e1547287-0', usage_metadata={'input_tokens': 7, 'output_tokens': 3, 'total_tokens': 10})]\nfor\nchunk\nin\nmodel\n.\nstream\n(\n\"cat\"\n)\n:\nprint\n(\nchunk\n.\ncontent\n,\nend\n=\n\"|\"\n)\nc|a|t||\nPlease see the implementation of\n_astream\nin the model! If you do not implement it, then no output will stream.!\nasync\nfor\nchunk\nin\nmodel\n.\nastream\n(\n\"cat\"\n)\n:\nprint\n(\nchunk\n.\ncontent\n,\nend\n=\n\"|\"\n)\nc|a|t||\nLet's try to use the astream events API which will also help double check that all the callbacks were implemented!\nasync\nfor\nevent\nin\nmodel\n.\nastream_events\n(\n\"cat\"\n,\nversion\n=\n\"v1\"\n)\n:\nprint\n(\nevent\n)\n{'event': 'on_chat_model_start', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'name': 'ChatParrotLink', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='c', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 1, 'total_tokens': 4})}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='a', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a')}, 'parent_ids': []}\n{'event': 'on_chat_model_end', 'name': 'ChatParrotLink', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6})}, 'parent_ids': []}\nContributing\nâ€‹\nWe appreciate all chat model integration contributions.\nHere's a checklist to help make sure your contribution gets added to LangChain:\nDocumentation:\nThe model contains doc-strings for all initialization arguments, as these will be surfaced in the\nAPI Reference\n.\nThe class doc-string for the model contains a link to the model API if the model is powered by a service.\nTests:\nAdd unit or integration tests to the overridden methods. Verify that\ninvoke\n,\nainvoke\n,\nbatch\n,\nstream\nwork if you've over-ridden the corresponding code.\nStreaming (if you're implementing it):\nImplement the _stream method to get streaming working\nStop Token Behavior:\nStop token should be respected\nStop token should be INCLUDED as part of the response\nSecret API Keys:\nIf your model connects to an API it will likely accept API keys as part of its initialization. Use Pydantic's\nSecretStr\ntype for secrets, so they don't get accidentally printed out when folks print the model.\nIdentifying Params:\nInclude a\nmodel_name\nin identifying params\nOptimizations:\nConsider providing native async support to reduce the overhead from the model!\nProvided a native async of\n_agenerate\n(used by\nainvoke\n)\nProvided a native async of\n_astream\n(used by\nastream\n)\nNext steps\nâ€‹\nYou've now learned how to create your own custom chat models.\nNext, check out the other how-to guides chat models in this section, like\nhow to get a model to return structured output\nor\nhow to track chat model token usage\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/custom_embeddings/",
    "How-to guides\nCustom Embeddings\nOn this page\nCustom Embeddings\nLangChain is integrated with many\n3rd party embedding models\n. In this guide we'll show you how to create a custom Embedding class, in case a built-in one does not already exist. Embeddings are critical in natural language processing applications as they convert text into a numerical form that algorithms can understand, thereby enabling a wide range of applications such as similarity search, text classification, and clustering.\nImplementing embeddings using the standard\nEmbeddings\ninterface will allow your embeddings to be utilized in existing\nLangChain\nabstractions (e.g., as the embeddings powering a\nVectorStore\nor cached using\nCacheBackedEmbeddings\n).\nInterface\nâ€‹\nThe current\nEmbeddings\nabstraction in LangChain is designed to operate on text data. In this implementation, the inputs are either single strings or lists of strings, and the outputs are lists of numerical arrays (vectors), where each vector represents\nan embedding of the input text into some n-dimensional space.\nYour custom embedding must implement the following methods:\nMethod/Property\nDescription\nRequired/Optional\nembed_documents(texts)\nGenerates embeddings for a list of strings.\nRequired\nembed_query(text)\nGenerates an embedding for a single text query.\nRequired\naembed_documents(texts)\nAsynchronously generates embeddings for a list of strings.\nOptional\naembed_query(text)\nAsynchronously generates an embedding for a single text query.\nOptional\nThese methods ensure that your embedding model can be integrated seamlessly into the LangChain framework, providing both synchronous and asynchronous capabilities for scalability and performance optimization.\nnote\nEmbeddings\ndo not currently implement the\nRunnable\ninterface and are also\nnot\ninstances of pydantic\nBaseModel\n.\nEmbedding queries vs documents\nâ€‹\nThe\nembed_query\nand\nembed_documents\nmethods are required. These methods both operate\non string inputs. The accessing of\nDocument.page_content\nattributes is handled\nby the vector store using the embedding model for legacy reasons.\nembed_query\ntakes in a single string and returns a single embedding as a list of floats.\nIf your model has different modes for embedding queries vs the underlying documents, you can\nimplement this method to handle that.\nembed_documents\ntakes in a list of strings and returns a list of embeddings as a list of lists of floats.\nnote\nembed_documents\ntakes in a list of plain text, not a list of LangChain\nDocument\nobjects. The name of this method\nmay change in future versions of LangChain.\nImplementation\nâ€‹\nAs an example, we'll implement a simple embeddings model that returns a constant vector. This model is for illustrative purposes only.\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\nembeddings\nimport\nEmbeddings\nclass\nParrotLinkEmbeddings\n(\nEmbeddings\n)\n:\n\"\"\"ParrotLink embedding model integration.\n# TODO: Populate with relevant params.\nKey init args â€” completion params:\nmodel: str\nName of ParrotLink model to use.\nSee full list of supported init args and their descriptions in the params section.\n# TODO: Replace with relevant init params.\nInstantiate:\n.. code-block:: python\nfrom langchain_parrot_link import ParrotLinkEmbeddings\nembed = ParrotLinkEmbeddings(\nmodel=\"...\",\n# api_key=\"...\",\n# other params...\n)\nEmbed single text:\n.. code-block:: python\ninput_text = \"The meaning of life is 42\"\nembed.embed_query(input_text)\n.. code-block:: python\n# TODO: Example output.\n# TODO: Delete if token-level streaming isn't supported.\nEmbed multiple text:\n.. code-block:: python\ninput_texts = [\"Document 1...\", \"Document 2...\"]\nembed.embed_documents(input_texts)\n.. code-block:: python\n# TODO: Example output.\n# TODO: Delete if native async isn't supported.\nAsync:\n.. code-block:: python\nawait embed.aembed_query(input_text)\n# multiple:\n# await embed.aembed_documents(input_texts)\n.. code-block:: python\n# TODO: Example output.\n\"\"\"\ndef\n__init__\n(\nself\n,\nmodel\n:\nstr\n)\n:\nself\n.\nmodel\n=\nmodel\ndef\nembed_documents\n(\nself\n,\ntexts\n:\nList\n[\nstr\n]\n)\n-\n>\nList\n[\nList\n[\nfloat\n]\n]\n:\n\"\"\"Embed search docs.\"\"\"\nreturn\n[\n[\n0.5\n,\n0.6\n,\n0.7\n]\nfor\n_\nin\ntexts\n]\ndef\nembed_query\n(\nself\n,\ntext\n:\nstr\n)\n-\n>\nList\n[\nfloat\n]\n:\n\"\"\"Embed query text.\"\"\"\nreturn\nself\n.\nembed_documents\n(\n[\ntext\n]\n)\n[\n0\n]\n# optional: add custom async implementations here\n# you can also delete these, and the base class will\n# use the default implementation, which calls the sync\n# version in an async executor:\n# async def aembed_documents(self, texts: List[str]) -> List[List[float]]:\n#     \"\"\"Asynchronous Embed search docs.\"\"\"\n#     ...\n# async def aembed_query(self, text: str) -> List[float]:\n#     \"\"\"Asynchronous Embed query text.\"\"\"\n#     ...\nAPI Reference:\nEmbeddings\nLet's test it ðŸ§ª\nâ€‹\nembeddings\n=\nParrotLinkEmbeddings\n(\n\"test-model\"\n)\nprint\n(\nembeddings\n.\nembed_documents\n(\n[\n\"Hello\"\n,\n\"world\"\n]\n)\n)\nprint\n(\nembeddings\n.\nembed_query\n(\n\"Hello\"\n)\n)\n[[0.5, 0.6, 0.7], [0.5, 0.6, 0.7]]\n[0.5, 0.6, 0.7]\nContributing\nâ€‹\nWe welcome contributions of Embedding models to the LangChain code base.\nIf you aim to contribute an embedding model for a new provider (e.g., with a new set of dependencies or SDK), we encourage you to publish your implementation in a separate\nlangchain-*\nintegration package. This will enable you to appropriately manage dependencies and version your package. Please refer to our\ncontributing guide\nfor a walkthrough of this process.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/custom_llm/",
    "How-to guides\nHow to create a custom LLM class\nOn this page\nHow to create a custom LLM class\nThis notebook goes over how to create a custom LLM wrapper, in case you want to use your own LLM or a different wrapper than one that is supported in LangChain.\nWrapping your LLM with the standard\nLLM\ninterface allow you to use your LLM in existing LangChain programs with minimal code modifications.\nAs an bonus, your LLM will automatically become a LangChain\nRunnable\nand will benefit from some optimizations out of the box, async support, the\nastream_events\nAPI, etc.\ncaution\nYou are currently on a page documenting the use of\ntext completion models\n. Many of the latest and most popular models are\nchat completion models\n.\nUnless you are specifically using more advanced prompting techniques, you are probably looking for\nthis page instead\n.\nImplementation\nâ€‹\nThere are only two required things that a custom LLM needs to implement:\nMethod\nDescription\n_call\nTakes in a string and some optional stop words, and returns a string. Used by\ninvoke\n.\n_llm_type\nA property that returns a string, used for logging purposes only.\nOptional implementations:\nMethod\nDescription\n_identifying_params\nUsed to help with identifying the model and printing the LLM; should return a dictionary. This is a\n@property\n.\n_acall\nProvides an async native implementation of\n_call\n, used by\nainvoke\n.\n_stream\nMethod to stream the output token by token.\n_astream\nProvides an async native implementation of\n_stream\n; in newer LangChain versions, defaults to\n_stream\n.\nLet's implement a simple custom LLM that just returns the first n characters of the input.\nfrom\ntyping\nimport\nAny\n,\nDict\n,\nIterator\n,\nList\n,\nMapping\n,\nOptional\nfrom\nlangchain_core\n.\ncallbacks\n.\nmanager\nimport\nCallbackManagerForLLMRun\nfrom\nlangchain_core\n.\nlanguage_models\n.\nllms\nimport\nLLM\nfrom\nlangchain_core\n.\noutputs\nimport\nGenerationChunk\nclass\nCustomLLM\n(\nLLM\n)\n:\n\"\"\"A custom chat model that echoes the first `n` characters of the input.\nWhen contributing an implementation to LangChain, carefully document\nthe model including the initialization parameters, include\nan example of how to initialize the model and include any relevant\nlinks to the underlying models documentation or API.\nExample:\n.. code-block:: python\nmodel = CustomChatModel(n=2)\nresult = model.invoke([HumanMessage(content=\"hello\")])\nresult = model.batch([[HumanMessage(content=\"hello\")],\n[HumanMessage(content=\"world\")]])\n\"\"\"\nn\n:\nint\n\"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\ndef\n_call\n(\nself\n,\nprompt\n:\nstr\n,\nstop\n:\nOptional\n[\nList\n[\nstr\n]\n]\n=\nNone\n,\nrun_manager\n:\nOptional\n[\nCallbackManagerForLLMRun\n]\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n-\n>\nstr\n:\n\"\"\"Run the LLM on the given input.\nOverride this method to implement the LLM logic.\nArgs:\nprompt: The prompt to generate from.\nstop: Stop words to use when generating. Model output is cut off at the\nfirst occurrence of any of the stop substrings.\nIf stop tokens are not supported consider raising NotImplementedError.\nrun_manager: Callback manager for the run.\n**kwargs: Arbitrary additional keyword arguments. These are usually passed\nto the model provider API call.\nReturns:\nThe model output as a string. Actual completions SHOULD NOT include the prompt.\n\"\"\"\nif\nstop\nis\nnot\nNone\n:\nraise\nValueError\n(\n\"stop kwargs are not permitted.\"\n)\nreturn\nprompt\n[\n:\nself\n.\nn\n]\ndef\n_stream\n(\nself\n,\nprompt\n:\nstr\n,\nstop\n:\nOptional\n[\nList\n[\nstr\n]\n]\n=\nNone\n,\nrun_manager\n:\nOptional\n[\nCallbackManagerForLLMRun\n]\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n-\n>\nIterator\n[\nGenerationChunk\n]\n:\n\"\"\"Stream the LLM on the given prompt.\nThis method should be overridden by subclasses that support streaming.\nIf not implemented, the default behavior of calls to stream will be to\nfallback to the non-streaming version of the model and return\nthe output as a single chunk.\nArgs:\nprompt: The prompt to generate from.\nstop: Stop words to use when generating. Model output is cut off at the\nfirst occurrence of any of these substrings.\nrun_manager: Callback manager for the run.\n**kwargs: Arbitrary additional keyword arguments. These are usually passed\nto the model provider API call.\nReturns:\nAn iterator of GenerationChunks.\n\"\"\"\nfor\nchar\nin\nprompt\n[\n:\nself\n.\nn\n]\n:\nchunk\n=\nGenerationChunk\n(\ntext\n=\nchar\n)\nif\nrun_manager\n:\nrun_manager\n.\non_llm_new_token\n(\nchunk\n.\ntext\n,\nchunk\n=\nchunk\n)\nyield\nchunk\n@property\ndef\n_identifying_params\n(\nself\n)\n-\n>\nDict\n[\nstr\n,\nAny\n]\n:\n\"\"\"Return a dictionary of identifying parameters.\"\"\"\nreturn\n{\n# The model name allows users to specify custom token counting\n# rules in LLM monitoring applications (e.g., in LangSmith users\n# can provide per token pricing for their model and monitor\n# costs for the given LLM.)\n\"model_name\"\n:\n\"CustomChatModel\"\n,\n}\n@property\ndef\n_llm_type\n(\nself\n)\n-\n>\nstr\n:\n\"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\nreturn\n\"custom\"\nAPI Reference:\nCallbackManagerForLLMRun\n|\nLLM\n|\nGenerationChunk\nLet's test it ðŸ§ª\nâ€‹\nThis LLM will implement the standard\nRunnable\ninterface of LangChain which many of the LangChain abstractions support!\nllm\n=\nCustomLLM\n(\nn\n=\n5\n)\nprint\n(\nllm\n)\n\u001b[1mCustomLLM\u001b[0m\nParams: {'model_name': 'CustomChatModel'}\nllm\n.\ninvoke\n(\n\"This is a foobar thing\"\n)\n'This '\nawait\nllm\n.\nainvoke\n(\n\"world\"\n)\n'world'\nllm\n.\nbatch\n(\n[\n\"woof woof woof\"\n,\n\"meow meow meow\"\n]\n)\n['woof ', 'meow ']\nawait\nllm\n.\nabatch\n(\n[\n\"woof woof woof\"\n,\n\"meow meow meow\"\n]\n)\n['woof ', 'meow ']\nasync\nfor\ntoken\nin\nllm\n.\nastream\n(\n\"hello\"\n)\n:\nprint\n(\ntoken\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nh|e|l|l|o|\nLet's confirm that in integrates nicely with other\nLangChain\nAPIs.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nAPI Reference:\nChatPromptTemplate\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"you are a bot\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n]\n)\nllm\n=\nCustomLLM\n(\nn\n=\n7\n)\nchain\n=\nprompt\n|\nllm\nidx\n=\n0\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n{\n\"input\"\n:\n\"hello there!\"\n}\n,\nversion\n=\n\"v1\"\n)\n:\nprint\n(\nevent\n)\nidx\n+=\n1\nif\nidx\n>\n7\n:\n# Truncate\nbreak\n{'event': 'on_chain_start', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}\n{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}\n{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}, 'output': ChatPromptValue(messages=[SystemMessage(content='you are a bot'), HumanMessage(content='hello there!')])}}\n{'event': 'on_llm_start', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'prompts': ['System: you are a bot\\nHuman: hello there!']}}}\n{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'S'}}\n{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'S'}}\n{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'y'}}\n{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'y'}}\nContributing\nâ€‹\nWe appreciate all chat model integration contributions.\nHere's a checklist to help make sure your contribution gets added to LangChain:\nDocumentation:\nThe model contains doc-strings for all initialization arguments, as these will be surfaced in the\nAPIReference\n.\nThe class doc-string for the model contains a link to the model API if the model is powered by a service.\nTests:\nAdd unit or integration tests to the overridden methods. Verify that\ninvoke\n,\nainvoke\n,\nbatch\n,\nstream\nwork if you've over-ridden the corresponding code.\nStreaming (if you're implementing it):\nMake sure to invoke the\non_llm_new_token\ncallback\non_llm_new_token\nis invoked BEFORE yielding the chunk\nStop Token Behavior:\nStop token should be respected\nStop token should be INCLUDED as part of the response\nSecret API Keys:\nIf your model connects to an API it will likely accept API keys as part of its initialization. Use Pydantic's\nSecretStr\ntype for secrets, so they don't get accidentally printed out when folks print the model.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/custom_retriever/",
    "How-to guides\nCustom Retriever\nOn this page\nHow to create a custom Retriever\nOverview\nâ€‹\nMany LLM applications involve retrieving information from external data sources using a\nRetriever\n.\nA retriever is responsible for retrieving a list of relevant\nDocuments\nto a given user\nquery\n.\nThe retrieved documents are often formatted into prompts that are fed into an LLM, allowing the LLM to use the information in the to generate an appropriate response (e.g., answering a user question based on a knowledge base).\nInterface\nâ€‹\nTo create your own retriever, you need to extend the\nBaseRetriever\nclass and implement the following methods:\nMethod\nDescription\nRequired/Optional\n_get_relevant_documents\nGet documents relevant to a query.\nRequired\n_aget_relevant_documents\nImplement to provide async native support.\nOptional\nThe logic inside of\n_get_relevant_documents\ncan involve arbitrary calls to a database or to the web using requests.\ntip\nBy inherting from\nBaseRetriever\n, your retriever automatically becomes a LangChain\nRunnable\nand will gain the standard\nRunnable\nfunctionality out of the box!\ninfo\nYou can use a\nRunnableLambda\nor\nRunnableGenerator\nto implement a retriever.\nThe main benefit of implementing a retriever as a\nBaseRetriever\nvs. a\nRunnableLambda\n(a custom\nrunnable function\n) is that a\nBaseRetriever\nis a well\nknown LangChain entity so some tooling for monitoring may implement specialized behavior for retrievers. Another difference\nis that a\nBaseRetriever\nwill behave slightly differently from\nRunnableLambda\nin some APIs; e.g., the\nstart\nevent\nin\nastream_events\nAPI will be\non_retriever_start\ninstead of\non_chain_start\n.\nExample\nâ€‹\nLet's implement a toy retriever that returns all documents whose text contains the text in the user query.\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\ncallbacks\nimport\nCallbackManagerForRetrieverRun\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_core\n.\nretrievers\nimport\nBaseRetriever\nclass\nToyRetriever\n(\nBaseRetriever\n)\n:\n\"\"\"A toy retriever that contains the top k documents that contain the user query.\nThis retriever only implements the sync method _get_relevant_documents.\nIf the retriever were to involve file access or network access, it could benefit\nfrom a native async implementation of `_aget_relevant_documents`.\nAs usual, with Runnables, there's a default async implementation that's provided\nthat delegates to the sync implementation running on another thread.\n\"\"\"\ndocuments\n:\nList\n[\nDocument\n]\n\"\"\"List of documents to retrieve from.\"\"\"\nk\n:\nint\n\"\"\"Number of top results to return\"\"\"\ndef\n_get_relevant_documents\n(\nself\n,\nquery\n:\nstr\n,\n*\n,\nrun_manager\n:\nCallbackManagerForRetrieverRun\n)\n-\n>\nList\n[\nDocument\n]\n:\n\"\"\"Sync implementations for retriever.\"\"\"\nmatching_documents\n=\n[\n]\nfor\ndocument\nin\nself\n.\ndocuments\n:\nif\nlen\n(\nmatching_documents\n)\n>\nself\n.\nk\n:\nreturn\nmatching_documents\nif\nquery\n.\nlower\n(\n)\nin\ndocument\n.\npage_content\n.\nlower\n(\n)\n:\nmatching_documents\n.\nappend\n(\ndocument\n)\nreturn\nmatching_documents\n# Optional: Provide a more efficient native implementation by overriding\n# _aget_relevant_documents\n# async def _aget_relevant_documents(\n#     self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\n# ) -> List[Document]:\n#     \"\"\"Asynchronously get documents relevant to a query.\n#     Args:\n#         query: String to find relevant documents for\n#         run_manager: The callbacks handler to use\n#     Returns:\n#         List of relevant documents\n#     \"\"\"\nAPI Reference:\nCallbackManagerForRetrieverRun\n|\nDocument\n|\nBaseRetriever\nTest it ðŸ§ª\nâ€‹\ndocuments\n=\n[\nDocument\n(\npage_content\n=\n\"Dogs are great companions, known for their loyalty and friendliness.\"\n,\nmetadata\n=\n{\n\"type\"\n:\n\"dog\"\n,\n\"trait\"\n:\n\"loyalty\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Cats are independent pets that often enjoy their own space.\"\n,\nmetadata\n=\n{\n\"type\"\n:\n\"cat\"\n,\n\"trait\"\n:\n\"independence\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Goldfish are popular pets for beginners, requiring relatively simple care.\"\n,\nmetadata\n=\n{\n\"type\"\n:\n\"fish\"\n,\n\"trait\"\n:\n\"low maintenance\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Parrots are intelligent birds capable of mimicking human speech.\"\n,\nmetadata\n=\n{\n\"type\"\n:\n\"bird\"\n,\n\"trait\"\n:\n\"intelligence\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Rabbits are social animals that need plenty of space to hop around.\"\n,\nmetadata\n=\n{\n\"type\"\n:\n\"rabbit\"\n,\n\"trait\"\n:\n\"social\"\n}\n,\n)\n,\n]\nretriever\n=\nToyRetriever\n(\ndocuments\n=\ndocuments\n,\nk\n=\n3\n)\nretriever\n.\ninvoke\n(\n\"that\"\n)\n[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}),\nDocument(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]\nIt's a\nrunnable\nso it'll benefit from the standard Runnable Interface! ðŸ¤©\nawait\nretriever\n.\nainvoke\n(\n\"that\"\n)\n[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}),\nDocument(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]\nretriever\n.\nbatch\n(\n[\n\"dog\"\n,\n\"cat\"\n]\n)\n[[Document(page_content='Dogs are great companions, known for their loyalty and friendliness.', metadata={'type': 'dog', 'trait': 'loyalty'})],\n[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'})]]\nasync\nfor\nevent\nin\nretriever\n.\nastream_events\n(\n\"bar\"\n,\nversion\n=\n\"v1\"\n)\n:\nprint\n(\nevent\n)\n{'event': 'on_retriever_start', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'name': 'ToyRetriever', 'tags': [], 'metadata': {}, 'data': {'input': 'bar'}}\n{'event': 'on_retriever_stream', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'tags': [], 'metadata': {}, 'name': 'ToyRetriever', 'data': {'chunk': []}}\n{'event': 'on_retriever_end', 'name': 'ToyRetriever', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'tags': [], 'metadata': {}, 'data': {'output': []}}\nContributing\nâ€‹\nWe appreciate contributions of interesting retrievers!\nHere's a checklist to help make sure your contribution gets added to LangChain:\nDocumentation:\nThe retriever contains doc-strings for all initialization arguments, as these will be surfaced in the\nAPI Reference\n.\nThe class doc-string for the model contains a link to any relevant APIs used for the retriever (e.g., if the retriever is retrieving from wikipedia, it'll be good to link to the wikipedia API!)\nTests:\nAdd unit or integration tests to verify that\ninvoke\nand\nainvoke\nwork.\nOptimizations:\nIf the retriever is connecting to external data sources (e.g., an API or a file), it'll almost certainly benefit from an async native optimization!\nProvide a native async implementation of\n_aget_relevant_documents\n(used by\nainvoke\n)\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/custom_tools/",
    "How-to guides\nHow to create tools\nOn this page\nHow to create tools\nWhen constructing an\nagent\n, you will need to provide it with a list of\nTools\nthat it can use. Besides the actual function that is called, the Tool consists of several components:\nAttribute\nType\nDescription\nname\nstr\nMust be unique within a set of tools provided to an LLM or agent.\ndescription\nstr\nDescribes what the tool does. Used as context by the LLM or agent.\nargs_schema\npydantic.BaseModel\nOptional but recommended, and required if using callback handlers. It can be used to provide more information (e.g., few-shot examples) or validation for expected parameters.\nreturn_direct\nboolean\nOnly relevant for agents. When True, after invoking the given tool, the agent will stop and return the result direcly to the user.\nLangChain supports the creation of tools from:\nFunctions;\nLangChain\nRunnables\n;\nBy sub-classing from\nBaseTool\n-- This is the most flexible method, it provides the largest degree of control, at the expense of more effort and code.\nCreating tools from functions may be sufficient for most use cases, and can be done via a simple\n@tool decorator\n. If more configuration is needed-- e.g., specification of both sync and async implementations-- one can also use the\nStructuredTool.from_function\nclass method.\nIn this guide we provide an overview of these methods.\ntip\nModels will perform better if the tools have well chosen names, descriptions and JSON schemas.\nCreating tools from functions\nâ€‹\n@tool decorator\nâ€‹\nThis\n@tool\ndecorator is the simplest way to define a custom tool. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function's docstring as the tool's description - so a docstring MUST be provided.\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\n# Let's inspect some of the attributes associated with the tool.\nprint\n(\nmultiply\n.\nname\n)\nprint\n(\nmultiply\n.\ndescription\n)\nprint\n(\nmultiply\n.\nargs\n)\nAPI Reference:\ntool\nmultiply\nMultiply two numbers.\n{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\nOr create an\nasync\nimplementation, like this:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\nasync\ndef\namultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\nAPI Reference:\ntool\nNote that\n@tool\nsupports parsing of annotations, nested schemas, and other features:\nfrom\ntyping\nimport\nAnnotated\n,\nList\n@tool\ndef\nmultiply_by_max\n(\na\n:\nAnnotated\n[\nint\n,\n\"scale factor\"\n]\n,\nb\n:\nAnnotated\n[\nList\n[\nint\n]\n,\n\"list of ints over which to take maximum\"\n]\n,\n)\n-\n>\nint\n:\n\"\"\"Multiply a by the maximum of b.\"\"\"\nreturn\na\n*\nmax\n(\nb\n)\nprint\n(\nmultiply_by_max\n.\nargs_schema\n.\nmodel_json_schema\n(\n)\n)\n{'description': 'Multiply a by the maximum of b.',\n'properties': {'a': {'description': 'scale factor',\n'title': 'A',\n'type': 'integer'},\n'b': {'description': 'list of ints over which to take maximum',\n'items': {'type': 'integer'},\n'title': 'B',\n'type': 'array'}},\n'required': ['a', 'b'],\n'title': 'multiply_by_maxSchema',\n'type': 'object'}\nYou can also customize the tool name and JSON args by passing them into the tool decorator.\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nCalculatorInput\n(\nBaseModel\n)\n:\na\n:\nint\n=\nField\n(\ndescription\n=\n\"first number\"\n)\nb\n:\nint\n=\nField\n(\ndescription\n=\n\"second number\"\n)\n@tool\n(\n\"multiplication-tool\"\n,\nargs_schema\n=\nCalculatorInput\n,\nreturn_direct\n=\nTrue\n)\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\n# Let's inspect some of the attributes associated with the tool.\nprint\n(\nmultiply\n.\nname\n)\nprint\n(\nmultiply\n.\ndescription\n)\nprint\n(\nmultiply\n.\nargs\n)\nprint\n(\nmultiply\n.\nreturn_direct\n)\nmultiplication-tool\nMultiply two numbers.\n{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}\nTrue\nDocstring parsing\nâ€‹\n@tool\ncan optionally parse\nGoogle Style docstrings\nand associate the docstring components (such as arg descriptions) to the relevant parts of the tool schema. To toggle this behavior, specify\nparse_docstring\n:\n@tool\n(\nparse_docstring\n=\nTrue\n)\ndef\nfoo\n(\nbar\n:\nstr\n,\nbaz\n:\nint\n)\n-\n>\nstr\n:\n\"\"\"The foo.\nArgs:\nbar: The bar.\nbaz: The baz.\n\"\"\"\nreturn\nbar\nprint\n(\nfoo\n.\nargs_schema\n.\nmodel_json_schema\n(\n)\n)\n{'description': 'The foo.',\n'properties': {'bar': {'description': 'The bar.',\n'title': 'Bar',\n'type': 'string'},\n'baz': {'description': 'The baz.', 'title': 'Baz', 'type': 'integer'}},\n'required': ['bar', 'baz'],\n'title': 'fooSchema',\n'type': 'object'}\ncaution\nBy default,\n@tool(parse_docstring=True)\nwill raise\nValueError\nif the docstring does not parse correctly. See\nAPI Reference\nfor detail and examples.\nStructuredTool\nâ€‹\nThe\nStructuredTool.from_function\nclass method provides a bit more configurability than the\n@tool\ndecorator, without requiring much additional code.\nfrom\nlangchain_core\n.\ntools\nimport\nStructuredTool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\nasync\ndef\namultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\ncalculator\n=\nStructuredTool\n.\nfrom_function\n(\nfunc\n=\nmultiply\n,\ncoroutine\n=\namultiply\n)\nprint\n(\ncalculator\n.\ninvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\n)\n)\nprint\n(\nawait\ncalculator\n.\nainvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n5\n}\n)\n)\nAPI Reference:\nStructuredTool\n6\n10\nTo configure it:\nclass\nCalculatorInput\n(\nBaseModel\n)\n:\na\n:\nint\n=\nField\n(\ndescription\n=\n\"first number\"\n)\nb\n:\nint\n=\nField\n(\ndescription\n=\n\"second number\"\n)\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\ncalculator\n=\nStructuredTool\n.\nfrom_function\n(\nfunc\n=\nmultiply\n,\nname\n=\n\"Calculator\"\n,\ndescription\n=\n\"multiply numbers\"\n,\nargs_schema\n=\nCalculatorInput\n,\nreturn_direct\n=\nTrue\n,\n# coroutine= ... <- you can specify an async method if desired as well\n)\nprint\n(\ncalculator\n.\ninvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\n)\n)\nprint\n(\ncalculator\n.\nname\n)\nprint\n(\ncalculator\n.\ndescription\n)\nprint\n(\ncalculator\n.\nargs\n)\n6\nCalculator\nmultiply numbers\n{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}\nCreating tools from Runnables\nâ€‹\nLangChain\nRunnables\nthat accept string or\ndict\ninput can be converted to tools using the\nas_tool\nmethod, which allows for the specification of names, descriptions, and additional schema information for arguments.\nExample usage:\nfrom\nlangchain_core\n.\nlanguage_models\nimport\nGenericFakeChatModel\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"human\"\n,\n\"Hello. Please respond in the style of {answer_style}.\"\n)\n]\n)\n# Placeholder LLM\nllm\n=\nGenericFakeChatModel\n(\nmessages\n=\niter\n(\n[\n\"hello matey\"\n]\n)\n)\nchain\n=\nprompt\n|\nllm\n|\nStrOutputParser\n(\n)\nas_tool\n=\nchain\n.\nas_tool\n(\nname\n=\n\"Style responder\"\n,\ndescription\n=\n\"Description of when to use tool.\"\n)\nas_tool\n.\nargs\nAPI Reference:\nGenericFakeChatModel\n|\nStrOutputParser\n|\nChatPromptTemplate\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_95770/2548361071.py:14: LangChainBetaWarning: This API is in beta and may change in the future.\nas_tool = chain.as_tool(\n{'answer_style': {'title': 'Answer Style', 'type': 'string'}}\nSee\nthis guide\nfor more detail.\nSubclass BaseTool\nâ€‹\nYou can define a custom tool by sub-classing from\nBaseTool\n. This provides maximal control over the tool definition, but requires writing more code.\nfrom\ntyping\nimport\nOptional\nfrom\nlangchain_core\n.\ncallbacks\nimport\n(\nAsyncCallbackManagerForToolRun\n,\nCallbackManagerForToolRun\n,\n)\nfrom\nlangchain_core\n.\ntools\nimport\nBaseTool\nfrom\nlangchain_core\n.\ntools\n.\nbase\nimport\nArgsSchema\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nCalculatorInput\n(\nBaseModel\n)\n:\na\n:\nint\n=\nField\n(\ndescription\n=\n\"first number\"\n)\nb\n:\nint\n=\nField\n(\ndescription\n=\n\"second number\"\n)\n# Note: It's important that every field has type hints. BaseTool is a\n# Pydantic class and not having type hints can lead to unexpected behavior.\nclass\nCustomCalculatorTool\n(\nBaseTool\n)\n:\nname\n:\nstr\n=\n\"Calculator\"\ndescription\n:\nstr\n=\n\"useful for when you need to answer questions about math\"\nargs_schema\n:\nOptional\n[\nArgsSchema\n]\n=\nCalculatorInput\nreturn_direct\n:\nbool\n=\nTrue\ndef\n_run\n(\nself\n,\na\n:\nint\n,\nb\n:\nint\n,\nrun_manager\n:\nOptional\n[\nCallbackManagerForToolRun\n]\n=\nNone\n)\n-\n>\nint\n:\n\"\"\"Use the tool.\"\"\"\nreturn\na\n*\nb\nasync\ndef\n_arun\n(\nself\n,\na\n:\nint\n,\nb\n:\nint\n,\nrun_manager\n:\nOptional\n[\nAsyncCallbackManagerForToolRun\n]\n=\nNone\n,\n)\n-\n>\nint\n:\n\"\"\"Use the tool asynchronously.\"\"\"\n# If the calculation is cheap, you can just delegate to the sync implementation\n# as shown below.\n# If the sync calculation is expensive, you should delete the entire _arun method.\n# LangChain will automatically provide a better implementation that will\n# kick off the task in a thread to make sure it doesn't block other async code.\nreturn\nself\n.\n_run\n(\na\n,\nb\n,\nrun_manager\n=\nrun_manager\n.\nget_sync\n(\n)\n)\nAPI Reference:\nAsyncCallbackManagerForToolRun\n|\nCallbackManagerForToolRun\n|\nBaseTool\nmultiply\n=\nCustomCalculatorTool\n(\n)\nprint\n(\nmultiply\n.\nname\n)\nprint\n(\nmultiply\n.\ndescription\n)\nprint\n(\nmultiply\n.\nargs\n)\nprint\n(\nmultiply\n.\nreturn_direct\n)\nprint\n(\nmultiply\n.\ninvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\n)\n)\nprint\n(\nawait\nmultiply\n.\nainvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\n)\n)\nCalculator\nuseful for when you need to answer questions about math\n{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}\nTrue\n6\n6\nHow to create async tools\nâ€‹\nLangChain Tools implement the\nRunnable interface ðŸƒ\n.\nAll Runnables expose the\ninvoke\nand\nainvoke\nmethods (as well as other methods like\nbatch\n,\nabatch\n,\nastream\netc).\nSo even if you only provide an\nsync\nimplementation of a tool, you could still use the\nainvoke\ninterface, but there\nare some important things to know:\nLangChain's by default provides an async implementation that assumes that the function is expensive to compute, so it'll delegate execution to another thread.\nIf you're working in an async codebase, you should create async tools rather than sync tools, to avoid incuring a small overhead due to that thread.\nIf you need both sync and async implementations, use\nStructuredTool.from_function\nor sub-class from\nBaseTool\n.\nIf implementing both sync and async, and the sync code is fast to run, override the default LangChain async implementation and simply call the sync code.\nYou CANNOT and SHOULD NOT use the sync\ninvoke\nwith an\nasync\ntool.\nfrom\nlangchain_core\n.\ntools\nimport\nStructuredTool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\ncalculator\n=\nStructuredTool\n.\nfrom_function\n(\nfunc\n=\nmultiply\n)\nprint\n(\ncalculator\n.\ninvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\n)\n)\nprint\n(\nawait\ncalculator\n.\nainvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n5\n}\n)\n)\n# Uses default LangChain async implementation incurs small overhead\nAPI Reference:\nStructuredTool\n6\n10\nfrom\nlangchain_core\n.\ntools\nimport\nStructuredTool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\nasync\ndef\namultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\ncalculator\n=\nStructuredTool\n.\nfrom_function\n(\nfunc\n=\nmultiply\n,\ncoroutine\n=\namultiply\n)\nprint\n(\ncalculator\n.\ninvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\n)\n)\nprint\n(\nawait\ncalculator\n.\nainvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n5\n}\n)\n)\n# Uses use provided amultiply without additional overhead\nAPI Reference:\nStructuredTool\n6\n10\nYou should not and cannot use\n.invoke\nwhen providing only an async definition.\n@tool\nasync\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\ntry\n:\nmultiply\n.\ninvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\n)\nexcept\nNotImplementedError\n:\nprint\n(\n\"Raised not implemented error. You should not be doing this.\"\n)\nRaised not implemented error. You should not be doing this.\nHandling Tool Errors\nâ€‹\nIf you're using tools with agents, you will likely need an error handling strategy, so the agent can recover from the error and continue execution.\nA simple strategy is to throw a\nToolException\nfrom inside the tool and specify an error handler using\nhandle_tool_errors\n.\nWhen the error handler is specified, the exception will be caught and the error handler will decide which output to return from the tool.\nYou can set\nhandle_tool_errors\nto\nTrue\n, a string value, or a function. If it's a function, the function should take a\nToolException\nas a parameter and return a value.\nPlease note that only raising a\nToolException\nwon't be effective. You need to first set the\nhandle_tool_errors\nof the tool because its default value is\nFalse\n.\nfrom\nlangchain_core\n.\ntools\nimport\nToolException\ndef\nget_weather\n(\ncity\n:\nstr\n)\n-\n>\nint\n:\n\"\"\"Get weather for the given city.\"\"\"\nraise\nToolException\n(\nf\"Error: There is no city by the name of\n{\ncity\n}\n.\"\n)\nAPI Reference:\nToolException\nHere's an example with the default\nhandle_tool_errors=True\nbehavior.\nget_weather_tool\n=\nStructuredTool\n.\nfrom_function\n(\nfunc\n=\nget_weather\n,\nhandle_tool_errors\n=\nTrue\n,\n)\nget_weather_tool\n.\ninvoke\n(\n{\n\"city\"\n:\n\"foobar\"\n}\n)\n'Error: There is no city by the name of foobar.'\nWe can set\nhandle_tool_errors\nto a string that will always be returned.\nget_weather_tool\n=\nStructuredTool\n.\nfrom_function\n(\nfunc\n=\nget_weather\n,\nhandle_tool_errors\n=\n\"There is no such city, but it's probably above 0K there!\"\n,\n)\nget_weather_tool\n.\ninvoke\n(\n{\n\"city\"\n:\n\"foobar\"\n}\n)\n\"There is no such city, but it's probably above 0K there!\"\nHandling the error using a function:\ndef\n_handle_error\n(\nerror\n:\nToolException\n)\n-\n>\nstr\n:\nreturn\nf\"The following errors occurred during tool execution: `\n{\nerror\n.\nargs\n[\n0\n]\n}\n`\"\nget_weather_tool\n=\nStructuredTool\n.\nfrom_function\n(\nfunc\n=\nget_weather\n,\nhandle_tool_errors\n=\n_handle_error\n,\n)\nget_weather_tool\n.\ninvoke\n(\n{\n\"city\"\n:\n\"foobar\"\n}\n)\n'The following errors occurred during tool execution: `Error: There is no city by the name of foobar.`'\nReturning artifacts of Tool execution\nâ€‹\nSometimes there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns custom objects like Documents, we may want to pass some view or metadata about this output to the model without passing the raw output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\nThe Tool and\nToolMessage\ninterfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the ToolMessage.content) and those parts which are meant for use outside the model (ToolMessage.artifact).\nRequires\nlangchain-core >= 0.2.19\nThis functionality was added in\nlangchain-core == 0.2.19\n. Please make sure your package is up to date.\nIf we want our tool to distinguish between message content and other artifacts, we need to specify\nresponse_format=\"content_and_artifact\"\nwhen defining our tool and make sure that we return a tuple of (content, artifact):\nimport\nrandom\nfrom\ntyping\nimport\nList\n,\nTuple\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\ngenerate_random_ints\n(\nmin\n:\nint\n,\nmax\n:\nint\n,\nsize\n:\nint\n)\n-\n>\nTuple\n[\nstr\n,\nList\n[\nint\n]\n]\n:\n\"\"\"Generate size random ints in the range [min, max].\"\"\"\narray\n=\n[\nrandom\n.\nrandint\n(\nmin\n,\nmax\n)\nfor\n_\nin\nrange\n(\nsize\n)\n]\ncontent\n=\nf\"Successfully generated array of\n{\nsize\n}\nrandom ints in [\n{\nmin\n}\n,\n{\nmax\n}\n].\"\nreturn\ncontent\n,\narray\nAPI Reference:\ntool\nIf we invoke our tool directly with the tool arguments, we'll get back just the content part of the output:\ngenerate_random_ints\n.\ninvoke\n(\n{\n\"min\"\n:\n0\n,\n\"max\"\n:\n9\n,\n\"size\"\n:\n10\n}\n)\n'Successfully generated array of 10 random ints in [0, 9].'\nIf we invoke our tool with a ToolCall (like the ones generated by tool-calling models), we'll get back a ToolMessage that contains both the content and artifact generated by the Tool:\ngenerate_random_ints\n.\ninvoke\n(\n{\n\"name\"\n:\n\"generate_random_ints\"\n,\n\"args\"\n:\n{\n\"min\"\n:\n0\n,\n\"max\"\n:\n9\n,\n\"size\"\n:\n10\n}\n,\n\"id\"\n:\n\"123\"\n,\n# required\n\"type\"\n:\n\"tool_call\"\n,\n# required\n}\n)\nToolMessage(content='Successfully generated array of 10 random ints in [0, 9].', name='generate_random_ints', tool_call_id='123', artifact=[4, 8, 2, 4, 1, 0, 9, 5, 8, 1])\nWe can do the same when subclassing BaseTool:\nfrom\nlangchain_core\n.\ntools\nimport\nBaseTool\nclass\nGenerateRandomFloats\n(\nBaseTool\n)\n:\nname\n:\nstr\n=\n\"generate_random_floats\"\ndescription\n:\nstr\n=\n\"Generate size random floats in the range [min, max].\"\nresponse_format\n:\nstr\n=\n\"content_and_artifact\"\nndigits\n:\nint\n=\n2\ndef\n_run\n(\nself\n,\nmin\n:\nfloat\n,\nmax\n:\nfloat\n,\nsize\n:\nint\n)\n-\n>\nTuple\n[\nstr\n,\nList\n[\nfloat\n]\n]\n:\nrange_\n=\nmax\n-\nmin\narray\n=\n[\nround\n(\nmin\n+\n(\nrange_\n*\nrandom\n.\nrandom\n(\n)\n)\n,\nndigits\n=\nself\n.\nndigits\n)\nfor\n_\nin\nrange\n(\nsize\n)\n]\ncontent\n=\nf\"Generated\n{\nsize\n}\nfloats in [\n{\nmin\n}\n,\n{\nmax\n}\n], rounded to\n{\nself\n.\nndigits\n}\ndecimals.\"\nreturn\ncontent\n,\narray\n# Optionally define an equivalent async method\n# async def _arun(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:\n#     ...\nAPI Reference:\nBaseTool\nrand_gen\n=\nGenerateRandomFloats\n(\nndigits\n=\n4\n)\nrand_gen\n.\ninvoke\n(\n{\n\"name\"\n:\n\"generate_random_floats\"\n,\n\"args\"\n:\n{\n\"min\"\n:\n0.1\n,\n\"max\"\n:\n3.3333\n,\n\"size\"\n:\n3\n}\n,\n\"id\"\n:\n\"123\"\n,\n\"type\"\n:\n\"tool_call\"\n,\n}\n)\nToolMessage(content='Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.', name='generate_random_floats', tool_call_id='123', artifact=[1.5566, 0.5134, 2.7914])\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/debugging/",
    "How-to guides\nHow to debug your LLM apps\nOn this page\nHow to debug your LLM apps\nLike building any type of software, at some point you'll need to debug when building with LLMs. A model call will fail, or model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created.\nThere are three main methods for debugging:\nVerbose Mode: This adds print statements for \"important\" events in your chain.\nDebug Mode: This add logging statements for ALL events in your chain.\nLangSmith Tracing: This logs events to\nLangSmith\nto allow for visualization there.\nVerbose Mode\nDebug Mode\nLangSmith Tracing\nFree\nâœ…\nâœ…\nâœ…\nUI\nâŒ\nâŒ\nâœ…\nPersisted\nâŒ\nâŒ\nâœ…\nSee all events\nâŒ\nâœ…\nâœ…\nSee \"important\" events\nâœ…\nâŒ\nâœ…\nRuns Locally\nâœ…\nâœ…\nâŒ\nTracing\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\nOr, if in a notebook, you can set them with:\nimport\ngetpass\nimport\nos\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nLet's suppose we have an agent, and want to visualize the actions it takes and tool outputs it receives. Without any debugging, here's what we see:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nfrom\nlangchain\n.\nagents\nimport\nAgentExecutor\n,\ncreate_tool_calling_agent\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_tavily\nimport\nTavilySearch\ntools\n=\n[\nTavilySearch\n(\nmax_results\n=\n5\n,\ntopic\n=\n\"general\"\n)\n]\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant.\"\n,\n)\n,\n(\n\"placeholder\"\n,\n\"{chat_history}\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n(\n\"placeholder\"\n,\n\"{agent_scratchpad}\"\n)\n,\n]\n)\n# Construct the Tools agent\nagent\n=\ncreate_tool_calling_agent\n(\nllm\n,\ntools\n,\nprompt\n)\n# Create an agent executor by passing in the agent and tools\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n)\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"Who directed the 2023 film Oppenheimer and what is their age in days?\"\n}\n)\nAPI Reference:\nChatPromptTemplate\n{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?',\n'output': \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = (\\\\text{Current Year} - \\\\text{Birth Year}) \\\\times 365 + \\\\text{Extra Days for Leap Years} + \\\\text{Days from Birthday to Today's Date} \\\\]\\n\\nLet's calculate:\\n\\n- From July 30, 1970, to July 30, 2023, is 53 years.\\n- From July 30, 2023, to December 7, 2023, is 130 days.\\n\\nLeap years between 1970 and 2023 (every 4 years, except century years not divisible by 400):\\n1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020. That's 13 leap years.\\n\\nSo, his age in days is:\\n\\\\[ 53 \\\\times 365 + 13 + 130 = 19345 + 13 + 130 = 19488 \\\\text{ days} \\\\]\\n\\nChristopher Nolan is **19,488 days old** as of today.\"}\nWe don't get much output, but since we set up LangSmith we can easily see what happened under the hood:\nhttps://smith.langchain.com/public/a89ff88f-9ddc-4757-a395-3a1b365655bf/r\nset_debug\nand\nset_verbose\nâ€‹\nIf you're prototyping in Jupyter Notebooks or running Python scripts, it can be helpful to print out the intermediate steps of a chain run.\nThere are a number of ways to enable printing at varying degrees of verbosity.\nNote: These still work even with LangSmith enabled, so you can have both turned on and running at the same time\nset_verbose(True)\nâ€‹\nSetting the\nverbose\nflag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic.\nfrom\nlangchain\n.\nglobals\nimport\nset_verbose\nset_verbose\n(\nTrue\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n)\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"Who directed the 2023 film Oppenheimer and what is their age in days?\"\n}\n)\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `tavily_search` with `{'query': 'director of the 2023 film Oppenheimer'}`\n\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'director of the 2023 film Oppenheimer', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Oppenheimer (film) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Oppenheimer_(film)', 'content': \"Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Toggle the table of contents Contents move to sidebar hide (Top) 1 Plot 2 Cast 3 ProductionToggle Production subsection 3.1 Development 3.2 Writing 3.3 Casting 3.4 Filming 3.5 Post-production 4 Music 5 Marketing 6 ReleaseToggle Release subsection 6.1 Theatrical 6.1.1 Classifications and censorship 6.1.2 Bhagavad Gita controversy 6.2 Home media 7 ReceptionToggle Reception subsection 7.1 Box office 7.1.1 United States and Canada 7.1.2 Japan 7.1.3 Other territories 7.2 Critical response 7.3 Influence on legislation 8 Accuracy and omissions 9 Accolades 10 See also 11 Notes 12 References 13 Further reading 14 External links Oppenheimer (film) 70 languages Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾ AzÉ™rbaycanca à¦¬à¦¾à¦‚à¦²à¦¾ Ð‘ÐµÐ»Ð°Ñ€ÑƒÑÐºÐ°Ñ à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ Ð‘ÑŠÐ»Ð³Ð°Ñ€ÑÐºÐ¸ Bosanski CatalÃ  ÄŒeÅ¡tina Cymraeg Dansk Deutsch à¤¡à¥‹à¤Ÿà¥‡à¤²à¥€ Eesti Î•Î»Î»Î·Î½Î¹ÎºÎ¬ EspaÃ±ol Euskara ÙØ§Ø±Ø³ÛŒ FranÃ§ais Gaeilge Galego í•œêµ­ì–´ Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶ à¤¹à¤¿à¤¨à¥à¤¦à¥€ Ido Bahasa Indonesia Italiano ×¢×‘×¨×™×ª Jawa áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ ÒšÐ°Ð·Ð°Ò›ÑˆÐ° Latina LatvieÅ¡u LietuviÅ³ Magyar ÐœÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸ à´®à´²à´¯à´¾à´³à´‚ à¤®à¤°à¤¾à¤ à¥€ Ù…ØµØ±Ù‰ Ù…Ø§Ø²ÙØ±ÙˆÙ†ÛŒ Bahasa Melayu Nederlands à¤¨à¥‡à¤ªà¤¾à¤²à¥€ æ—¥æœ¬èªž Norsk bokmÃ¥l OÊ»zbekcha / ÑžÐ·Ð±ÐµÐºÑ‡Ð° à¨ªà©°à¨œà¨¾à¨¬à©€ Polski PortuguÃªs RomÃ¢nÄƒ Ð ÑƒÑÑÐºÐ¸Ð¹ Shqip Simple English SlovenÄina SlovenÅ¡Äina Ú©ÙˆØ±Ø¯ÛŒ Ð¡Ñ€Ð¿ÑÐºÐ¸ / srpski Suomi Svenska à®¤à®®à®¿à®´à¯ à°¤à±†à°²à±à°—à± à¹„à¸—à¸¢ Ð¢Ð¾Ò·Ð¸ÐºÓ£ TÃ¼rkÃ§e Ð£ÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ° Ø§Ø±Ø¯Ùˆ Tiáº¿ng Viá»‡t ç²µèªž ä¸­æ–‡ Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Expand all Edit interlanguage links Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiquote Wikidata item From Wikipedia, the free encyclopedia 2023 film by Christopher Nolan | Oppenheimer | | --- | | Theatrical release poster | | Directed by | Christopher Nolan | | Screenplay by | Christopher Nolan | | Based on | American Prometheus by Kai Bird Martin J. Sherwin | | Produced by | Emma Thomas Charles Roven Christopher Nolan | | Starring | Cillian Murphy Emily Blunt Matt Damon Robert Downey Jr. Florence Pugh Josh Hartnett Casey Affleck Rami Malek Kenneth Branagh | | Cinematography | Hoyte van Hoytema | | Edited by | Jennifer Lame | | Music by | Ludwig GÃ¶ransson | | Production companies | Universal Pictures[1][2] Syncopy[1][2] Atlas Entertainment[1][2] Breakheart Films[2] Peters Creek Entertainment[2] Gadget Films[1][3] | | Distributed by | Universal Pictures | | Release dates | July 11, 2023 (2023-07-11) (Le Grand Rex) July 21, 2023 (2023-07-21) (United States and United Kingdom) | | Running time | 180 minutes[4] | | Countries | United States United Kingdom | | Language | English | | Budget | $100 million[5] | | Box office | $975.8 million[6][7] | Oppenheimer is a 2023 epic biographical drama film written, produced, and directed by Christopher Nolan. [8] It follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film dramatizes Oppenheimer's studies, his direction of the Los Alamos Laboratory and his 1954 security hearing. Oppenheimer received critical acclaim and grossed $975 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film of all time at the time of its release.\", 'score': 0.9475027, 'raw_content': None}, {'title': 'Oppenheimer | Cast, Film, Length, Plot, Actors, Awards, & Facts ...', 'url': 'https://www.britannica.com/topic/Oppenheimer-film', 'content': 'J. Robert Oppenheimer Robert Downey, Jr. Oppenheimer # Oppenheimer Oppenheimer, American and British dramatic biographical film, released in 2023, that explores the life and legacy of the American physicist J. Robert Oppenheimer, who played a key role in the development of the atomic bomb. Robert Oppenheimer (2005). Film criticsâ€™ reaction to Oppenheimer was overwhelmingly positive. Oppenheimer grossed more than $300 million domestically and more than $600 million internationally by the end of November 2023, making it the second highest grossing R-rated film of all time. The film also dominated the Academy Awards nominations, garnering 13 nominations compared with the 8 for Greta Gerwigâ€™s Barbie, which opened the same weekend as Oppenheimer but topped Nolanâ€™s film at the box office.', 'score': 0.76194656, 'raw_content': None}, {'title': 'Oppenheimer (2023) - Full cast & crew - IMDb', 'url': 'https://www.imdb.com/title/tt15398776/fullcredits/', 'content': 'Oppenheimer (2023) - Cast and crew credits, including actors, actresses, directors, writers and more. Menu. ... Oscars Pride Month American Black Film Festival Summer Watch Guide STARmeter Awards Awards Central Festival Central All Events. ... second unit director: visual effects (uncredited) Francesca Kaimer Millea.', 'score': 0.683948, 'raw_content': None}, {'title': \"'Oppenheimer' director Christopher Nolan says the film is his darkest - NPR\", 'url': 'https://www.npr.org/2023/08/14/1193448291/oppenheimer-director-christopher-nolan', 'content': '# \\'Like it or not, we live in Oppenheimer\\'s world,\\' says director Christopher Nolan #### \\'Like it or not, we live in Oppenheimer\\'s world,\\' says director Christopher Nolan But he says the story of Robert Oppenheimer, known as the father of the atomic bomb, stayed with him in a way his other films didn\\'t. Nolan says he was drawn to the tension of Oppenheimer\\'s story â€” particularly the disconnect between the joy the physicist felt at the success of the Trinity test, and the horror that later resulted. Writer, director and producer Christopher Nolan says Oppenheimer is the \"darkest\" of all the films he\\'s worked on. Writer, director and producer Christopher Nolan says Oppenheimer is the \"darkest\" of all the films he\\'s worked on.', 'score': 0.6255073, 'raw_content': None}, {'title': 'An extended interview with Christopher Nolan, director of Oppenheimer', 'url': 'https://thebulletin.org/premium/2023-07/an-extended-interview-with-christopher-nolan-director-of-oppenheimer/', 'content': 'A group of Manhattan Project scientists and engineers also focused on wider public education on nuclear weapons and energy (and science generally) through the creation of the Bulletin of the Atomic Scientists; Oppenheimer served as the first chair of the magazineâ€™s Board of Sponsors.[5] As time has passed, more evidence has come to light of the bias and unfairness of the process that Dr. Oppenheimer was subjected to while the evidence of his loyalty and love of country have only been further affirmed.â€[8] Decades after the fact, records of the Oppenheimer security hearing made it clear that, rather than any disloyalty to the nation, it was his principled opposition to development of the hydrogen bombâ€”a nuclear fusion-based weapon of immensely greater power than the fission weapons used to decimate Hiroshima and Nagasaki in 1945â€”that was key to the decision to essentially bar him from government service. Robert Oppenheimer, Los Alamos, Manhattan Project, Nolan, atomic bomb, movie', 'score': 0.32472825, 'raw_content': None}], 'response_time': 0.94}\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `tavily_search` with `{'query': 'birthdate of the director of the 2023 film Oppenheimer'}`\n\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'birthdate of the director of the 2023 film Oppenheimer', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Oppenheimer (film) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Oppenheimer_(film)', 'content': \"Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Toggle the table of contents Contents move to sidebar hide (Top) 1 Plot 2 Cast 3 ProductionToggle Production subsection 3.1 Development 3.2 Writing 3.3 Casting 3.4 Filming 3.5 Post-production 4 Music 5 Marketing 6 ReleaseToggle Release subsection 6.1 Theatrical 6.1.1 Classifications and censorship 6.1.2 Bhagavad Gita controversy 6.2 Home media 7 ReceptionToggle Reception subsection 7.1 Box office 7.1.1 United States and Canada 7.1.2 Japan 7.1.3 Other territories 7.2 Critical response 7.3 Influence on legislation 8 Accuracy and omissions 9 Accolades 10 See also 11 Notes 12 References 13 Further reading 14 External links Oppenheimer (film) 70 languages Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾ AzÉ™rbaycanca à¦¬à¦¾à¦‚à¦²à¦¾ Ð‘ÐµÐ»Ð°Ñ€ÑƒÑÐºÐ°Ñ à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ Ð‘ÑŠÐ»Ð³Ð°Ñ€ÑÐºÐ¸ Bosanski CatalÃ  ÄŒeÅ¡tina Cymraeg Dansk Deutsch à¤¡à¥‹à¤Ÿà¥‡à¤²à¥€ Eesti Î•Î»Î»Î·Î½Î¹ÎºÎ¬ EspaÃ±ol Euskara ÙØ§Ø±Ø³ÛŒ FranÃ§ais Gaeilge Galego í•œêµ­ì–´ Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶ à¤¹à¤¿à¤¨à¥à¤¦à¥€ Ido Bahasa Indonesia Italiano ×¢×‘×¨×™×ª Jawa áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ ÒšÐ°Ð·Ð°Ò›ÑˆÐ° Latina LatvieÅ¡u LietuviÅ³ Magyar ÐœÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸ à´®à´²à´¯à´¾à´³à´‚ à¤®à¤°à¤¾à¤ à¥€ Ù…ØµØ±Ù‰ Ù…Ø§Ø²ÙØ±ÙˆÙ†ÛŒ Bahasa Melayu Nederlands à¤¨à¥‡à¤ªà¤¾à¤²à¥€ æ—¥æœ¬èªž Norsk bokmÃ¥l OÊ»zbekcha / ÑžÐ·Ð±ÐµÐºÑ‡Ð° à¨ªà©°à¨œà¨¾à¨¬à©€ Polski PortuguÃªs RomÃ¢nÄƒ Ð ÑƒÑÑÐºÐ¸Ð¹ Shqip Simple English SlovenÄina SlovenÅ¡Äina Ú©ÙˆØ±Ø¯ÛŒ Ð¡Ñ€Ð¿ÑÐºÐ¸ / srpski Suomi Svenska à®¤à®®à®¿à®´à¯ à°¤à±†à°²à±à°—à± à¹„à¸—à¸¢ Ð¢Ð¾Ò·Ð¸ÐºÓ£ TÃ¼rkÃ§e Ð£ÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ° Ø§Ø±Ø¯Ùˆ Tiáº¿ng Viá»‡t ç²µèªž ä¸­æ–‡ Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Expand all Edit interlanguage links Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiquote Wikidata item From Wikipedia, the free encyclopedia 2023 film by Christopher Nolan | Oppenheimer | | --- | | Theatrical release poster | | Directed by | Christopher Nolan | | Screenplay by | Christopher Nolan | | Based on | American Prometheus by Kai Bird Martin J. Sherwin | | Produced by | Emma Thomas Charles Roven Christopher Nolan | | Starring | Cillian Murphy Emily Blunt Matt Damon Robert Downey Jr. Florence Pugh Josh Hartnett Casey Affleck Rami Malek Kenneth Branagh | | Cinematography | Hoyte van Hoytema | | Edited by | Jennifer Lame | | Music by | Ludwig GÃ¶ransson | | Production companies | Universal Pictures[1][2] Syncopy[1][2] Atlas Entertainment[1][2] Breakheart Films[2] Peters Creek Entertainment[2] Gadget Films[1][3] | | Distributed by | Universal Pictures | | Release dates | July 11, 2023 (2023-07-11) (Le Grand Rex) July 21, 2023 (2023-07-21) (United States and United Kingdom) | | Running time | 180 minutes[4] | | Countries | United States United Kingdom | | Language | English | | Budget | $100 million[5] | | Box office | $975.8 million[6][7] | Oppenheimer is a 2023 epic biographical drama film written, produced, and directed by Christopher Nolan. [8] It follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film dramatizes Oppenheimer's studies, his direction of the Los Alamos Laboratory and his 1954 security hearing. Oppenheimer received critical acclaim and grossed $975 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film of all time at the time of its release.\", 'score': 0.9092728, 'raw_content': None}, {'title': 'Oppenheimer (movie) - Simple English Wikipedia, the free encyclopedia', 'url': 'https://simple.wikipedia.org/wiki/Oppenheimer_(movie)', 'content': 'Oppenheimer (movie) - Simple English Wikipedia, the free encyclopedia Oppenheimer (movie) Oppenheimer is a 2023 epic biographical thriller movie written and directed by Christopher Nolan. Robert Oppenheimer, a theoretical physicist who helped create the first nuclear weapons as part of the Manhattan Project. With $975 million at the box office, Oppenheimer is the highest-grossing biographical movie of all time, beating Bohemian Rhapsody (2018).[5][6] Josh Hartnett as Ernest Lawrence, a Nobel-winning nuclear physicist who worked with Oppenheimer at the University of California, Berkeley. Dylan Arnold as Frank Oppenheimer, Robertâ€™s younger brother and a particle physicist who worked on the Manhattan Project. Retrieved from \"https://simple.wikipedia.org/w/index.php?title=Oppenheimer_(movie)&oldid=10077836\" *   2023 movies Oppenheimer (movie)', 'score': 0.7961819, 'raw_content': None}, {'title': 'Oppenheimer | Cast, Film, Length, Plot, Actors, Awards, & Facts ...', 'url': 'https://www.britannica.com/topic/Oppenheimer-film', 'content': 'J. Robert Oppenheimer Robert Downey, Jr. Oppenheimer # Oppenheimer Oppenheimer, American and British dramatic biographical film, released in 2023, that explores the life and legacy of the American physicist J. Robert Oppenheimer, who played a key role in the development of the atomic bomb. Robert Oppenheimer (2005). Film criticsâ€™ reaction to Oppenheimer was overwhelmingly positive. Oppenheimer grossed more than $300 million domestically and more than $600 million internationally by the end of November 2023, making it the second highest grossing R-rated film of all time. The film also dominated the Academy Awards nominations, garnering 13 nominations compared with the 8 for Greta Gerwigâ€™s Barbie, which opened the same weekend as Oppenheimer but topped Nolanâ€™s film at the box office.', 'score': 0.6854659, 'raw_content': None}, {'title': 'Oppenheimer (2023) - IMDb', 'url': 'https://www.imdb.com/title/tt15398776/', 'content': \"Oppenheimer (2023) - IMDb Oppenheimer IMDb RATING Robert Oppenheimer, the physicist who had a large hand in the development of the atomic bombs that brought an end to World War II.A dramatization of the life story of J. Robert Oppenheimer, the physicist who had a large hand in the development of the atomic bombs that brought an end to World War II.A dramatization of the life story of J. Robert Oppenheimer, the physicist who had a large hand in the development of the atomic bombs that brought an end to World War II. J. Robert Oppenheimer Cillian Murphy and the cast of Oppenheimer discuss what it's like to work with a singular director like Christopher Nolan. J. Robert Oppenheimer: Albert?\", 'score': 0.5951402, 'raw_content': None}, {'title': 'Oppenheimer (film) - Wikiwand', 'url': 'https://www.wikiwand.com/en/articles/Oppenheimer_(2023_film)', 'content': \"Development Kai Bird (pictured) and Martin J. Sherwin are the authors of J. Robert Oppenheimer's biography American Prometheus (2005), on which the film is based.. Director Sam Mendes was interested in adapting the 2005 J. Robert Oppenheimer biography American Prometheus by Kai Bird and Martin J. Sherwin.After that project failed to materialize, the book was optioned by various filmmakers over\", 'score': 0.3386242, 'raw_content': None}], 'response_time': 4.11}\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `tavily_search` with `{'query': 'birthdate of Christopher Nolan'}`\nresponded: The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\nTo calculate Christopher Nolan's age in days, I need to find his birthdate. Let me find that information for you.\n\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'birthdate of Christopher Nolan', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Christopher Nolan - Age, Family, Bio | Famous Birthdays', 'url': 'https://www.famousbirthdays.com/people/christopher-nolan.html', 'content': 'Christopher Nolan Director Birthday July 30, 1970 Birth Sign Leo Birthplace London, England Age 54 years old #10,366 Most Popular About British-American director, screenwriter, and producer who first received acclaim for his 2000 indie suspense thriller Memento. He then shifted from art-house films to blockbusters with the box office hits The Dark Knight, Inception, and Interstellar. He won his first Academy Awards for Best Director and Best Picture for his 2023 film Oppenheimer. Trivia In 2003, he approached Warner Bros. with his pitch for a new Batman franchise more grounded in a realistic world than a comic book world. He signed a contract with the studio, and produced three Batman features from 2005 to 2012: Batman Begins, The Dark Knight and The Dark Knight Rises.', 'score': 0.8939131, 'raw_content': None}, {'title': 'Christopher Nolan | Biography, Movies, Batman, Oppenheimer, & Facts ...', 'url': 'https://www.britannica.com/biography/Christopher-Nolan-British-director', 'content': 'Christopher Nolan (born July 30, 1970, London, England) is a British film director and writer acclaimed for his noirish visual aesthetic and unconventional, often highly conceptual narratives. In 2024 Nolan won an Academy Award for best director for Oppenheimer (2023), which was also named best picture. Nolanâ€™s breakthrough came with the 2000 film Memento, a sleeper hit that he adapted from a short story written by his brother Jonathan Nolan. The film was a critical and popular success and garnered the Nolan brothers an Academy Award nomination for best original screenplay. Nolanâ€™s highly anticipated Batman Begins (2005), starring Christian Bale, focuses on the superheroâ€™s origins and features settings and a tone that are grimmer and more realistic than those of previous Batman films. Nolanâ€™s 2023 film, Oppenheimer, depicts American theoretical physicist  J.', 'score': 0.88822687, 'raw_content': None}, {'title': 'Christopher Nolan: Biography, Movie Director, Filmmaker', 'url': 'https://www.biography.com/movies-tv/christopher-nolan', 'content': 'Opt-Out Icon Christopher Nolan is an Academy Award-winning movie director and screenwriter whoâ€™s helmed several hit films, including Inception, The Dark Knight, Interstellar, and Oppenheimer. We may earn commission from links on this page, but we only recommend products we back. Christopher Nolan is a British-American filmmaker known for his complex storytelling in big-budget movies such as Inception (2010), Interstellar (2014) and Tenet (2020). Play Icon We may earn commission from links on this page, but we only recommend products we back. Biography and associated logos are trademarks of A+E NetworksÂ®protected in the US and other countries around the globe. Opt-Out Icon', 'score': 0.29651213, 'raw_content': None}, {'title': 'Christopher Nolan \"Film Director\" - Biography, Age and Married', 'url': 'https://biographyhost.com/p/christopher-nolan-biography.html', 'content': 'Christopher Nolan is a renowned British-American filmmaker celebrated for his innovative storytelling in films like Oppenheimer and Inception. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films.', 'score': 0.21290259, 'raw_content': None}, {'title': 'Christopher Nolan - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Christopher_Nolan', 'content': 'Following a positive word of mouth and screenings in 500 theatres, it earned $40\\xa0million.[41] Memento premiered at the Venice Film Festival in September 2000 to critical acclaim.[42] Joe Morgenstern of The Wall Street Journal wrote in his review, \"I can\\'t remember when a movie has seemed so clever, strangely affecting and slyly funny at the very same time.\"[43] In the book The Philosophy of Neo-Noir, Basil Smith drew a comparison with John Locke\\'s An Essay Concerning Human Understanding, which argues that conscious memories constitute our identities â€“ a theme Nolan explores in the film.[44] Memento earned Nolan many accolades, including nominations for an Academy Award and a Golden Globe Award for Best Screenplay, as well as two Independent Spirit Awards: Best Director and Best Screenplay.[45][46] Six critics listed it as one of the best films of the 2000s.[47] In 2001, Nolan and Emma Thomas founded the production company Syncopy Inc.[48][b]', 'score': 0.15323243, 'raw_content': None}], 'response_time': 2.47}\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `tavily_search` with `{'query': 'current date'}`\nresponded: Christopher Nolan, the director of the 2023 film **Oppenheimer**, was born on **July 30, 1970**.\nTo calculate his age in days as of today, we can use the following formula:\n\\[ \\text{Age in days} = (\\text{Current Date} - \\text{Birthdate}) \\]\nLet's calculate this now.\n\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'current date', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': \"Today's Date - CalendarDate.com\", 'url': 'https://www.calendardate.com/todays.htm', 'content': \"Details about today's date with count of days, weeks, and months, Sun and Moon cycles, Zodiac signs and holidays. Sunday June 15, 2025 . Home; Calendars. 2025 Calendar; ... Current Season Today: Spring with 6 days until the start of Summer. S. Hemishpere flip seasons - i.e. Winter is Summer.\", 'score': 0.63152665, 'raw_content': None}, {'title': \"What is the date today | Today's Date\", 'url': 'https://www.datetoday.info/', 'content': 'Find out the current date and time in different time zones and formats, such as UTC, America/Los_Angeles, ISO 8601, RFC 2822, Unix Epoch, etc. Learn more about the day of the week, the day of the year, the week number, the month number, and the remaining days of the year.', 'score': 0.60049355, 'raw_content': None}, {'title': 'Current Time', 'url': 'https://www.timeanddate.com/', 'content': 'Current Time. Monday Jun 9, 2025 Washington DC, District of Columbia, USA. Set home location. 5:39: 55 am. World Clock. World Clock. Current local time around the world. Personal World Clock. Set the current time of your favorite locations across time zones. World Clock: current time around the globe', 'score': 0.45914948, 'raw_content': None}, {'title': 'What time is it - Exact time - Any time zone - vClock', 'url': 'https://vclock.com/time/', 'content': 'Online clock. What time is it in different regions of United States, Canada, Australia, Europe and the World. What time is it - Exact time - Any time zone - vClock ... On this website, you can find out the current time and date in any country and city in the world. You can also view the time difference between your location and that of another', 'score': 0.15111576, 'raw_content': None}, {'title': 'Time.is - exact time, any time zone', 'url': 'https://time.is/', 'content': '7 million locations, 58 languages, synchronized with atomic clock time.', 'score': 0.08800977, 'raw_content': None}], 'response_time': 2.62}\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `tavily_search` with `{'query': 'days between July 30, 1970 and current date'}`\nresponded: Christopher Nolan, the director of the 2023 film **Oppenheimer**, was born on **July 30, 1970**.\nTo calculate his age in days as of today, we can use the following formula:\n\\[ \\text{Age in days} = (\\text{Current Date} - \\text{Birthdate}) \\]\nLet's calculate this now.\n\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'days between July 30, 1970 and current date', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Date Calculator', 'url': 'https://www.calculator.net/date-calculator.html', 'content': 'Days Between Two Dates. Find the number of years, months, weeks, and days between dates. Click \"Settings\" to define holidays. ... The months of April, June, September, and November have 30 days, while the rest have 31 days except for February, which has 28 days in a standard year, and 29 in a leap year. ... to 1 day in 3,030 years with respect', 'score': 0.15738304, 'raw_content': None}, {'title': 'Days Calculator (Days Between Dates)', 'url': 'https://www.gigacalculator.com/calculators/days-between-dates-calculator.php', 'content': 'Days calculator to count how many days between any two dates. Find out how many days there are between any two dates, e.g. days between today and date X in the future, or date Y in the past and today. Calculate how many days you have to a deadline with this free days between dates calculator. Days calculator online for time between dates, including days since or days from a given date.', 'score': 0.15232232, 'raw_content': None}, {'title': 'Days Calculator', 'url': 'https://time-calculator.net/days.html', 'content': 'The days calculator can find the days or duration between two dates and also gives the time interval in years, months, and days. Start Date: Today. End Date: Today. Include last day (+1 day) = Calculate. Ã— Reset. Result:', 'score': 0.1465877, 'raw_content': None}, {'title': 'Date Calculator - Add Days to Date & Days Between Dates', 'url': 'https://timedatecalc.com/date-calculator', 'content': \"How to Add Days to Date. Enter the start date To get started, enter the start date to which you need to add/subtract days (today's date is initially displayed). Use the calendar for more convenient date selection. Enter the number of days Next, enter the time value you need to add or subtract from the start date (years, months, weeks, days).\", 'score': 0.14245868, 'raw_content': None}, {'title': 'Date Duration Calculator: Days Between Dates - timeanddate.com', 'url': 'https://www.timeanddate.com/date/duration.html', 'content': 'Help and Example Use. Some typical uses for the Date Calculators; API Services for Developers. API for Business Date Calculators; Date Calculators. Time and Date Duration - Calculate duration, with both date and time included; Date Calculator - Add or subtract days, months, years; Weekday Calculator - What day is this date?; Birthday Calculator - Find when you are 1 billion seconds old', 'score': 0.12024263, 'raw_content': None}], 'response_time': 2.27}\u001b[0m\u001b[32;1m\u001b[1;3mChristopher Nolan was born on July 30, 1970. As of today, June 15, 2025, he is 19,944 days old.\u001b[0m\n\u001b[1m> Finished chain.\u001b[0m\n{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?',\n'output': 'Christopher Nolan was born on July 30, 1970. As of today, June 15, 2025, he is 19,944 days old.'}\nset_debug(True)\nâ€‹\nSetting the global\ndebug\nflag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs.\nfrom\nlangchain\n.\nglobals\nimport\nset_debug\nset_debug\n(\nTrue\n)\nset_verbose\n(\nFalse\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n)\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"Who directed the 2023 film Oppenheimer and what is their age in days?\"\n}\n)\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n\u001b[0m{\n\"input\": \"Who directed the 2023 film Oppenheimer and what is their age in days?\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\n\u001b[0m{\n\"input\": \"\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n\u001b[0m{\n\"input\": \"\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n\u001b[0m{\n\"input\": \"\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\n\u001b[0m{\n\"input\": \"\"\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n\u001b[0m{\n\"output\": []\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [1ms] Exiting Chain run with output:\n\u001b[0m{\n\"agent_scratchpad\": []\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [2ms] Exiting Chain run with output:\n\u001b[0m{\n\"input\": \"Who directed the 2023 film Oppenheimer and what is their age in days?\",\n\"intermediate_steps\": [],\n\"agent_scratchpad\": []\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n\u001b[0m{\n\"input\": \"Who directed the 2023 film Oppenheimer and what is their age in days?\",\n\"intermediate_steps\": [],\n\"agent_scratchpad\": []\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n\u001b[0m[outputs]\n\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n\u001b[0m{\n\"prompts\": [\n\"System: You are a helpful assistant.\\nHuman: Who directed the 2023 film Oppenheimer and what is their age in days?\"\n]\n}\n\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] [2.87s] Exiting LLM run with output:\n\u001b[0m{\n\"generations\": [\n[\n{\n\"text\": \"\",\n\"generation_info\": {\n\"finish_reason\": \"tool_calls\",\n\"model_name\": \"gpt-4-turbo-2024-04-09\",\n\"system_fingerprint\": \"fp_de235176ee\",\n\"service_tier\": \"default\"\n},\n\"type\": \"ChatGenerationChunk\",\n\"message\": {\n\"lc\": 1,\n\"type\": \"constructor\",\n\"id\": [\n\"langchain\",\n\"schema\",\n\"messages\",\n\"AIMessageChunk\"\n],\n\"kwargs\": {\n\"content\": \"\",\n\"additional_kwargs\": {\n\"tool_calls\": [\n{\n\"index\": 0,\n\"id\": \"call_7470602CBXe0TCtzU9kNddmI\",\n\"function\": {\n\"arguments\": \"{\\\"query\\\": \\\"director of the 2023 film Oppenheimer\\\"}\",\n\"name\": \"tavily_search\"\n},\n\"type\": \"function\"\n},\n{\n\"index\": 1,\n\"id\": \"call_NcqiDSEUVpwfSKBTSUDRwJTQ\",\n\"function\": {\n\"arguments\": \"{\\\"query\\\": \\\"birth date of Christopher Nolan\\\"}\",\n\"name\": \"tavily_search\"\n},\n\"type\": \"function\"\n}\n]\n},\n\"response_metadata\": {\n\"finish_reason\": \"tool_calls\",\n\"model_name\": \"gpt-4-turbo-2024-04-09\",\n\"system_fingerprint\": \"fp_de235176ee\",\n\"service_tier\": \"default\"\n},\n\"type\": \"AIMessageChunk\",\n\"id\": \"run--421b146e-04d7-4e72-8c1d-68c9b92995fe\",\n\"tool_calls\": [\n{\n\"name\": \"tavily_search\",\n\"args\": {\n\"query\": \"director of the 2023 film Oppenheimer\"\n},\n\"id\": \"call_7470602CBXe0TCtzU9kNddmI\",\n\"type\": \"tool_call\"\n},\n{\n\"name\": \"tavily_search\",\n\"args\": {\n\"query\": \"birth date of Christopher Nolan\"\n},\n\"id\": \"call_NcqiDSEUVpwfSKBTSUDRwJTQ\",\n\"type\": \"tool_call\"\n}\n],\n\"tool_call_chunks\": [\n{\n\"name\": \"tavily_search\",\n\"args\": \"{\\\"query\\\": \\\"director of the 2023 film Oppenheimer\\\"}\",\n\"id\": \"call_7470602CBXe0TCtzU9kNddmI\",\n\"index\": 0,\n\"type\": \"tool_call_chunk\"\n},\n{\n\"name\": \"tavily_search\",\n\"args\": \"{\\\"query\\\": \\\"birth date of Christopher Nolan\\\"}\",\n\"id\": \"call_NcqiDSEUVpwfSKBTSUDRwJTQ\",\n\"index\": 1,\n\"type\": \"tool_call_chunk\"\n}\n],\n\"invalid_tool_calls\": []\n}\n}\n}\n]\n],\n\"llm_output\": null,\n\"run\": null,\n\"type\": \"LLMResult\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] Entering Parser run with input:\n\u001b[0m[inputs]\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] [0ms] Exiting Parser run with output:\n\u001b[0m[outputs]\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] [2.88s] Exiting Chain run with output:\n\u001b[0m[outputs]\n\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:tavily_search] Entering Tool run with input:\n\u001b[0m\"{'query': 'director of the 2023 film Oppenheimer'}\"\n\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:tavily_search] [2.11s] Exiting Tool run with output:\n\u001b[0m\"{'query': 'director of the 2023 film Oppenheimer', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Oppenheimer (film) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Oppenheimer_(film)', 'content': \"Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Toggle the table of contents Contents move to sidebar hide (Top) 1 Plot 2 Cast 3 ProductionToggle Production subsection 3.1 Development 3.2 Writing 3.3 Casting 3.4 Filming 3.5 Post-production 4 Music 5 Marketing 6 ReleaseToggle Release subsection 6.1 Theatrical 6.1.1 Classifications and censorship 6.1.2 Bhagavad Gita controversy 6.2 Home media 7 ReceptionToggle Reception subsection 7.1 Box office 7.1.1 United States and Canada 7.1.2 Japan 7.1.3 Other territories 7.2 Critical response 7.3 Influence on legislation 8 Accuracy and omissions 9 Accolades 10 See also 11 Notes 12 References 13 Further reading 14 External links Oppenheimer (film) 70 languages Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾ AzÉ™rbaycanca à¦¬à¦¾à¦‚à¦²à¦¾ Ð‘ÐµÐ»Ð°Ñ€ÑƒÑÐºÐ°Ñ à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ Ð‘ÑŠÐ»Ð³Ð°Ñ€ÑÐºÐ¸ Bosanski CatalÃ  ÄŒeÅ¡tina Cymraeg Dansk Deutsch à¤¡à¥‹à¤Ÿà¥‡à¤²à¥€ Eesti Î•Î»Î»Î·Î½Î¹ÎºÎ¬ EspaÃ±ol Euskara ÙØ§Ø±Ø³ÛŒ FranÃ§ais Gaeilge Galego í•œêµ­ì–´ Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶ à¤¹à¤¿à¤¨à¥à¤¦à¥€ Ido Bahasa Indonesia Italiano ×¢×‘×¨×™×ª Jawa áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ ÒšÐ°Ð·Ð°Ò›ÑˆÐ° Latina LatvieÅ¡u LietuviÅ³ Magyar ÐœÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸ à´®à´²à´¯à´¾à´³à´‚ à¤®à¤°à¤¾à¤ à¥€ Ù…ØµØ±Ù‰ Ù…Ø§Ø²ÙØ±ÙˆÙ†ÛŒ Bahasa Melayu Nederlands à¤¨à¥‡à¤ªà¤¾à¤²à¥€ æ—¥æœ¬èªž Norsk bokmÃ¥l OÊ»zbekcha / ÑžÐ·Ð±ÐµÐºÑ‡Ð° à¨ªà©°à¨œà¨¾à¨¬à©€ Polski PortuguÃªs RomÃ¢nÄƒ Ð ÑƒÑÑÐºÐ¸Ð¹ Shqip Simple English SlovenÄina SlovenÅ¡Äina Ú©ÙˆØ±Ø¯ÛŒ Ð¡Ñ€Ð¿ÑÐºÐ¸ / srpski Suomi Svenska à®¤à®®à®¿à®´à¯ à°¤à±†à°²à±à°—à± à¹„à¸—à¸¢ Ð¢Ð¾Ò·Ð¸ÐºÓ£ TÃ¼rkÃ§e Ð£ÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ° Ø§Ø±Ø¯Ùˆ Tiáº¿ng Viá»‡t ç²µèªž ä¸­æ–‡ Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Expand all Edit interlanguage links Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiquote Wikidata item From Wikipedia, the free encyclopedia 2023 film by Christopher Nolan | Oppenheimer | | --- | | Theatrical release poster | | Directed by | Christopher Nolan | | Screenplay by | Christopher Nolan | | Based on | American Prometheus by Kai Bird Martin J. Sherwin | | Produced by | Emma Thomas Charles Roven Christopher Nolan | | Starring | Cillian Murphy Emily Blunt Matt Damon Robert Downey Jr. Florence Pugh Josh Hartnett Casey Affleck Rami Malek Kenneth Branagh | | Cinematography | Hoyte van Hoytema | | Edited by | Jennifer Lame | | Music by | Ludwig GÃ¶ransson | | Production companies | Universal Pictures[1][2] Syncopy[1][2] Atlas Entertainment[1][2] Breakheart Films[2] Peters Creek Entertainment[2] Gadget Films[1][3] | | Distributed by | Universal Pictures | | Release dates | July 11, 2023 (2023-07-11) (Le Grand Rex) July 21, 2023 (2023-07-21) (United States and United Kingdom) | | Running time | 180 minutes[4] | | Countries | United States United Kingdom | | Language | English | | Budget | $100 million[5] | | Box office | $975.8 million[6][7] | Oppenheimer is a 2023 epic biographical drama film written, produced, and directed by Christopher Nolan. [8] It follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film dramatizes Oppenheimer's studies, his direction of the Los Alamos Laboratory and his 1954 security hearing. Oppenheimer received critical acclaim and grossed $975 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film of all time at the time of its release.\", 'score': 0.9475027, 'raw_content': None}, {'title': 'Oppenheimer | Cast, Film, Length, Plot, Actors, Awards, & Facts ...', 'url': 'https://www.britannica.com/topic/Oppenheimer-film', 'content': 'J. Robert Oppenheimer Robert Downey, Jr. Oppenheimer # Oppenheimer Oppenheimer, American and British dramatic biographical film, released in 2023, that explores the life and legacy of the American physicist J. Robert Oppenheimer, who played a key role in the development of the atomic bomb. Robert Oppenheimer (2005). Film criticsâ€™ reaction to Oppenheimer was overwhelmingly positive. Oppenheimer grossed more than $300 million domestically and more than $600 million internationally by the end of November 2023, making it the second highest grossing R-rated film of all time. The film also dominated the Academy Awards nominations, garnering 13 nominations compared with the 8 for Greta Gerwigâ€™s Barbie, which opened the same weekend as Oppenheimer but topped Nolanâ€™s film at the box office.', 'score': 0.76194656, 'raw_content': None}, {'title': 'Oppenheimer (2023) - Full cast & crew - IMDb', 'url': 'https://www.imdb.com/title/tt15398776/fullcredits/', 'content': 'Oppenheimer (2023) - Cast and crew credits, including actors, actresses, directors, writers and more. Menu. ... Oscars Pride Month American Black Film Festival Summer Watch Guide STARmeter Awards Awards Central Festival Central All Events. ... second unit director: visual effects (uncredited) Francesca Kaimer Millea.', 'score': 0.683948, 'raw_content': None}, {'title': \"'Oppenheimer' director Christopher Nolan says the film is his darkest - NPR\", 'url': 'https://www.npr.org/2023/08/14/1193448291/oppenheimer-director-christopher-nolan', 'content': '# \\'Like it or not, we live in Oppenheimer\\'s world,\\' says director Christopher Nolan #### \\'Like it or not, we live in Oppenheimer\\'s world,\\' says director Christopher Nolan But he says the story of Robert Oppenheimer, known as the father of the atomic bomb, stayed with him in a way his other films didn\\'t. Nolan says he was drawn to the tension of Oppenheimer\\'s story â€” particularly the disconnect between the joy the physicist felt at the success of the Trinity test, and the horror that later resulted. Writer, director and producer Christopher Nolan says Oppenheimer is the \"darkest\" of all the films he\\'s worked on. Writer, director and producer Christopher Nolan says Oppenheimer is the \"darkest\" of all the films he\\'s worked on.', 'score': 0.6255073, 'raw_content': None}, {'title': 'An extended interview with Christopher Nolan, director of Oppenheimer', 'url': 'https://thebulletin.org/premium/2023-07/an-extended-interview-with-christopher-nolan-director-of-oppenheimer/', 'content': 'A group of Manhattan Project scientists and engineers also focused on wider public education on nuclear weapons and energy (and science generally) through the creation of the Bulletin of the Atomic Scientists; Oppenheimer served as the first chair of the magazineâ€™s Board of Sponsors.[5] As time has passed, more evidence has come to light of the bias and unfairness of the process that Dr. Oppenheimer was subjected to while the evidence of his loyalty and love of country have only been further affirmed.â€[8] Decades after the fact, records of the Oppenheimer security hearing made it clear that, rather than any disloyalty to the nation, it was his principled opposition to development of the hydrogen bombâ€”a nuclear fusion-based weapon of immensely greater power than the fission weapons used to decimate Hiroshima and Nagasaki in 1945â€”that was key to the decision to essentially bar him from government service. Robert Oppenheimer, Los Alamos, Manhattan Project, Nolan, atomic bomb, movie', 'score': 0.32472825, 'raw_content': None}], 'response_time': 1.39}\"\n\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:tavily_search] Entering Tool run with input:\n\u001b[0m\"{'query': 'birth date of Christopher Nolan'}\"\n\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:tavily_search] [1.11s] Exiting Tool run with output:\n\u001b[0m\"{'query': 'birth date of Christopher Nolan', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Christopher Nolan | Biography, Movies, Batman, Oppenheimer, & Facts ...', 'url': 'https://www.britannica.com/biography/Christopher-Nolan-British-director', 'content': 'Christopher Nolan (born July 30, 1970, London, England) is a British film director and writer acclaimed for his noirish visual aesthetic and unconventional, often highly conceptual narratives. In 2024 Nolan won an Academy Award for best director for Oppenheimer (2023), which was also named best picture. Nolanâ€™s breakthrough came with the 2000 film Memento, a sleeper hit that he adapted from a short story written by his brother Jonathan Nolan. The film was a critical and popular success and garnered the Nolan brothers an Academy Award nomination for best original screenplay. Nolanâ€™s highly anticipated Batman Begins (2005), starring Christian Bale, focuses on the superheroâ€™s origins and features settings and a tone that are grimmer and more realistic than those of previous Batman films. Nolanâ€™s 2023 film, Oppenheimer, depicts American theoretical physicist  J.', 'score': 0.8974172, 'raw_content': None}, {'title': 'Christopher Nolan - IMDb', 'url': 'https://m.imdb.com/name/nm0634240/', 'content': 'Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most', 'score': 0.5087155, 'raw_content': None}, {'title': 'Christopher Nolan: Biography, Movie Director, Filmmaker', 'url': 'https://www.biography.com/movies-tv/christopher-nolan', 'content': 'Opt-Out Icon Christopher Nolan is an Academy Award-winning movie director and screenwriter whoâ€™s helmed several hit films, including Inception, The Dark Knight, Interstellar, and Oppenheimer. We may earn commission from links on this page, but we only recommend products we back. Christopher Nolan is a British-American filmmaker known for his complex storytelling in big-budget movies such as Inception (2010), Interstellar (2014) and Tenet (2020). Play Icon We may earn commission from links on this page, but we only recommend products we back. Biography and associated logos are trademarks of A+E NetworksÂ®protected in the US and other countries around the globe. Opt-Out Icon', 'score': 0.28185803, 'raw_content': None}, {'title': 'Christopher Nolan \"Film Director\" - Biography, Age and Married', 'url': 'https://biographyhost.com/p/christopher-nolan-biography.html', 'content': 'Christopher Nolan is a renowned British-American filmmaker celebrated for his innovative storytelling in films like Oppenheimer and Inception. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films.', 'score': 0.19905913, 'raw_content': None}, {'title': 'Christopher Nolan - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Christopher_Nolan', 'content': 'Following a positive word of mouth and screenings in 500 theatres, it earned $40\\xa0million.[41] Memento premiered at the Venice Film Festival in September 2000 to critical acclaim.[42] Joe Morgenstern of The Wall Street Journal wrote in his review, \"I can\\'t remember when a movie has seemed so clever, strangely affecting and slyly funny at the very same time.\"[43] In the book The Philosophy of Neo-Noir, Basil Smith drew a comparison with John Locke\\'s An Essay Concerning Human Understanding, which argues that conscious memories constitute our identities â€“ a theme Nolan explores in the film.[44] Memento earned Nolan many accolades, including nominations for an Academy Award and a Golden Globe Award for Best Screenplay, as well as two Independent Spirit Awards: Best Director and Best Screenplay.[45][46] Six critics listed it as one of the best films of the 2000s.[47] In 2001, Nolan and Emma Thomas founded the production company Syncopy Inc.[48][b]', 'score': 0.1508904, 'raw_content': None}], 'response_time': 0.74}\"\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\n\u001b[0m{\n\"input\": \"\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n\u001b[0m{\n\"input\": \"\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n\u001b[0m{\n\"input\": \"\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\n\u001b[0m{\n\"input\": \"\"\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n\u001b[0m[outputs]\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [1ms] Exiting Chain run with output:\n\u001b[0m[outputs]\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [2ms] Exiting Chain run with output:\n\u001b[0m[outputs]\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n\u001b[0m[inputs]\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n\u001b[0m[outputs]\n\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n\u001b[0m{\n\"prompts\": [\n\"System: You are a helpful assistant.\\nHuman: Who directed the 2023 film Oppenheimer and what is their age in days?\\nAI: \\nTool: {\\\"query\\\": \\\"director of the 2023 film Oppenheimer\\\", \\\"follow_up_questions\\\": null, \\\"answer\\\": null, \\\"images\\\": [], \\\"results\\\": [{\\\"title\\\": \\\"Oppenheimer (film) - Wikipedia\\\", \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\\\", \\\"content\\\": \\\"Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Toggle the table of contents Contents move to sidebar hide (Top) 1 Plot 2 Cast 3 ProductionToggle Production subsection 3.1 Development 3.2 Writing 3.3 Casting 3.4 Filming 3.5 Post-production 4 Music 5 Marketing 6 ReleaseToggle Release subsection 6.1 Theatrical 6.1.1 Classifications and censorship 6.1.2 Bhagavad Gita controversy 6.2 Home media 7 ReceptionToggle Reception subsection 7.1 Box office 7.1.1 United States and Canada 7.1.2 Japan 7.1.3 Other territories 7.2 Critical response 7.3 Influence on legislation 8 Accuracy and omissions 9 Accolades 10 See also 11 Notes 12 References 13 Further reading 14 External links Oppenheimer (film) 70 languages Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾ AzÉ™rbaycanca à¦¬à¦¾à¦‚à¦²à¦¾ Ð‘ÐµÐ»Ð°Ñ€ÑƒÑÐºÐ°Ñ à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ Ð‘ÑŠÐ»Ð³Ð°Ñ€ÑÐºÐ¸ Bosanski CatalÃ  ÄŒeÅ¡tina Cymraeg Dansk Deutsch à¤¡à¥‹à¤Ÿà¥‡à¤²à¥€ Eesti Î•Î»Î»Î·Î½Î¹ÎºÎ¬ EspaÃ±ol Euskara ÙØ§Ø±Ø³ÛŒ FranÃ§ais Gaeilge Galego í•œêµ­ì–´ Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶ à¤¹à¤¿à¤¨à¥à¤¦à¥€ Ido Bahasa Indonesia Italiano ×¢×‘×¨×™×ª Jawa áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ ÒšÐ°Ð·Ð°Ò›ÑˆÐ° Latina LatvieÅ¡u LietuviÅ³ Magyar ÐœÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸ à´®à´²à´¯à´¾à´³à´‚ à¤®à¤°à¤¾à¤ à¥€ Ù…ØµØ±Ù‰ Ù…Ø§Ø²ÙØ±ÙˆÙ†ÛŒ Bahasa Melayu Nederlands à¤¨à¥‡à¤ªà¤¾à¤²à¥€ æ—¥æœ¬èªž Norsk bokmÃ¥l OÊ»zbekcha / ÑžÐ·Ð±ÐµÐºÑ‡Ð° à¨ªà©°à¨œà¨¾à¨¬à©€ Polski PortuguÃªs RomÃ¢nÄƒ Ð ÑƒÑÑÐºÐ¸Ð¹ Shqip Simple English SlovenÄina SlovenÅ¡Äina Ú©ÙˆØ±Ø¯ÛŒ Ð¡Ñ€Ð¿ÑÐºÐ¸ / srpski Suomi Svenska à®¤à®®à®¿à®´à¯ à°¤à±†à°²à±à°—à± à¹„à¸—à¸¢ Ð¢Ð¾Ò·Ð¸ÐºÓ£ TÃ¼rkÃ§e Ð£ÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ° Ø§Ø±Ø¯Ùˆ Tiáº¿ng Viá»‡t ç²µèªž ä¸­æ–‡ Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Expand all Edit interlanguage links Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiquote Wikidata item From Wikipedia, the free encyclopedia 2023 film by Christopher Nolan | Oppenheimer | | --- | | Theatrical release poster | | Directed by | Christopher Nolan | | Screenplay by | Christopher Nolan | | Based on | American Prometheus by Kai Bird Martin J. Sherwin | | Produced by | Emma Thomas Charles Roven Christopher Nolan | | Starring | Cillian Murphy Emily Blunt Matt Damon Robert Downey Jr. Florence Pugh Josh Hartnett Casey Affleck Rami Malek Kenneth Branagh | | Cinematography | Hoyte van Hoytema | | Edited by | Jennifer Lame | | Music by | Ludwig GÃ¶ransson | | Production companies | Universal Pictures[1][2] Syncopy[1][2] Atlas Entertainment[1][2] Breakheart Films[2] Peters Creek Entertainment[2] Gadget Films[1][3] | | Distributed by | Universal Pictures | | Release dates | July 11, 2023 (2023-07-11) (Le Grand Rex) July 21, 2023 (2023-07-21) (United States and United Kingdom) | | Running time | 180 minutes[4] | | Countries | United States United Kingdom | | Language | English | | Budget | $100 million[5] | | Box office | $975.8 million[6][7] | Oppenheimer is a 2023 epic biographical drama film written, produced, and directed by Christopher Nolan. [8] It follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film dramatizes Oppenheimer's studies, his direction of the Los Alamos Laboratory and his 1954 security hearing. Oppenheimer received critical acclaim and grossed $975 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film of all time at the time of its release.\\\", \\\"score\\\": 0.9475027, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Oppenheimer | Cast, Film, Length, Plot, Actors, Awards, & Facts ...\\\", \\\"url\\\": \\\"https://www.britannica.com/topic/Oppenheimer-film\\\", \\\"content\\\": \\\"J. Robert Oppenheimer Robert Downey, Jr. Oppenheimer # Oppenheimer Oppenheimer, American and British dramatic biographical film, released in 2023, that explores the life and legacy of the American physicist J. Robert Oppenheimer, who played a key role in the development of the atomic bomb. Robert Oppenheimer (2005). Film criticsâ€™ reaction to Oppenheimer was overwhelmingly positive. Oppenheimer grossed more than $300 million domestically and more than $600 million internationally by the end of November 2023, making it the second highest grossing R-rated film of all time. The film also dominated the Academy Awards nominations, garnering 13 nominations compared with the 8 for Greta Gerwigâ€™s Barbie, which opened the same weekend as Oppenheimer but topped Nolanâ€™s film at the box office.\\\", \\\"score\\\": 0.76194656, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Oppenheimer (2023) - Full cast & crew - IMDb\\\", \\\"url\\\": \\\"https://www.imdb.com/title/tt15398776/fullcredits/\\\", \\\"content\\\": \\\"Oppenheimer (2023) - Cast and crew credits, including actors, actresses, directors, writers and more. Menu. ... Oscars Pride Month American Black Film Festival Summer Watch Guide STARmeter Awards Awards Central Festival Central All Events. ... second unit director: visual effects (uncredited) Francesca Kaimer Millea.\\\", \\\"score\\\": 0.683948, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"'Oppenheimer' director Christopher Nolan says the film is his darkest - NPR\\\", \\\"url\\\": \\\"https://www.npr.org/2023/08/14/1193448291/oppenheimer-director-christopher-nolan\\\", \\\"content\\\": \\\"# 'Like it or not, we live in Oppenheimer's world,' says director Christopher Nolan #### 'Like it or not, we live in Oppenheimer's world,' says director Christopher Nolan But he says the story of Robert Oppenheimer, known as the father of the atomic bomb, stayed with him in a way his other films didn't. Nolan says he was drawn to the tension of Oppenheimer's story â€” particularly the disconnect between the joy the physicist felt at the success of the Trinity test, and the horror that later resulted. Writer, director and producer Christopher Nolan says Oppenheimer is the \\\\\\\"darkest\\\\\\\" of all the films he's worked on. Writer, director and producer Christopher Nolan says Oppenheimer is the \\\\\\\"darkest\\\\\\\" of all the films he's worked on.\\\", \\\"score\\\": 0.6255073, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"An extended interview with Christopher Nolan, director of Oppenheimer\\\", \\\"url\\\": \\\"https://thebulletin.org/premium/2023-07/an-extended-interview-with-christopher-nolan-director-of-oppenheimer/\\\", \\\"content\\\": \\\"A group of Manhattan Project scientists and engineers also focused on wider public education on nuclear weapons and energy (and science generally) through the creation of the Bulletin of the Atomic Scientists; Oppenheimer served as the first chair of the magazineâ€™s Board of Sponsors.[5] As time has passed, more evidence has come to light of the bias and unfairness of the process that Dr. Oppenheimer was subjected to while the evidence of his loyalty and love of country have only been further affirmed.â€[8] Decades after the fact, records of the Oppenheimer security hearing made it clear that, rather than any disloyalty to the nation, it was his principled opposition to development of the hydrogen bombâ€”a nuclear fusion-based weapon of immensely greater power than the fission weapons used to decimate Hiroshima and Nagasaki in 1945â€”that was key to the decision to essentially bar him from government service. Robert Oppenheimer, Los Alamos, Manhattan Project, Nolan, atomic bomb, movie\\\", \\\"score\\\": 0.32472825, \\\"raw_content\\\": null}], \\\"response_time\\\": 1.39}\\nTool: {\\\"query\\\": \\\"birth date of Christopher Nolan\\\", \\\"follow_up_questions\\\": null, \\\"answer\\\": null, \\\"images\\\": [], \\\"results\\\": [{\\\"title\\\": \\\"Christopher Nolan | Biography, Movies, Batman, Oppenheimer, & Facts ...\\\", \\\"url\\\": \\\"https://www.britannica.com/biography/Christopher-Nolan-British-director\\\", \\\"content\\\": \\\"Christopher Nolan (born July 30, 1970, London, England) is a British film director and writer acclaimed for his noirish visual aesthetic and unconventional, often highly conceptual narratives. In 2024 Nolan won an Academy Award for best director for Oppenheimer (2023), which was also named best picture. Nolanâ€™s breakthrough came with the 2000 film Memento, a sleeper hit that he adapted from a short story written by his brother Jonathan Nolan. The film was a critical and popular success and garnered the Nolan brothers an Academy Award nomination for best original screenplay. Nolanâ€™s highly anticipated Batman Begins (2005), starring Christian Bale, focuses on the superheroâ€™s origins and features settings and a tone that are grimmer and more realistic than those of previous Batman films. Nolanâ€™s 2023 film, Oppenheimer, depicts American theoretical physicist  J.\\\", \\\"score\\\": 0.8974172, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Christopher Nolan - IMDb\\\", \\\"url\\\": \\\"https://m.imdb.com/name/nm0634240/\\\", \\\"content\\\": \\\"Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most\\\", \\\"score\\\": 0.5087155, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Christopher Nolan: Biography, Movie Director, Filmmaker\\\", \\\"url\\\": \\\"https://www.biography.com/movies-tv/christopher-nolan\\\", \\\"content\\\": \\\"Opt-Out Icon Christopher Nolan is an Academy Award-winning movie director and screenwriter whoâ€™s helmed several hit films, including Inception, The Dark Knight, Interstellar, and Oppenheimer. We may earn commission from links on this page, but we only recommend products we back. Christopher Nolan is a British-American filmmaker known for his complex storytelling in big-budget movies such as Inception (2010), Interstellar (2014) and Tenet (2020). Play Icon We may earn commission from links on this page, but we only recommend products we back. Biography and associated logos are trademarks of A+E NetworksÂ®protected in the US and other countries around the globe. Opt-Out Icon\\\", \\\"score\\\": 0.28185803, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Christopher Nolan \\\\\\\"Film Director\\\\\\\" - Biography, Age and Married\\\", \\\"url\\\": \\\"https://biographyhost.com/p/christopher-nolan-biography.html\\\", \\\"content\\\": \\\"Christopher Nolan is a renowned British-American filmmaker celebrated for his innovative storytelling in films like Oppenheimer and Inception. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films. Christopher Nolan is a British-American filmmaker renowned for his innovative storytelling and visually stunning films.\\\", \\\"score\\\": 0.19905913, \\\"raw_content\\\": null}, {\\\"title\\\": \\\"Christopher Nolan - Wikipedia\\\", \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Christopher_Nolan\\\", \\\"content\\\": \\\"Following a positive word of mouth and screenings in 500 theatres, it earned $40Â million.[41] Memento premiered at the Venice Film Festival in September 2000 to critical acclaim.[42] Joe Morgenstern of The Wall Street Journal wrote in his review, \\\\\\\"I can't remember when a movie has seemed so clever, strangely affecting and slyly funny at the very same time.\\\\\\\"[43] In the book The Philosophy of Neo-Noir, Basil Smith drew a comparison with John Locke's An Essay Concerning Human Understanding, which argues that conscious memories constitute our identities â€“ a theme Nolan explores in the film.[44] Memento earned Nolan many accolades, including nominations for an Academy Award and a Golden Globe Award for Best Screenplay, as well as two Independent Spirit Awards: Best Director and Best Screenplay.[45][46] Six critics listed it as one of the best films of the 2000s.[47] In 2001, Nolan and Emma Thomas founded the production company Syncopy Inc.[48][b]\\\", \\\"score\\\": 0.1508904, \\\"raw_content\\\": null}], \\\"response_time\\\": 0.74}\"\n]\n}\n\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] [10.98s] Exiting LLM run with output:\n\u001b[0m{\n\"generations\": [\n[\n{\n\"text\": \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = \\\\text{Current Date} - \\\\text{Birth Date} \\\\]\\n\\nLet's calculate:\\n\\n- Birthdate: July 30, 1970\\n- Today's Date: December 7, 2023\\n\\nUsing a date calculator or similar method, we find that Christopher Nolan is approximately **19,480 days old** as of today.\",\n\"generation_info\": {\n\"finish_reason\": \"stop\",\n\"model_name\": \"gpt-4-turbo-2024-04-09\",\n\"system_fingerprint\": \"fp_de235176ee\",\n\"service_tier\": \"default\"\n},\n\"type\": \"ChatGenerationChunk\",\n\"message\": {\n\"lc\": 1,\n\"type\": \"constructor\",\n\"id\": [\n\"langchain\",\n\"schema\",\n\"messages\",\n\"AIMessageChunk\"\n],\n\"kwargs\": {\n\"content\": \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = \\\\text{Current Date} - \\\\text{Birth Date} \\\\]\\n\\nLet's calculate:\\n\\n- Birthdate: July 30, 1970\\n- Today's Date: December 7, 2023\\n\\nUsing a date calculator or similar method, we find that Christopher Nolan is approximately **19,480 days old** as of today.\",\n\"response_metadata\": {\n\"finish_reason\": \"stop\",\n\"model_name\": \"gpt-4-turbo-2024-04-09\",\n\"system_fingerprint\": \"fp_de235176ee\",\n\"service_tier\": \"default\"\n},\n\"type\": \"AIMessageChunk\",\n\"id\": \"run--21b0c760-dbf4-45e1-89fd-d1edfa1eb9d5\",\n\"tool_calls\": [],\n\"invalid_tool_calls\": []\n}\n}\n}\n]\n],\n\"llm_output\": null,\n\"run\": null,\n\"type\": \"LLMResult\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] Entering Parser run with input:\n\u001b[0m[inputs]\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] [0ms] Exiting Parser run with output:\n\u001b[0m[outputs]\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] [10.99s] Exiting Chain run with output:\n\u001b[0m[outputs]\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [17.09s] Exiting Chain run with output:\n\u001b[0m{\n\"output\": \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = \\\\text{Current Date} - \\\\text{Birth Date} \\\\]\\n\\nLet's calculate:\\n\\n- Birthdate: July 30, 1970\\n- Today's Date: December 7, 2023\\n\\nUsing a date calculator or similar method, we find that Christopher Nolan is approximately **19,480 days old** as of today.\"\n}\n{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?',\n'output': \"The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.\\n\\nChristopher Nolan was born on **July 30, 1970**. To calculate his age in days as of today:\\n\\n1. First, determine the total number of days from his birthdate to today.\\n2. Use the formula: \\\\[ \\\\text{Age in days} = \\\\text{Current Date} - \\\\text{Birth Date} \\\\]\\n\\nLet's calculate:\\n\\n- Birthdate: July 30, 1970\\n- Today's Date: December 7, 2023\\n\\nUsing a date calculator or similar method, we find that Christopher Nolan is approximately **19,480 days old** as of today.\"}\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/document_loader_csv/",
    "How-to guides\nHow to load CSVs\nOn this page\nHow to load CSVs\nA\ncomma-separated values (CSV)\nfile is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\nLangChain implements a\nCSV Loader\nthat will load CSV files into a sequence of\nDocument\nobjects. Each row of the CSV file is translated to one document.\nfrom\nlangchain_community\n.\ndocument_loaders\n.\ncsv_loader\nimport\nCSVLoader\nfile_path\n=\n\"../integrations/document_loaders/example_data/mlb_teams_2012.csv\"\nloader\n=\nCSVLoader\n(\nfile_path\n=\nfile_path\n)\ndata\n=\nloader\n.\nload\n(\n)\nfor\nrecord\nin\ndata\n[\n:\n2\n]\n:\nprint\n(\nrecord\n)\npage_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 0}\npage_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 1}\nCustomizing the CSV parsing and loading\nâ€‹\nCSVLoader\nwill accept a\ncsv_args\nkwarg that supports customization of arguments passed to Python's\ncsv.DictReader\n. See the\ncsv module\ndocumentation for more information of what csv args are supported.\nloader\n=\nCSVLoader\n(\nfile_path\n=\nfile_path\n,\ncsv_args\n=\n{\n\"delimiter\"\n:\n\",\"\n,\n\"quotechar\"\n:\n'\"'\n,\n\"fieldnames\"\n:\n[\n\"MLB Team\"\n,\n\"Payroll in millions\"\n,\n\"Wins\"\n]\n,\n}\n,\n)\ndata\n=\nloader\n.\nload\n(\n)\nfor\nrecord\nin\ndata\n[\n:\n2\n]\n:\nprint\n(\nrecord\n)\npage_content='MLB Team: Team\\nPayroll in millions: \"Payroll (millions)\"\\nWins: \"Wins\"' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 0}\npage_content='MLB Team: Nationals\\nPayroll in millions: 81.34\\nWins: 98' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 1}\nSpecify a column to identify the document source\nâ€‹\nThe\n\"source\"\nkey on\nDocument\nmetadata can be set using a column of the CSV. Use the\nsource_column\nargument to specify a source for the document created from each row. Otherwise\nfile_path\nwill be used as the source for all documents created from the CSV file.\nThis is useful when using documents loaded from CSV files for chains that answer questions using sources.\nloader\n=\nCSVLoader\n(\nfile_path\n=\nfile_path\n,\nsource_column\n=\n\"Team\"\n)\ndata\n=\nloader\n.\nload\n(\n)\nfor\nrecord\nin\ndata\n[\n:\n2\n]\n:\nprint\n(\nrecord\n)\npage_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98' metadata={'source': 'Nationals', 'row': 0}\npage_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97' metadata={'source': 'Reds', 'row': 1}\nLoad from a string\nâ€‹\nPython's\ntempfile\ncan be used when working with CSV strings directly.\nimport\ntempfile\nfrom\nio\nimport\nStringIO\nstring_data\n=\n\"\"\"\n\"Team\", \"Payroll (millions)\", \"Wins\"\n\"Nationals\",     81.34, 98\n\"Reds\",          82.20, 97\n\"Yankees\",      197.96, 95\n\"Giants\",       117.62, 94\n\"\"\"\n.\nstrip\n(\n)\nwith\ntempfile\n.\nNamedTemporaryFile\n(\ndelete\n=\nFalse\n,\nmode\n=\n\"w+\"\n)\nas\ntemp_file\n:\ntemp_file\n.\nwrite\n(\nstring_data\n)\ntemp_file_path\n=\ntemp_file\n.\nname\nloader\n=\nCSVLoader\n(\nfile_path\n=\ntemp_file_path\n)\ndata\n=\nloader\n.\nload\n(\n)\nfor\nrecord\nin\ndata\n[\n:\n2\n]\n:\nprint\n(\nrecord\n)\npage_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98' metadata={'source': 'Nationals', 'row': 0}\npage_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97' metadata={'source': 'Reds', 'row': 1}\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/document_loader_directory/",
    "How-to guides\nHow to load documents from a directory\nOn this page\nHow to load documents from a directory\nLangChain's\nDirectoryLoader\nimplements functionality for reading files from disk into LangChain\nDocument\nobjects. Here we demonstrate:\nHow to load from a filesystem, including use of wildcard patterns;\nHow to use multithreading for file I/O;\nHow to use custom loader classes to parse specific file types (e.g., code);\nHow to handle errors, such as those due to decoding.\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nDirectoryLoader\nDirectoryLoader\naccepts a\nloader_cls\nkwarg, which defaults to\nUnstructuredLoader\n.\nUnstructured\nsupports parsing for a number of formats, such as PDF and HTML. Here we use it to read in a markdown (.md) file.\nWe can use the\nglob\nparameter to control which files to load. Note that here it doesn't load the\n.rst\nfile or the\n.html\nfiles.\nloader\n=\nDirectoryLoader\n(\n\"../\"\n,\nglob\n=\n\"**/*.md\"\n)\ndocs\n=\nloader\n.\nload\n(\n)\nlen\n(\ndocs\n)\n20\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n[\n:\n100\n]\n)\nSecurity\nLangChain has a large ecosystem of integrations with various external resources like local\nShow a progress bar\nâ€‹\nBy default a progress bar will not be shown. To show a progress bar, install the\ntqdm\nlibrary (e.g.\npip install tqdm\n), and set the\nshow_progress\nparameter to\nTrue\n.\nloader\n=\nDirectoryLoader\n(\n\"../\"\n,\nglob\n=\n\"**/*.md\"\n,\nshow_progress\n=\nTrue\n)\ndocs\n=\nloader\n.\nload\n(\n)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u0000\u0000â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 54.56it/s]\nUse multithreading\nâ€‹\nBy default the loading happens in one thread. In order to utilize several threads set the\nuse_multithreading\nflag to true.\nloader\n=\nDirectoryLoader\n(\n\"../\"\n,\nglob\n=\n\"**/*.md\"\n,\nuse_multithreading\n=\nTrue\n)\ndocs\n=\nloader\n.\nload\n(\n)\nChange loader class\nâ€‹\nBy default this uses the\nUnstructuredLoader\nclass. To customize the loader, specify the loader class in the\nloader_cls\nkwarg. Below we show an example using\nTextLoader\n:\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nTextLoader\nloader\n=\nDirectoryLoader\n(\n\"../\"\n,\nglob\n=\n\"**/*.md\"\n,\nloader_cls\n=\nTextLoader\n)\ndocs\n=\nloader\n.\nload\n(\n)\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n[\n:\n100\n]\n)\n# Security\nLangChain has a large ecosystem of integrations with various external resources like loc\nNotice that while the\nUnstructuredLoader\nparses Markdown headers,\nTextLoader\ndoes not.\nIf you need to load Python source code files, use the\nPythonLoader\n:\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nPythonLoader\nloader\n=\nDirectoryLoader\n(\n\"../../../../../\"\n,\nglob\n=\n\"**/*.py\"\n,\nloader_cls\n=\nPythonLoader\n)\nAuto-detect file encodings with TextLoader\nâ€‹\nDirectoryLoader\ncan help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\npath\n=\n\"../../../libs/langchain/tests/unit_tests/examples/\"\nloader\n=\nDirectoryLoader\n(\npath\n,\nglob\n=\n\"**/*.txt\"\n,\nloader_cls\n=\nTextLoader\n)\nA. Default Behavior\nâ€‹\nBy default we raise an error:\nloader\n.\nload\n(\n)\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n115 def load(self) -> List[Document]:\n116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n180 else:\n181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n184 if pbar:\n185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n218     else:\n219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n221 finally:\n222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n209 try:\n--> 210     for subdoc in loader.lazy_load():\n211         yield subdoc\n212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n54                 continue\n55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n57 except Exception as e:\n58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\nThe file\nexample-non-utf8.txt\nuses a different encoding, so the\nload()\nfunction fails with a helpful message indicating which file failed decoding.\nWith the default behavior of\nTextLoader\nany failure to load any of the documents will fail the whole loading process and no documents are loaded.\nB. Silent fail\nâ€‹\nWe can pass the parameter\nsilent_errors\nto the\nDirectoryLoader\nto skip the files which could not be loaded and continue the load process.\nloader\n=\nDirectoryLoader\n(\npath\n,\nglob\n=\n\"**/*.txt\"\n,\nloader_cls\n=\nTextLoader\n,\nsilent_errors\n=\nTrue\n)\ndocs\n=\nloader\n.\nload\n(\n)\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\ndoc_sources\n=\n[\ndoc\n.\nmetadata\n[\n\"source\"\n]\nfor\ndoc\nin\ndocs\n]\ndoc_sources\n['../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt']\nC. Auto detect encodings\nâ€‹\nWe can also ask\nTextLoader\nto auto detect the file encoding before failing, by passing the\nautodetect_encoding\nto the loader class.\ntext_loader_kwargs\n=\n{\n\"autodetect_encoding\"\n:\nTrue\n}\nloader\n=\nDirectoryLoader\n(\npath\n,\nglob\n=\n\"**/*.txt\"\n,\nloader_cls\n=\nTextLoader\n,\nloader_kwargs\n=\ntext_loader_kwargs\n)\ndocs\n=\nloader\n.\nload\n(\n)\ndoc_sources\n=\n[\ndoc\n.\nmetadata\n[\n\"source\"\n]\nfor\ndoc\nin\ndocs\n]\ndoc_sources\n['../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt',\n'../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt']\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/document_loader_html/",
    "How-to guides\nHow to load HTML\nOn this page\nHow to load HTML\nThe HyperText Markup Language or\nHTML\nis the standard markup language for documents designed to be displayed in a web browser.\nThis covers how to load\nHTML\ndocuments into a LangChain\nDocument\nobjects that we can use downstream.\nParsing HTML files often requires specialized tools. Here we demonstrate parsing via\nUnstructured\nand\nBeautifulSoup4\n, which can be installed via pip. Head over to the integrations page to find integrations with additional services, such as\nAzure AI Document Intelligence\nor\nFireCrawl\n.\nLoading HTML with Unstructured\nâ€‹\n%\npip install unstructured\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nUnstructuredHTMLLoader\nfile_path\n=\n\"../../docs/integrations/document_loaders/example_data/fake-content.html\"\nloader\n=\nUnstructuredHTMLLoader\n(\nfile_path\n)\ndata\n=\nloader\n.\nload\n(\n)\nprint\n(\ndata\n)\n[Document(page_content='My First Heading\\n\\nMy first paragraph.', metadata={'source': '../../docs/integrations/document_loaders/example_data/fake-content.html'})]\nLoading HTML with BeautifulSoup4\nâ€‹\nWe can also use\nBeautifulSoup4\nto load HTML documents using the\nBSHTMLLoader\n.  This will extract the text from the HTML into\npage_content\n, and the page title as\ntitle\ninto\nmetadata\n.\n%\npip install bs4\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nBSHTMLLoader\nloader\n=\nBSHTMLLoader\n(\nfile_path\n)\ndata\n=\nloader\n.\nload\n(\n)\nprint\n(\ndata\n)\n[Document(page_content='\\nTest Title\\n\\n\\nMy First Heading\\nMy first paragraph.\\n\\n\\n', metadata={'source': '../../docs/integrations/document_loaders/example_data/fake-content.html', 'title': 'Test Title'})]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/document_loader_json/",
    "How-to guides\nHow to load JSON\nOn this page\nHow to load JSON\nJSON (JavaScript Object Notation)\nis an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attributeâ€“value pairs and arrays (or other serializable values).\nJSON Lines\nis a file format where each line is a valid JSON value.\nLangChain implements a\nJSONLoader\nto convert JSON and JSONL data into LangChain\nDocument\nobjects. It uses a specified\njq schema\nto parse the JSON files, allowing for the extraction of specific fields into the content\nand metadata of the LangChain Document.\nIt uses the\njq\npython package. Check out this\nmanual\nfor a detailed documentation of the\njq\nsyntax.\nHere we will demonstrate:\nHow to load JSON and JSONL data into the content of a LangChain\nDocument\n;\nHow to load JSON and JSONL data into metadata associated with a\nDocument\n.\n#!pip install jq\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nJSONLoader\nimport\njson\nfrom\npathlib\nimport\nPath\nfrom\npprint\nimport\npprint\nfile_path\n=\n'./example_data/facebook_chat.json'\ndata\n=\njson\n.\nloads\n(\nPath\n(\nfile_path\n)\n.\nread_text\n(\n)\n)\npprint\n(\ndata\n)\n{'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},\n'is_still_participant': True,\n'joinable_mode': {'link': '', 'mode': 1},\n'magic_words': [],\n'messages': [{'content': 'Bye!',\n'sender_name': 'User 2',\n'timestamp_ms': 1675597571851},\n{'content': 'Oh no worries! Bye',\n'sender_name': 'User 1',\n'timestamp_ms': 1675597435669},\n{'content': 'No Im sorry it was my mistake, the blue one is not '\n'for sale',\n'sender_name': 'User 2',\n'timestamp_ms': 1675596277579},\n{'content': 'I thought you were selling the blue one!',\n'sender_name': 'User 1',\n'timestamp_ms': 1675595140251},\n{'content': 'Im not interested in this bag. Im interested in the '\n'blue one!',\n'sender_name': 'User 1',\n'timestamp_ms': 1675595109305},\n{'content': 'Here is $129',\n'sender_name': 'User 2',\n'timestamp_ms': 1675595068468},\n{'photos': [{'creation_timestamp': 1675595059,\n'uri': 'url_of_some_picture.jpg'}],\n'sender_name': 'User 2',\n'timestamp_ms': 1675595060730},\n{'content': 'Online is at least $100',\n'sender_name': 'User 2',\n'timestamp_ms': 1675595045152},\n{'content': 'How much do you want?',\n'sender_name': 'User 1',\n'timestamp_ms': 1675594799696},\n{'content': 'Goodmorning! $50 is too low.',\n'sender_name': 'User 2',\n'timestamp_ms': 1675577876645},\n{'content': 'Hi! Im interested in your bag. Im offering $50. Let '\n'me know if you are interested. Thanks!',\n'sender_name': 'User 1',\n'timestamp_ms': 1675549022673}],\n'participants': [{'name': 'User 1'}, {'name': 'User 2'}],\n'thread_path': 'inbox/User 1 and User 2 chat',\n'title': 'User 1 and User 2 chat'}\nUsing\nJSONLoader\nâ€‹\nSuppose we are interested in extracting the values under the\ncontent\nfield within the\nmessages\nkey of the JSON data. This can easily be done through the\nJSONLoader\nas shown below.\nJSON file\nâ€‹\nloader\n=\nJSONLoader\n(\nfile_path\n=\n'./example_data/facebook_chat.json'\n,\njq_schema\n=\n'.messages[].content'\n,\ntext_content\n=\nFalse\n)\ndata\n=\nloader\n.\nload\n(\n)\npprint\n(\ndata\n)\n[Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1}),\nDocument(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2}),\nDocument(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3}),\nDocument(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4}),\nDocument(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5}),\nDocument(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6}),\nDocument(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7}),\nDocument(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8}),\nDocument(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9}),\nDocument(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10}),\nDocument(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11})]\nJSON Lines file\nâ€‹\nIf you want to load documents from a JSON Lines file, you pass\njson_lines=True\nand specify\njq_schema\nto extract\npage_content\nfrom a single JSON object.\nfile_path\n=\n'./example_data/facebook_chat_messages.jsonl'\npprint\n(\nPath\n(\nfile_path\n)\n.\nread_text\n(\n)\n)\n('{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\n'\n'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no '\n'worries! Bye\"}\\n'\n'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im '\n'sorry it was my mistake, the blue one is not for sale\"}\\n')\nloader\n=\nJSONLoader\n(\nfile_path\n=\n'./example_data/facebook_chat_messages.jsonl'\n,\njq_schema\n=\n'.content'\n,\ntext_content\n=\nFalse\n,\njson_lines\n=\nTrue\n)\ndata\n=\nloader\n.\nload\n(\n)\npprint\n(\ndata\n)\n[Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),\nDocument(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),\nDocument(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]\nAnother option is to set\njq_schema='.'\nand provide\ncontent_key\n:\nloader\n=\nJSONLoader\n(\nfile_path\n=\n'./example_data/facebook_chat_messages.jsonl'\n,\njq_schema\n=\n'.'\n,\ncontent_key\n=\n'sender_name'\n,\njson_lines\n=\nTrue\n)\ndata\n=\nloader\n.\nload\n(\n)\npprint\n(\ndata\n)\n[Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),\nDocument(page_content='User 1', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),\nDocument(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]\nJSON file with jq schema\ncontent_key\nâ€‹\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True.\nEnsure that content_key is compatible and can be parsed using the jq schema.\nfile_path\n=\n'./sample.json'\npprint\n(\nPath\n(\nfile_path\n)\n.\nread_text\n(\n)\n)\n{\"data\": [\n{\"attributes\": {\n\"message\": \"message1\",\n\"tags\": [\n\"tag1\"]},\n\"id\": \"1\"},\n{\"attributes\": {\n\"message\": \"message2\",\n\"tags\": [\n\"tag2\"]},\n\"id\": \"2\"}]}\nloader\n=\nJSONLoader\n(\nfile_path\n=\nfile_path\n,\njq_schema\n=\n\".data[]\"\n,\ncontent_key\n=\n\".attributes.message\"\n,\nis_content_key_jq_parsable\n=\nTrue\n,\n)\ndata\n=\nloader\n.\nload\n(\n)\npprint\n(\ndata\n)\n[Document(page_content='message1', metadata={'source': '/path/to/sample.json', 'seq_num': 1}),\nDocument(page_content='message2', metadata={'source': '/path/to/sample.json', 'seq_num': 2})]\nExtracting metadata\nâ€‹\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\nThe following demonstrates how metadata can be extracted using the\nJSONLoader\n.\nThere are some key changes to be noted. In the previous example where we didn't collect the metadata, we managed to directly specify in the schema where the value for the\npage_content\ncan be extracted from.\n.messages[].content\nIn the current example, we have to tell the loader to iterate over the records in the\nmessages\nfield. The jq_schema then has to be:\n.messages[]\nThis allows us to pass the records (dict) into the\nmetadata_func\nthat has to be implemented. The\nmetadata_func\nis responsible for identifying which pieces of information in the record should be included in the metadata stored in the final\nDocument\nobject.\nAdditionally, we now have to explicitly specify in the loader, via the\ncontent_key\nargument, the key from the record where the value for the\npage_content\nneeds to be extracted from.\n# Define the metadata extraction function.\ndef\nmetadata_func\n(\nrecord\n:\ndict\n,\nmetadata\n:\ndict\n)\n-\n>\ndict\n:\nmetadata\n[\n\"sender_name\"\n]\n=\nrecord\n.\nget\n(\n\"sender_name\"\n)\nmetadata\n[\n\"timestamp_ms\"\n]\n=\nrecord\n.\nget\n(\n\"timestamp_ms\"\n)\nreturn\nmetadata\nloader\n=\nJSONLoader\n(\nfile_path\n=\n'./example_data/facebook_chat.json'\n,\njq_schema\n=\n'.messages[]'\n,\ncontent_key\n=\n\"content\"\n,\nmetadata_func\n=\nmetadata_func\n)\ndata\n=\nloader\n.\nload\n(\n)\npprint\n(\ndata\n)\n[Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),\nDocument(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),\nDocument(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),\nDocument(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),\nDocument(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),\nDocument(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),\nDocument(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),\nDocument(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),\nDocument(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),\nDocument(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),\nDocument(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]\nNow, you will see that the documents contain the metadata associated with the content we extracted.\nThe\nmetadata_func\nâ€‹\nAs shown above, the\nmetadata_func\naccepts the default metadata generated by the\nJSONLoader\n. This allows full control to the user with respect to how the metadata is formatted.\nFor example, the default metadata contains the\nsource\nand the\nseq_num\nkeys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the\nmetadata_func\nto rename the default keys and use the ones from the JSON data.\nThe example below shows how we can modify the\nsource\nto only contain information of the file source relative to the\nlangchain\ndirectory.\n# Define the metadata extraction function.\ndef\nmetadata_func\n(\nrecord\n:\ndict\n,\nmetadata\n:\ndict\n)\n-\n>\ndict\n:\nmetadata\n[\n\"sender_name\"\n]\n=\nrecord\n.\nget\n(\n\"sender_name\"\n)\nmetadata\n[\n\"timestamp_ms\"\n]\n=\nrecord\n.\nget\n(\n\"timestamp_ms\"\n)\nif\n\"source\"\nin\nmetadata\n:\nsource\n=\nmetadata\n[\n\"source\"\n]\n.\nsplit\n(\n\"/\"\n)\nsource\n=\nsource\n[\nsource\n.\nindex\n(\n\"langchain\"\n)\n:\n]\nmetadata\n[\n\"source\"\n]\n=\n\"/\"\n.\njoin\n(\nsource\n)\nreturn\nmetadata\nloader\n=\nJSONLoader\n(\nfile_path\n=\n'./example_data/facebook_chat.json'\n,\njq_schema\n=\n'.messages[]'\n,\ncontent_key\n=\n\"content\"\n,\nmetadata_func\n=\nmetadata_func\n)\ndata\n=\nloader\n.\nload\n(\n)\npprint\n(\ndata\n)\n[Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),\nDocument(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),\nDocument(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),\nDocument(page_content='I thought you were selling the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),\nDocument(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),\nDocument(page_content='Here is $129', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),\nDocument(page_content='', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),\nDocument(page_content='Online is at least $100', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),\nDocument(page_content='How much do you want?', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),\nDocument(page_content='Goodmorning! $50 is too low.', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),\nDocument(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]\nCommon JSON structures with jq schema\nâ€‹\nThe list below provides a reference to the possible\njq_schema\nthe user can use to extract content from the JSON data depending on the structure.\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]\njq_schema   -> \".[].text\"\nJSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}\njq_schema   -> \".key[].text\"\nJSON        -> [\"...\", \"...\", \"...\"]\njq_schema   -> \".[]\"\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/document_loader_markdown/",
    "How-to guides\nHow to load Markdown\nOn this page\nHow to load Markdown\nMarkdown\nis a lightweight markup language for creating formatted text using a plain-text editor.\nHere we cover how to load\nMarkdown\ndocuments into LangChain\nDocument\nobjects that we can use downstream.\nWe will cover:\nBasic usage;\nParsing of Markdown into elements such as titles, list items, and text.\nLangChain implements an\nUnstructuredMarkdownLoader\nobject which requires the\nUnstructured\npackage. First we install it:\n%\npip install\n\"unstructured[md]\"\nnltk\nBasic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain's readme:\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nUnstructuredMarkdownLoader\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nmarkdown_path\n=\n\"../../../README.md\"\nloader\n=\nUnstructuredMarkdownLoader\n(\nmarkdown_path\n)\ndata\n=\nloader\n.\nload\n(\n)\nassert\nlen\n(\ndata\n)\n==\n1\nassert\nisinstance\n(\ndata\n[\n0\n]\n,\nDocument\n)\nreadme_content\n=\ndata\n[\n0\n]\n.\npage_content\nprint\n(\nreadme_content\n[\n:\n250\n]\n)\nAPI Reference:\nDocument\nðŸ¦œï¸ðŸ”— LangChain\nâš¡ Build context-aware reasoning applications âš¡\nLooking for the JS/TS library? Check out LangChain.js.\nTo help you ship LangChain apps to production faster, check out LangSmith.\nLangSmith is a unified developer platform for building,\nRetain Elements\nâ€‹\nUnder the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying\nmode=\"elements\"\n.\nloader\n=\nUnstructuredMarkdownLoader\n(\nmarkdown_path\n,\nmode\n=\n\"elements\"\n)\ndata\n=\nloader\n.\nload\n(\n)\nprint\n(\nf\"Number of documents:\n{\nlen\n(\ndata\n)\n}\n\\n\"\n)\nfor\ndocument\nin\ndata\n[\n:\n2\n]\n:\nprint\n(\nf\"\n{\ndocument\n}\n\\n\"\n)\nNumber of documents: 66\npage_content='ðŸ¦œï¸ðŸ”— LangChain' metadata={'source': '../../../README.md', 'category_depth': 0, 'last_modified': '2024-06-28T15:20:01', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../..', 'filename': 'README.md', 'category': 'Title'}\npage_content='âš¡ Build context-aware reasoning applications âš¡' metadata={'source': '../../../README.md', 'last_modified': '2024-06-28T15:20:01', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../..', 'filename': 'README.md', 'category': 'NarrativeText'}\nNote that in this case we recover three distinct element types:\nprint\n(\nset\n(\ndocument\n.\nmetadata\n[\n\"category\"\n]\nfor\ndocument\nin\ndata\n)\n)\n{'ListItem', 'NarrativeText', 'Title'}\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/document_loader_office_file/",
    "How-to guides\nHow to load Microsoft Office files\nOn this page\nHow to load Microsoft Office files\nThe\nMicrosoft Office\nsuite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\nThis covers how to load commonly used file formats including\nDOCX\n,\nXLSX\nand\nPPTX\ndocuments into a LangChain\nDocument\nobject that we can use downstream.\nLoading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader\nâ€‹\nAzure AI Document Intelligence\n(formerly known as\nAzure Form Recognizer\n) is machine-learning\nbased service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from\ndigital or scanned PDFs, images, Office and HTML files. Document Intelligence supports\nPDF\n,\nJPEG/JPG\n,\nPNG\n,\nBMP\n,\nTIFF\n,\nHEIF\n,\nDOCX\n,\nXLSX\n,\nPPTX\nand\nHTML\n.\nThis\ncurrent implementation\nof a loader using\nDocument Intelligence\ncan incorporate content page-wise and turn it into LangChain documents. The default output format is markdown, which can be easily chained with\nMarkdownHeaderTextSplitter\nfor semantic document chunking. You can also use\nmode=\"single\"\nor\nmode=\"page\"\nto return pure texts in a single page or document split by page.\nPrerequisite\nâ€‹\nAn Azure AI Document Intelligence resource in one of the 3 preview regions:\nEast US\n,\nWest US2\n,\nWest Europe\n- follow\nthis document\nto create one if you don't have. You will be passing\n<endpoint>\nand\n<key>\nas parameters to the loader.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain langchain\n-\ncommunity azure\n-\nai\n-\ndocumentintelligence\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nAzureAIDocumentIntelligenceLoader\nfile_path\n=\n\"<filepath>\"\nendpoint\n=\n\"<endpoint>\"\nkey\n=\n\"<key>\"\nloader\n=\nAzureAIDocumentIntelligenceLoader\n(\napi_endpoint\n=\nendpoint\n,\napi_key\n=\nkey\n,\nfile_path\n=\nfile_path\n,\napi_model\n=\n\"prebuilt-layout\"\n)\ndocuments\n=\nloader\n.\nload\n(\n)\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/document_loader_pdf/",
    "How-to guides\nHow to load PDFs\nOn this page\nHow to load PDFs\nPortable Document Format (PDF)\n, standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\nThis guide covers how to\nload\nPDF\ndocuments into the LangChain\nDocument\nformat that we use downstream.\nText in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:\nAgglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;\nRun\nOCR\non images to detect text therein;\nClassify text as belonging to paragraphs, lists, tables, or other structures;\nStructure text into table rows and columns, or key-value pairs.\nLangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.\nWe will demonstrate these approaches on a\nsample file\n:\nfile_path\n=\n(\n\"../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\"\n)\nA note on multimodal models\nMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. We demonstrate an example of this in the\nUse of multimodal models\nsection below.\nSimple and fast text extraction\nâ€‹\nIf you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of\nDocument\nobjects-- one per page-- containing a single string of the page's text in the Document's\npage_content\nattribute. It will not parse text in images or scanned PDF pages. Under the hood it uses the\npypdf\nPython library.\nLangChain\ndocument loaders\nimplement\nlazy_load\nand its async variant,\nalazy_load\n, which return iterators of\nDocument\nobjects. We will use these below.\n%\npip install\n-\nqU pypdf\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nPyPDFLoader\nloader\n=\nPyPDFLoader\n(\nfile_path\n)\npages\n=\n[\n]\nasync\nfor\npage\nin\nloader\n.\nalazy_load\n(\n)\n:\npages\n.\nappend\n(\npage\n)\nprint\n(\nf\"\n{\npages\n[\n0\n]\n.\nmetadata\n}\n\\n\"\n)\nprint\n(\npages\n[\n0\n]\n.\npage_content\n)\n{'source': '../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf', 'page': 0}\nLayoutParser : A Uniï¬ed Toolkit for Deep\nLearning Based Document Image Analysis\nZejiang Shen1( ï¿½), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\nLee4, Jacob Carlson3, and Weining Li5\n1Allen Institute for AI\nshannons@allenai.org\n2Brown University\nruochen zhang@brown.edu\n3Harvard University\n{melissadell,jacob carlson }@fas.harvard.edu\n4University of Washington\nbcgl@cs.washington.edu\n5University of Waterloo\nw422li@uwaterloo.ca\nAbstract. Recent advances in document image analysis (DIA) have been\nprimarily driven by the application of neural networks. Ideally, research\noutcomes could be easily deployed in production and extended for further\ninvestigation. However, various factors like loosely organized codebases\nand sophisticated model conï¬gurations complicate the easy reuse of im-\nportant innovations by a wide audience. Though there have been on-going\neï¬€orts to improve reusability and simplify deep learning (DL) model\ndevelopment in disciplines like natural language processing and computer\nvision, none of them are optimized for challenges in the domain of DIA.\nThis represents a major gap in the existing toolkit, as DIA is central to\nacademic research across a wide range of disciplines in the social sciences\nand humanities. This paper introduces LayoutParser , an open-source\nlibrary for streamlining the usage of DL in DIA research and applica-\ntions. The core LayoutParser library comes with a set of simple and\nintuitive interfaces for applying and customizing DL models for layout de-\ntection, character recognition, and many other document processing tasks.\nTo promote extensibility, LayoutParser also incorporates a community\nplatform for sharing both pre-trained models and full document digiti-\nzation pipelines. We demonstrate that LayoutParser is helpful for both\nlightweight and large-scale digitization pipelines in real-word use cases.\nThe library is publicly available at https://layout-parser.github.io .\nKeywords: Document Image Analysis Â·Deep Learning Â·Layout Analysis\nÂ·Character Recognition Â·Open Source library Â·Toolkit.\n1 Introduction\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\ndocument image analysis (DIA) tasks including document image classiï¬cation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\nNote that the metadata of each document stores the corresponding page number.\nVector search over PDFs\nâ€‹\nOnce we have loaded PDFs into LangChain\nDocument\nobjects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain\nembeddings\nmodel will suffice.\n%\npip install\n-\nqU langchain\n-\nopenai\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"OpenAI API Key:\"\n)\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nvector_store\n=\nInMemoryVectorStore\n.\nfrom_documents\n(\npages\n,\nOpenAIEmbeddings\n(\n)\n)\ndocs\n=\nvector_store\n.\nsimilarity_search\n(\n\"What is LayoutParser?\"\n,\nk\n=\n2\n)\nfor\ndoc\nin\ndocs\n:\nprint\n(\nf\"Page\n{\ndoc\n.\nmetadata\n[\n'page'\n]\n}\n:\n{\ndoc\n.\npage_content\n[\n:\n300]\n}\n\\n\"\n)\nAPI Reference:\nInMemoryVectorStore\nPage 13: 14 Z. Shen et al.\n6 Conclusion\nLayoutParser provides a comprehensive toolkit for deep learning-based document\nimage analysis. The oï¬€-the-shelf library is easy to install, and can be used to\nbuild ï¬‚exible and accurate pipelines for processing documents with complicated\nstructures. It also supports hi\nPage 0: LayoutParser : A Uniï¬ed Toolkit for Deep\nLearning Based Document Image Analysis\nZejiang Shen1( ï¿½), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\nLee4, Jacob Carlson3, and Weining Li5\n1Allen Institute for AI\nshannons@allenai.org\n2Brown University\nruochen zhang@brown.edu\n3Harvard University\nLayout analysis and extraction of text from images\nâ€‹\nIf you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of\nDocument\nobjects, where each object represents a structure on the page. The Document's metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).\nUnder the hood it uses the\nlangchain-unstructured\nlibrary. See the\nintegration docs\nfor more information about using\nUnstructured\nwith LangChain.\nUnstructured supports multiple parameters for PDF parsing:\nstrategy\n(e.g.,\n\"fast\"\nor\n\"hi-res\"\n)\nAPI or local processing. You will need an API key to use the API.\nThe\nhi-res\nstrategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See\nlocal parsing\nsection below for considerations when running locally.\n%\npip install\n-\nqU langchain\n-\nunstructured\nimport\ngetpass\nimport\nos\nif\n\"UNSTRUCTURED_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"UNSTRUCTURED_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Unstructured API Key:\"\n)\nUnstructured API Key: Â·Â·Â·Â·Â·Â·Â·Â·\nAs before, we initialize a loader and load documents lazily:\nfrom\nlangchain_unstructured\nimport\nUnstructuredLoader\nloader\n=\nUnstructuredLoader\n(\nfile_path\n=\nfile_path\n,\nstrategy\n=\n\"hi_res\"\n,\npartition_via_api\n=\nTrue\n,\ncoordinates\n=\nTrue\n,\n)\ndocs\n=\n[\n]\nfor\ndoc\nin\nloader\n.\nlazy_load\n(\n)\n:\ndocs\n.\nappend\n(\ndoc\n)\nINFO: Preparing to split document for partition.\nINFO: Starting page number set to 1\nINFO: Allow failed set to 0\nINFO: Concurrency level set to 5\nINFO: Splitting pages 1 to 16 (16 total)\nINFO: Determined optimal split size of 4 pages.\nINFO: Partitioning 4 files with 4 page(s) each.\nINFO: Partitioning set #1 (pages 1-4).\nINFO: Partitioning set #2 (pages 5-8).\nINFO: Partitioning set #3 (pages 9-12).\nINFO: Partitioning set #4 (pages 13-16).\nINFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\nINFO: Successfully partitioned set #1, elements added to the final result.\nINFO: Successfully partitioned set #2, elements added to the final result.\nINFO: Successfully partitioned set #3, elements added to the final result.\nINFO: Successfully partitioned set #4, elements added to the final result.\nHere we recover 171 distinct structures over the 16 page document:\nprint\n(\nlen\n(\ndocs\n)\n)\n171\nWe can use the document metadata to recover content from a single page:\nfirst_page_docs\n=\n[\ndoc\nfor\ndoc\nin\ndocs\nif\ndoc\n.\nmetadata\n.\nget\n(\n\"page_number\"\n)\n==\n1\n]\nfor\ndoc\nin\nfirst_page_docs\n:\nprint\n(\ndoc\n.\npage_content\n)\nLayoutParser: A Uniï¬ed Toolkit for Deep Learning Based Document Image Analysis\n1 2 0 2 n u J 1 2 ] V C . s c [ 2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\nZejiang ShenÂ® (<), Ruochen Zhang?, Melissa DellÂ®, Benjamin Charles Germain Lee?, Jacob CarlsonÂ®, and Weining LiÂ®\n1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\nAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conï¬gurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eï¬€orts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.\nKeywords: Document Image Analysis Â· Deep Learning Â· Layout Analysis Â· Character Recognition Â· Open Source library Â· Toolkit.\n1 Introduction\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classiï¬cation [11,\nExtracting tables and other structures\nâ€‹\nEach\nDocument\nwe load represents a structure, like a title, paragraph, or table.\nSome structures may be of special interest for indexing or question-answering tasks. These structures may be:\nClassified for easy identification;\nParsed into a more structured representation.\nBelow, we identify and extract a table:\nClick to expand code for rendering pages\n%pip install -qU matplotlib PyMuPDF pillow\nimport\nfitz\nimport\nmatplotlib\n.\npatches\nas\npatches\nimport\nmatplotlib\n.\npyplot\nas\nplt\nfrom\nPIL\nimport\nImage\ndef\nplot_pdf_with_boxes\n(\npdf_page\n,\nsegments\n)\n:\npix\n=\npdf_page\n.\nget_pixmap\n(\n)\npil_image\n=\nImage\n.\nfrombytes\n(\n\"RGB\"\n,\n[\npix\n.\nwidth\n,\npix\n.\nheight\n]\n,\npix\n.\nsamples\n)\nfig\n,\nax\n=\nplt\n.\nsubplots\n(\n1\n,\nfigsize\n=\n(\n10\n,\n10\n)\n)\nax\n.\nimshow\n(\npil_image\n)\ncategories\n=\nset\n(\n)\ncategory_to_color\n=\n{\n\"Title\"\n:\n\"orchid\"\n,\n\"Image\"\n:\n\"forestgreen\"\n,\n\"Table\"\n:\n\"tomato\"\n,\n}\nfor\nsegment\nin\nsegments\n:\npoints\n=\nsegment\n[\n\"coordinates\"\n]\n[\n\"points\"\n]\nlayout_width\n=\nsegment\n[\n\"coordinates\"\n]\n[\n\"layout_width\"\n]\nlayout_height\n=\nsegment\n[\n\"coordinates\"\n]\n[\n\"layout_height\"\n]\nscaled_points\n=\n[\n(\nx\n*\npix\n.\nwidth\n/\nlayout_width\n,\ny\n*\npix\n.\nheight\n/\nlayout_height\n)\nfor\nx\n,\ny\nin\npoints\n]\nbox_color\n=\ncategory_to_color\n.\nget\n(\nsegment\n[\n\"category\"\n]\n,\n\"deepskyblue\"\n)\ncategories\n.\nadd\n(\nsegment\n[\n\"category\"\n]\n)\nrect\n=\npatches\n.\nPolygon\n(\nscaled_points\n,\nlinewidth\n=\n1\n,\nedgecolor\n=\nbox_color\n,\nfacecolor\n=\n\"none\"\n)\nax\n.\nadd_patch\n(\nrect\n)\n# Make legend\nlegend_handles\n=\n[\npatches\n.\nPatch\n(\ncolor\n=\n\"deepskyblue\"\n,\nlabel\n=\n\"Text\"\n)\n]\nfor\ncategory\nin\n[\n\"Title\"\n,\n\"Image\"\n,\n\"Table\"\n]\n:\nif\ncategory\nin\ncategories\n:\nlegend_handles\n.\nappend\n(\npatches\n.\nPatch\n(\ncolor\n=\ncategory_to_color\n[\ncategory\n]\n,\nlabel\n=\ncategory\n)\n)\nax\n.\naxis\n(\n\"off\"\n)\nax\n.\nlegend\n(\nhandles\n=\nlegend_handles\n,\nloc\n=\n\"upper right\"\n)\nplt\n.\ntight_layout\n(\n)\nplt\n.\nshow\n(\n)\ndef\nrender_page\n(\ndoc_list\n:\nlist\n,\npage_number\n:\nint\n,\nprint_text\n=\nTrue\n)\n-\n>\nNone\n:\npdf_page\n=\nfitz\n.\nopen\n(\nfile_path\n)\n.\nload_page\n(\npage_number\n-\n1\n)\npage_docs\n=\n[\ndoc\nfor\ndoc\nin\ndoc_list\nif\ndoc\n.\nmetadata\n.\nget\n(\n\"page_number\"\n)\n==\npage_number\n]\nsegments\n=\n[\ndoc\n.\nmetadata\nfor\ndoc\nin\npage_docs\n]\nplot_pdf_with_boxes\n(\npdf_page\n,\nsegments\n)\nif\nprint_text\n:\nfor\ndoc\nin\npage_docs\n:\nprint\n(\nf\"\n{\ndoc\n.\npage_content\n}\n\\n\"\n)\nrender_page\n(\ndocs\n,\n5\n)\nLayoutParser: A Uniï¬ed Toolkit for DL-Based DIA\n5\nTable 1: Current layout detection models in the LayoutParser model zoo\nDataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiï¬c documents Layouts of scanned modern magazines and scientiï¬c reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiï¬c and business document Layouts of history Japanese documents\n1 For each dataset, we train several models of diï¬€erent sizes for diï¬€erent needs (the trade-oï¬€ between accuracy vs. computational cost). For â€œbase modelâ€ and â€œlarge modelâ€, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diï¬€erent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.\nlayout data structures, which are optimized for eï¬ƒciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniï¬ed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.\n3.1 Layout Detection Models\nIn LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Diï¬€erent from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:\n1 import layoutparser as lp 2 image = cv2 . imread ( \" image_file \" ) # load images 3 model = lp . De t e c tro n2 Lay outM odel ( \" lp :// PubLayNet / f as t er _ r c nn _ R _ 50 _ F P N_ 3 x / config \" ) 4 5 layout = model . detect ( image )\nLayoutParser provides a wealth of pre-trained model weights using various datasets covering diï¬€erent languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are signiï¬cantly diï¬€erent from the training dataset. As document structures and layouts vary greatly in diï¬€erent domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name lp://<dataset-name>/<model-architecture-name>.\nNote that although the table text is collapsed into a single string in the document's content, the metadata contains a representation of its rows and columns:\nfrom\nIPython\n.\ndisplay\nimport\nHTML\n,\ndisplay\nsegments\n=\n[\ndoc\n.\nmetadata\nfor\ndoc\nin\ndocs\nif\ndoc\n.\nmetadata\n.\nget\n(\n\"page_number\"\n)\n==\n5\nand\ndoc\n.\nmetadata\n.\nget\n(\n\"category\"\n)\n==\n\"Table\"\n]\ndisplay\n(\nHTML\n(\nsegments\n[\n0\n]\n[\n\"text_as_html\"\n]\n)\n)\nTable 1: Current layout detection models in the LayoutParser model zoo\nDataset\nBase Model1\nLarge Model Notes\nPubLayNet [38]\nF/M\nLayouts of modern scientific documents\nPRImA\nM\nLayouts of scanned modern magazines and scientific reports\nNewspaper\nF\nLayouts of scanned US newspapers from the 20th century\nTableBank [18]\nF\nTable region on modern scientific and business document\nHJDataset\nF/M\nLayouts of history Japanese documents\nExtracting text from specific sections\nâ€‹\nStructures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding\nDocument\nobjects.\nBelow, we extract all text associated with the document's \"Conclusion\" section:\nrender_page\n(\ndocs\n,\n14\n,\nprint_text\n=\nFalse\n)\nconclusion_docs\n=\n[\n]\nparent_id\n=\n-\n1\nfor\ndoc\nin\ndocs\n:\nif\ndoc\n.\nmetadata\n[\n\"category\"\n]\n==\n\"Title\"\nand\n\"Conclusion\"\nin\ndoc\n.\npage_content\n:\nparent_id\n=\ndoc\n.\nmetadata\n[\n\"element_id\"\n]\nif\ndoc\n.\nmetadata\n.\nget\n(\n\"parent_id\"\n)\n==\nparent_id\n:\nconclusion_docs\n.\nappend\n(\ndoc\n)\nfor\ndoc\nin\nconclusion_docs\n:\nprint\n(\ndoc\n.\npage_content\n)\nLayoutParser provides a comprehensive toolkit for deep learning-based document image analysis. The oï¬€-the-shelf library is easy to install, and can be used to build ï¬‚exible and accurate pipelines for processing documents with complicated structures. It also supports high-level customization and enables easy labeling and training of DL models on unique document image datasets. The LayoutParser community platform facilitates sharing DL models and DIA pipelines, inviting discussion and promoting code reproducibility and reusability. The LayoutParser team is committed to keeping the library updated continuously and bringing the most recent advances in DL-based DIA, such as multi-modal document modeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.\nAcknowledgements We thank the anonymous reviewers for their comments and suggestions. This project is supported in part by NSF Grant OIA-2033558 and funding from the Harvard Data Science Initiative and Harvard Catalyst. Zejiang Shen thanks Doug Downey for suggestions.\nExtracting text from images\nâ€‹\nOCR is run on images, enabling the extraction of text therein:\nrender_page\n(\ndocs\n,\n11\n)\nLayoutParser: A Uniï¬ed Toolkit for DL-Based DIA\nfocuses on precision, eï¬ƒciency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and ï¬‚exibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.\n11\n5.1 A Comprehensive Historical Document Digitization Pipeline\nThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, ob- taining a structured representation of historical document scans is often extremely complicated. In this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to gener- ate high-quality structured data from historical Japanese ï¬rm ï¬nancial ta- bles with complicated layouts. The pipeline applies two layout models to identify diï¬€erent levels of document structures and two customized OCR engines for optimized character recog- nition accuracy.\nâ€˜Active Learning Layout Annotate Layout Dataset | +â€”â€” Annotation Toolkit A4 Deep Learning Layout Layout Detection Model Training & Inference, A Post-processing â€” Handy Data Structures & \\ Lo orajport 7 ) Al Pls for Layout Data A4 Default and Customized Text Recognition 0CR Models Â¥ Visualization & Export Layout Structure Visualization & Storage The Japanese Document Helpful LayoutParser Modules Digitization Pipeline\nAs shown in Figure 4 (a), the document contains columns of text written vertically 15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have vari- able widths, and hence cannot be eas- ily identiï¬ed via rule-based methods. Within each column, words are sepa- rated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.\nFig. 5: Illustration of how LayoutParser helps with the historical document digi- tization pipeline.\n15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.\nNote that the text from the figure on the right is extracted and incorporated into the content of the\nDocument\n.\nLocal parsing\nâ€‹\nParsing locally requires the installation of additional dependencies.\nPoppler\n(PDF analysis)\nLinux:\napt-get install poppler-utils\nMac:\nbrew install poppler\nWindows:\nhttps://github.com/oschwartz10612/poppler-windows\nTesseract\n(OCR)\nLinux:\napt-get install tesseract-ocr\nMac:\nbrew install tesseract\nWindows:\nhttps://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\nWe will also need to install the\nunstructured\nPDF extras:\n%\npip install\n-\nqU\n\"unstructured[pdf]\"\nWe can then use the\nUnstructuredLoader\nmuch the same way, forgoing the API key and\npartition_via_api\nsetting:\nloader_local\n=\nUnstructuredLoader\n(\nfile_path\n=\nfile_path\n,\nstrategy\n=\n\"hi_res\"\n,\n)\ndocs_local\n=\n[\n]\nfor\ndoc\nin\nloader_local\n.\nlazy_load\n(\n)\n:\ndocs_local\n.\nappend\n(\ndoc\n)\nWARNING: This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\nINFO: Reading PDF for file: /Users/chestercurme/repos/langchain/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Detecting page elements ...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: padding image by 20 for structure detection\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: padding image by 20 for structure detection\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nINFO: Processing entire page OCR with tesseract...\nThe list of documents can then be processed similarly to those obtained from the API.\nUse of multimodal models\nâ€‹\nMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a \"one-dimensional\" string representation.\nIn principle we can use any LangChain\nchat model\nthat supports multimodal inputs. A list of these models is documented\nhere\n. Below we use OpenAI's\ngpt-4o-mini\n.\nFirst we define a short utility function to convert a PDF page to a base64-encoded image:\n%\npip install\n-\nqU PyMuPDF pillow langchain\n-\nopenai\nimport\nbase64\nimport\nio\nimport\nfitz\nfrom\nPIL\nimport\nImage\ndef\npdf_page_to_base64\n(\npdf_path\n:\nstr\n,\npage_number\n:\nint\n)\n:\npdf_document\n=\nfitz\n.\nopen\n(\npdf_path\n)\npage\n=\npdf_document\n.\nload_page\n(\npage_number\n-\n1\n)\n# input is one-indexed\npix\n=\npage\n.\nget_pixmap\n(\n)\nimg\n=\nImage\n.\nfrombytes\n(\n\"RGB\"\n,\n[\npix\n.\nwidth\n,\npix\n.\nheight\n]\n,\npix\n.\nsamples\n)\nbuffer\n=\nio\n.\nBytesIO\n(\n)\nimg\n.\nsave\n(\nbuffer\n,\nformat\n=\n\"PNG\"\n)\nreturn\nbase64\n.\nb64encode\n(\nbuffer\n.\ngetvalue\n(\n)\n)\n.\ndecode\n(\n\"utf-8\"\n)\nfrom\nIPython\n.\ndisplay\nimport\nImage\nas\nIPImage\nfrom\nIPython\n.\ndisplay\nimport\ndisplay\nbase64_image\n=\npdf_page_to_base64\n(\nfile_path\n,\n11\n)\ndisplay\n(\nIPImage\n(\ndata\n=\nbase64\n.\nb64decode\n(\nbase64_image\n)\n)\n)\nWe can then query the model in the\nusual way\n. Below we ask it a question on related to the diagram on the page.\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nquery\n=\n\"What is the name of the first step in the pipeline?\"\nmessage\n=\nHumanMessage\n(\ncontent\n=\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\nquery\n}\n,\n{\n\"type\"\n:\n\"image_url\"\n,\n\"image_url\"\n:\n{\n\"url\"\n:\nf\"data:image/jpeg;base64,\n{\nbase64_image\n}\n\"\n}\n,\n}\n,\n]\n,\n)\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nprint\n(\nresponse\n.\ncontent\n)\nAPI Reference:\nHumanMessage\nINFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n``````output\nThe first step in the pipeline is \"Annotate Layout Dataset.\"\nOther PDF loaders\nâ€‹\nFor a list of available LangChain PDF loaders, please see\nthis table\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/document_loader_web/",
    "How-to guides\nHow to load web pages\nOn this page\nHow to load web pages\nThis guide covers how to\nload\nweb pages into the LangChain\nDocument\nformat that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.\nLangChain integrates with a host of parsers that are appropriate for web pages. The right parser will depend on your needs. Below we demonstrate two possibilities:\nSimple and fast\nparsing, in which we recover one\nDocument\nper web page with its content represented as a \"flattened\" string;\nAdvanced\nparsing, in which we recover multiple\nDocument\nobjects per page, allowing one to identify and traverse sections, links, tables, and other structures.\nSetup\nâ€‹\nFor the \"simple and fast\" parsing, we will need\nlangchain-community\nand the\nbeautifulsoup4\nlibrary:\n%\npip install\n-\nqU langchain\n-\ncommunity beautifulsoup4\nFor advanced parsing, we will use\nlangchain-unstructured\n:\n%\npip install\n-\nqU langchain\n-\nunstructured\nSimple and fast text extraction\nâ€‹\nIf you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of\nDocument\nobjects -- one per page -- containing a single string of the page's text. Under the hood it uses the\nbeautifulsoup4\nPython library.\nLangChain document loaders implement\nlazy_load\nand its async variant,\nalazy_load\n, which return iterators of\nDocument objects\n. We will use these below.\nimport\nbs4\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\npage_url\n=\n\"https://python.langchain.com/docs/how_to/chatbots_memory/\"\nloader\n=\nWebBaseLoader\n(\nweb_paths\n=\n[\npage_url\n]\n)\ndocs\n=\n[\n]\nasync\nfor\ndoc\nin\nloader\n.\nalazy_load\n(\n)\n:\ndocs\n.\nappend\n(\ndoc\n)\nassert\nlen\n(\ndocs\n)\n==\n1\ndoc\n=\ndocs\n[\n0\n]\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\nprint\n(\nf\"\n{\ndoc\n.\nmetadata\n}\n\\n\"\n)\nprint\n(\ndoc\n.\npage_content\n[\n:\n500\n]\n.\nstrip\n(\n)\n)\n{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'title': 'How to add memory to chatbots | \\uf8ffÃ¼Â¶ÃºÃ”âˆÃ¨\\uf8ffÃ¼Ã®Ã³ LangChain', 'description': 'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:', 'language': 'en'}\nHow to add memory to chatbots | ï£¿Ã¼Â¶ÃºÃ”âˆÃ¨ï£¿Ã¼Ã®Ã³ LangChain\nSkip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1ï£¿Ã¼Ã­Â¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingd\nThis is essentially a dump of the text from the page's HTML. It may contain extraneous information like headings and navigation bars. If you are familiar with the expected HTML, you can specify desired\n<div>\nclasses and other parameters via BeautifulSoup. Below we parse only the body text of the article:\nloader\n=\nWebBaseLoader\n(\nweb_paths\n=\n[\npage_url\n]\n,\nbs_kwargs\n=\n{\n\"parse_only\"\n:\nbs4\n.\nSoupStrainer\n(\nclass_\n=\n\"theme-doc-markdown markdown\"\n)\n,\n}\n,\nbs_get_text_kwargs\n=\n{\n\"separator\"\n:\n\" | \"\n,\n\"strip\"\n:\nTrue\n}\n,\n)\ndocs\n=\n[\n]\nasync\nfor\ndoc\nin\nloader\n.\nalazy_load\n(\n)\n:\ndocs\n.\nappend\n(\ndoc\n)\nassert\nlen\n(\ndocs\n)\n==\n1\ndoc\n=\ndocs\n[\n0\n]\nprint\n(\nf\"\n{\ndoc\n.\nmetadata\n}\n\\n\"\n)\nprint\n(\ndoc\n.\npage_content\n[\n:\n500\n]\n)\n{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/'}\nHow to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We'll go into more detail on a few techniq\nprint\n(\ndoc\n.\npage_content\n[\n-\n500\n:\n]\n)\na greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.'), | HumanMessage(content='What did I say my name was?'), | AIMessage(content='You introduced yourself as Nemo. How can I assist you today, Nemo?')] | Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.\nNote that this required advance technical knowledge of how the body text is represented in the underlying HTML.\nWe can parameterize\nWebBaseLoader\nwith a variety of settings, allowing for specification of request headers, rate limits, and parsers and other kwargs for BeautifulSoup. See its\nAPI reference\nfor detail.\nAdvanced parsing\nâ€‹\nThis method is appropriate if we want more granular control or processing of the page content. Below, instead of generating one\nDocument\nper page and controlling its content via BeautifulSoup, we generate multiple\nDocument\nobjects representing distinct structures on a page. These structures can include section titles and their corresponding body texts, lists or enumerations, tables, and more.\nUnder the hood it uses the\nlangchain-unstructured\nlibrary. See the\nintegration docs\nfor more information about using\nUnstructured\nwith LangChain.\nfrom\nlangchain_unstructured\nimport\nUnstructuredLoader\npage_url\n=\n\"https://python.langchain.com/docs/how_to/chatbots_memory/\"\nloader\n=\nUnstructuredLoader\n(\nweb_url\n=\npage_url\n)\ndocs\n=\n[\n]\nasync\nfor\ndoc\nin\nloader\n.\nalazy_load\n(\n)\n:\ndocs\n.\nappend\n(\ndoc\n)\nINFO: Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\nINFO: NumExpr defaulting to 8 threads.\nNote that with no advance knowledge of the page HTML structure, we recover a natural organization of the body text:\nfor\ndoc\nin\ndocs\n[\n:\n5\n]\n:\nprint\n(\ndoc\n.\npage_content\n)\nHow to add memory to chatbots\nA key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\nSimply stuffing previous messages into a chat model prompt.\nThe above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\nMore complex modifications like synthesizing summaries for long running conversations.\nERROR! Session/line number was not unique in database. History logging moved to new session 2747\nExtracting content from specific sections\nâ€‹\nEach\nDocument\nobject represents an element of the page. Its metadata contains useful information, such as its category:\nfor\ndoc\nin\ndocs\n[\n:\n5\n]\n:\nprint\n(\nf\"\n{\ndoc\n.\nmetadata\n[\n'category'\n]\n}\n:\n{\ndoc\n.\npage_content\n}\n\"\n)\nTitle: How to add memory to chatbots\nNarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\nListItem: Simply stuffing previous messages into a chat model prompt.\nListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\nListItem: More complex modifications like synthesizing summaries for long running conversations.\nElements may also have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding\nDocument\nobjects.\nAs an example, below we load the content of the \"Setup\" sections for two web pages:\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nasync\ndef\n_get_setup_docs_from_url\n(\nurl\n:\nstr\n)\n-\n>\nList\n[\nDocument\n]\n:\nloader\n=\nUnstructuredLoader\n(\nweb_url\n=\nurl\n)\nsetup_docs\n=\n[\n]\nparent_id\n=\n-\n1\nasync\nfor\ndoc\nin\nloader\n.\nalazy_load\n(\n)\n:\nif\ndoc\n.\nmetadata\n[\n\"category\"\n]\n==\n\"Title\"\nand\ndoc\n.\npage_content\n.\nstartswith\n(\n\"Setup\"\n)\n:\nparent_id\n=\ndoc\n.\nmetadata\n[\n\"element_id\"\n]\nif\ndoc\n.\nmetadata\n.\nget\n(\n\"parent_id\"\n)\n==\nparent_id\n:\nsetup_docs\n.\nappend\n(\ndoc\n)\nreturn\nsetup_docs\npage_urls\n=\n[\n\"https://python.langchain.com/docs/how_to/chatbots_memory/\"\n,\n\"https://python.langchain.com/docs/how_to/chatbots_tools/\"\n,\n]\nsetup_docs\n=\n[\n]\nfor\nurl\nin\npage_urls\n:\npage_setup_docs\n=\nawait\n_get_setup_docs_from_url\n(\nurl\n)\nsetup_docs\n.\nextend\n(\npage_setup_docs\n)\nAPI Reference:\nDocument\nfrom\ncollections\nimport\ndefaultdict\nsetup_text\n=\ndefaultdict\n(\nstr\n)\nfor\ndoc\nin\nsetup_docs\n:\nurl\n=\ndoc\n.\nmetadata\n[\n\"url\"\n]\nsetup_text\n[\nurl\n]\n+=\nf\"\n{\ndoc\n.\npage_content\n}\n\\n\"\ndict\n(\nsetup_text\n)\n{'https://python.langchain.com/docs/how_to/chatbots_memory/': \"You'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\\n%pip install --upgrade --quiet langchain langchain-openai\\n\\n# Set env var OPENAI_API_KEY or load from a .env file:\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\\nYou should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m\\n[0mNote: you may need to restart the kernel to use updated packages.\\n\",\n'https://python.langchain.com/docs/how_to/chatbots_tools/': \"For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\\nYou'll need to sign up for an account on the Tavily website, and install the following packages:\\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python\\n\\n# Set env var OPENAI_API_KEY or load from a .env file:\\nimport dotenv\\n\\ndotenv.load_dotenv()\\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\\n\"}\nVector search over page content\nâ€‹\nOnce we have loaded the page contents into LangChain\nDocument\nobjects, we can index them (e.g., for a RAG application) in the usual way. Below we use OpenAI\nembeddings\n, although any LangChain embeddings model will suffice.\n%\npip install\n-\nqU langchain\n-\nopenai\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"OpenAI API Key:\"\n)\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nvector_store\n=\nInMemoryVectorStore\n.\nfrom_documents\n(\nsetup_docs\n,\nOpenAIEmbeddings\n(\n)\n)\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\n\"Install Tavily\"\n,\nk\n=\n2\n)\nfor\ndoc\nin\nretrieved_docs\n:\nprint\n(\nf\"Page\n{\ndoc\n.\nmetadata\n[\n'url'\n]\n}\n:\n{\ndoc\n.\npage_content\n[\n:\n300]\n}\n\\n\"\n)\nAPI Reference:\nInMemoryVectorStore\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\nOther web page loaders\nâ€‹\nFor a list of available LangChain web page loaders, please see\nthis table\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/dynamic_chain/",
    "How-to guides\nHow to create a dynamic (self-constructing) chain\nHow to create a dynamic (self-constructing) chain\nPrerequisites\nThis guide assumes familiarity with the following:\nLangChain Expression Language (LCEL)\nHow to turn any function into a runnable\nSometimes we want to construct parts of a chain at runtime, depending on the chain inputs (\nrouting\nis the most common example of this). We can create dynamic chains like this using a very useful property of RunnableLambda's, which is that if a RunnableLambda returns a Runnable, that Runnable is itself invoked. Let's see an example.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\n# | echo: false\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n)\nfrom\noperator\nimport\nitemgetter\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnable\n,\nRunnablePassthrough\n,\nchain\ncontextualize_instructions\n=\n\"\"\"Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text).\"\"\"\ncontextualize_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\ncontextualize_instructions\n)\n,\n(\n\"placeholder\"\n,\n\"{chat_history}\"\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n]\n)\ncontextualize_question\n=\ncontextualize_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\nqa_instructions\n=\n(\n\"\"\"Answer the user question given the following context:\\n\\n{context}.\"\"\"\n)\nqa_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nqa_instructions\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n]\n)\n@chain\ndef\ncontextualize_if_needed\n(\ninput_\n:\ndict\n)\n-\n>\nRunnable\n:\nif\ninput_\n.\nget\n(\n\"chat_history\"\n)\n:\n# NOTE: This is returning another Runnable, not an actual output.\nreturn\ncontextualize_question\nelse\n:\nreturn\nRunnablePassthrough\n(\n)\n|\nitemgetter\n(\n\"question\"\n)\n@chain\ndef\nfake_retriever\n(\ninput_\n:\ndict\n)\n-\n>\nstr\n:\nreturn\n\"egypt's population in 2024 is about 111 million\"\nfull_chain\n=\n(\nRunnablePassthrough\n.\nassign\n(\nquestion\n=\ncontextualize_if_needed\n)\n.\nassign\n(\ncontext\n=\nfake_retriever\n)\n|\nqa_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\n)\nfull_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"what about egypt\"\n,\n\"chat_history\"\n:\n[\n(\n\"human\"\n,\n\"what's the population of indonesia\"\n)\n,\n(\n\"ai\"\n,\n\"about 276 million\"\n)\n,\n]\n,\n}\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnable\n|\nRunnablePassthrough\n|\nchain\n\"Egypt's population in 2024 is about 111 million.\"\nThe key here is that\ncontextualize_if_needed\nreturns another Runnable and not an actual output. This returned Runnable is itself run when the full chain is executed.\nLooking at the trace we can see that, since we passed in chat_history, we executed the contextualize_question chain as part of the full chain:\nhttps://smith.langchain.com/public/9e0ae34c-4082-4f3f-beed-34a2a2f4c991/r\nNote that the streaming, batching, etc. capabilities of the returned Runnable are all preserved\nfor\nchunk\nin\ncontextualize_if_needed\n.\nstream\n(\n{\n\"question\"\n:\n\"what about egypt\"\n,\n\"chat_history\"\n:\n[\n(\n\"human\"\n,\n\"what's the population of indonesia\"\n)\n,\n(\n\"ai\"\n,\n\"about 276 million\"\n)\n,\n]\n,\n}\n)\n:\nprint\n(\nchunk\n)\nWhat\nis the population of Egypt?\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/embed_text/",
    "How-to guides\nText embedding models\nOn this page\nText embedding models\ninfo\nHead to\nIntegrations\nfor documentation on built-in integrations with text embedding model providers.\nThe Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\nEmbeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\nThe base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former,\n.embed_documents\n, takes as input multiple texts, while the latter,\n.embed_query\n, takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).\n.embed_query\nwill return a list of floats, whereas\n.embed_documents\nreturns a list of lists of floats.\nGet started\nâ€‹\nSetup\nâ€‹\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings_model\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nembed_documents\nâ€‹\nEmbed list of texts\nâ€‹\nUse\n.embed_documents\nto embed a list of strings, recovering a list of embeddings:\nembeddings\n=\nembeddings_model\n.\nembed_documents\n(\n[\n\"Hi there!\"\n,\n\"Oh, hello!\"\n,\n\"What's your name?\"\n,\n\"My friends call me World\"\n,\n\"Hello World!\"\n]\n)\nlen\n(\nembeddings\n)\n,\nlen\n(\nembeddings\n[\n0\n]\n)\n(5, 1536)\nembed_query\nâ€‹\nEmbed single query\nâ€‹\nUse\n.embed_query\nto embed a single piece of text (e.g., for the purpose of comparing to other embedded pieces of texts).\nembedded_query\n=\nembeddings_model\n.\nembed_query\n(\n\"What was the name mentioned in the conversation?\"\n)\nembedded_query\n[\n:\n5\n]\n[0.0053587136790156364,\n-0.0004999046213924885,\n0.038883671164512634,\n-0.003001077566295862,\n-0.00900818221271038]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/ensemble_retriever/",
    "How-to guides\nHow to combine results from multiple retrievers\nOn this page\nHow to combine results from multiple retrievers\nThe\nEnsembleRetriever\nsupports ensembling of results from multiple\nretrievers\n. It is initialized with a list of\nBaseRetriever\nobjects. EnsembleRetrievers rerank the results of the constituent retrievers based on the\nReciprocal Rank Fusion\nalgorithm.\nBy leveraging the strengths of different algorithms, the\nEnsembleRetriever\ncan achieve better performance than any single algorithm.\nThe most common pattern is to combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity), because their strengths are complementary. It is also known as \"hybrid search\". The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity.\nBasic usage\nâ€‹\nBelow we demonstrate ensembling of a\nBM25Retriever\nwith a retriever derived from the\nFAISS vector store\n.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  rank_bm25\n>\n/\ndev\n/\nnull\nfrom\nlangchain\n.\nretrievers\nimport\nEnsembleRetriever\nfrom\nlangchain_community\n.\nretrievers\nimport\nBM25Retriever\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\ndoc_list_1\n=\n[\n\"I like apples\"\n,\n\"I like oranges\"\n,\n\"Apples and oranges are fruits\"\n,\n]\n# initialize the bm25 retriever and faiss retriever\nbm25_retriever\n=\nBM25Retriever\n.\nfrom_texts\n(\ndoc_list_1\n,\nmetadatas\n=\n[\n{\n\"source\"\n:\n1\n}\n]\n*\nlen\n(\ndoc_list_1\n)\n)\nbm25_retriever\n.\nk\n=\n2\ndoc_list_2\n=\n[\n\"You like apples\"\n,\n\"You like oranges\"\n,\n]\nembedding\n=\nOpenAIEmbeddings\n(\n)\nfaiss_vectorstore\n=\nFAISS\n.\nfrom_texts\n(\ndoc_list_2\n,\nembedding\n,\nmetadatas\n=\n[\n{\n\"source\"\n:\n2\n}\n]\n*\nlen\n(\ndoc_list_2\n)\n)\nfaiss_retriever\n=\nfaiss_vectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n2\n}\n)\n# initialize the ensemble retriever\nensemble_retriever\n=\nEnsembleRetriever\n(\nretrievers\n=\n[\nbm25_retriever\n,\nfaiss_retriever\n]\n,\nweights\n=\n[\n0.5\n,\n0.5\n]\n)\ndocs\n=\nensemble_retriever\n.\ninvoke\n(\n\"apples\"\n)\ndocs\n[Document(page_content='I like apples', metadata={'source': 1}),\nDocument(page_content='You like apples', metadata={'source': 2}),\nDocument(page_content='Apples and oranges are fruits', metadata={'source': 1}),\nDocument(page_content='You like oranges', metadata={'source': 2})]\nRuntime Configuration\nâ€‹\nWe can also configure the individual retrievers at runtime using\nconfigurable fields\n. Below we update the \"top-k\" parameter for the FAISS retriever specifically:\nfrom\nlangchain_core\n.\nrunnables\nimport\nConfigurableField\nfaiss_retriever\n=\nfaiss_vectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n2\n}\n)\n.\nconfigurable_fields\n(\nsearch_kwargs\n=\nConfigurableField\n(\nid\n=\n\"search_kwargs_faiss\"\n,\nname\n=\n\"Search Kwargs\"\n,\ndescription\n=\n\"The search kwargs to use\"\n,\n)\n)\nensemble_retriever\n=\nEnsembleRetriever\n(\nretrievers\n=\n[\nbm25_retriever\n,\nfaiss_retriever\n]\n,\nweights\n=\n[\n0.5\n,\n0.5\n]\n)\nAPI Reference:\nConfigurableField\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"search_kwargs_faiss\"\n:\n{\n\"k\"\n:\n1\n}\n}\n}\ndocs\n=\nensemble_retriever\n.\ninvoke\n(\n\"apples\"\n,\nconfig\n=\nconfig\n)\ndocs\n[Document(page_content='I like apples', metadata={'source': 1}),\nDocument(page_content='You like apples', metadata={'source': 2}),\nDocument(page_content='Apples and oranges are fruits', metadata={'source': 1})]\nNotice that this only returns one source from the FAISS retriever, because we pass in the relevant configuration at run time\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/example_selectors_langsmith/",
    "How-to guides\nHow to select examples from a LangSmith dataset\nOn this page\nHow to select examples from a LangSmith dataset\nðŸ“š\nPrerequisites\nChat models\nFew-shot-prompting\nLangSmith\nðŸ“¦\nCompatibility\nThe code in this guide requires\nlangsmith>=0.1.101\n,\nlangchain-core>=0.2.34\n. Please ensure you have the correct packages installed.\nLangSmith\ndatasets have built-in support for similarity search, making them a great tool for building and querying few-shot examples.\nIn this guide we'll see how to use an indexed LangSmith dataset as a few-shot example selector.\nSetup\nâ€‹\nBefore getting started make sure you've\ncreated a LangSmith account\nand set your credentials:\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"LANGSMITH_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Set LangSmith API key:\\n\\n\"\n)\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nSet LangSmith API key:\nÂ·Â·Â·Â·Â·Â·Â·Â·\nWe'll need to install the\nlangsmith\nSDK. In this example we'll also make use of\nlangchain\n,\nlangchain-openai\n, and\nlangchain-benchmarks\n:\n%\npip install\n-\nqU\n\"langsmith>=0.1.101\"\n\"langchain-core>=0.2.34\"\nlangchain langchain\n-\nopenai langchain\n-\nbenchmarks\nNow we'll clone a public dataset and turn on indexing for the dataset. We can also turn on indexing via the\nLangSmith UI\n.\nWe'll clone the\nMultiverse math few shot example dataset\n.\nThis enables searching over the dataset and will make sure that anytime we update/add examples they are also indexed.\nfrom\nlangsmith\nimport\nClient\nas\nLangSmith\nls_client\n=\nLangSmith\n(\n)\ndataset_name\n=\n\"multiverse-math-few-shot-examples-v2\"\ndataset_public_url\n=\n(\n\"https://smith.langchain.com/public/620596ee-570b-4d2b-8c8f-f828adbe5242/d\"\n)\nls_client\n.\nclone_public_dataset\n(\ndataset_public_url\n)\ndataset_id\n=\nls_client\n.\nread_dataset\n(\ndataset_name\n=\ndataset_name\n)\n.\nid\nls_client\n.\nindex_dataset\n(\ndataset_id\n=\ndataset_id\n)\nQuerying dataset\nâ€‹\nIndexing can take a few seconds. Once the dataset is indexed, we can search for similar examples. Note that the input to the\nsimilar_examples\nmethod must have the same schema as the examples inputs. In this case our example inputs are a dictionary with a \"question\" key:\nexamples\n=\nls_client\n.\nsimilar_examples\n(\n{\n\"question\"\n:\n\"whats the negation of the negation of the negation of 3\"\n}\n,\nlimit\n=\n3\n,\ndataset_id\n=\ndataset_id\n,\n)\nlen\n(\nexamples\n)\n3\nexamples\n[\n0\n]\n.\ninputs\n[\n\"question\"\n]\n'evaluate the negation of -100'\nFor this dataset, the outputs are the conversation that followed the question in OpenAI message format:\nexamples\n[\n0\n]\n.\noutputs\n[\n\"conversation\"\n]\n[{'role': 'assistant',\n'content': None,\n'tool_calls': [{'id': 'toolu_01HTpq4cYNUac6F7omUc2Wz3',\n'type': 'function',\n'function': {'name': 'negate', 'arguments': '{\"a\": -100}'}}]},\n{'role': 'tool',\n'content': '-100.0',\n'tool_call_id': 'toolu_01HTpq4cYNUac6F7omUc2Wz3'},\n{'role': 'assistant', 'content': 'So the answer is 100.'},\n{'role': 'user',\n'content': '100 is incorrect. Please refer to the output of your tool call.'},\n{'role': 'assistant',\n'content': [{'text': \"You're right, my previous answer was incorrect. Let me re-evaluate using the tool output:\",\n'type': 'text'}],\n'tool_calls': [{'id': 'toolu_01XsJQboYghGDygQpPjJkeRq',\n'type': 'function',\n'function': {'name': 'negate', 'arguments': '{\"a\": -100}'}}]},\n{'role': 'tool',\n'content': '-100.0',\n'tool_call_id': 'toolu_01XsJQboYghGDygQpPjJkeRq'},\n{'role': 'assistant', 'content': 'The answer is -100.0'},\n{'role': 'user',\n'content': 'You have the correct numerical answer but are returning additional text. Please only respond with the numerical answer.'},\n{'role': 'assistant', 'content': '-100.0'}]\nCreating dynamic few-shot prompts\nâ€‹\nThe search returns the examples whose inputs are most similar to the query input. We can use this for few-shot prompting a model like so:\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nfrom\nlangchain_benchmarks\n.\ntool_usage\n.\ntasks\n.\nmultiverse_math\nimport\n(\nadd\n,\ncos\n,\ndivide\n,\nlog\n,\nmultiply\n,\nnegate\n,\npi\n,\npower\n,\nsin\n,\nsubtract\n,\n)\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\nlangsmith\nimport\nAsyncClient\nas\nAsyncLangSmith\nasync_ls_client\n=\nAsyncLangSmith\n(\n)\ndef\nsimilar_examples\n(\ninput_\n:\ndict\n)\n-\n>\ndict\n:\nexamples\n=\nls_client\n.\nsimilar_examples\n(\ninput_\n,\nlimit\n=\n5\n,\ndataset_id\n=\ndataset_id\n)\nreturn\n{\n**\ninput_\n,\n\"examples\"\n:\nexamples\n}\nasync\ndef\nasimilar_examples\n(\ninput_\n:\ndict\n)\n-\n>\ndict\n:\nexamples\n=\nawait\nasync_ls_client\n.\nsimilar_examples\n(\ninput_\n,\nlimit\n=\n5\n,\ndataset_id\n=\ndataset_id\n)\nreturn\n{\n**\ninput_\n,\n\"examples\"\n:\nexamples\n}\ndef\nconstruct_prompt\n(\ninput_\n:\ndict\n)\n-\n>\nlist\n:\ninstructions\n=\n\"\"\"You are great at using mathematical tools.\"\"\"\nexamples\n=\n[\n]\nfor\nex\nin\ninput_\n[\n\"examples\"\n]\n:\nexamples\n.\nappend\n(\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nex\n.\ninputs\n[\n\"question\"\n]\n}\n)\nfor\nmsg\nin\nex\n.\noutputs\n[\n\"conversation\"\n]\n:\nif\nmsg\n[\n\"role\"\n]\n==\n\"assistant\"\n:\nmsg\n[\n\"name\"\n]\n=\n\"example_assistant\"\nif\nmsg\n[\n\"role\"\n]\n==\n\"user\"\n:\nmsg\n[\n\"name\"\n]\n=\n\"example_user\"\nexamples\n.\nappend\n(\nmsg\n)\nreturn\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\ninstructions\n}\n,\n*\nexamples\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_\n[\n\"question\"\n]\n}\n,\n]\ntools\n=\n[\nadd\n,\ncos\n,\ndivide\n,\nlog\n,\nmultiply\n,\nnegate\n,\npi\n,\npower\n,\nsin\n,\nsubtract\n]\nllm\n=\ninit_chat_model\n(\n\"gpt-4o-2024-08-06\"\n)\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n)\nexample_selector\n=\nRunnableLambda\n(\nfunc\n=\nsimilar_examples\n,\nafunc\n=\nasimilar_examples\n)\nchain\n=\nexample_selector\n|\nconstruct_prompt\n|\nllm_with_tools\nAPI Reference:\nRunnableLambda\nai_msg\n=\nawait\nchain\n.\nainvoke\n(\n{\n\"question\"\n:\n\"whats the negation of the negation of 3\"\n}\n)\nai_msg\n.\ntool_calls\n[{'name': 'negate',\n'args': {'a': 3},\n'id': 'call_uMSdoTl6ehfHh5a6JQUb2NoZ',\n'type': 'tool_call'}]\nLooking at the LangSmith trace, we can see that relevant examples were pulled in in the\nsimilar_examples\nstep and passed as messages to ChatOpenAI:\nhttps://smith.langchain.com/public/9585e30f-765a-4ed9-b964-2211420cd2f8/r/fdea98d6-e90f-49d4-ac22-dfd012e9e0d9\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/example_selectors_length_based/",
    "How-to guides\nHow to select examples by length\nHow to select examples by length\nThis\nexample selector\nselects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.\nfrom\nlangchain_core\n.\nexample_selectors\nimport\nLengthBasedExampleSelector\nfrom\nlangchain_core\n.\nprompts\nimport\nFewShotPromptTemplate\n,\nPromptTemplate\n# Examples of a pretend task of creating antonyms.\nexamples\n=\n[\n{\n\"input\"\n:\n\"happy\"\n,\n\"output\"\n:\n\"sad\"\n}\n,\n{\n\"input\"\n:\n\"tall\"\n,\n\"output\"\n:\n\"short\"\n}\n,\n{\n\"input\"\n:\n\"energetic\"\n,\n\"output\"\n:\n\"lethargic\"\n}\n,\n{\n\"input\"\n:\n\"sunny\"\n,\n\"output\"\n:\n\"gloomy\"\n}\n,\n{\n\"input\"\n:\n\"windy\"\n,\n\"output\"\n:\n\"calm\"\n}\n,\n]\nexample_prompt\n=\nPromptTemplate\n(\ninput_variables\n=\n[\n\"input\"\n,\n\"output\"\n]\n,\ntemplate\n=\n\"Input: {input}\\nOutput: {output}\"\n,\n)\nexample_selector\n=\nLengthBasedExampleSelector\n(\n# The examples it has available to choose from.\nexamples\n=\nexamples\n,\n# The PromptTemplate being used to format the examples.\nexample_prompt\n=\nexample_prompt\n,\n# The maximum length that the formatted examples should be.\n# Length is measured by the get_text_length function below.\nmax_length\n=\n25\n,\n# The function used to get the length of a string, which is used\n# to determine which examples to include. It is commented out because\n# it is provided as a default value if none is specified.\n# get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n)\ndynamic_prompt\n=\nFewShotPromptTemplate\n(\n# We provide an ExampleSelector instead of examples.\nexample_selector\n=\nexample_selector\n,\nexample_prompt\n=\nexample_prompt\n,\nprefix\n=\n\"Give the antonym of every input\"\n,\nsuffix\n=\n\"Input: {adjective}\\nOutput:\"\n,\ninput_variables\n=\n[\n\"adjective\"\n]\n,\n)\nAPI Reference:\nLengthBasedExampleSelector\n|\nFewShotPromptTemplate\n|\nPromptTemplate\n# An example with small input, so it selects all examples.\nprint\n(\ndynamic_prompt\n.\nformat\n(\nadjective\n=\n\"big\"\n)\n)\nGive the antonym of every input\nInput: happy\nOutput: sad\nInput: tall\nOutput: short\nInput: energetic\nOutput: lethargic\nInput: sunny\nOutput: gloomy\nInput: windy\nOutput: calm\nInput: big\nOutput:\n# An example with long input, so it selects only one example.\nlong_string\n=\n\"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\nprint\n(\ndynamic_prompt\n.\nformat\n(\nadjective\n=\nlong_string\n)\n)\nGive the antonym of every input\nInput: happy\nOutput: sad\nInput: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\nOutput:\n# You can add an example to an example selector as well.\nnew_example\n=\n{\n\"input\"\n:\n\"big\"\n,\n\"output\"\n:\n\"small\"\n}\ndynamic_prompt\n.\nexample_selector\n.\nadd_example\n(\nnew_example\n)\nprint\n(\ndynamic_prompt\n.\nformat\n(\nadjective\n=\n\"enthusiastic\"\n)\n)\nGive the antonym of every input\nInput: happy\nOutput: sad\nInput: tall\nOutput: short\nInput: energetic\nOutput: lethargic\nInput: sunny\nOutput: gloomy\nInput: windy\nOutput: calm\nInput: big\nOutput: small\nInput: enthusiastic\nOutput:\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/example_selectors_mmr/",
    "How-to guides\nHow to select examples by maximal marginal relevance (MMR)\nHow to select examples by maximal marginal relevance (MMR)\nThe\nMaxMarginalRelevanceExampleSelector\nselects\nexamples\nbased on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\nexample_selectors\nimport\n(\nMaxMarginalRelevanceExampleSelector\n,\nSemanticSimilarityExampleSelector\n,\n)\nfrom\nlangchain_core\n.\nprompts\nimport\nFewShotPromptTemplate\n,\nPromptTemplate\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nexample_prompt\n=\nPromptTemplate\n(\ninput_variables\n=\n[\n\"input\"\n,\n\"output\"\n]\n,\ntemplate\n=\n\"Input: {input}\\nOutput: {output}\"\n,\n)\n# Examples of a pretend task of creating antonyms.\nexamples\n=\n[\n{\n\"input\"\n:\n\"happy\"\n,\n\"output\"\n:\n\"sad\"\n}\n,\n{\n\"input\"\n:\n\"tall\"\n,\n\"output\"\n:\n\"short\"\n}\n,\n{\n\"input\"\n:\n\"energetic\"\n,\n\"output\"\n:\n\"lethargic\"\n}\n,\n{\n\"input\"\n:\n\"sunny\"\n,\n\"output\"\n:\n\"gloomy\"\n}\n,\n{\n\"input\"\n:\n\"windy\"\n,\n\"output\"\n:\n\"calm\"\n}\n,\n]\nAPI Reference:\nMaxMarginalRelevanceExampleSelector\n|\nSemanticSimilarityExampleSelector\n|\nFewShotPromptTemplate\n|\nPromptTemplate\nexample_selector\n=\nMaxMarginalRelevanceExampleSelector\n.\nfrom_examples\n(\n# The list of examples available to select from.\nexamples\n,\n# The embedding class used to produce embeddings which are used to measure semantic similarity.\nOpenAIEmbeddings\n(\n)\n,\n# The VectorStore class that is used to store the embeddings and do a similarity search over.\nFAISS\n,\n# The number of examples to produce.\nk\n=\n2\n,\n)\nmmr_prompt\n=\nFewShotPromptTemplate\n(\n# We provide an ExampleSelector instead of examples.\nexample_selector\n=\nexample_selector\n,\nexample_prompt\n=\nexample_prompt\n,\nprefix\n=\n\"Give the antonym of every input\"\n,\nsuffix\n=\n\"Input: {adjective}\\nOutput:\"\n,\ninput_variables\n=\n[\n\"adjective\"\n]\n,\n)\n# Input is a feeling, so should select the happy/sad example as the first one\nprint\n(\nmmr_prompt\n.\nformat\n(\nadjective\n=\n\"worried\"\n)\n)\nGive the antonym of every input\nInput: happy\nOutput: sad\nInput: windy\nOutput: calm\nInput: worried\nOutput:\n# Let's compare this to what we would just get if we went solely off of similarity,\n# by using SemanticSimilarityExampleSelector instead of MaxMarginalRelevanceExampleSelector.\nexample_selector\n=\nSemanticSimilarityExampleSelector\n.\nfrom_examples\n(\n# The list of examples available to select from.\nexamples\n,\n# The embedding class used to produce embeddings which are used to measure semantic similarity.\nOpenAIEmbeddings\n(\n)\n,\n# The VectorStore class that is used to store the embeddings and do a similarity search over.\nFAISS\n,\n# The number of examples to produce.\nk\n=\n2\n,\n)\nsimilar_prompt\n=\nFewShotPromptTemplate\n(\n# We provide an ExampleSelector instead of examples.\nexample_selector\n=\nexample_selector\n,\nexample_prompt\n=\nexample_prompt\n,\nprefix\n=\n\"Give the antonym of every input\"\n,\nsuffix\n=\n\"Input: {adjective}\\nOutput:\"\n,\ninput_variables\n=\n[\n\"adjective\"\n]\n,\n)\nprint\n(\nsimilar_prompt\n.\nformat\n(\nadjective\n=\n\"worried\"\n)\n)\nGive the antonym of every input\nInput: happy\nOutput: sad\nInput: sunny\nOutput: gloomy\nInput: worried\nOutput:\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/example_selectors_ngram/",
    "How-to guides\nHow to select examples by n-gram overlap\nHow to select examples by n-gram overlap\nThe\nNGramOverlapExampleSelector\nselects and orders examples based on which examples are most similar to the input, according to an ngram overlap score. The ngram overlap score is a float between 0.0 and 1.0, inclusive.\nThe\nselector\nallows for a threshold score to be set. Examples with an ngram overlap score less than or equal to the threshold are excluded. The threshold is set to -1.0, by default, so will not exclude any examples, only reorder them. Setting the threshold to 0.0 will exclude examples that have no ngram overlaps with the input.\nfrom\nlangchain_community\n.\nexample_selectors\nimport\nNGramOverlapExampleSelector\nfrom\nlangchain_core\n.\nprompts\nimport\nFewShotPromptTemplate\n,\nPromptTemplate\nexample_prompt\n=\nPromptTemplate\n(\ninput_variables\n=\n[\n\"input\"\n,\n\"output\"\n]\n,\ntemplate\n=\n\"Input: {input}\\nOutput: {output}\"\n,\n)\n# Examples of a fictional translation task.\nexamples\n=\n[\n{\n\"input\"\n:\n\"See Spot run.\"\n,\n\"output\"\n:\n\"Ver correr a Spot.\"\n}\n,\n{\n\"input\"\n:\n\"My dog barks.\"\n,\n\"output\"\n:\n\"Mi perro ladra.\"\n}\n,\n{\n\"input\"\n:\n\"Spot can run.\"\n,\n\"output\"\n:\n\"Spot puede correr.\"\n}\n,\n]\nAPI Reference:\nFewShotPromptTemplate\n|\nPromptTemplate\nexample_selector\n=\nNGramOverlapExampleSelector\n(\n# The examples it has available to choose from.\nexamples\n=\nexamples\n,\n# The PromptTemplate being used to format the examples.\nexample_prompt\n=\nexample_prompt\n,\n# The threshold, at which selector stops.\n# It is set to -1.0 by default.\nthreshold\n=\n-\n1.0\n,\n# For negative threshold:\n# Selector sorts examples by ngram overlap score, and excludes none.\n# For threshold greater than 1.0:\n# Selector excludes all examples, and returns an empty list.\n# For threshold equal to 0.0:\n# Selector sorts examples by ngram overlap score,\n# and excludes those with no ngram overlap with input.\n)\ndynamic_prompt\n=\nFewShotPromptTemplate\n(\n# We provide an ExampleSelector instead of examples.\nexample_selector\n=\nexample_selector\n,\nexample_prompt\n=\nexample_prompt\n,\nprefix\n=\n\"Give the Spanish translation of every input\"\n,\nsuffix\n=\n\"Input: {sentence}\\nOutput:\"\n,\ninput_variables\n=\n[\n\"sentence\"\n]\n,\n)\n# An example input with large ngram overlap with \"Spot can run.\"\n# and no overlap with \"My dog barks.\"\nprint\n(\ndynamic_prompt\n.\nformat\n(\nsentence\n=\n\"Spot can run fast.\"\n)\n)\nGive the Spanish translation of every input\nInput: Spot can run.\nOutput: Spot puede correr.\nInput: See Spot run.\nOutput: Ver correr a Spot.\nInput: My dog barks.\nOutput: Mi perro ladra.\nInput: Spot can run fast.\nOutput:\n# You can add examples to NGramOverlapExampleSelector as well.\nnew_example\n=\n{\n\"input\"\n:\n\"Spot plays fetch.\"\n,\n\"output\"\n:\n\"Spot juega a buscar.\"\n}\nexample_selector\n.\nadd_example\n(\nnew_example\n)\nprint\n(\ndynamic_prompt\n.\nformat\n(\nsentence\n=\n\"Spot can run fast.\"\n)\n)\nGive the Spanish translation of every input\nInput: Spot can run.\nOutput: Spot puede correr.\nInput: See Spot run.\nOutput: Ver correr a Spot.\nInput: Spot plays fetch.\nOutput: Spot juega a buscar.\nInput: My dog barks.\nOutput: Mi perro ladra.\nInput: Spot can run fast.\nOutput:\n# You can set a threshold at which examples are excluded.\n# For example, setting threshold equal to 0.0\n# excludes examples with no ngram overlaps with input.\n# Since \"My dog barks.\" has no ngram overlaps with \"Spot can run fast.\"\n# it is excluded.\nexample_selector\n.\nthreshold\n=\n0.0\nprint\n(\ndynamic_prompt\n.\nformat\n(\nsentence\n=\n\"Spot can run fast.\"\n)\n)\nGive the Spanish translation of every input\nInput: Spot can run.\nOutput: Spot puede correr.\nInput: See Spot run.\nOutput: Ver correr a Spot.\nInput: Spot plays fetch.\nOutput: Spot juega a buscar.\nInput: Spot can run fast.\nOutput:\n# Setting small nonzero threshold\nexample_selector\n.\nthreshold\n=\n0.09\nprint\n(\ndynamic_prompt\n.\nformat\n(\nsentence\n=\n\"Spot can play fetch.\"\n)\n)\nGive the Spanish translation of every input\nInput: Spot can run.\nOutput: Spot puede correr.\nInput: Spot plays fetch.\nOutput: Spot juega a buscar.\nInput: Spot can play fetch.\nOutput:\n# Setting threshold greater than 1.0\nexample_selector\n.\nthreshold\n=\n1.0\n+\n1e-9\nprint\n(\ndynamic_prompt\n.\nformat\n(\nsentence\n=\n\"Spot can play fetch.\"\n)\n)\nGive the Spanish translation of every input\nInput: Spot can play fetch.\nOutput:\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/example_selectors_similarity/",
    "How-to guides\nHow to select examples by similarity\nHow to select examples by similarity\nThis object selects\nexamples\nbased on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_core\n.\nexample_selectors\nimport\nSemanticSimilarityExampleSelector\nfrom\nlangchain_core\n.\nprompts\nimport\nFewShotPromptTemplate\n,\nPromptTemplate\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nexample_prompt\n=\nPromptTemplate\n(\ninput_variables\n=\n[\n\"input\"\n,\n\"output\"\n]\n,\ntemplate\n=\n\"Input: {input}\\nOutput: {output}\"\n,\n)\n# Examples of a pretend task of creating antonyms.\nexamples\n=\n[\n{\n\"input\"\n:\n\"happy\"\n,\n\"output\"\n:\n\"sad\"\n}\n,\n{\n\"input\"\n:\n\"tall\"\n,\n\"output\"\n:\n\"short\"\n}\n,\n{\n\"input\"\n:\n\"energetic\"\n,\n\"output\"\n:\n\"lethargic\"\n}\n,\n{\n\"input\"\n:\n\"sunny\"\n,\n\"output\"\n:\n\"gloomy\"\n}\n,\n{\n\"input\"\n:\n\"windy\"\n,\n\"output\"\n:\n\"calm\"\n}\n,\n]\nAPI Reference:\nSemanticSimilarityExampleSelector\n|\nFewShotPromptTemplate\n|\nPromptTemplate\nexample_selector\n=\nSemanticSimilarityExampleSelector\n.\nfrom_examples\n(\n# The list of examples available to select from.\nexamples\n,\n# The embedding class used to produce embeddings which are used to measure semantic similarity.\nOpenAIEmbeddings\n(\n)\n,\n# The VectorStore class that is used to store the embeddings and do a similarity search over.\nChroma\n,\n# The number of examples to produce.\nk\n=\n1\n,\n)\nsimilar_prompt\n=\nFewShotPromptTemplate\n(\n# We provide an ExampleSelector instead of examples.\nexample_selector\n=\nexample_selector\n,\nexample_prompt\n=\nexample_prompt\n,\nprefix\n=\n\"Give the antonym of every input\"\n,\nsuffix\n=\n\"Input: {adjective}\\nOutput:\"\n,\ninput_variables\n=\n[\n\"adjective\"\n]\n,\n)\n# Input is a feeling, so should select the happy/sad example\nprint\n(\nsimilar_prompt\n.\nformat\n(\nadjective\n=\n\"worried\"\n)\n)\nGive the antonym of every input\nInput: happy\nOutput: sad\nInput: worried\nOutput:\n# Input is a measurement, so should select the tall/short example\nprint\n(\nsimilar_prompt\n.\nformat\n(\nadjective\n=\n\"large\"\n)\n)\nGive the antonym of every input\nInput: tall\nOutput: short\nInput: large\nOutput:\n# You can add new examples to the SemanticSimilarityExampleSelector as well\nsimilar_prompt\n.\nexample_selector\n.\nadd_example\n(\n{\n\"input\"\n:\n\"enthusiastic\"\n,\n\"output\"\n:\n\"apathetic\"\n}\n)\nprint\n(\nsimilar_prompt\n.\nformat\n(\nadjective\n=\n\"passionate\"\n)\n)\nGive the antonym of every input\nInput: enthusiastic\nOutput: apathetic\nInput: passionate\nOutput:\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/extraction_examples/",
    "How-to guides\nHow to use reference examples when doing extraction\nOn this page\nHow to use reference examples when doing extraction\nThe quality of extractions can often be improved by providing reference examples to the LLM.\nData extraction attempts to generate\nstructured representations\nof information found in text and other unstructured or semi-structured formats.\nTool-calling\nLLM features are often used in this context. This guide demonstrates how to build few-shot examples of tool calls to help steer the behavior of extraction and similar applications.\ntip\nWhile this guide focuses how to use examples with a tool calling model, this technique is generally applicable, and will work\nalso with JSON more or prompt based techniques.\nLangChain implements a\ntool-call attribute\non messages from LLMs that include tool calls. See our\nhow-to guide on tool calling\nfor more detail. To build reference examples for data extraction, we build a chat history containing a sequence of:\nHumanMessage\ncontaining example inputs;\nAIMessage\ncontaining example tool calls;\nToolMessage\ncontaining example tool outputs.\nLangChain adopts this convention for structuring tool calls into conversation across LLM model providers.\nFirst we build a prompt template that includes a placeholder for these messages:\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nMessagesPlaceholder\n# Define a custom prompt to provide instructions and any additional context.\n# 1) You can add examples into the prompt template to improve extraction quality\n# 2) Introduce additional parameters to take context into account (e.g., include metadata\n#    about the document from which the text was extracted.)\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are an expert extraction algorithm. \"\n\"Only extract relevant information from the text. \"\n\"If you do not know the value of an attribute asked \"\n\"to extract, return null for the attribute's value.\"\n,\n)\n,\n# â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“\nMessagesPlaceholder\n(\n\"examples\"\n)\n,\n# <-- EXAMPLES!\n# â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘\n(\n\"human\"\n,\n\"{text}\"\n)\n,\n]\n)\nAPI Reference:\nChatPromptTemplate\n|\nMessagesPlaceholder\nTest out the template:\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nHumanMessage\n,\n)\nprompt\n.\ninvoke\n(\n{\n\"text\"\n:\n\"this is some text\"\n,\n\"examples\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"testing 1 2 3\"\n)\n]\n}\n)\nAPI Reference:\nHumanMessage\nChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='testing 1 2 3', additional_kwargs={}, response_metadata={}), HumanMessage(content='this is some text', additional_kwargs={}, response_metadata={})])\nDefine the schema\nâ€‹\nLet's re-use the person schema from the\nextraction tutorial\n.\nfrom\ntyping\nimport\nList\n,\nOptional\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nPerson\n(\nBaseModel\n)\n:\n\"\"\"Information about a person.\"\"\"\n# ^ Doc-string for the entity Person.\n# This doc-string is sent to the LLM as the description of the schema Person,\n# and it can help to improve extraction results.\n# Note that:\n# 1. Each field is an `optional` -- this allows the model to decline to extract it!\n# 2. Each field has a `description` -- this description is used by the LLM.\n# Having a good description can help improve extraction results.\nname\n:\nOptional\n[\nstr\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The name of the person\"\n)\nhair_color\n:\nOptional\n[\nstr\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The color of the person's hair if known\"\n)\nheight_in_meters\n:\nOptional\n[\nstr\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Height in METERs\"\n)\nclass\nData\n(\nBaseModel\n)\n:\n\"\"\"Extracted data about people.\"\"\"\n# Creates a model so that we can extract multiple entities.\npeople\n:\nList\n[\nPerson\n]\nDefine reference examples\nâ€‹\nExamples can be defined as a list of input-output pairs.\nEach example contains an example\ninput\ntext and an example\noutput\nshowing what should be extracted from the text.\nimportant\nThis is a bit in the weeds, so feel free to skip.\nThe format of the example needs to match the API used (e.g., tool calling or JSON mode etc.).\nHere, the formatted examples will match the format expected for the tool calling API since that's what we're using.\nimport\nuuid\nfrom\ntyping\nimport\nDict\n,\nList\n,\nTypedDict\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nBaseMessage\n,\nHumanMessage\n,\nSystemMessage\n,\nToolMessage\n,\n)\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nExample\n(\nTypedDict\n)\n:\n\"\"\"A representation of an example consisting of text input and expected tool calls.\nFor extraction, the tool calls are represented as instances of pydantic model.\n\"\"\"\ninput\n:\nstr\n# This is the example text\ntool_calls\n:\nList\n[\nBaseModel\n]\n# Instances of pydantic model that should be extracted\ndef\ntool_example_to_messages\n(\nexample\n:\nExample\n)\n-\n>\nList\n[\nBaseMessage\n]\n:\n\"\"\"Convert an example into a list of messages that can be fed into an LLM.\nThis code is an adapter that converts our example to a list of messages\nthat can be fed into a chat model.\nThe list of messages per example corresponds to:\n1) HumanMessage: contains the content from which content should be extracted.\n2) AIMessage: contains the extracted information from the model\n3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.\nThe ToolMessage is required because some of the chat models are hyper-optimized for agents\nrather than for an extraction use case.\n\"\"\"\nmessages\n:\nList\n[\nBaseMessage\n]\n=\n[\nHumanMessage\n(\ncontent\n=\nexample\n[\n\"input\"\n]\n)\n]\ntool_calls\n=\n[\n]\nfor\ntool_call\nin\nexample\n[\n\"tool_calls\"\n]\n:\ntool_calls\n.\nappend\n(\n{\n\"id\"\n:\nstr\n(\nuuid\n.\nuuid4\n(\n)\n)\n,\n\"args\"\n:\ntool_call\n.\ndict\n(\n)\n,\n# The name of the function right now corresponds\n# to the name of the pydantic model\n# This is implicit in the API right now,\n# and will be improved over time.\n\"name\"\n:\ntool_call\n.\n__class__\n.\n__name__\n,\n}\n,\n)\nmessages\n.\nappend\n(\nAIMessage\n(\ncontent\n=\n\"\"\n,\ntool_calls\n=\ntool_calls\n)\n)\ntool_outputs\n=\nexample\n.\nget\n(\n\"tool_outputs\"\n)\nor\n[\n\"You have correctly called this tool.\"\n]\n*\nlen\n(\ntool_calls\n)\nfor\noutput\n,\ntool_call\nin\nzip\n(\ntool_outputs\n,\ntool_calls\n)\n:\nmessages\n.\nappend\n(\nToolMessage\n(\ncontent\n=\noutput\n,\ntool_call_id\n=\ntool_call\n[\n\"id\"\n]\n)\n)\nreturn\nmessages\nAPI Reference:\nAIMessage\n|\nBaseMessage\n|\nHumanMessage\n|\nSystemMessage\n|\nToolMessage\nNext let's define our examples and then convert them into message format.\nexamples\n=\n[\n(\n\"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\"\n,\nData\n(\npeople\n=\n[\n]\n)\n,\n)\n,\n(\n\"Fiona traveled far from France to Spain.\"\n,\nData\n(\npeople\n=\n[\nPerson\n(\nname\n=\n\"Fiona\"\n,\nheight_in_meters\n=\nNone\n,\nhair_color\n=\nNone\n)\n]\n)\n,\n)\n,\n]\nmessages\n=\n[\n]\nfor\ntext\n,\ntool_call\nin\nexamples\n:\nmessages\n.\nextend\n(\ntool_example_to_messages\n(\n{\n\"input\"\n:\ntext\n,\n\"tool_calls\"\n:\n[\ntool_call\n]\n}\n)\n)\nLet's test out the prompt\nexample_prompt\n=\nprompt\n.\ninvoke\n(\n{\n\"text\"\n:\n\"this is some text\"\n,\n\"examples\"\n:\nmessages\n}\n)\nfor\nmessage\nin\nexample_prompt\n.\nmessages\n:\nprint\n(\nf\"\n{\nmessage\n.\ntype\n}\n:\n{\nmessage\n}\n\"\n)\nsystem: content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\" additional_kwargs={} response_metadata={}\nhuman: content=\"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\" additional_kwargs={} response_metadata={}\nai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'Data', 'args': {'people': []}, 'id': '240159b1-1405-4107-a07c-3c6b91b3d5b7', 'type': 'tool_call'}]\ntool: content='You have correctly called this tool.' tool_call_id='240159b1-1405-4107-a07c-3c6b91b3d5b7'\nhuman: content='Fiona traveled far from France to Spain.' additional_kwargs={} response_metadata={}\nai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'Data', 'args': {'people': [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]}, 'id': '3fc521e4-d1d2-4c20-bf40-e3d72f1068da', 'type': 'tool_call'}]\ntool: content='You have correctly called this tool.' tool_call_id='3fc521e4-d1d2-4c20-bf40-e3d72f1068da'\nhuman: content='this is some text' additional_kwargs={} response_metadata={}\nCreate an extractor\nâ€‹\nLet's select an LLM. Because we are using tool-calling, we will need a model that supports a tool-calling feature. See\nthis table\nfor available LLMs.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nFollowing the\nextraction tutorial\n, we use the\n.with_structured_output\nmethod to structure model outputs according to the desired schema:\nrunnable\n=\nprompt\n|\nllm\n.\nwith_structured_output\n(\nschema\n=\nData\n,\nmethod\n=\n\"function_calling\"\n,\ninclude_raw\n=\nFalse\n,\n)\nWithout examples ðŸ˜¿\nâ€‹\nNotice that even capable models can fail with a\nvery simple\ntest case!\nfor\n_\nin\nrange\n(\n5\n)\n:\ntext\n=\n\"The solar system is large, but earth has only 1 moon.\"\nprint\n(\nrunnable\n.\ninvoke\n(\n{\n\"text\"\n:\ntext\n,\n\"examples\"\n:\n[\n]\n}\n)\n)\npeople=[Person(name='earth', hair_color='null', height_in_meters='null')]\n``````output\npeople=[Person(name='earth', hair_color='null', height_in_meters='null')]\n``````output\npeople=[]\n``````output\npeople=[Person(name='earth', hair_color='null', height_in_meters='null')]\n``````output\npeople=[]\nWith examples ðŸ˜»\nâ€‹\nReference examples helps to fix the failure!\nfor\n_\nin\nrange\n(\n5\n)\n:\ntext\n=\n\"The solar system is large, but earth has only 1 moon.\"\nprint\n(\nrunnable\n.\ninvoke\n(\n{\n\"text\"\n:\ntext\n,\n\"examples\"\n:\nmessages\n}\n)\n)\npeople=[]\n``````output\npeople=[]\n``````output\npeople=[]\n``````output\npeople=[]\n``````output\npeople=[]\nNote that we can see the few-shot examples as tool-calls in the\nLangsmith trace\n.\nAnd we retain performance on a positive sample:\nrunnable\n.\ninvoke\n(\n{\n\"text\"\n:\n\"My name is Harrison. My hair is black.\"\n,\n\"examples\"\n:\nmessages\n,\n}\n)\nData(people=[Person(name='Harrison', hair_color='black', height_in_meters=None)])\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/extraction_long_text/",
    "How-to guides\nHow to handle long text when doing extraction\nOn this page\nHow to handle long text when doing extraction\nWhen working with files, like PDFs, you're likely to encounter text that exceeds your language model's context window. To process this text, consider these strategies:\nChange LLM\nChoose a different LLM that supports a larger context window.\nBrute Force\nChunk the document, and extract content from each chunk.\nRAG\nChunk the document, index the chunks, and only extract content from a subset of chunks that look \"relevant\".\nKeep in mind that these strategies have different trade off and the best strategy likely depends on the application that you're designing!\nThis guide demonstrates how to implement strategies 2 and 3.\nSetup\nâ€‹\nFirst we'll install the dependencies needed for this guide:\n%\npip install\n-\nqU langchain\n-\ncommunity lxml faiss\n-\ncpu langchain\n-\nopenai\nNote: you may need to restart the kernel to use updated packages.\nNow we need some example data! Let's download an article about\ncars from wikipedia\nand load it as a LangChain\nDocument\n.\nimport\nre\nimport\nrequests\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nBSHTMLLoader\n# Download the content\nresponse\n=\nrequests\n.\nget\n(\n\"https://en.wikipedia.org/wiki/Car\"\n)\n# Write it to a file\nwith\nopen\n(\n\"car.html\"\n,\n\"w\"\n,\nencoding\n=\n\"utf-8\"\n)\nas\nf\n:\nf\n.\nwrite\n(\nresponse\n.\ntext\n)\n# Load it with an HTML parser\nloader\n=\nBSHTMLLoader\n(\n\"car.html\"\n)\ndocument\n=\nloader\n.\nload\n(\n)\n[\n0\n]\n# Clean up code\n# Replace consecutive new lines with a single new line\ndocument\n.\npage_content\n=\nre\n.\nsub\n(\n\"\\n\\n+\"\n,\n\"\\n\"\n,\ndocument\n.\npage_content\n)\nprint\n(\nlen\n(\ndocument\n.\npage_content\n)\n)\n78865\nDefine the schema\nâ€‹\nFollowing the\nextraction tutorial\n, we will use Pydantic to define the schema of information we wish to extract. In this case, we will extract a list of \"key developments\" (e.g., important historical events) that include a year and description.\nNote that we also include an\nevidence\nkey and instruct the model to provide in verbatim the relevant sentences of text from the article. This allows us to compare the extraction results to (the model's reconstruction of) text from the original document.\nfrom\ntyping\nimport\nList\n,\nOptional\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nMessagesPlaceholder\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nKeyDevelopment\n(\nBaseModel\n)\n:\n\"\"\"Information about a development in the history of cars.\"\"\"\nyear\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The year when there was an important historic development.\"\n)\ndescription\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"What happened in this year? What was the development?\"\n)\nevidence\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Repeat in verbatim the sentence(s) from which the year and description information were extracted\"\n,\n)\nclass\nExtractionData\n(\nBaseModel\n)\n:\n\"\"\"Extracted information about key developments in the history of cars.\"\"\"\nkey_developments\n:\nList\n[\nKeyDevelopment\n]\n# Define a custom prompt to provide instructions and any additional context.\n# 1) You can add examples into the prompt template to improve extraction quality\n# 2) Introduce additional parameters to take context into account (e.g., include metadata\n#    about the document from which the text was extracted.)\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are an expert at identifying key historic development in text. \"\n\"Only extract important historic developments. Extract nothing if no important information can be found in the text.\"\n,\n)\n,\n(\n\"human\"\n,\n\"{text}\"\n)\n,\n]\n)\nAPI Reference:\nChatPromptTemplate\n|\nMessagesPlaceholder\nCreate an extractor\nâ€‹\nLet's select an LLM. Because we are using tool-calling, we will need a model that supports a tool-calling feature. See\nthis table\nfor available LLMs.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nextractor\n=\nprompt\n|\nllm\n.\nwith_structured_output\n(\nschema\n=\nExtractionData\n,\ninclude_raw\n=\nFalse\n,\n)\nBrute force approach\nâ€‹\nSplit the documents into chunks such that each chunk fits into the context window of the LLMs.\nfrom\nlangchain_text_splitters\nimport\nTokenTextSplitter\ntext_splitter\n=\nTokenTextSplitter\n(\n# Controls the size of each chunk\nchunk_size\n=\n2000\n,\n# Controls overlap between chunks\nchunk_overlap\n=\n20\n,\n)\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\ndocument\n.\npage_content\n)\nUse\nbatch\nfunctionality to run the extraction in\nparallel\nacross each chunk!\ntip\nYou can often use .batch() to parallelize the extractions!\n.batch\nuses a threadpool under the hood to help you parallelize workloads.\nIf your model is exposed via an API, this will likely speed up your extraction flow!\n# Limit just to the first 3 chunks\n# so the code can be re-run quickly\nfirst_few\n=\ntexts\n[\n:\n3\n]\nextractions\n=\nextractor\n.\nbatch\n(\n[\n{\n\"text\"\n:\ntext\n}\nfor\ntext\nin\nfirst_few\n]\n,\n{\n\"max_concurrency\"\n:\n5\n}\n,\n# limit the concurrency by passing max concurrency!\n)\nMerge results\nâ€‹\nAfter extracting data from across the chunks, we'll want to merge the extractions together.\nkey_developments\n=\n[\n]\nfor\nextraction\nin\nextractions\n:\nkey_developments\n.\nextend\n(\nextraction\n.\nkey_developments\n)\nkey_developments\n[\n:\n10\n]\n[KeyDevelopment(year=1769, description='Nicolas-Joseph Cugnot built the first steam-powered road vehicle.', evidence='The French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769, while the Swiss inventor FranÃ§ois Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.'),\nKeyDevelopment(year=1808, description='FranÃ§ois Isaac de Rivaz designed and constructed the first internal combustion-powered automobile.', evidence='The French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769, while the Swiss inventor FranÃ§ois Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.'),\nKeyDevelopment(year=1886, description='Carl Benz invented the modern car, a practical, marketable automobile for everyday use, and patented his Benz Patent-Motorwagen.', evidence='The modern carâ€”a practical, marketable automobile for everyday useâ€”was invented in 1886, when the German inventor Carl Benz patented his Benz Patent-Motorwagen.'),\nKeyDevelopment(year=1901, description='The Oldsmobile Curved Dash became the first mass-produced car.', evidence='The 1901 Oldsmobile Curved Dash and the 1908 Ford Model T, both American cars, are widely considered the first mass-produced[3][4] and mass-affordable[5][6][7] cars, respectively.'),\nKeyDevelopment(year=1908, description='The Ford Model T became the first mass-affordable car.', evidence='The 1901 Oldsmobile Curved Dash and the 1908 Ford Model T, both American cars, are widely considered the first mass-produced[3][4] and mass-affordable[5][6][7] cars, respectively.'),\nKeyDevelopment(year=1885, description='Carl Benz built the original Benz Patent-Motorwagen, the first modern car.', evidence='The original Benz Patent-Motorwagen, the first modern car, built in 1885 and awarded the patent for the concept'),\nKeyDevelopment(year=1881, description='Gustave TrouvÃ© demonstrated a three-wheeled car powered by electricity.', evidence='In November 1881, French inventor Gustave TrouvÃ© demonstrated a three-wheeled car powered by electricity at the International Exposition of Electricity.'),\nKeyDevelopment(year=1888, description=\"Bertha Benz undertook the first road trip by car to prove the road-worthiness of her husband's invention.\", evidence=\"In August 1888, Bertha Benz, the wife and business partner of Carl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.\"),\nKeyDevelopment(year=1896, description='Benz designed and patented the first internal-combustion flat engine, called boxermotor.', evidence='In 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor.'),\nKeyDevelopment(year=1897, description='The first motor car in central Europe and one of the first factory-made cars in the world was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra), the PrÃ¤sident automobil.', evidence='The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the PrÃ¤sident automobil.')]\nRAG based approach\nâ€‹\nAnother simple idea is to chunk up the text, but instead of extracting information from every chunk, just focus on the most relevant chunks.\ncaution\nIt can be difficult to identify which chunks are relevant.\nFor example, in the\ncar\narticle we're using here, most of the article contains key development information. So by using\nRAG\n, we'll likely be throwing out a lot of relevant information.\nWe suggest experimenting with your use case and determining whether this approach works or not.\nTo implement the RAG based approach:\nChunk up your document(s) and index them (e.g., in a vectorstore);\nPrepend the\nextractor\nchain with a retrieval step using the vectorstore.\nHere's a simple example that relies on the\nFAISS\nvectorstore.\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\ndocument\n.\npage_content\n)\nvectorstore\n=\nFAISS\n.\nfrom_texts\n(\ntexts\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n}\n)\n# Only extract from first document\nAPI Reference:\nDocument\n|\nRunnableLambda\nIn this case the RAG extractor is only looking at the top document.\nrag_extractor\n=\n{\n\"text\"\n:\nretriever\n|\n(\nlambda\ndocs\n:\ndocs\n[\n0\n]\n.\npage_content\n)\n# fetch content of top doc\n}\n|\nextractor\nresults\n=\nrag_extractor\n.\ninvoke\n(\n\"Key developments associated with cars\"\n)\nfor\nkey_development\nin\nresults\n.\nkey_developments\n:\nprint\n(\nkey_development\n)\nyear=2006 description='Car-sharing services in the US experienced double-digit growth in revenue and membership.' evidence='in the US, some car-sharing services have experienced double-digit growth in revenue and membership growth between 2006 and 2007.'\nyear=2020 description='56 million cars were manufactured worldwide, with China producing the most.' evidence='In 2020, there were 56 million cars manufactured worldwide, down from 67 million the previous year. The automotive industry in China produces by far the most (20 million in 2020).'\nCommon issues\nâ€‹\nDifferent methods have their own pros and cons related to cost, speed, and accuracy.\nWatch out for these issues:\nChunking content means that the LLM can fail to extract information if the information is spread across multiple chunks.\nLarge chunk overlap may cause the same information to be extracted twice, so be prepared to de-duplicate!\nLLMs can make up data. If looking for a single fact across a large text and using a brute force approach, you may end up getting more made up data.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/extraction_parse/",
    "How-to guides\nHow to use prompting alone (no tool calling) to do extraction\nOn this page\nHow to use prompting alone (no tool calling) to do extraction\nTool calling\nfeatures are not required for generating structured output from LLMs. LLMs that are able to follow prompt instructions well can be tasked with outputting information in a given format.\nThis approach relies on designing good prompts and then parsing the output of the LLMs to make them extract information well.\nTo extract data without tool-calling features:\nInstruct the LLM to generate text following an expected format (e.g., JSON with a certain schema);\nUse\noutput parsers\nto structure the model response into a desired Python object.\nFirst we select a LLM:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\ntip\nThis tutorial is meant to be simple, but generally should really include reference examples to squeeze out performance!\nUsing PydanticOutputParser\nâ€‹\nThe following example uses the built-in\nPydanticOutputParser\nto parse the output of a chat model.\nfrom\ntyping\nimport\nList\n,\nOptional\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nPydanticOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\npydantic\nimport\nBaseModel\n,\nField\n,\nvalidator\nclass\nPerson\n(\nBaseModel\n)\n:\n\"\"\"Information about a person.\"\"\"\nname\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The name of the person\"\n)\nheight_in_meters\n:\nfloat\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The height of the person expressed in meters.\"\n)\nclass\nPeople\n(\nBaseModel\n)\n:\n\"\"\"Identifying information about all people in a text.\"\"\"\npeople\n:\nList\n[\nPerson\n]\n# Set up a parser\nparser\n=\nPydanticOutputParser\n(\npydantic_object\n=\nPeople\n)\n# Prompt\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\"\n,\n)\n,\n(\n\"human\"\n,\n\"{query}\"\n)\n,\n]\n)\n.\npartial\n(\nformat_instructions\n=\nparser\n.\nget_format_instructions\n(\n)\n)\nAPI Reference:\nPydanticOutputParser\n|\nChatPromptTemplate\nLet's take a look at what information is sent to the model\nquery\n=\n\"Anna is 23 years old and she is 6 feet tall\"\nprint\n(\nprompt\n.\nformat_prompt\n(\nquery\n=\nquery\n)\n.\nto_string\n(\n)\n)\nSystem: Answer the user query. Wrap the output in `json` tags\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\nHere is the output schema:\n\\`\\`\\`\n{\"$defs\": {\"Person\": {\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"description\": \"The name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"height_in_meters\": {\"description\": \"The height of the person expressed in meters.\", \"title\": \"Height In Meters\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"], \"title\": \"Person\", \"type\": \"object\"}}, \"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"items\": {\"$ref\": \"#/$defs/Person\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\n\\`\\`\\`\nHuman: Anna is 23 years old and she is 6 feet tall\nHaving defined our prompt, we simply chain together the prompt, model and output parser:\nchain\n=\nprompt\n|\nmodel\n|\nparser\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\nquery\n}\n)\nPeople(people=[Person(name='Anna', height_in_meters=1.8288)])\nCheck out the associated\nLangsmith trace\n.\nNote that the schema shows up in two places:\nIn the prompt, via\nparser.get_format_instructions()\n;\nIn the chain, to receive the formatted output and structure it into a Python object (in this case, the Pydantic object\nPeople\n).\nCustom Parsing\nâ€‹\nIf desired, it's easy to create a custom prompt and parser with\nLangChain\nand\nLCEL\n.\nTo create a custom parser, define a function to parse the output from the model (typically an\nAIMessage\n) into an object of your choice.\nSee below for a simple implementation of a JSON parser.\nimport\njson\nimport\nre\nfrom\ntyping\nimport\nList\n,\nOptional\nfrom\nlangchain_anthropic\n.\nchat_models\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\npydantic\nimport\nBaseModel\n,\nField\n,\nvalidator\nclass\nPerson\n(\nBaseModel\n)\n:\n\"\"\"Information about a person.\"\"\"\nname\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The name of the person\"\n)\nheight_in_meters\n:\nfloat\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The height of the person expressed in meters.\"\n)\nclass\nPeople\n(\nBaseModel\n)\n:\n\"\"\"Identifying information about all people in a text.\"\"\"\npeople\n:\nList\n[\nPerson\n]\n# Prompt\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"Answer the user query. Output your answer as JSON that  \"\n\"matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \"\n\"Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\"\n,\n)\n,\n(\n\"human\"\n,\n\"{query}\"\n)\n,\n]\n)\n.\npartial\n(\nschema\n=\nPeople\n.\nschema\n(\n)\n)\n# Custom parser\ndef\nextract_json\n(\nmessage\n:\nAIMessage\n)\n-\n>\nList\n[\ndict\n]\n:\n\"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.\nParameters:\ntext (str): The text containing the JSON content.\nReturns:\nlist: A list of extracted JSON strings.\n\"\"\"\ntext\n=\nmessage\n.\ncontent\n# Define the regular expression pattern to match JSON blocks\npattern\n=\nr\"\\`\\`\\`json(.*?)\\`\\`\\`\"\n# Find all non-overlapping matches of the pattern in the string\nmatches\n=\nre\n.\nfindall\n(\npattern\n,\ntext\n,\nre\n.\nDOTALL\n)\n# Return the list of matched JSON strings, stripping any leading or trailing whitespace\ntry\n:\nreturn\n[\njson\n.\nloads\n(\nmatch\n.\nstrip\n(\n)\n)\nfor\nmatch\nin\nmatches\n]\nexcept\nException\n:\nraise\nValueError\n(\nf\"Failed to parse:\n{\nmessage\n}\n\"\n)\nAPI Reference:\nAIMessage\n|\nChatPromptTemplate\nquery\n=\n\"Anna is 23 years old and she is 6 feet tall\"\nprint\n(\nprompt\n.\nformat_prompt\n(\nquery\n=\nquery\n)\n.\nto_string\n(\n)\n)\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\`\\`\\`json\n{'$defs': {'Person': {'description': 'Information about a person.', 'properties': {'name': {'description': 'The name of the person', 'title': 'Name', 'type': 'string'}, 'height_in_meters': {'description': 'The height of the person expressed in meters.', 'title': 'Height In Meters', 'type': 'number'}}, 'required': ['name', 'height_in_meters'], 'title': 'Person', 'type': 'object'}}, 'description': 'Identifying information about all people in a text.', 'properties': {'people': {'items': {'$ref': '#/$defs/Person'}, 'title': 'People', 'type': 'array'}}, 'required': ['people'], 'title': 'People', 'type': 'object'}\n\\`\\`\\`. Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\nHuman: Anna is 23 years old and she is 6 feet tall\nchain\n=\nprompt\n|\nmodel\n|\nextract_json\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\nquery\n}\n)\n[{'people': [{'name': 'Anna', 'height_in_meters': 1.83}]}]\nOther Libraries\nâ€‹\nIf you're looking at extracting using a parsing approach, check out the\nKor\nlibrary. It's written by one of the\nLangChain\nmaintainers and it\nhelps to craft a prompt that takes examples into account, allows controlling formats (e.g., JSON or CSV) and expresses the schema in TypeScript. It seems to work pretty!\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/fallbacks/",
    "How-to guides\nHow to add fallbacks to a runnable\nOn this page\nHow to add fallbacks to a runnable\nWhen working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That's why we've introduced the concept of fallbacks.\nA\nfallback\nis an alternative plan that may be used in an emergency.\nCrucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don't just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there.\nFallback for LLM API Errors\nâ€‹\nThis is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things.\nIMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain langchain\n-\nopenai\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_openai\nimport\nChatOpenAI\nFirst, let's mock out what happens if we hit a RateLimitError from OpenAI\nfrom\nunittest\n.\nmock\nimport\npatch\nimport\nhttpx\nfrom\nopenai\nimport\nRateLimitError\nrequest\n=\nhttpx\n.\nRequest\n(\n\"GET\"\n,\n\"/\"\n)\nresponse\n=\nhttpx\n.\nResponse\n(\n200\n,\nrequest\n=\nrequest\n)\nerror\n=\nRateLimitError\n(\n\"rate limit\"\n,\nresponse\n=\nresponse\n,\nbody\n=\n\"\"\n)\n# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc\nopenai_llm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\nmax_retries\n=\n0\n)\nanthropic_llm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\nllm\n=\nopenai_llm\n.\nwith_fallbacks\n(\n[\nanthropic_llm\n]\n)\n# Let's use just the OpenAI LLm first, to show that we run into an error\nwith\npatch\n(\n\"openai.resources.chat.completions.Completions.create\"\n,\nside_effect\n=\nerror\n)\n:\ntry\n:\nprint\n(\nopenai_llm\n.\ninvoke\n(\n\"Why did the chicken cross the road?\"\n)\n)\nexcept\nRateLimitError\n:\nprint\n(\n\"Hit error\"\n)\nHit error\n# Now let's try with fallbacks to Anthropic\nwith\npatch\n(\n\"openai.resources.chat.completions.Completions.create\"\n,\nside_effect\n=\nerror\n)\n:\ntry\n:\nprint\n(\nllm\n.\ninvoke\n(\n\"Why did the chicken cross the road?\"\n)\n)\nexcept\nRateLimitError\n:\nprint\n(\n\"Hit error\"\n)\ncontent=' I don\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\' convention.\\n\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False\nWe can use our \"LLM with Fallbacks\" as we would a normal LLM.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You're a nice assistant who always includes a compliment in your response\"\n,\n)\n,\n(\n\"human\"\n,\n\"Why did the {animal} cross the road\"\n)\n,\n]\n)\nchain\n=\nprompt\n|\nllm\nwith\npatch\n(\n\"openai.resources.chat.completions.Completions.create\"\n,\nside_effect\n=\nerror\n)\n:\ntry\n:\nprint\n(\nchain\n.\ninvoke\n(\n{\n\"animal\"\n:\n\"kangaroo\"\n}\n)\n)\nexcept\nRateLimitError\n:\nprint\n(\n\"Hit error\"\n)\nAPI Reference:\nChatPromptTemplate\ncontent=\" I don't actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\\n\\n- To get to the other side (the classic joke answer!)\\n\\n- It was trying to find some food or water \\n\\n- It was trying to find a mate during mating season\\n\\n- It was fleeing from a predator or perceived threat\\n\\n- It was disoriented and crossed accidentally \\n\\n- It was following a herd of other kangaroos who were crossing\\n\\n- It wanted a change of scenery or environment \\n\\n- It was trying to reach a new habitat or territory\\n\\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher.\" additional_kwargs={} example=False\nFallback for Sequences\nâ€‹\nWe can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt.\n# First let's create a chain with a ChatModel\n# We add in a string output parser here so the outputs between the two are the same type\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nchat_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You're a nice assistant who always includes a compliment in your response\"\n,\n)\n,\n(\n\"human\"\n,\n\"Why did the {animal} cross the road\"\n)\n,\n]\n)\n# Here we're going to use a bad model name to easily create a chain that will error\nchat_model\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-fake\"\n)\nbad_chain\n=\nchat_prompt\n|\nchat_model\n|\nStrOutputParser\n(\n)\nAPI Reference:\nStrOutputParser\n# Now lets create a chain with the normal OpenAI model\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nOpenAI\nprompt_template\n=\n\"\"\"Instructions: You should always include a compliment in your response.\nQuestion: Why did the {animal} cross the road?\"\"\"\nprompt\n=\nPromptTemplate\n.\nfrom_template\n(\nprompt_template\n)\nllm\n=\nOpenAI\n(\n)\ngood_chain\n=\nprompt\n|\nllm\nAPI Reference:\nPromptTemplate\n# We can now create a final chain which combines the two\nchain\n=\nbad_chain\n.\nwith_fallbacks\n(\n[\ngood_chain\n]\n)\nchain\n.\ninvoke\n(\n{\n\"animal\"\n:\n\"turtle\"\n}\n)\n'\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.'\nFallback for Long Inputs\nâ€‹\nOne of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length.\nshort_llm\n=\nChatOpenAI\n(\n)\nlong_llm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-3.5-turbo-16k\"\n)\nllm\n=\nshort_llm\n.\nwith_fallbacks\n(\n[\nlong_llm\n]\n)\ninputs\n=\n\"What is the next number: \"\n+\n\", \"\n.\njoin\n(\n[\n\"one\"\n,\n\"two\"\n]\n*\n3000\n)\ntry\n:\nprint\n(\nshort_llm\n.\ninvoke\n(\ninputs\n)\n)\nexcept\nException\nas\ne\n:\nprint\n(\ne\n)\nThis model's maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages.\ntry\n:\nprint\n(\nllm\n.\ninvoke\n(\ninputs\n)\n)\nexcept\nException\nas\ne\n:\nprint\n(\ne\n)\ncontent='The next number in the sequence is two.' additional_kwargs={} example=False\nFallback to Better Model\nâ€‹\nOften times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4.\nfrom\nlangchain\n.\noutput_parsers\nimport\nDatetimeOutputParser\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\"\n)\n# In this case we are going to do the fallbacks on the LLM + output parser level\n# Because the error will get raised in the OutputParser\nopenai_35\n=\nChatOpenAI\n(\n)\n|\nDatetimeOutputParser\n(\n)\nopenai_4\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4\"\n)\n|\nDatetimeOutputParser\n(\n)\nonly_35\n=\nprompt\n|\nopenai_35\nfallback_4\n=\nprompt\n|\nopenai_35\n.\nwith_fallbacks\n(\n[\nopenai_4\n]\n)\ntry\n:\nprint\n(\nonly_35\n.\ninvoke\n(\n{\n\"event\"\n:\n\"the superbowl in 1994\"\n}\n)\n)\nexcept\nException\nas\ne\n:\nprint\n(\nf\"Error:\n{\ne\n}\n\"\n)\nError: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z\ntry\n:\nprint\n(\nfallback_4\n.\ninvoke\n(\n{\n\"event\"\n:\n\"the superbowl in 1994\"\n}\n)\n)\nexcept\nException\nas\ne\n:\nprint\n(\nf\"Error:\n{\ne\n}\n\"\n)\n1994-01-30 15:30:00\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/filter_messages/",
    "How-to guides\nHow to filter messages\nOn this page\nHow to filter messages\nIn more complex chains and agents we might track state with a list of\nmessages\n. This list can start to accumulate messages from multiple different models, speakers, sub-chains, etc., and we may only want to pass subsets of this full list of messages to each model call in the chain/agent.\nThe\nfilter_messages\nutility makes it easy to filter messages by type, id, or name.\nBasic usage\nâ€‹\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nHumanMessage\n,\nSystemMessage\n,\nfilter_messages\n,\n)\nmessages\n=\n[\nSystemMessage\n(\n\"you are a good assistant\"\n,\nid\n=\n\"1\"\n)\n,\nHumanMessage\n(\n\"example input\"\n,\nid\n=\n\"2\"\n,\nname\n=\n\"example_user\"\n)\n,\nAIMessage\n(\n\"example output\"\n,\nid\n=\n\"3\"\n,\nname\n=\n\"example_assistant\"\n)\n,\nHumanMessage\n(\n\"real input\"\n,\nid\n=\n\"4\"\n,\nname\n=\n\"bob\"\n)\n,\nAIMessage\n(\n\"real output\"\n,\nid\n=\n\"5\"\n,\nname\n=\n\"alice\"\n)\n,\n]\nfilter_messages\n(\nmessages\n,\ninclude_types\n=\n\"human\"\n)\nAPI Reference:\nAIMessage\n|\nHumanMessage\n|\nSystemMessage\n|\nfilter_messages\n[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\nHumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4')]\nfilter_messages\n(\nmessages\n,\nexclude_names\n=\n[\n\"example_user\"\n,\n\"example_assistant\"\n]\n)\n[SystemMessage(content='you are a good assistant', additional_kwargs={}, response_metadata={}, id='1'),\nHumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'),\nAIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]\nfilter_messages\n(\nmessages\n,\ninclude_types\n=\n[\nHumanMessage\n,\nAIMessage\n]\n,\nexclude_ids\n=\n[\n\"3\"\n]\n)\n[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\nHumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'),\nAIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]\nChaining\nâ€‹\nfilter_messages\ncan be used imperatively (like above) or declaratively, making it easy to compose with other components in a chain:\n%\npip install\n-\nqU langchain\n-\nanthropic\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n,\ntemperature\n=\n0\n)\n# Notice we don't pass in messages. This creates\n# a RunnableLambda that takes messages as input\nfilter_\n=\nfilter_messages\n(\nexclude_names\n=\n[\n\"example_user\"\n,\n\"example_assistant\"\n]\n)\nchain\n=\nfilter_\n|\nllm\nchain\n.\ninvoke\n(\nmessages\n)\nAIMessage(content=[], additional_kwargs={}, response_metadata={'id': 'msg_01At8GtCiJ79M29yvNwCiQaB', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 16, 'output_tokens': 3, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--b3db2b91-0edf-4c48-99e7-35e641b8229d-0', usage_metadata={'input_tokens': 16, 'output_tokens': 3, 'total_tokens': 19, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\nLooking at the LangSmith trace we can see that before the messages are passed to the model they are filtered:\nhttps://smith.langchain.com/public/f808a724-e072-438e-9991-657cc9e7e253/r\nLooking at just the filter_, we can see that it's a Runnable object that can be invoked like all Runnables:\nfilter_\n.\ninvoke\n(\nmessages\n)\n[SystemMessage(content='you are a good assistant', additional_kwargs={}, response_metadata={}, id='1'),\nHumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'),\nAIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]\nAPI reference\nâ€‹\nFor a complete description of all arguments head to the API reference:\nhttps://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.filter_messages.html\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/hybrid/",
    "How-to guides\nHybrid Search\nOn this page\nHybrid Search\nThe standard search in LangChain is done by vector similarity. However, a number of\nvector store\nimplementations (Astra DB, ElasticSearch, Neo4J, AzureSearch, Qdrant...) also support more advanced search combining vector similarity search and other search techniques (full-text, BM25, and so on). This is generally referred to as \"Hybrid\" search.\nStep 1: Make sure the vectorstore you are using supports hybrid search\nAt the moment, there is no unified way to perform hybrid search in LangChain. Each vectorstore may have their own way to do it. This is generally exposed as a keyword argument that is passed in during\nsimilarity_search\n.\nBy reading the documentation or source code, figure out whether the vectorstore you are using supports hybrid search, and, if so, how to use it.\nStep 2: Add that parameter as a configurable field for the chain\nThis will let you easily call the chain and configure any relevant flags at runtime. See\nthis documentation\nfor more information on configuration.\nStep 3: Call the chain with that configurable field\nNow, at runtime you can call this chain with configurable field.\nCode Example\nâ€‹\nLet's see a concrete example of what this looks like in code. We will use the Cassandra/CQL interface of Astra DB for this example.\nInstall the following Python package:\n!pip install\n\"cassio>=0.1.7\"\nGet the\nconnection secrets\n.\nInitialize cassio:\nimport\ncassio\ncassio\n.\ninit\n(\ndatabase_id\n=\n\"Your database ID\"\n,\ntoken\n=\n\"Your application token\"\n,\nkeyspace\n=\n\"Your key space\"\n,\n)\nCreate the Cassandra VectorStore with a standard\nindex analyzer\n. The index analyzer is needed to enable term matching.\nfrom\ncassio\n.\ntable\n.\ncql\nimport\nSTANDARD_ANALYZER\nfrom\nlangchain_community\n.\nvectorstores\nimport\nCassandra\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\n)\nvectorstore\n=\nCassandra\n(\nembedding\n=\nembeddings\n,\ntable_name\n=\n\"test_hybrid\"\n,\nbody_index_options\n=\n[\nSTANDARD_ANALYZER\n]\n,\nsession\n=\nNone\n,\nkeyspace\n=\nNone\n,\n)\nvectorstore\n.\nadd_texts\n(\n[\n\"In 2023, I visited Paris\"\n,\n\"In 2022, I visited New York\"\n,\n\"In 2021, I visited New Orleans\"\n,\n]\n)\nIf we do a standard similarity search, we get all the documents:\nvectorstore\n.\nas_retriever\n(\n)\n.\ninvoke\n(\n\"What city did I visit last?\"\n)\n[Document(page_content='In 2022, I visited New York'),\nDocument(page_content='In 2023, I visited Paris'),\nDocument(page_content='In 2021, I visited New Orleans')]\nThe Astra DB vectorstore\nbody_search\nargument can be used to filter the search on the term\nnew\n.\nvectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"body_search\"\n:\n\"new\"\n}\n)\n.\ninvoke\n(\n\"What city did I visit last?\"\n)\n[Document(page_content='In 2022, I visited New York'),\nDocument(page_content='In 2021, I visited New Orleans')]\nWe can now create the chain that we will use to do question-answering over\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\n(\nConfigurableField\n,\nRunnablePassthrough\n,\n)\nfrom\nlangchain_openai\nimport\nChatOpenAI\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nConfigurableField\n|\nRunnablePassthrough\nThis is basic question-answering chain set up.\ntemplate\n=\n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nmodel\n=\nChatOpenAI\n(\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\nHere we mark the retriever as having a configurable field. All vectorstore retrievers have\nsearch_kwargs\nas a field. This is just a dictionary, with vectorstore specific fields\nconfigurable_retriever\n=\nretriever\n.\nconfigurable_fields\n(\nsearch_kwargs\n=\nConfigurableField\n(\nid\n=\n\"search_kwargs\"\n,\nname\n=\n\"Search Kwargs\"\n,\ndescription\n=\n\"The search kwargs to use\"\n,\n)\n)\nWe can now create the chain using our configurable retriever\nchain\n=\n(\n{\n\"context\"\n:\nconfigurable_retriever\n,\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n)\nchain\n.\ninvoke\n(\n\"What city did I visit last?\"\n)\nParis\nWe can now invoke the chain with configurable options.\nsearch_kwargs\nis the id of the configurable field. The value is the search kwargs to use for Astra DB.\nchain\n.\ninvoke\n(\n\"What city did I visit last?\"\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"search_kwargs\"\n:\n{\n\"body_search\"\n:\n\"new\"\n}\n}\n}\n,\n)\nNew York\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/indexing/",
    "How-to guides\nHow to use the LangChain indexing API\nOn this page\nHow to use the LangChain indexing API\nHere, we will look at a basic indexing workflow using the LangChain indexing API.\nThe indexing API lets you load and keep in sync documents from any source into a\nvector store\n. Specifically, it helps:\nAvoid writing duplicated content into the vector store\nAvoid re-writing unchanged content\nAvoid re-computing embeddings over unchanged content\nAll of which should save you time and money, as well as improve your vector search results.\nCrucially, the indexing API will work even with documents that have gone through several\ntransformation steps (e.g., via text chunking) with respect to the original source documents.\nHow it works\nâ€‹\nLangChain indexing makes use of a record manager (\nRecordManager\n) that keeps track of document writes into the vector store.\nWhen indexing content, hashes are computed for each document, and the following information is stored in the record manager:\nthe document hash (hash of both page content and metadata)\nwrite time\nthe source id -- each document should include information in its metadata to allow us to determine the ultimate source of this document\nDeletion modes\nâ€‹\nWhen indexing documents into a vector store, it's possible that some existing documents in the vector store should be deleted. In certain situations you may want to remove any existing documents that are derived from the same sources as the new documents being indexed. In others you may want to delete all existing documents wholesale. The indexing API deletion modes let you pick the behavior you want:\nCleanup Mode\nDe-Duplicates Content\nParallelizable\nCleans Up Deleted Source Docs\nCleans Up Mutations of Source Docs and/or Derived Docs\nClean Up Timing\nNone\nâœ…\nâœ…\nâŒ\nâŒ\n-\nIncremental\nâœ…\nâœ…\nâŒ\nâœ…\nContinuously\nFull\nâœ…\nâŒ\nâœ…\nâœ…\nAt end of indexing\nScoped_Full\nâœ…\nâœ…\nâŒ\nâœ…\nAt end of indexing\nNone\ndoes not do any automatic clean up, allowing the user to manually do clean up of old content.\nincremental\n,\nfull\nand\nscoped_full\noffer the following automated clean up:\nIf the content of the source document or derived documents has\nchanged\n, all 3 modes will clean up (delete) previous versions of the content.\nIf the source document has been\ndeleted\n(meaning it is not included in the documents currently being indexed), the\nfull\ncleanup mode will delete it from the vector store correctly, but the\nincremental\nand\nscoped_full\nmode will not.\nWhen content is mutated (e.g., the source PDF file was revised) there will be a period of time during indexing when both the new and old versions may be returned to the user. This happens after the new content was written, but before the old version was deleted.\nincremental\nindexing minimizes this period of time as it is able to do clean up continuously, as it writes.\nfull\nand\nscoped_full\nmode does the clean up after all batches have been written.\nRequirements\nâ€‹\nDo not use with a store that has been pre-populated with content independently of the indexing API, as the record manager will not know that records have been inserted previously.\nOnly works with LangChain\nvectorstore\n's that support:\ndocument addition by id (\nadd_documents\nmethod with\nids\nargument)\ndelete by id (\ndelete\nmethod with\nids\nargument)\nCompatible Vectorstores:\nAerospike\n,\nAnalyticDB\n,\nAstraDB\n,\nAwaDB\n,\nAzureCosmosDBNoSqlVectorSearch\n,\nAzureCosmosDBVectorSearch\n,\nAzureSearch\n,\nBagel\n,\nCassandra\n,\nChroma\n,\nCouchbaseVectorStore\n,\nDashVector\n,\nDatabricksVectorSearch\n,\nDeepLake\n,\nDingo\n,\nElasticVectorSearch\n,\nElasticsearchStore\n,\nFAISS\n,\nHanaDB\n,\nMilvus\n,\nMongoDBAtlasVectorSearch\n,\nMyScale\n,\nOpenSearchVectorSearch\n,\nPGVector\n,\nPinecone\n,\nQdrant\n,\nRedis\n,\nRockset\n,\nScaNN\n,\nSingleStoreDB\n,\nSupabaseVectorStore\n,\nSurrealDBStore\n,\nTimescaleVector\n,\nVald\n,\nVDMS\n,\nVearch\n,\nVespaStore\n,\nWeaviate\n,\nYellowbrick\n,\nZepVectorStore\n,\nTencentVectorDB\n,\nOpenSearchVectorSearch\n.\nCaution\nâ€‹\nThe record manager relies on a time-based mechanism to determine what content can be cleaned up (when using\nfull\nor\nincremental\nor\nscoped_full\ncleanup modes).\nIf two tasks run back-to-back, and the first task finishes before the clock time changes, then the second task may not be able to clean up content.\nThis is unlikely to be an issue in actual settings for the following reasons:\nThe RecordManager uses higher resolution timestamps.\nThe data would need to change between the first and the second tasks runs, which becomes unlikely if the time interval between the tasks is small.\nIndexing tasks typically take more than a few ms.\nQuickstart\nâ€‹\nfrom\nlangchain\n.\nindexes\nimport\nSQLRecordManager\n,\nindex\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_elasticsearch\nimport\nElasticsearchStore\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nAPI Reference:\nDocument\nInitialize a vector store and set up the embeddings:\ncollection_name\n=\n\"test_index\"\nembedding\n=\nOpenAIEmbeddings\n(\n)\nvectorstore\n=\nElasticsearchStore\n(\nes_url\n=\n\"http://localhost:9200\"\n,\nindex_name\n=\n\"test_index\"\n,\nembedding\n=\nembedding\n)\nInitialize a record manager with an appropriate namespace.\nSuggestion:\nUse a namespace that takes into account both the vector store and the collection name in the vector store; e.g., 'redis/my_docs', 'chromadb/my_docs' or 'postgres/my_docs'.\nnamespace\n=\nf\"elasticsearch/\n{\ncollection_name\n}\n\"\nrecord_manager\n=\nSQLRecordManager\n(\nnamespace\n,\ndb_url\n=\n\"sqlite:///record_manager_cache.sql\"\n)\nCreate a schema before using the record manager.\nrecord_manager\n.\ncreate_schema\n(\n)\nLet's index some test documents:\ndoc1\n=\nDocument\n(\npage_content\n=\n\"kitty\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"kitty.txt\"\n}\n)\ndoc2\n=\nDocument\n(\npage_content\n=\n\"doggy\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"doggy.txt\"\n}\n)\nIndexing into an empty vector store:\ndef\n_clear\n(\n)\n:\n\"\"\"Hacky helper method to clear content. See the `full` mode section to understand why it works.\"\"\"\nindex\n(\n[\n]\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"full\"\n,\nsource_id_key\n=\n\"source\"\n)\nNone\ndeletion mode\nâ€‹\nThis mode does not do automatic clean up of old versions of content; however, it still takes care of content de-duplication.\n_clear\n(\n)\nindex\n(\n[\ndoc1\n,\ndoc1\n,\ndoc1\n,\ndoc1\n,\ndoc1\n]\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\nNone\n,\nsource_id_key\n=\n\"source\"\n,\n)\n{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n_clear\n(\n)\nindex\n(\n[\ndoc1\n,\ndoc2\n]\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\nNone\n,\nsource_id_key\n=\n\"source\"\n)\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\nSecond time around all content will be skipped:\nindex\n(\n[\ndoc1\n,\ndoc2\n]\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\nNone\n,\nsource_id_key\n=\n\"source\"\n)\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n\"incremental\"\ndeletion mode\nâ€‹\n_clear\n(\n)\nindex\n(\n[\ndoc1\n,\ndoc2\n]\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"incremental\"\n,\nsource_id_key\n=\n\"source\"\n,\n)\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\nIndexing again should result in both documents getting\nskipped\n-- also skipping the embedding operation!\nindex\n(\n[\ndoc1\n,\ndoc2\n]\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"incremental\"\n,\nsource_id_key\n=\n\"source\"\n,\n)\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\nIf we provide no documents with incremental indexing mode, nothing will change.\nindex\n(\n[\n]\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"incremental\"\n,\nsource_id_key\n=\n\"source\"\n)\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\nIf we mutate a document, the new version will be written and all old versions sharing the same source will be deleted.\nchanged_doc_2\n=\nDocument\n(\npage_content\n=\n\"puppy\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"doggy.txt\"\n}\n)\nindex\n(\n[\nchanged_doc_2\n]\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"incremental\"\n,\nsource_id_key\n=\n\"source\"\n,\n)\n{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 1}\n\"full\"\ndeletion mode\nâ€‹\nIn\nfull\nmode the user should pass the\nfull\nuniverse of content that should be indexed into the indexing function.\nAny documents that are not passed into the indexing function and are present in the vectorstore will be deleted!\nThis behavior is useful to handle deletions of source documents.\n_clear\n(\n)\nall_docs\n=\n[\ndoc1\n,\ndoc2\n]\nindex\n(\nall_docs\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"full\"\n,\nsource_id_key\n=\n\"source\"\n)\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\nSay someone deleted the first doc:\ndel\nall_docs\n[\n0\n]\nall_docs\n[Document(page_content='doggy', metadata={'source': 'doggy.txt'})]\nUsing full mode will clean up the deleted content as well.\nindex\n(\nall_docs\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"full\"\n,\nsource_id_key\n=\n\"source\"\n)\n{'num_added': 0, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\nSource\nâ€‹\nThe metadata attribute contains a field called\nsource\n. This source should be pointing at the\nultimate\nprovenance associated with the given document.\nFor example, if these documents are representing chunks of some parent document, the\nsource\nfor both documents should be the same and reference the parent document.\nIn general,\nsource\nshould always be specified. Only use a\nNone\n, if you\nnever\nintend to use\nincremental\nmode, and for some reason can't specify the\nsource\nfield correctly.\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\ndoc1\n=\nDocument\n(\npage_content\n=\n\"kitty kitty kitty kitty kitty\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"kitty.txt\"\n}\n)\ndoc2\n=\nDocument\n(\npage_content\n=\n\"doggy doggy the doggy\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"doggy.txt\"\n}\n)\nnew_docs\n=\nCharacterTextSplitter\n(\nseparator\n=\n\"t\"\n,\nkeep_separator\n=\nTrue\n,\nchunk_size\n=\n12\n,\nchunk_overlap\n=\n2\n)\n.\nsplit_documents\n(\n[\ndoc1\n,\ndoc2\n]\n)\nnew_docs\n[Document(page_content='kitty kit', metadata={'source': 'kitty.txt'}),\nDocument(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}),\nDocument(page_content='tty kitty', metadata={'source': 'kitty.txt'}),\nDocument(page_content='doggy doggy', metadata={'source': 'doggy.txt'}),\nDocument(page_content='the doggy', metadata={'source': 'doggy.txt'})]\n_clear\n(\n)\nindex\n(\nnew_docs\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"incremental\"\n,\nsource_id_key\n=\n\"source\"\n,\n)\n{'num_added': 5, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\nchanged_doggy_docs\n=\n[\nDocument\n(\npage_content\n=\n\"woof woof\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"doggy.txt\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"woof woof woof\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"doggy.txt\"\n}\n)\n,\n]\nThis should delete the old versions of documents associated with\ndoggy.txt\nsource and replace them with the new versions.\nindex\n(\nchanged_doggy_docs\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"incremental\"\n,\nsource_id_key\n=\n\"source\"\n,\n)\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 2}\nvectorstore\n.\nsimilarity_search\n(\n\"dog\"\n,\nk\n=\n30\n)\n[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}),\nDocument(page_content='woof woof woof', metadata={'source': 'doggy.txt'}),\nDocument(page_content='tty kitty', metadata={'source': 'kitty.txt'}),\nDocument(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}),\nDocument(page_content='kitty kit', metadata={'source': 'kitty.txt'})]\nUsing with loaders\nâ€‹\nIndexing can accept either an iterable of documents or else any loader.\nAttention:\nThe loader\nmust\nset source keys correctly.\nfrom\nlangchain_core\n.\ndocument_loaders\nimport\nBaseLoader\nclass\nMyCustomLoader\n(\nBaseLoader\n)\n:\ndef\nlazy_load\n(\nself\n)\n:\ntext_splitter\n=\nCharacterTextSplitter\n(\nseparator\n=\n\"t\"\n,\nkeep_separator\n=\nTrue\n,\nchunk_size\n=\n12\n,\nchunk_overlap\n=\n2\n)\ndocs\n=\n[\nDocument\n(\npage_content\n=\n\"woof woof\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"doggy.txt\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"woof woof woof\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"doggy.txt\"\n}\n)\n,\n]\nyield\nfrom\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\ndef\nload\n(\nself\n)\n:\nreturn\nlist\n(\nself\n.\nlazy_load\n(\n)\n)\nAPI Reference:\nBaseLoader\n_clear\n(\n)\nloader\n=\nMyCustomLoader\n(\n)\nloader\n.\nload\n(\n)\n[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}),\nDocument(page_content='woof woof woof', metadata={'source': 'doggy.txt'})]\nindex\n(\nloader\n,\nrecord_manager\n,\nvectorstore\n,\ncleanup\n=\n\"full\"\n,\nsource_id_key\n=\n\"source\"\n)\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\nvectorstore\n.\nsimilarity_search\n(\n\"dog\"\n,\nk\n=\n30\n)\n[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}),\nDocument(page_content='woof woof woof', metadata={'source': 'doggy.txt'})]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/inspect/",
    "How-to guides\nHow to inspect runnables\nOn this page\nHow to inspect runnables\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Expression Language (LCEL)\nChaining runnables\nOnce you create a runnable with\nLangChain Expression Language\n, you may often want to inspect it to get a better sense for what is going on. This notebook covers some methods for doing so.\nThis guide shows some ways you can programmatically introspect the internal steps of chains. If you are instead interested in debugging issues in your chain, see\nthis section\ninstead.\nFirst, let's create an example chain. We will create one that does retrieval:\n%\npip install\n-\nqU langchain langchain\n-\nopenai faiss\n-\ncpu tiktoken\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\n,\nOpenAIEmbeddings\nvectorstore\n=\nFAISS\n.\nfrom_texts\n(\n[\n\"harrison worked at kensho\"\n]\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\ntemplate\n=\n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nmodel\n=\nChatOpenAI\n(\n)\nchain\n=\n(\n{\n\"context\"\n:\nretriever\n,\n\"question\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\nGet a graph\nâ€‹\nYou can use the\nget_graph()\nmethod to get a graph representation of the runnable:\nchain\n.\nget_graph\n(\n)\nPrint a graph\nâ€‹\nWhile that is not super legible, you can use the\nprint_ascii()\nmethod to show that graph in a way that's easier to understand:\nchain\n.\nget_graph\n(\n)\n.\nprint_ascii\n(\n)\n+---------------------------------+\n| Parallel<context,question>Input |\n+---------------------------------+\n**               **\n***                   ***\n**                         **\n+----------------------+              +-------------+\n| VectorStoreRetriever |              | Passthrough |\n+----------------------+              +-------------+\n**               **\n***         ***\n**     **\n+----------------------------------+\n| Parallel<context,question>Output |\n+----------------------------------+\n*\n*\n*\n+--------------------+\n| ChatPromptTemplate |\n+--------------------+\n*\n*\n*\n+------------+\n| ChatOpenAI |\n+------------+\n*\n*\n*\n+-----------------+\n| StrOutputParser |\n+-----------------+\n*\n*\n*\n+-----------------------+\n| StrOutputParserOutput |\n+-----------------------+\nGet the prompts\nâ€‹\nYou may want to see just the prompts that are used in a chain with the\nget_prompts()\nmethod:\nchain\n.\nget_prompts\n(\n)\n[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])]\nNext steps\nâ€‹\nYou've now learned how to introspect your composed LCEL chains.\nNext, check out the other how-to guides on runnables in this section, or the related how-to guide on\ndebugging your chains\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/lcel_cheatsheet/",
    "How-to guides\nLangChain Expression Language Cheatsheet\nOn this page\nLangChain Expression Language Cheatsheet\nThis is a quick reference for all the most important\nLCEL\nprimitives. For more advanced usage see the\nLCEL how-to guides\nand the\nfull API reference\n.\nInvoke a runnable\nâ€‹\nRunnable.invoke()\n/\nRunnable.ainvoke()\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nrunnable\n=\nRunnableLambda\n(\nlambda\nx\n:\nstr\n(\nx\n)\n)\nrunnable\n.\ninvoke\n(\n5\n)\n# Async variant:\n# await runnable.ainvoke(5)\nAPI Reference:\nRunnableLambda\n'5'\nBatch a runnable\nâ€‹\nRunnable.batch()\n/\nRunnable.abatch()\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nrunnable\n=\nRunnableLambda\n(\nlambda\nx\n:\nstr\n(\nx\n)\n)\nrunnable\n.\nbatch\n(\n[\n7\n,\n8\n,\n9\n]\n)\n# Async variant:\n# await runnable.abatch([7, 8, 9])\nAPI Reference:\nRunnableLambda\n['7', '8', '9']\nStream a runnable\nâ€‹\nRunnable.stream()\n/\nRunnable.astream()\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\ndef\nfunc\n(\nx\n)\n:\nfor\ny\nin\nx\n:\nyield\nstr\n(\ny\n)\nrunnable\n=\nRunnableLambda\n(\nfunc\n)\nfor\nchunk\nin\nrunnable\n.\nstream\n(\nrange\n(\n5\n)\n)\n:\nprint\n(\nchunk\n)\n# Async variant:\n# async for chunk in await runnable.astream(range(5)):\n#     print(chunk)\nAPI Reference:\nRunnableLambda\n0\n1\n2\n3\n4\nCompose runnables\nâ€‹\nPipe operator\n|\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\n{\n\"foo\"\n:\nx\n}\n)\nrunnable2\n=\nRunnableLambda\n(\nlambda\nx\n:\n[\nx\n]\n*\n2\n)\nchain\n=\nrunnable1\n|\nrunnable2\nchain\n.\ninvoke\n(\n2\n)\nAPI Reference:\nRunnableLambda\n[{'foo': 2}, {'foo': 2}]\nInvoke runnables in parallel\nâ€‹\nRunnableParallel\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnableParallel\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\n{\n\"foo\"\n:\nx\n}\n)\nrunnable2\n=\nRunnableLambda\n(\nlambda\nx\n:\n[\nx\n]\n*\n2\n)\nchain\n=\nRunnableParallel\n(\nfirst\n=\nrunnable1\n,\nsecond\n=\nrunnable2\n)\nchain\n.\ninvoke\n(\n2\n)\nAPI Reference:\nRunnableLambda\n|\nRunnableParallel\n{'first': {'foo': 2}, 'second': [2, 2]}\nTurn any function into a runnable\nâ€‹\nRunnableLambda\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\ndef\nfunc\n(\nx\n)\n:\nreturn\nx\n+\n5\nrunnable\n=\nRunnableLambda\n(\nfunc\n)\nrunnable\n.\ninvoke\n(\n2\n)\nAPI Reference:\nRunnableLambda\n7\nMerge input and output dicts\nâ€‹\nRunnablePassthrough.assign\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnablePassthrough\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\nx\n[\n\"foo\"\n]\n+\n7\n)\nchain\n=\nRunnablePassthrough\n.\nassign\n(\nbar\n=\nrunnable1\n)\nchain\n.\ninvoke\n(\n{\n\"foo\"\n:\n10\n}\n)\nAPI Reference:\nRunnableLambda\n|\nRunnablePassthrough\n{'foo': 10, 'bar': 17}\nInclude input dict in output dict\nâ€‹\nRunnablePassthrough\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\n(\nRunnableLambda\n,\nRunnableParallel\n,\nRunnablePassthrough\n,\n)\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\nx\n[\n\"foo\"\n]\n+\n7\n)\nchain\n=\nRunnableParallel\n(\nbar\n=\nrunnable1\n,\nbaz\n=\nRunnablePassthrough\n(\n)\n)\nchain\n.\ninvoke\n(\n{\n\"foo\"\n:\n10\n}\n)\nAPI Reference:\nRunnableLambda\n|\nRunnableParallel\n|\nRunnablePassthrough\n{'bar': 17, 'baz': {'foo': 10}}\nAdd default invocation args\nâ€‹\nRunnable.bind\nâ€‹\nfrom\ntyping\nimport\nOptional\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\ndef\nfunc\n(\nmain_arg\n:\ndict\n,\nother_arg\n:\nOptional\n[\nstr\n]\n=\nNone\n)\n-\n>\ndict\n:\nif\nother_arg\n:\nreturn\n{\n**\nmain_arg\n,\n**\n{\n\"foo\"\n:\nother_arg\n}\n}\nreturn\nmain_arg\nrunnable1\n=\nRunnableLambda\n(\nfunc\n)\nbound_runnable1\n=\nrunnable1\n.\nbind\n(\nother_arg\n=\n\"bye\"\n)\nbound_runnable1\n.\ninvoke\n(\n{\n\"bar\"\n:\n\"hello\"\n}\n)\nAPI Reference:\nRunnableLambda\n{'bar': 'hello', 'foo': 'bye'}\nAdd fallbacks\nâ€‹\nRunnable.with_fallbacks\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\nx\n+\n\"foo\"\n)\nrunnable2\n=\nRunnableLambda\n(\nlambda\nx\n:\nstr\n(\nx\n)\n+\n\"foo\"\n)\nchain\n=\nrunnable1\n.\nwith_fallbacks\n(\n[\nrunnable2\n]\n)\nchain\n.\ninvoke\n(\n5\n)\nAPI Reference:\nRunnableLambda\n'5foo'\nAdd retries\nâ€‹\nRunnable.with_retry\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\ncounter\n=\n-\n1\ndef\nfunc\n(\nx\n)\n:\nglobal\ncounter\ncounter\n+=\n1\nprint\n(\nf\"attempt with\n{\ncounter\n=\n}\n\"\n)\nreturn\nx\n/\ncounter\nchain\n=\nRunnableLambda\n(\nfunc\n)\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n)\nchain\n.\ninvoke\n(\n2\n)\nAPI Reference:\nRunnableLambda\nattempt with counter=0\nattempt with counter=1\n2.0\nConfigure runnable execution\nâ€‹\nRunnableConfig\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnableParallel\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\n{\n\"foo\"\n:\nx\n}\n)\nrunnable2\n=\nRunnableLambda\n(\nlambda\nx\n:\n[\nx\n]\n*\n2\n)\nrunnable3\n=\nRunnableLambda\n(\nlambda\nx\n:\nstr\n(\nx\n)\n)\nchain\n=\nRunnableParallel\n(\nfirst\n=\nrunnable1\n,\nsecond\n=\nrunnable2\n,\nthird\n=\nrunnable3\n)\nchain\n.\ninvoke\n(\n7\n,\nconfig\n=\n{\n\"max_concurrency\"\n:\n2\n}\n)\nAPI Reference:\nRunnableLambda\n|\nRunnableParallel\n{'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}\nAdd default config to runnable\nâ€‹\nRunnable.with_config\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnableParallel\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\n{\n\"foo\"\n:\nx\n}\n)\nrunnable2\n=\nRunnableLambda\n(\nlambda\nx\n:\n[\nx\n]\n*\n2\n)\nrunnable3\n=\nRunnableLambda\n(\nlambda\nx\n:\nstr\n(\nx\n)\n)\nchain\n=\nRunnableParallel\n(\nfirst\n=\nrunnable1\n,\nsecond\n=\nrunnable2\n,\nthird\n=\nrunnable3\n)\nconfigured_chain\n=\nchain\n.\nwith_config\n(\nmax_concurrency\n=\n2\n)\nchain\n.\ninvoke\n(\n7\n)\nAPI Reference:\nRunnableLambda\n|\nRunnableParallel\n{'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}\nMake runnable attributes configurable\nâ€‹\nRunnable.with_configurable_fields\nâ€‹\nfrom\ntyping\nimport\nAny\n,\nOptional\nfrom\nlangchain_core\n.\nrunnables\nimport\n(\nConfigurableField\n,\nRunnableConfig\n,\nRunnableSerializable\n,\n)\nclass\nFooRunnable\n(\nRunnableSerializable\n[\ndict\n,\ndict\n]\n)\n:\noutput_key\n:\nstr\ndef\ninvoke\n(\nself\n,\ninput\n:\nAny\n,\nconfig\n:\nOptional\n[\nRunnableConfig\n]\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n-\n>\nlist\n:\nreturn\nself\n.\n_call_with_config\n(\nself\n.\nsubtract_seven\n,\ninput\n,\nconfig\n,\n**\nkwargs\n)\ndef\nsubtract_seven\n(\nself\n,\ninput\n:\ndict\n)\n-\n>\ndict\n:\nreturn\n{\nself\n.\noutput_key\n:\ninput\n[\n\"foo\"\n]\n-\n7\n}\nrunnable1\n=\nFooRunnable\n(\noutput_key\n=\n\"bar\"\n)\nconfigurable_runnable1\n=\nrunnable1\n.\nconfigurable_fields\n(\noutput_key\n=\nConfigurableField\n(\nid\n=\n\"output_key\"\n)\n)\nconfigurable_runnable1\n.\ninvoke\n(\n{\n\"foo\"\n:\n10\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"output_key\"\n:\n\"not bar\"\n}\n}\n)\nAPI Reference:\nConfigurableField\n|\nRunnableConfig\n|\nRunnableSerializable\n{'not bar': 3}\nconfigurable_runnable1\n.\ninvoke\n(\n{\n\"foo\"\n:\n10\n}\n)\n{'bar': 3}\nMake chain components configurable\nâ€‹\nRunnable.with_configurable_alternatives\nâ€‹\nfrom\ntyping\nimport\nAny\n,\nOptional\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\n,\nRunnableLambda\n,\nRunnableParallel\nclass\nListRunnable\n(\nRunnableSerializable\n[\nAny\n,\nlist\n]\n)\n:\ndef\ninvoke\n(\nself\n,\ninput\n:\nAny\n,\nconfig\n:\nOptional\n[\nRunnableConfig\n]\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n-\n>\nlist\n:\nreturn\nself\n.\n_call_with_config\n(\nself\n.\nlistify\n,\ninput\n,\nconfig\n,\n**\nkwargs\n)\ndef\nlistify\n(\nself\n,\ninput\n:\nAny\n)\n-\n>\nlist\n:\nreturn\n[\ninput\n]\nclass\nStrRunnable\n(\nRunnableSerializable\n[\nAny\n,\nstr\n]\n)\n:\ndef\ninvoke\n(\nself\n,\ninput\n:\nAny\n,\nconfig\n:\nOptional\n[\nRunnableConfig\n]\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n-\n>\nlist\n:\nreturn\nself\n.\n_call_with_config\n(\nself\n.\nstrify\n,\ninput\n,\nconfig\n,\n**\nkwargs\n)\ndef\nstrify\n(\nself\n,\ninput\n:\nAny\n)\n-\n>\nstr\n:\nreturn\nstr\n(\ninput\n)\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\n{\n\"foo\"\n:\nx\n}\n)\nconfigurable_runnable\n=\nListRunnable\n(\n)\n.\nconfigurable_alternatives\n(\nConfigurableField\n(\nid\n=\n\"second_step\"\n)\n,\ndefault_key\n=\n\"list\"\n,\nstring\n=\nStrRunnable\n(\n)\n)\nchain\n=\nrunnable1\n|\nconfigurable_runnable\nchain\n.\ninvoke\n(\n7\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"second_step\"\n:\n\"string\"\n}\n}\n)\nAPI Reference:\nRunnableConfig\n|\nRunnableLambda\n|\nRunnableParallel\n\"{'foo': 7}\"\nchain\n.\ninvoke\n(\n7\n)\n[{'foo': 7}]\nBuild a chain dynamically based on input\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnableParallel\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\n{\n\"foo\"\n:\nx\n}\n)\nrunnable2\n=\nRunnableLambda\n(\nlambda\nx\n:\n[\nx\n]\n*\n2\n)\nchain\n=\nRunnableLambda\n(\nlambda\nx\n:\nrunnable1\nif\nx\n>\n6\nelse\nrunnable2\n)\nchain\n.\ninvoke\n(\n7\n)\nAPI Reference:\nRunnableLambda\n|\nRunnableParallel\n{'foo': 7}\nchain\n.\ninvoke\n(\n5\n)\n[5, 5]\nGenerate a stream of events\nâ€‹\nRunnable.astream_events\nâ€‹\n# | echo: false\nimport\nnest_asyncio\nnest_asyncio\n.\napply\n(\n)\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnableParallel\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\n{\n\"foo\"\n:\nx\n}\n,\nname\n=\n\"first\"\n)\nasync\ndef\nfunc\n(\nx\n)\n:\nfor\n_\nin\nrange\n(\n5\n)\n:\nyield\nx\nrunnable2\n=\nRunnableLambda\n(\nfunc\n,\nname\n=\n\"second\"\n)\nchain\n=\nrunnable1\n|\nrunnable2\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"bar\"\n,\nversion\n=\n\"v2\"\n)\n:\nprint\n(\nf\"event=\n{\nevent\n[\n'event'\n]\n}\n| name=\n{\nevent\n[\n'name'\n]\n}\n| data=\n{\nevent\n[\n'data'\n]\n}\n\"\n)\nAPI Reference:\nRunnableLambda\n|\nRunnableParallel\nevent=on_chain_start | name=RunnableSequence | data={'input': 'bar'}\nevent=on_chain_start | name=first | data={}\nevent=on_chain_stream | name=first | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_start | name=second | data={}\nevent=on_chain_end | name=first | data={'output': {'foo': 'bar'}, 'input': 'bar'}\nevent=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\nevent=on_chain_end | name=second | data={'output': {'foo': 'bar'}, 'input': {'foo': 'bar'}}\nevent=on_chain_end | name=RunnableSequence | data={'output': {'foo': 'bar'}}\nYield batched outputs as they complete\nâ€‹\nRunnable.batch_as_completed\n/\nRunnable.abatch_as_completed\nâ€‹\nimport\ntime\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnableParallel\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\ntime\n.\nsleep\n(\nx\n)\nor\nprint\n(\nf\"slept\n{\nx\n}\n\"\n)\n)\nfor\nidx\n,\nresult\nin\nrunnable1\n.\nbatch_as_completed\n(\n[\n5\n,\n1\n]\n)\n:\nprint\n(\nidx\n,\nresult\n)\n# Async variant:\n# async for idx, result in runnable1.abatch_as_completed([5, 1]):\n#     print(idx, result)\nAPI Reference:\nRunnableLambda\n|\nRunnableParallel\nslept 1\n1 None\nslept 5\n0 None\nReturn subset of output dict\nâ€‹\nRunnable.pick\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnablePassthrough\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\nx\n[\n\"baz\"\n]\n+\n5\n)\nchain\n=\nRunnablePassthrough\n.\nassign\n(\nfoo\n=\nrunnable1\n)\n.\npick\n(\n[\n\"foo\"\n,\n\"bar\"\n]\n)\nchain\n.\ninvoke\n(\n{\n\"bar\"\n:\n\"hi\"\n,\n\"baz\"\n:\n2\n}\n)\nAPI Reference:\nRunnableLambda\n|\nRunnablePassthrough\n{'foo': 7, 'bar': 'hi'}\nDeclaratively make a batched version of a runnable\nâ€‹\nRunnable.map\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\nlist\n(\nrange\n(\nx\n)\n)\n)\nrunnable2\n=\nRunnableLambda\n(\nlambda\nx\n:\nx\n+\n5\n)\nchain\n=\nrunnable1\n|\nrunnable2\n.\nmap\n(\n)\nchain\n.\ninvoke\n(\n3\n)\nAPI Reference:\nRunnableLambda\n[5, 6, 7]\nGet a graph representation of a runnable\nâ€‹\nRunnable.get_graph\nâ€‹\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnableParallel\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\n{\n\"foo\"\n:\nx\n}\n)\nrunnable2\n=\nRunnableLambda\n(\nlambda\nx\n:\n[\nx\n]\n*\n2\n)\nrunnable3\n=\nRunnableLambda\n(\nlambda\nx\n:\nstr\n(\nx\n)\n)\nchain\n=\nrunnable1\n|\nRunnableParallel\n(\nsecond\n=\nrunnable2\n,\nthird\n=\nrunnable3\n)\nchain\n.\nget_graph\n(\n)\n.\nprint_ascii\n(\n)\nAPI Reference:\nRunnableLambda\n|\nRunnableParallel\n+-------------+\n| LambdaInput |\n+-------------+\n*\n*\n*\n+------------------------------+\n| Lambda(lambda x: {'foo': x}) |\n+------------------------------+\n*\n*\n*\n+-----------------------------+\n| Parallel<second,third>Input |\n+-----------------------------+\n****                  ***\n****                         ****\n**                                 **\n+---------------------------+               +--------------------------+\n| Lambda(lambda x: [x] * 2) |               | Lambda(lambda x: str(x)) |\n+---------------------------+               +--------------------------+\n****                  ***\n****          ****\n**      **\n+------------------------------+\n| Parallel<second,third>Output |\n+------------------------------+\nGet all prompts in a chain\nâ€‹\nRunnable.get_prompts\nâ€‹\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nprompt1\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"good ai\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n]\n)\nprompt2\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"really good ai\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n(\n\"ai\"\n,\n\"{ai_output}\"\n)\n,\n(\n\"human\"\n,\n\"{input2}\"\n)\n,\n]\n)\nfake_llm\n=\nRunnableLambda\n(\nlambda\nprompt\n:\n\"i am good ai\"\n)\nchain\n=\nprompt1\n.\nassign\n(\nai_output\n=\nfake_llm\n)\n|\nprompt2\n|\nfake_llm\nfor\ni\n,\nprompt\nin\nenumerate\n(\nchain\n.\nget_prompts\n(\n)\n)\n:\nprint\n(\nf\"**prompt\n{\ni\n=\n}\n**\\n\"\n)\nprint\n(\nprompt\n.\npretty_repr\n(\n)\n)\nprint\n(\n\"\\n\"\n*\n3\n)\nAPI Reference:\nChatPromptTemplate\n|\nRunnableLambda\n**prompt i=0**\n================================ System Message ================================\ngood ai\n================================ Human Message =================================\n{input}\n**prompt i=1**\n================================ System Message ================================\nreally good ai\n================================ Human Message =================================\n{input}\n================================== AI Message ==================================\n{ai_output}\n================================ Human Message =================================\n{input2}\nAdd lifecycle listeners\nâ€‹\nRunnable.with_listeners\nâ€‹\nimport\ntime\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\nlangchain_core\n.\ntracers\n.\nschemas\nimport\nRun\ndef\non_start\n(\nrun_obj\n:\nRun\n)\n:\nprint\n(\n\"start_time:\"\n,\nrun_obj\n.\nstart_time\n)\ndef\non_end\n(\nrun_obj\n:\nRun\n)\n:\nprint\n(\n\"end_time:\"\n,\nrun_obj\n.\nend_time\n)\nrunnable1\n=\nRunnableLambda\n(\nlambda\nx\n:\ntime\n.\nsleep\n(\nx\n)\n)\nchain\n=\nrunnable1\n.\nwith_listeners\n(\non_start\n=\non_start\n,\non_end\n=\non_end\n)\nchain\n.\ninvoke\n(\n2\n)\nAPI Reference:\nRunnableLambda\n|\nRun\nstart_time: 2024-05-17 23:04:00.951065+00:00\nend_time: 2024-05-17 23:04:02.958765+00:00\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/llm_caching/",
    "How-to guides\nHow to cache LLM responses\nOn this page\nHow to cache LLM responses\nLangChain provides an optional\ncaching\nlayer for LLMs. This is useful for two reasons:\nIt can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\nIt can speed up your application by reducing the number of API calls you make to the LLM provider.\n%\npip install\n-\nqU langchain_openai langchain_community\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\n# Please manually enter OpenAI Key\nfrom\nlangchain_core\n.\nglobals\nimport\nset_llm_cache\nfrom\nlangchain_openai\nimport\nOpenAI\n# To make the caching really obvious, let's use a slower and older model.\n# Caching supports newer chat models as well.\nllm\n=\nOpenAI\n(\nmodel\n=\n\"gpt-3.5-turbo-instruct\"\n,\nn\n=\n2\n,\nbest_of\n=\n2\n)\nAPI Reference:\nset_llm_cache\n%\n%\ntime\nfrom\nlangchain_core\n.\ncaches\nimport\nInMemoryCache\nset_llm_cache\n(\nInMemoryCache\n(\n)\n)\n# The first time, it is not yet in cache, so it should take longer\nllm\n.\ninvoke\n(\n\"Tell me a joke\"\n)\nAPI Reference:\nInMemoryCache\nCPU times: user 546 ms, sys: 379 ms, total: 925 ms\nWall time: 1.11 s\n\"\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"\n%\n%\ntime\n# The second time it is, so it goes faster\nllm\n.\ninvoke\n(\n\"Tell me a joke\"\n)\nCPU times: user 192 Âµs, sys: 77 Âµs, total: 269 Âµs\nWall time: 270 Âµs\n\"\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"\nSQLite Cache\nâ€‹\n!rm\n.\nlangchain\n.\ndb\n# We can do the same thing with a SQLite cache\nfrom\nlangchain_community\n.\ncache\nimport\nSQLiteCache\nset_llm_cache\n(\nSQLiteCache\n(\ndatabase_path\n=\n\".langchain.db\"\n)\n)\n%\n%\ntime\n# The first time, it is not yet in cache, so it should take longer\nllm\n.\ninvoke\n(\n\"Tell me a joke\"\n)\nCPU times: user 10.6 ms, sys: 4.21 ms, total: 14.8 ms\nWall time: 851 ms\n\"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"\n%\n%\ntime\n# The second time it is, so it goes faster\nllm\n.\ninvoke\n(\n\"Tell me a joke\"\n)\nCPU times: user 59.7 ms, sys: 63.6 ms, total: 123 ms\nWall time: 134 ms\n\"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/llm_token_usage_tracking/",
    "How-to guides\nHow to track token usage for LLMs\nOn this page\nHow to track token usage for LLMs\nTracking\ntoken\nusage to calculate cost is an important part of putting your app in production. This guide goes over how to obtain this information from your LangChain model calls.\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLLMs\nUsing LangSmith\nâ€‹\nYou can use\nLangSmith\nto help track token usage in your LLM application. See the\nLangSmith quick start guide\n.\nUsing callbacks\nâ€‹\nThere are some API-specific callback context managers that allow you to track token usage across multiple calls. You'll need to check whether such an integration is available for your particular model.\nIf such an integration is not available for your model, you can create a custom callback manager by adapting the implementation of the\nOpenAI callback manager\n.\nOpenAI\nâ€‹\nLet's first look at an extremely simple example of tracking token usage for a single Chat model call.\ndanger\nThe callback handler does not currently support streaming token counts for legacy language models (e.g.,\nlangchain_openai.OpenAI\n). For support in a streaming context, refer to the corresponding guide for chat models\nhere\n.\nSingle call\nâ€‹\nfrom\nlangchain_community\n.\ncallbacks\nimport\nget_openai_callback\nfrom\nlangchain_openai\nimport\nOpenAI\nllm\n=\nOpenAI\n(\nmodel_name\n=\n\"gpt-3.5-turbo-instruct\"\n)\nwith\nget_openai_callback\n(\n)\nas\ncb\n:\nresult\n=\nllm\n.\ninvoke\n(\n\"Tell me a joke\"\n)\nprint\n(\nresult\n)\nprint\n(\n\"---\"\n)\nprint\n(\n)\nprint\n(\nf\"Total Tokens:\n{\ncb\n.\ntotal_tokens\n}\n\"\n)\nprint\n(\nf\"Prompt Tokens:\n{\ncb\n.\nprompt_tokens\n}\n\"\n)\nprint\n(\nf\"Completion Tokens:\n{\ncb\n.\ncompletion_tokens\n}\n\"\n)\nprint\n(\nf\"Total Cost (USD): $\n{\ncb\n.\ntotal_cost\n}\n\"\n)\nWhy don't scientists trust atoms?\nBecause they make up everything.\n---\nTotal Tokens: 18\nPrompt Tokens: 4\nCompletion Tokens: 14\nTotal Cost (USD): $3.4e-05\nMultiple calls\nâ€‹\nAnything inside the context manager will get tracked. Here's an example of using it to track multiple calls in sequence to a chain. This will also work for an agent which may use multiple steps.\nfrom\nlangchain_community\n.\ncallbacks\nimport\nget_openai_callback\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nOpenAI\nllm\n=\nOpenAI\n(\nmodel_name\n=\n\"gpt-3.5-turbo-instruct\"\n)\ntemplate\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Tell me a joke about {topic}\"\n)\nchain\n=\ntemplate\n|\nllm\nwith\nget_openai_callback\n(\n)\nas\ncb\n:\nresponse\n=\nchain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"birds\"\n}\n)\nprint\n(\nresponse\n)\nresponse\n=\nchain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"fish\"\n}\n)\nprint\n(\n\"--\"\n)\nprint\n(\nresponse\n)\nprint\n(\n)\nprint\n(\n\"---\"\n)\nprint\n(\nf\"Total Tokens:\n{\ncb\n.\ntotal_tokens\n}\n\"\n)\nprint\n(\nf\"Prompt Tokens:\n{\ncb\n.\nprompt_tokens\n}\n\"\n)\nprint\n(\nf\"Completion Tokens:\n{\ncb\n.\ncompletion_tokens\n}\n\"\n)\nprint\n(\nf\"Total Cost (USD): $\n{\ncb\n.\ntotal_cost\n}\n\"\n)\nAPI Reference:\nPromptTemplate\nWhy did the chicken go to the seance?\nTo talk to the other side of the road!\n--\nWhy did the fish need a lawyer?\nBecause it got caught in a net!\n---\nTotal Tokens: 50\nPrompt Tokens: 12\nCompletion Tokens: 38\nTotal Cost (USD): $9.400000000000001e-05\nStreaming\nâ€‹\ndanger\nget_openai_callback\ndoes not currently support streaming token counts for legacy language models (e.g.,\nlangchain_openai.OpenAI\n). If you want to count tokens correctly in a streaming context, there are a number of options:\nUse chat models as described in\nthis guide\n;\nImplement a\ncustom callback handler\nthat uses appropriate tokenizers to count the tokens;\nUse a monitoring platform such as\nLangSmith\n.\nNote that when using legacy language models in a streaming context, token counts are not updated:\nfrom\nlangchain_community\n.\ncallbacks\nimport\nget_openai_callback\nfrom\nlangchain_openai\nimport\nOpenAI\nllm\n=\nOpenAI\n(\nmodel_name\n=\n\"gpt-3.5-turbo-instruct\"\n)\nwith\nget_openai_callback\n(\n)\nas\ncb\n:\nfor\nchunk\nin\nllm\n.\nstream\n(\n\"Tell me a joke\"\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"\"\n,\nflush\n=\nTrue\n)\nprint\n(\nresult\n)\nprint\n(\n\"---\"\n)\nprint\n(\n)\nprint\n(\nf\"Total Tokens:\n{\ncb\n.\ntotal_tokens\n}\n\"\n)\nprint\n(\nf\"Prompt Tokens:\n{\ncb\n.\nprompt_tokens\n}\n\"\n)\nprint\n(\nf\"Completion Tokens:\n{\ncb\n.\ncompletion_tokens\n}\n\"\n)\nprint\n(\nf\"Total Cost (USD): $\n{\ncb\n.\ntotal_cost\n}\n\"\n)\nWhy don't scientists trust atoms?\nBecause they make up everything!\nWhy don't scientists trust atoms?\nBecause they make up everything.\n---\nTotal Tokens: 0\nPrompt Tokens: 0\nCompletion Tokens: 0\nTotal Cost (USD): $0.0\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/local_llms/",
    "How-to guides\nRun models locally\nOn this page\nRun models locally\nUse case\nâ€‹\nThe popularity of projects like\nllama.cpp\n,\nOllama\n,\nGPT4All\n,\nllamafile\n, and others underscore the demand to run LLMs locally (on your own device).\nThis has at least two important benefits:\nPrivacy\n: Your data is not sent to a third party, and it is not subject to the terms of service of a commercial service\nCost\n: There is no inference fee, which is important for token-intensive applications (e.g.,\nlong-running simulations\n, summarization)\nOverview\nâ€‹\nRunning an LLM locally requires a few things:\nOpen-source LLM\n: An open-source LLM that can be freely modified and shared\nInference\n: Ability to run this LLM on your device w/ acceptable latency\nOpen-source LLMs\nâ€‹\nUsers can now gain access to a rapidly growing set of\nopen-source LLMs\n.\nThese LLMs can be assessed across at least two dimensions (see figure):\nBase model\n: What is the base-model and how was it trained?\nFine-tuning approach\n: Was the base-model fine-tuned and, if so, what\nset of instructions\nwas used?\nThe relative performance of these models can be assessed using several leaderboards, including:\nLmSys\nGPT4All\nHuggingFace\nInference\nâ€‹\nA few frameworks for this have emerged to support inference of open-source LLMs on various devices:\nllama.cpp\n: C++ implementation of llama inference code with\nweight optimization / quantization\ngpt4all\n: Optimized C backend for inference\nollama\n: Bundles model weights and environment into an app that runs on device and serves the LLM\nllamafile\n: Bundles model weights and everything needed to run the model in a single file, allowing you to run the LLM locally from this file without any additional installation steps\nIn general, these frameworks will do a few things:\nQuantization\n: Reduce the memory footprint of the raw model weights\nEfficient implementation for inference\n: Support inference on consumer hardware (e.g., CPU or laptop GPU)\nIn particular, see\nthis excellent post\non the importance of quantization.\nWith less precision, we radically decrease the memory needed to store the LLM in memory.\nIn addition, we can see the importance of GPU memory bandwidth\nsheet\n!\nA Mac M2 Max is 5-6x faster than a M1 for inference due to the larger GPU memory bandwidth.\nFormatting prompts\nâ€‹\nSome providers have\nchat model\nwrappers that takes care of formatting your input prompt for the specific local model you're using. However, if you are prompting local models with a\ntext-in/text-out LLM\nwrapper, you may need to use a prompt tailored for your specific model.\nThis can\nrequire the inclusion of special tokens\n.\nHere's an example for LLaMA 2\n.\nQuickstart\nâ€‹\nOllama\nis one way to easily run inference on macOS.\nThe instructions\nhere\nprovide details, which we summarize:\nDownload and run\nthe app\nFrom command line, fetch a model from this\nlist of options\n: e.g.,\nollama pull gpt-oss:20b\nWhen the app is running, all models are automatically served on\nlocalhost:11434\n%\npip install\n-\nqU langchain_ollama\nfrom\nlangchain_ollama\nimport\nChatOllama\nllm\n=\nChatOllama\n(\nmodel\n=\n\"gpt-oss:20b\"\n,\nvalidate_model_on_init\n=\nTrue\n)\nllm\n.\ninvoke\n(\n\"The first man on the moon was ...\"\n)\n.\ncontent\n'...Neil Armstrong!\\n\\nOn July 20, 1969, Neil Armstrong became the first person to set foot on the lunar surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind\" as he stepped off the lunar module Eagle onto the Moon\\'s surface.\\n\\nWould you like to know more about the Apollo 11 mission or Neil Armstrong\\'s achievements?'\nStream tokens as they are being generated:\nfor\nchunk\nin\nllm\n.\nstream\n(\n\"The first man on the moon was ...\"\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n...|\n``````output\nNeil| Armstrong|,| an| American| astronaut|.| He| stepped| out| of| the| lunar| module| Eagle| and| onto| the| surface| of| the| Moon| on| July| |20|,| |196|9|,| famously| declaring|:| \"|That|'s| one| small| step| for| man|,| one| giant| leap| for| mankind|.\"||\nOllama also includes a chat model wrapper that handles formatting conversation turns:\nfrom\nlangchain_ollama\nimport\nChatOllama\nchat_model\n=\nChatOllama\n(\nmodel\n=\n\"llama3.1:8b\"\n)\nchat_model\n.\ninvoke\n(\n\"Who was the first man on the moon?\"\n)\nAIMessage(content='The answer is a historic one!\\n\\nThe first man to walk on the Moon was Neil Armstrong, an American astronaut and commander of the Apollo 11 mission. On July 20, 1969, Armstrong stepped out of the lunar module Eagle onto the surface of the Moon, famously declaring:\\n\\n\"That\\'s one small step for man, one giant leap for mankind.\"\\n\\nArmstrong was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the Moon during the mission. Michael Collins remained in orbit around the Moon in the command module Columbia.\\n\\nNeil Armstrong passed away on August 25, 2012, but his legacy as a pioneering astronaut and engineer continues to inspire people around the world!', response_metadata={'model': 'llama3.1:8b', 'created_at': '2024-08-01T00:38:29.176717Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 10681861417, 'load_duration': 34270292, 'prompt_eval_count': 19, 'prompt_eval_duration': 6209448000, 'eval_count': 141, 'eval_duration': 4432022000}, id='run-7bed57c5-7f54-4092-912c-ae49073dcd48-0', usage_metadata={'input_tokens': 19, 'output_tokens': 141, 'total_tokens': 160})\nEnvironment\nâ€‹\nInference speed is a challenge when running models locally (see above).\nTo minimize latency, it is desirable to run models locally on GPU, which ships with many consumer laptops\ne.g., Apple devices\n.\nAnd even with GPU, the available GPU memory bandwidth (as noted above) is important.\nRunning Apple silicon GPU\nâ€‹\nollama\nand\nllamafile\nwill automatically utilize the GPU on Apple devices.\nOther frameworks require the user to set up the environment to utilize the Apple GPU.\nFor example,\nllama.cpp\npython bindings can be configured to use the GPU via\nMetal\n.\nMetal is a graphics and compute API created by Apple providing near-direct access to the GPU.\nSee the\nllama.cpp\nsetup\nhere\nto enable this.\nIn particular, ensure that conda is using the correct virtual environment that you created (\nminiforge3\n).\ne.g., for me:\nconda activate /Users/rlm/miniforge3/envs/llama\nWith the above confirmed, then:\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir\nLLMs\nâ€‹\nThere are various ways to gain access to quantized model weights.\nHuggingFace\n- Many quantized model are available for download and can be run with framework such as\nllama.cpp\n. You can also download models in\nllamafile\nformat\nfrom HuggingFace.\ngpt4all\n- The model explorer offers a leaderboard of metrics and associated quantized models available for download\nollama\n- Several models can be accessed directly via\npull\nOllama\n\u0000â€‹\nWith\nOllama\n, fetch a model via\nollama pull <model family>:<tag>\n.\nllm\n=\nChatOllama\n(\nmodel\n=\n\"gpt-oss:20b\"\n)\nllm\n.\ninvoke\n(\n\"The first man on the moon was ... think step by step\"\n)\n' Sure! Here\\'s the answer, broken down step by step:\\n\\nThe first man on the moon was... Neil Armstrong.\\n\\nHere\\'s how I arrived at that answer:\\n\\n1. The first manned mission to land on the moon was Apollo 11.\\n2. The mission included three astronauts: Neil Armstrong, Edwin \"Buzz\" Aldrin, and Michael Collins.\\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind.\"\\n\\nSo, the first man on the moon was Neil Armstrong!'\nLlama.cpp\nâ€‹\nLlama.cpp is compatible with a\nbroad set of models\n.\nFor example, below we run inference on\nllama2-13b\nwith 4 bit quantization downloaded from\nHuggingFace\n.\nAs noted above, see the\nAPI reference\nfor the full set of parameters.\nFrom the\nllama.cpp API reference docs\n, a few are worth commenting on:\nn_gpu_layers\n: number of layers to be loaded into GPU memory\nValue: 1\nMeaning: Only one layer of the model will be loaded into GPU memory (1 is often sufficient).\nn_batch\n: number of tokens the model should process in parallel\nValue: n_batch\nMeaning: It's recommended to choose a value between 1 and n_ctx (which in this case is set to 2048)\nn_ctx\n: Token context window\nValue: 2048\nMeaning: The model will consider a window of 2048 tokens at a time\nf16_kv\n: whether the model should use half-precision for the key/value cache\nValue: True\nMeaning: The model will use half-precision, which can be more memory efficient; Metal only supports True.\n%\nenv CMAKE_ARGS\n=\n\"-DLLAMA_METAL=on\"\n%\nenv FORCE_CMAKE\n=\n1\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  llama\n-\ncpp\n-\npython\n-\n-\nno\n-\ncache\n-\ndir\nfrom\nlangchain_community\n.\nllms\nimport\nLlamaCpp\nfrom\nlangchain_core\n.\ncallbacks\nimport\nCallbackManager\n,\nStreamingStdOutCallbackHandler\nllm\n=\nLlamaCpp\n(\nmodel_path\n=\n\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\"\n,\nn_gpu_layers\n=\n1\n,\nn_batch\n=\n512\n,\nn_ctx\n=\n2048\n,\nf16_kv\n=\nTrue\n,\ncallback_manager\n=\nCallbackManager\n(\n[\nStreamingStdOutCallbackHandler\n(\n)\n]\n)\n,\nverbose\n=\nTrue\n,\n)\nAPI Reference:\nCallbackManager\n|\nStreamingStdOutCallbackHandler\nThe console log will show the below to indicate Metal was enabled properly from steps above:\nggml_metal_init: allocating\nggml_metal_init: using MPS\nllm\n.\ninvoke\n(\n\"The first man on the moon was ... Let's think step by step\"\n)\nLlama.generate: prefix-match hit\n``````output\nand use logical reasoning to figure out who the first man on the moon was.\nHere are some clues:\n1. The first man on the moon was an American.\n2. He was part of the Apollo 11 mission.\n3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\n4. His last name is Armstrong.\nNow, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\nTherefore, the first man on the moon was Neil Armstrong!\n``````output\nllama_print_timings:        load time =  9623.21 ms\nllama_print_timings:      sample time =   143.77 ms /   203 runs   (    0.71 ms per token,  1412.01 tokens per second)\nllama_print_timings: prompt eval time =   485.94 ms /     7 tokens (   69.42 ms per token,    14.40 tokens per second)\nllama_print_timings:        eval time =  6385.16 ms /   202 runs   (   31.61 ms per token,    31.64 tokens per second)\nllama_print_timings:       total time =  7279.28 ms\n\" and use logical reasoning to figure out who the first man on the moon was.\\n\\nHere are some clues:\\n\\n1. The first man on the moon was an American.\\n2. He was part of the Apollo 11 mission.\\n3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\\n4. His last name is Armstrong.\\n\\nNow, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\\nTherefore, the first man on the moon was Neil Armstrong!\"\nGPT4All\nâ€‹\nWe can use model weights downloaded from\nGPT4All\nmodel explorer.\nSimilar to what is shown above, we can run inference and use\nthe API reference\nto set parameters of interest.\n%\npip install gpt4all\nfrom\nlangchain_community\n.\nllms\nimport\nGPT4All\nllm\n=\nGPT4All\n(\nmodel\n=\n\"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\"\n)\nllm\n.\ninvoke\n(\n\"The first man on the moon was ... Let's think step by step\"\n)\n\".\\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon's surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these\"\nllamafile\nâ€‹\nOne of the simplest ways to run an LLM locally is using a\nllamafile\n. All you need to do is:\nDownload a llamafile from\nHuggingFace\nMake the file executable\nRun the file\nllamafiles bundle model weights and a\nspecially-compiled\nversion of\nllama.cpp\ninto a single file that can run on most computers without any additional dependencies. They also come with an embedded inference server that provides an\nAPI\nfor interacting with your model.\nHere's a simple bash script that shows all 3 setup steps:\n# Download a llamafile from HuggingFace\nwget https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n# Make the file executable. On Windows, instead just rename the file to end in \".exe\".\nchmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n# Start the model server. Listens at http://localhost:8080 by default.\n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser\nAfter you run the above setup steps, you can use LangChain to interact with your model:\nfrom\nlangchain_community\n.\nllms\n.\nllamafile\nimport\nLlamafile\nllm\n=\nLlamafile\n(\n)\nllm\n.\ninvoke\n(\n\"The first man on the moon was ... Let's think step by step.\"\n)\n\"\\nFirstly, let's imagine the scene where Neil Armstrong stepped onto the moon. This happened in 1969. The first man on the moon was Neil Armstrong. We already know that.\\n2nd, let's take a step back. Neil Armstrong didn't have any special powers. He had to land his spacecraft safely on the moon without injuring anyone or causing any damage. If he failed to do this, he would have been killed along with all those people who were on board the spacecraft.\\n3rd, let's imagine that Neil Armstrong successfully landed his spacecraft on the moon and made it back to Earth safely. The next step was for him to be hailed as a hero by his people back home. It took years before Neil Armstrong became an American hero.\\n4th, let's take another step back. Let's imagine that Neil Armstrong wasn't hailed as a hero, and instead, he was just forgotten. This happened in the 1970s. Neil Armstrong wasn't recognized for his remarkable achievement on the moon until after he died.\\n5th, let's take another step back. Let's imagine that Neil Armstrong didn't die in the 1970s and instead, lived to be a hundred years old. This happened in 2036. In the year 2036, Neil Armstrong would have been a centenarian.\\nNow, let's think about the present. Neil Armstrong is still alive. He turned 95 years old on July 20th, 2018. If he were to die now, his achievement of becoming the first human being to set foot on the moon would remain an unforgettable moment in history.\\nI hope this helps you understand the significance and importance of Neil Armstrong's achievement on the moon!\"\nPrompts\nâ€‹\nSome LLMs will benefit from specific prompts.\nFor example, LLaMA will use\nspecial tokens\n.\nWe can use\nConditionalPromptSelector\nto set prompt based on the model type.\n# Set our LLM\nllm\n=\nLlamaCpp\n(\nmodel_path\n=\n\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\"\n,\nn_gpu_layers\n=\n1\n,\nn_batch\n=\n512\n,\nn_ctx\n=\n2048\n,\nf16_kv\n=\nTrue\n,\ncallback_manager\n=\nCallbackManager\n(\n[\nStreamingStdOutCallbackHandler\n(\n)\n]\n)\n,\nverbose\n=\nTrue\n,\n)\nSet the associated prompt based upon the model version.\nfrom\nlangchain\n.\nchains\n.\nprompt_selector\nimport\nConditionalPromptSelector\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nDEFAULT_LLAMA_SEARCH_PROMPT\n=\nPromptTemplate\n(\ninput_variables\n=\n[\n\"question\"\n]\n,\ntemplate\n=\n\"\"\"<<SYS>> \\n You are an assistant tasked with improving Google search \\\nresults. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that \\\nare similar to this question. The output should be a numbered list of questions \\\nand each should have a question mark at the end: \\n\\n {question} [/INST]\"\"\"\n,\n)\nDEFAULT_SEARCH_PROMPT\n=\nPromptTemplate\n(\ninput_variables\n=\n[\n\"question\"\n]\n,\ntemplate\n=\n\"\"\"You are an assistant tasked with improving Google search \\\nresults. Generate THREE Google search queries that are similar to \\\nthis question. The output should be a numbered list of questions and each \\\nshould have a question mark at the end: {question}\"\"\"\n,\n)\nQUESTION_PROMPT_SELECTOR\n=\nConditionalPromptSelector\n(\ndefault_prompt\n=\nDEFAULT_SEARCH_PROMPT\n,\nconditionals\n=\n[\n(\nlambda\nllm\n:\nisinstance\n(\nllm\n,\nLlamaCpp\n)\n,\nDEFAULT_LLAMA_SEARCH_PROMPT\n)\n]\n,\n)\nprompt\n=\nQUESTION_PROMPT_SELECTOR\n.\nget_prompt\n(\nllm\n)\nprompt\nAPI Reference:\nPromptTemplate\nPromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='<<SYS>> \\n You are an assistant tasked with improving Google search results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \\n\\n {question} [/INST]', template_format='f-string', validate_template=True)\n# Chain\nchain\n=\nprompt\n|\nllm\nquestion\n=\n\"What NFL team won the Super Bowl in the year that Justin Bieber was born?\"\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\nquestion\n}\n)\nSure! Here are three similar search queries with a question mark at the end:\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?\n``````output\nllama_print_timings:        load time = 14943.19 ms\nllama_print_timings:      sample time =    72.93 ms /   101 runs   (    0.72 ms per token,  1384.87 tokens per second)\nllama_print_timings: prompt eval time = 14942.95 ms /    93 tokens (  160.68 ms per token,     6.22 tokens per second)\nllama_print_timings:        eval time =  3430.85 ms /   100 runs   (   34.31 ms per token,    29.15 tokens per second)\nllama_print_timings:       total time = 18578.26 ms\n'  Sure! Here are three similar search queries with a question mark at the end:\\n\\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?'\nWe also can use the LangChain Prompt Hub to fetch and / or store prompts that are model specific.\nThis will work with your\nLangSmith API key\n.\nFor example,\nhere\nis a prompt for RAG with LLaMA-specific tokens.\nUse cases\nâ€‹\nGiven an\nllm\ncreated from one of the models above, you can use it for\nmany use cases\n.\nFor example, you can implement a\nRAG application\nusing the chat models demonstrated here.\nIn general, use cases for local LLMs can be driven by at least two factors:\nPrivacy\n: private data (e.g., journals, etc) that a user does not want to share\nCost\n: text preprocessing (extraction/tagging), summarization, and agent simulations are token-use-intensive tasks\nIn addition,\nhere\nis an overview on fine-tuning, which can utilize open-source LLMs.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/logprobs/",
    "How-to guides\nHow to get log probabilities\nOn this page\nHow to get log probabilities\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nTokens\nCertain\nchat models\ncan be configured to return token-level log probabilities representing the likelihood of a given token. This guide walks through how to get this information in LangChain.\nOpenAI\nâ€‹\nInstall the LangChain x OpenAI package and set your API key\n%\npip install\n-\nqU langchain\n-\nopenai\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nFor the OpenAI API to return log probabilities we need to configure the\nlogprobs=True\nparam. Then, the logprobs are included on each output\nAIMessage\nas part of the\nresponse_metadata\n:\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\n.\nbind\n(\nlogprobs\n=\nTrue\n)\nmsg\n=\nllm\n.\ninvoke\n(\n(\n\"human\"\n,\n\"how are you today\"\n)\n)\nmsg\n.\nresponse_metadata\n[\n\"logprobs\"\n]\n[\n\"content\"\n]\n[\n:\n5\n]\n[{'token': 'I', 'bytes': [73], 'logprob': -0.26341408, 'top_logprobs': []},\n{'token': \"'m\",\n'bytes': [39, 109],\n'logprob': -0.48584133,\n'top_logprobs': []},\n{'token': ' just',\n'bytes': [32, 106, 117, 115, 116],\n'logprob': -0.23484154,\n'top_logprobs': []},\n{'token': ' a',\n'bytes': [32, 97],\n'logprob': -0.0018291725,\n'top_logprobs': []},\n{'token': ' computer',\n'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],\n'logprob': -0.052299336,\n'top_logprobs': []}]\nAnd are part of streamed Message chunks as well:\nct\n=\n0\nfull\n=\nNone\nfor\nchunk\nin\nllm\n.\nstream\n(\n(\n\"human\"\n,\n\"how are you today\"\n)\n)\n:\nif\nct\n<\n5\n:\nfull\n=\nchunk\nif\nfull\nis\nNone\nelse\nfull\n+\nchunk\nif\n\"logprobs\"\nin\nfull\n.\nresponse_metadata\n:\nprint\n(\nfull\n.\nresponse_metadata\n[\n\"logprobs\"\n]\n[\n\"content\"\n]\n)\nelse\n:\nbreak\nct\n+=\n1\n[]\n[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}]\n[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}]\n[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}]\n[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.0022134194, 'top_logprobs': []}]\nNext steps\nâ€‹\nYou've now learned how to get logprobs from OpenAI models in LangChain.\nNext, check out the other how-to guides chat models in this section, like\nhow to get a model to return structured output\nor\nhow to track token usage\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/long_context_reorder/",
    "How-to guides\nHow to reorder retrieved results to mitigate the \"lost in the middle\" effect\nHow to reorder retrieved results to mitigate the \"lost in the middle\" effect\nSubstantial performance degradations in\nRAG\napplications have been\ndocumented\nas the number of retrieved documents grows (e.g., beyond ten). In brief: models are liable to miss relevant information in the middle of long contexts.\nBy contrast, queries against vector stores will typically return documents in descending order of relevance (e.g., as measured by cosine similarity of\nembeddings\n).\nTo mitigate the\n\"lost in the middle\"\neffect, you can re-order documents after retrieval such that the most relevant documents are positioned at extrema (e.g., the first and last pieces of context), and the least relevant documents are positioned in the middle. In some cases this can help surface the most relevant information to LLMs.\nThe\nLongContextReorder\ndocument transformer implements this re-ordering procedure. Below we demonstrate an example.\n%\npip install\n-\nqU langchain langchain\n-\ncommunity langchain\n-\nopenai\nFirst we embed some artificial documents and index them in a basic in-memory vector store. We will use\nOpenAI\nembeddings, but any LangChain vector store or embeddings model will suffice.\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\n# Get embeddings.\nembeddings\n=\nOpenAIEmbeddings\n(\n)\ntexts\n=\n[\n\"Basquetball is a great sport.\"\n,\n\"Fly me to the moon is one of my favourite songs.\"\n,\n\"The Celtics are my favourite team.\"\n,\n\"This is a document about the Boston Celtics\"\n,\n\"I simply love going to the movies\"\n,\n\"The Boston Celtics won the game by 20 points\"\n,\n\"This is just a random text.\"\n,\n\"Elden Ring is one of the best games in the last 15 years.\"\n,\n\"L. Kornet is one of the best Celtics players.\"\n,\n\"Larry Bird was an iconic NBA player.\"\n,\n]\n# Create a retriever\nretriever\n=\nInMemoryVectorStore\n.\nfrom_texts\n(\ntexts\n,\nembedding\n=\nembeddings\n)\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n10\n}\n)\nquery\n=\n\"What can you tell me about the Celtics?\"\n# Get relevant documents ordered by relevance score\ndocs\n=\nretriever\n.\ninvoke\n(\nquery\n)\nfor\ndoc\nin\ndocs\n:\nprint\n(\nf\"-\n{\ndoc\n.\npage_content\n}\n\"\n)\nAPI Reference:\nInMemoryVectorStore\n- The Celtics are my favourite team.\n- This is a document about the Boston Celtics\n- The Boston Celtics won the game by 20 points\n- L. Kornet is one of the best Celtics players.\n- Basquetball is a great sport.\n- Larry Bird was an iconic NBA player.\n- This is just a random text.\n- I simply love going to the movies\n- Fly me to the moon is one of my favourite songs.\n- Elden Ring is one of the best games in the last 15 years.\nNote that documents are returned in descending order of relevance to the query. The\nLongContextReorder\ndocument transformer will implement the re-ordering described above:\nfrom\nlangchain_community\n.\ndocument_transformers\nimport\nLongContextReorder\n# Reorder the documents:\n# Less relevant document will be at the middle of the list and more\n# relevant elements at beginning / end.\nreordering\n=\nLongContextReorder\n(\n)\nreordered_docs\n=\nreordering\n.\ntransform_documents\n(\ndocs\n)\n# Confirm that the 4 relevant documents are at beginning and end.\nfor\ndoc\nin\nreordered_docs\n:\nprint\n(\nf\"-\n{\ndoc\n.\npage_content\n}\n\"\n)\n- This is a document about the Boston Celtics\n- L. Kornet is one of the best Celtics players.\n- Larry Bird was an iconic NBA player.\n- I simply love going to the movies\n- Elden Ring is one of the best games in the last 15 years.\n- Fly me to the moon is one of my favourite songs.\n- This is just a random text.\n- Basquetball is a great sport.\n- The Boston Celtics won the game by 20 points\n- The Celtics are my favourite team.\nBelow, we show how to incorporate the re-ordered documents into a simple question-answering chain:\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\nimport\ncreate_stuff_documents_chain\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nprompt_template\n=\n\"\"\"\nGiven these texts:\n-----\n{context}\n-----\nPlease answer the following question:\n{query}\n\"\"\"\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\nprompt_template\n,\ninput_variables\n=\n[\n\"context\"\n,\n\"query\"\n]\n,\n)\n# Create and invoke the chain:\nchain\n=\ncreate_stuff_documents_chain\n(\nllm\n,\nprompt\n)\nresponse\n=\nchain\n.\ninvoke\n(\n{\n\"context\"\n:\nreordered_docs\n,\n\"query\"\n:\nquery\n}\n)\nprint\n(\nresponse\n)\nAPI Reference:\nPromptTemplate\nThe Boston Celtics are a professional basketball team known for their rich history and success in the NBA. L. Kornet is recognized as one of the best players on the team, and the Celtics recently won a game by 20 points. The Celtics are favored by some fans, as indicated by the statement, \"The Celtics are my favourite team.\" Overall, they have a strong following and are considered a significant part of basketball culture.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/",
    "How-to guides\nHow to split Markdown by Headers\nOn this page\nHow to split Markdown by Headers\nMotivation\nâ€‹\nMany chat or Q+A applications involve chunking input documents prior to embedding and vector storage.\nThese notes\nfrom Pinecone provide some useful tips:\nWhen a full paragraph or document is embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text.\nAs mentioned, chunking often aims to keep text with common context together. With this in mind, we might want to specifically honor the structure of the document itself. For example, a markdown file is organized by headers. Creating chunks within specific header groups is an intuitive idea. To address this challenge, we can use\nMarkdownHeaderTextSplitter\n. This will split a markdown file by a specified set of headers.\nFor example, if we want to split this markdown:\nmd = '# Foo\\n\\n ## Bar\\n\\nHi this is Jim  \\nHi this is Joe\\n\\n ## Baz\\n\\n Hi this is Molly'\nWe can specify the headers to split on:\n[(\"#\", \"Header 1\"),(\"##\", \"Header 2\")]\nAnd content is grouped or split by common headers:\n{'content': 'Hi this is Jim  \\nHi this is Joe', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Bar'}}\n{'content': 'Hi this is Molly', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Baz'}}\nLet's have a look at some examples below.\nBasic usage:\nâ€‹\n%\npip install\n-\nqU langchain\n-\ntext\n-\nsplitters\nfrom\nlangchain_text_splitters\nimport\nMarkdownHeaderTextSplitter\nmarkdown_document\n=\n\"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"\nheaders_to_split_on\n=\n[\n(\n\"#\"\n,\n\"Header 1\"\n)\n,\n(\n\"##\"\n,\n\"Header 2\"\n)\n,\n(\n\"###\"\n,\n\"Header 3\"\n)\n,\n]\nmarkdown_splitter\n=\nMarkdownHeaderTextSplitter\n(\nheaders_to_split_on\n)\nmd_header_splits\n=\nmarkdown_splitter\n.\nsplit_text\n(\nmarkdown_document\n)\nmd_header_splits\n[Document(page_content='Hi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),\nDocument(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),\nDocument(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]\ntype\n(\nmd_header_splits\n[\n0\n]\n)\nlangchain_core.documents.base.Document\nBy default,\nMarkdownHeaderTextSplitter\nstrips headers being split on from the output chunk's content. This can be disabled by setting\nstrip_headers = False\n.\nmarkdown_splitter\n=\nMarkdownHeaderTextSplitter\n(\nheaders_to_split_on\n,\nstrip_headers\n=\nFalse\n)\nmd_header_splits\n=\nmarkdown_splitter\n.\nsplit_text\n(\nmarkdown_document\n)\nmd_header_splits\n[Document(page_content='# Foo  \\n## Bar  \\nHi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),\nDocument(page_content='### Boo  \\nHi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),\nDocument(page_content='## Baz  \\nHi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]\nnote\nThe default\nMarkdownHeaderTextSplitter\nstrips white spaces and new lines. To preserve the original formatting of your Markdown documents, check out\nExperimentalMarkdownSyntaxTextSplitter\n.\nHow to return Markdown lines as separate documents\nâ€‹\nBy default,\nMarkdownHeaderTextSplitter\naggregates lines based on the headers specified in\nheaders_to_split_on\n. We can disable this by specifying\nreturn_each_line\n:\nmarkdown_splitter\n=\nMarkdownHeaderTextSplitter\n(\nheaders_to_split_on\n,\nreturn_each_line\n=\nTrue\n,\n)\nmd_header_splits\n=\nmarkdown_splitter\n.\nsplit_text\n(\nmarkdown_document\n)\nmd_header_splits\n[Document(page_content='Hi this is Jim', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),\nDocument(page_content='Hi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),\nDocument(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),\nDocument(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]\nNote that here header information is retained in the\nmetadata\nfor each document.\nHow to constrain chunk size:\nâ€‹\nWithin each markdown group we can then apply any text splitter we want, such as\nRecursiveCharacterTextSplitter\n, which allows for further control of the chunk size.\nmarkdown_document\n=\n\"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\nheaders_to_split_on\n=\n[\n(\n\"#\"\n,\n\"Header 1\"\n)\n,\n(\n\"##\"\n,\n\"Header 2\"\n)\n,\n]\n# MD splits\nmarkdown_splitter\n=\nMarkdownHeaderTextSplitter\n(\nheaders_to_split_on\n=\nheaders_to_split_on\n,\nstrip_headers\n=\nFalse\n)\nmd_header_splits\n=\nmarkdown_splitter\n.\nsplit_text\n(\nmarkdown_document\n)\n# Char-level splits\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nchunk_size\n=\n250\nchunk_overlap\n=\n30\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\nchunk_size\n,\nchunk_overlap\n=\nchunk_overlap\n)\n# Split\nsplits\n=\ntext_splitter\n.\nsplit_documents\n(\nmd_header_splits\n)\nsplits\n[Document(page_content='# Intro  \\n## History  \\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),\nDocument(page_content='Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),\nDocument(page_content='## Rise and divergence  \\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),\nDocument(page_content='#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),\nDocument(page_content='## Implementations  \\nImplementations of Markdown are available for over a dozen programming languages.', metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'})]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/merge_message_runs/",
    "How-to guides\nHow to merge consecutive messages of the same type\nOn this page\nHow to merge consecutive messages of the same type\nCertain models do not support passing in consecutive\nmessages\nof the same type (a.k.a. \"runs\" of the same message type).\nThe\nmerge_message_runs\nutility makes it easy to merge consecutive messages of the same type.\nSetup\nâ€‹\n%\npip install\n-\nqU langchain\n-\ncore langchain\n-\nanthropic\nBasic usage\nâ€‹\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nHumanMessage\n,\nSystemMessage\n,\nmerge_message_runs\n,\n)\nmessages\n=\n[\nSystemMessage\n(\n\"you're a good assistant.\"\n)\n,\nSystemMessage\n(\n\"you always respond with a joke.\"\n)\n,\nHumanMessage\n(\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"i wonder why it's called langchain\"\n}\n]\n)\n,\nHumanMessage\n(\n\"and who is harrison chasing anyways\"\n)\n,\nAIMessage\n(\n'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n)\n,\nAIMessage\n(\n\"Why, he's probably chasing after the last cup of coffee in the office!\"\n)\n,\n]\nmerged\n=\nmerge_message_runs\n(\nmessages\n)\nprint\n(\n\"\\n\\n\"\n.\njoin\n(\n[\nrepr\n(\nx\n)\nfor\nx\nin\nmerged\n]\n)\n)\nAPI Reference:\nAIMessage\n|\nHumanMessage\n|\nSystemMessage\n|\nmerge_message_runs\nSystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\", additional_kwargs={}, response_metadata={})\nHumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called langchain\"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={})\nAIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})\nNotice that if the contents of one of the messages to merge is a list of content blocks then the merged message will have a list of content blocks. And if both messages to merge have string contents then those are concatenated with a newline character.\nChaining\nâ€‹\nmerge_message_runs\ncan be used imperatively (like above) or declaratively, making it easy to compose with other components in a chain:\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n,\ntemperature\n=\n0\n)\n# Notice we don't pass in messages. This creates\n# a RunnableLambda that takes messages as input\nmerger\n=\nmerge_message_runs\n(\n)\nchain\n=\nmerger\n|\nllm\nchain\n.\ninvoke\n(\nmessages\n)\nAIMessage(content='\\n\\nAs for the actual answer, LangChain is named for connecting (chaining) language models together with other components. And Harrison Chase is one of the co-founders of LangChain, not someone being chased! \\n\\nBut I like to think he\\'s running after runaway tokens that escaped from the embedding space. \"Come back here, you vectors!\"', additional_kwargs={}, response_metadata={'id': 'msg_018MF8xBrM1ztw69XTx3Uxcy', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 84, 'output_tokens': 80, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--caa1b9d6-a554-40ad-95cd-268938d8223b-0', usage_metadata={'input_tokens': 84, 'output_tokens': 80, 'total_tokens': 164, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\nLooking at the LangSmith trace we can see that before the messages are passed to the model they are merged:\nhttps://smith.langchain.com/public/ab558677-cac9-4c59-9066-1ecce5bcd87c/r\nLooking at just the merger, we can see that it's a Runnable object that can be invoked like all Runnables:\nmerger\n.\ninvoke\n(\nmessages\n)\n[SystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called langchain\"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={}),\nAIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})]\nmerge_message_runs\ncan also be placed after a prompt:\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nprompt\n=\nChatPromptTemplate\n(\n[\n(\n\"system\"\n,\n\"You're great a {skill}\"\n)\n,\n(\n\"system\"\n,\n\"You're also great at explaining things\"\n)\n,\n(\n\"human\"\n,\n\"{query}\"\n)\n,\n]\n)\nchain\n=\nprompt\n|\nmerger\n|\nllm\nchain\n.\ninvoke\n(\n{\n\"skill\"\n:\n\"math\"\n,\n\"query\"\n:\n\"what's the definition of a convergent series\"\n}\n)\nAPI Reference:\nChatPromptTemplate\nAIMessage(content=\"# Definition of a Convergent Series\\n\\nA series is a sum of terms in a sequence, typically written as:\\n\\n$$\\\\sum_{n=1}^{\\\\infty} a_n = a_1 + a_2 + a_3 + \\\\ldots$$\\n\\nA series is called **convergent** if the sequence of partial sums approaches a finite limit.\\n\\n## Formal Definition\\n\\nLet's define the sequence of partial sums:\\n$$S_N = \\\\sum_{n=1}^{N} a_n = a_1 + a_2 + \\\\ldots + a_N$$\\n\\nA series $\\\\sum_{n=1}^{\\\\infty} a_n$ is convergent if and only if:\\n- The limit of the partial sums exists and is finite\\n- That is, there exists a finite number $S$ such that $\\\\lim_{N \\\\to \\\\infty} S_N = S$\\n\\nIf this limit exists, we say the series converges to $S$, and we write:\\n$$\\\\sum_{n=1}^{\\\\infty} a_n = S$$\\n\\nIf the limit doesn't exist or is infinite, the series is called divergent.\", additional_kwargs={}, response_metadata={'id': 'msg_018ypyi2MTjV6S7jCydSqDn9', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 29, 'output_tokens': 273, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'}, id='run--5de0ca29-d031-48f7-bc75-671eade20b74-0', usage_metadata={'input_tokens': 29, 'output_tokens': 273, 'total_tokens': 302, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})\nLangSmith Trace\nAPI reference\nâ€‹\nFor a complete description of all arguments head to the\nAPI reference\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/message_history/",
    "How-to guides\nHow to add message history\nOn this page\nHow to add message history\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChaining runnables\nPrompt templates\nChat Messages\nLangGraph persistence\nnote\nThis guide previously covered the\nRunnableWithMessageHistory\nabstraction. You can access this version of the guide in the\nv0.2 docs\n.\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of\nLangGraph persistence\nto incorporate\nmemory\ninto new LangChain applications.\nIf your code is already relying on\nRunnableWithMessageHistory\nor\nBaseChatMessageHistory\n, you do\nnot\nneed to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses\nRunnableWithMessageHistory\nwill continue to work as expected.\nPlease see\nHow to migrate to LangGraph Memory\nfor more details.\nPassing conversation state into and out a chain is vital when building a chatbot. LangGraph implements a built-in persistence layer, allowing chain states to be automatically persisted in memory, or external backends such as SQLite, Postgres or Redis. Details can be found in the LangGraph\npersistence documentation\n.\nIn this guide we demonstrate how to add persistence to arbitrary LangChain runnables by wrapping them in a minimal LangGraph application. This lets us persist the message history and other elements of the chain's state, simplifying the development of multi-turn applications. It also supports multiple threads, enabling a single application to interact separately with multiple users.\nSetup\nâ€‹\nLet's initialize a chat model:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nExample: message inputs\nâ€‹\nAdding memory to a\nchat model\nprovides a simple example. Chat models accept a list of messages as input and output a message. LangGraph includes a built-in\nMessagesState\nthat we can use for this purpose.\nBelow, we:\nDefine the graph state to be a list of messages;\nAdd a single node to the graph that calls a chat model;\nCompile the graph with an in-memory checkpointer to store messages between runs.\ninfo\nThe output of a LangGraph application is its\nstate\n. This can be any Python type, but in this context it will typically be a\nTypedDict\nthat matches the schema of your runnable.\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nMessagesState\n,\nStateGraph\n# Define a new graph\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define the function that calls the model\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\nresponse\n=\nllm\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\n# Update message history with response:\nreturn\n{\n\"messages\"\n:\nresponse\n}\n# Define the (single) node in the graph\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\n# Add memory\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nAPI Reference:\nHumanMessage\n|\nMemorySaver\n|\nStateGraph\nWhen we run the application, we pass in a configuration\ndict\nthat specifies a\nthread_id\n. This ID is used to distinguish conversational threads (e.g., between different users).\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc123\"\n}\n}\nWe can then invoke the application:\nquery\n=\n\"Hi! I'm Bob.\"\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n# output contains all messages in state\n==================================\u001b[1m Ai Message \u001b[0m==================================\nIt's nice to meet you, Bob! I'm Claude, an AI assistant created by Anthropic. How can I help you today?\nquery\n=\n\"What's my name?\"\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYour name is Bob, as you introduced yourself at the beginning of our conversation.\nNote that states are separated for different threads. If we issue the same query to a thread with a new\nthread_id\n, the model indicates that it does not know the answer:\nquery\n=\n\"What's my name?\"\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc234\"\n}\n}\ninput_messages\n=\n[\nHumanMessage\n(\nquery\n)\n]\noutput\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nI'm afraid I don't actually know your name. As an AI assistant, I don't have personal information about you unless you provide it to me directly.\nExample: dictionary inputs\nâ€‹\nLangChain runnables often accept multiple inputs via separate keys in a single\ndict\nargument. A common example is a prompt template with multiple parameters.\nWhereas before our runnable was a chat model, here we chain together a prompt template and chat model.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nMessagesPlaceholder\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"Answer in {language}.\"\n)\n,\nMessagesPlaceholder\n(\nvariable_name\n=\n\"messages\"\n)\n,\n]\n)\nrunnable\n=\nprompt\n|\nllm\nAPI Reference:\nChatPromptTemplate\n|\nMessagesPlaceholder\nFor this scenario, we define the graph state to include these parameters (in addition to the message history). We then define a single-node graph in the same way as before.\nNote that in the below state:\nUpdates to the\nmessages\nlist will append messages;\nUpdates to the\nlanguage\nstring will overwrite the string.\nfrom\ntyping\nimport\nSequence\nfrom\nlangchain_core\n.\nmessages\nimport\nBaseMessage\nfrom\nlanggraph\n.\ngraph\n.\nmessage\nimport\nadd_messages\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\nclass\nState\n(\nTypedDict\n)\n:\nmessages\n:\nAnnotated\n[\nSequence\n[\nBaseMessage\n]\n,\nadd_messages\n]\nlanguage\n:\nstr\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nState\n)\ndef\ncall_model\n(\nstate\n:\nState\n)\n:\nresponse\n=\nrunnable\n.\ninvoke\n(\nstate\n)\n# Update message history with response:\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nAPI Reference:\nBaseMessage\n|\nadd_messages\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc345\"\n}\n}\ninput_dict\n=\n{\n\"messages\"\n:\n[\nHumanMessage\n(\n\"Hi, I'm Bob.\"\n)\n]\n,\n\"language\"\n:\n\"Spanish\"\n,\n}\noutput\n=\napp\n.\ninvoke\n(\ninput_dict\n,\nconfig\n)\noutput\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n==================================\u001b[1m Ai Message \u001b[0m==================================\nÂ¡Hola, Bob! Es un placer conocerte.\nManaging message history\nâ€‹\nThe message history (and other elements of the application state) can be accessed via\n.get_state\n:\nstate\n=\napp\n.\nget_state\n(\nconfig\n)\n.\nvalues\nprint\n(\nf\"Language:\n{\nstate\n[\n'language'\n]\n}\n\"\n)\nfor\nmessage\nin\nstate\n[\n\"messages\"\n]\n:\nmessage\n.\npretty_print\n(\n)\nLanguage: Spanish\n================================\u001b[1m Human Message \u001b[0m=================================\nHi, I'm Bob.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nÂ¡Hola, Bob! Es un placer conocerte.\nWe can also update the state via\n.update_state\n. For example, we can manually append a new message:\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\n_\n=\napp\n.\nupdate_state\n(\nconfig\n,\n{\n\"messages\"\n:\n[\nHumanMessage\n(\n\"Test\"\n)\n]\n}\n)\nAPI Reference:\nHumanMessage\nstate\n=\napp\n.\nget_state\n(\nconfig\n)\n.\nvalues\nprint\n(\nf\"Language:\n{\nstate\n[\n'language'\n]\n}\n\"\n)\nfor\nmessage\nin\nstate\n[\n\"messages\"\n]\n:\nmessage\n.\npretty_print\n(\n)\nLanguage: Spanish\n================================\u001b[1m Human Message \u001b[0m=================================\nHi, I'm Bob.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nÂ¡Hola, Bob! Es un placer conocerte.\n================================\u001b[1m Human Message \u001b[0m=================================\nTest\nFor details on managing state, including deleting messages, see the LangGraph documentation:\nHow to delete messages\nHow to view and update past graph state\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/migrate_agent/",
    "How-to guides\nHow to migrate from legacy LangChain agents to LangGraph\nOn this page\nHow to migrate from legacy LangChain agents to LangGraph\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nAgents\nLangGraph\nTool calling\nHere we focus on how to move from legacy LangChain agents to more flexible\nLangGraph\nagents.\nLangChain agents (the\nAgentExecutor\nin particular) have multiple configuration parameters.\nIn this notebook we will show how those parameters map to the LangGraph react agent executor using the\ncreate_react_agent\nprebuilt helper method.\nnote\nIn LangGraph, the graph replaces LangChain's agent executor. It manages the agent's cycles and tracks the scratchpad as messages within its state. The LangChain \"agent\" corresponds to the prompt and LLM you've provided.\nPrerequisites\nâ€‹\nThis how-to guide uses OpenAI as the LLM. Install the dependencies to run.\n%\n%\ncapture\n-\n-\nno\n-\nstderr\n%\npip install\n-\nU langgraph langchain langchain\n-\nopenai\nThen, set your OpenAI API key.\nimport\ngetpass\nimport\nos\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"OpenAI API key:\\n\"\n)\nBasic Usage\nâ€‹\nFor basic creation and usage of a tool-calling ReAct-style agent, the functionality is the same. First, let's define a model and tool(s), then we'll use those to create an agent.\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\n@tool\ndef\nmagic_function\n(\ninput\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Applies a magic function to an input.\"\"\"\nreturn\ninput\n+\n2\ntools\n=\n[\nmagic_function\n]\nquery\n=\n\"what is the value of magic_function(3)?\"\nAPI Reference:\ntool\nFor the LangChain\nAgentExecutor\n, we define a prompt with a placeholder for the agent's scratchpad. The agent can be invoked as follows:\nfrom\nlangchain\n.\nagents\nimport\nAgentExecutor\n,\ncreate_tool_calling_agent\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n# Placeholders fill up a **list** of messages\n(\n\"placeholder\"\n,\n\"{agent_scratchpad}\"\n)\n,\n]\n)\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n)\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\nquery\n}\n)\nAPI Reference:\nChatPromptTemplate\n{'input': 'what is the value of magic_function(3)?',\n'output': 'The value of `magic_function(3)` is 5.'}\nLangGraph's\nreact agent executor\nmanages a state that is defined by a list of messages. It will continue to process the list until there are no tool calls in the agent's output. To kick it off, we input a list of messages. The output will contain the entire state of the graph-- in this case, the conversation history.\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n)\nmessages\n=\nlanggraph_agent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n)\n{\n\"input\"\n:\nquery\n,\n\"output\"\n:\nmessages\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\ncontent\n,\n}\nAPI Reference:\ncreate_react_agent\n{'input': 'what is the value of magic_function(3)?',\n'output': 'The value of `magic_function(3)` is 5.'}\nmessage_history\n=\nmessages\n[\n\"messages\"\n]\nnew_query\n=\n\"Pardon?\"\nmessages\n=\nlanggraph_agent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\nmessage_history\n+\n[\n(\n\"human\"\n,\nnew_query\n)\n]\n}\n)\n{\n\"input\"\n:\nnew_query\n,\n\"output\"\n:\nmessages\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\ncontent\n,\n}\n{'input': 'Pardon?',\n'output': 'The result of applying `magic_function` to the input value 3 is 5.'}\nPrompt Templates\nâ€‹\nWith legacy LangChain agents you have to pass in a prompt template. You can use this to control the agent.\nWith LangGraph\nreact agent executor\n, by default there is no prompt. You can achieve similar control over the agent in a few ways:\nPass in a system message as input\nInitialize the agent with a system message\nInitialize the agent with a function to transform messages in the graph state before passing to the model.\nInitialize the agent with a\nRunnable\nto transform messages in the graph state before passing to the model. This includes passing prompt templates as well.\nLet's take a look at all of these below. We will pass in custom instructions to get the agent to respond in Spanish.\nFirst up, using\nAgentExecutor\n:\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant. Respond only in Spanish.\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n# Placeholders fill up a **list** of messages\n(\n\"placeholder\"\n,\n\"{agent_scratchpad}\"\n)\n,\n]\n)\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n)\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\nquery\n}\n)\n{'input': 'what is the value of magic_function(3)?',\n'output': 'El valor de magic_function(3) es 5.'}\nNow, let's pass a custom system message to\nreact agent executor\n.\nLangGraph's prebuilt\ncreate_react_agent\ndoes not take a prompt template directly as a parameter, but instead takes a\nprompt\nparameter. This modifies the graph state before the llm is called, and can be one of four values:\nA\nSystemMessage\n, which is added to the beginning of the list of messages.\nA\nstring\n, which is converted to a\nSystemMessage\nand added to the beginning of the list of messages.\nA\nCallable\n, which should take in full graph state. The output is then passed to the language model.\nOr a\nRunnable\n, which should take in full graph state. The output is then passed to the language model.\nHere's how it looks in action:\nfrom\nlangchain_core\n.\nmessages\nimport\nSystemMessage\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nsystem_message\n=\n\"You are a helpful assistant. Respond only in Spanish.\"\n# This could also be a SystemMessage object\n# system_message = SystemMessage(content=\"You are a helpful assistant. Respond only in Spanish.\")\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\nsystem_message\n)\nmessages\n=\nlanggraph_agent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\nquery\n)\n]\n}\n)\nAPI Reference:\nSystemMessage\n|\ncreate_react_agent\nWe can also pass in an arbitrary function or a runnable. This function/runnable should take in a graph state and output a list of messages.\nWe can do all types of arbitrary formatting of messages here. In this case, let's add a SystemMessage to the start of the list of messages and append another user message at the end.\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\n,\nSystemMessage\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nfrom\nlanggraph\n.\nprebuilt\n.\nchat_agent_executor\nimport\nAgentState\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant. Respond only in Spanish.\"\n)\n,\n(\n\"placeholder\"\n,\n\"{messages}\"\n)\n,\n(\n\"user\"\n,\n\"Also say 'Pandamonium!' after the answer.\"\n)\n,\n]\n)\n# alternatively, this can be passed as a function, e.g.\n# def prompt(state: AgentState):\n#     return (\n#         [SystemMessage(content=\"You are a helpful assistant. Respond only in Spanish.\")] +\n#         state[\"messages\"] +\n#         [HumanMessage(content=\"Also say 'Pandamonium!' after the answer.\")]\n#     )\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\nprompt\n)\nmessages\n=\nlanggraph_agent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n)\nprint\n(\n{\n\"input\"\n:\nquery\n,\n\"output\"\n:\nmessages\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\ncontent\n,\n}\n)\nAPI Reference:\nHumanMessage\n|\nSystemMessage\n|\ncreate_react_agent\n{'input': 'what is the value of magic_function(3)?', 'output': 'El valor de magic_function(3) es 5. Â¡Pandamonium!'}\nMemory\nâ€‹\nIn LangChain\nâ€‹\nWith LangChain's\nAgentExecutor\n, you could add chat\nMemory\nso it can engage in a multi-turn conversation.\nfrom\nlangchain\n.\nagents\nimport\nAgentExecutor\n,\ncreate_tool_calling_agent\nfrom\nlangchain_core\n.\nchat_history\nimport\nInMemoryChatMessageHistory\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\n.\nhistory\nimport\nRunnableWithMessageHistory\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\nmemory\n=\nInMemoryChatMessageHistory\n(\nsession_id\n=\n\"test-session\"\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant.\"\n)\n,\n# First put the history\n(\n\"placeholder\"\n,\n\"{chat_history}\"\n)\n,\n# Then the new input\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n# Finally the scratchpad\n(\n\"placeholder\"\n,\n\"{agent_scratchpad}\"\n)\n,\n]\n)\n@tool\ndef\nmagic_function\n(\ninput\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Applies a magic function to an input.\"\"\"\nreturn\ninput\n+\n2\ntools\n=\n[\nmagic_function\n]\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n)\nagent_with_chat_history\n=\nRunnableWithMessageHistory\n(\nagent_executor\n,\n# This is needed because in most real world scenarios, a session id is needed\n# It isn't really used here because we are using a simple in memory ChatMessageHistory\nlambda\nsession_id\n:\nmemory\n,\ninput_messages_key\n=\n\"input\"\n,\nhistory_messages_key\n=\n\"chat_history\"\n,\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"session_id\"\n:\n\"test-session\"\n}\n}\nprint\n(\nagent_with_chat_history\n.\ninvoke\n(\n{\n\"input\"\n:\n\"Hi, I'm polly! What's the output of magic_function of 3?\"\n}\n,\nconfig\n)\n[\n\"output\"\n]\n)\nprint\n(\n\"---\"\n)\nprint\n(\nagent_with_chat_history\n.\ninvoke\n(\n{\n\"input\"\n:\n\"Remember my name?\"\n}\n,\nconfig\n)\n[\n\"output\"\n]\n)\nprint\n(\n\"---\"\n)\nprint\n(\nagent_with_chat_history\n.\ninvoke\n(\n{\n\"input\"\n:\n\"what was that output again?\"\n}\n,\nconfig\n)\n[\n\"output\"\n]\n)\nAPI Reference:\nInMemoryChatMessageHistory\n|\nChatPromptTemplate\n|\nRunnableWithMessageHistory\n|\ntool\nThe output of the magic function when the input is 3 is 5.\n---\nYes, you mentioned your name is Polly.\n---\nThe output of the magic function when the input is 3 is 5.\nIn LangGraph\nâ€‹\nMemory is just\npersistence\n, aka\ncheckpointing\n.\nAdd a\ncheckpointer\nto the agent and you get chat memory for free.\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\n# an in-memory checkpointer\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nsystem_message\n=\n\"You are a helpful assistant.\"\n# This could also be a SystemMessage object\n# system_message = SystemMessage(content=\"You are a helpful assistant. Respond only in Spanish.\")\nmemory\n=\nMemorySaver\n(\n)\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\nsystem_message\n,\ncheckpointer\n=\nmemory\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"test-thread\"\n}\n}\nprint\n(\nlanggraph_agent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"Hi, I'm polly! What's the output of magic_function of 3?\"\n)\n]\n}\n,\nconfig\n,\n)\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\ncontent\n)\nprint\n(\n\"---\"\n)\nprint\n(\nlanggraph_agent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"Remember my name?\"\n)\n]\n}\n,\nconfig\n)\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\ncontent\n)\nprint\n(\n\"---\"\n)\nprint\n(\nlanggraph_agent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"what was that output again?\"\n)\n]\n}\n,\nconfig\n)\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\ncontent\n)\nAPI Reference:\nMemorySaver\n|\ncreate_react_agent\nThe output of the magic function for the input 3 is 5.\n---\nYes, you mentioned that your name is Polly.\n---\nThe output of the magic function for the input 3 was 5.\nIterating through steps\nâ€‹\nIn LangChain\nâ€‹\nWith LangChain's\nAgentExecutor\n, you could iterate over the steps using the\nstream\n(or async\nastream\n) methods or the\niter\nmethod. LangGraph supports stepwise iteration using\nstream\nfrom\nlangchain\n.\nagents\nimport\nAgentExecutor\n,\ncreate_tool_calling_agent\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant.\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n# Placeholders fill up a **list** of messages\n(\n\"placeholder\"\n,\n\"{agent_scratchpad}\"\n)\n,\n]\n)\n@tool\ndef\nmagic_function\n(\ninput\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Applies a magic function to an input.\"\"\"\nreturn\ninput\n+\n2\ntools\n=\n[\nmagic_function\n]\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\nprompt\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n)\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"input\"\n:\nquery\n}\n)\n:\nprint\n(\nstep\n)\nAPI Reference:\nChatPromptTemplate\n|\ntool\n{'actions': [ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log=\"\\nInvoking: `magic_function` with `{'input': 3}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{\"input\":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_yyetzabaDBRX9Ml2KyqfKzZM')], 'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{\"input\":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])]}\n{'steps': [AgentStep(action=ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log=\"\\nInvoking: `magic_function` with `{'input': 3}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{\"input\":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_yyetzabaDBRX9Ml2KyqfKzZM'), observation=5)], 'messages': [FunctionMessage(content='5', additional_kwargs={}, response_metadata={}, name='magic_function')]}\n{'output': 'The value of `magic_function(3)` is 5.', 'messages': [AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={}, response_metadata={})]}\nIn LangGraph\nâ€‹\nIn LangGraph, things are handled natively using\nstream\nor the asynchronous\nastream\nmethod.\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nfrom\nlanggraph\n.\nprebuilt\n.\nchat_agent_executor\nimport\nAgentState\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant.\"\n)\n,\n(\n\"placeholder\"\n,\n\"{messages}\"\n)\n,\n]\n)\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\nprompt\n)\nfor\nstep\nin\nlanggraph_agent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n,\nstream_mode\n=\n\"updates\"\n)\n:\nprint\n(\nstep\n)\nAPI Reference:\ncreate_react_agent\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IHTMrjvIHn8gFOX42FstIpr9', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 61, 'total_tokens': 75, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1a6970da-163a-4e4d-b9b7-7e73b1057f42-0', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_IHTMrjvIHn8gFOX42FstIpr9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 14, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}\n{'tools': {'messages': [ToolMessage(content='5', name='magic_function', id='51a9d3e4-734d-426f-a5a1-c6597e4efe25', tool_call_id='call_IHTMrjvIHn8gFOX42FstIpr9')]}}\n{'agent': {'messages': [AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 84, 'total_tokens': 98, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a20a4ee344', 'finish_reason': 'stop', 'logprobs': None}, id='run-73001576-a3dc-4552-8d81-c9ce8aec05b3-0', usage_metadata={'input_tokens': 84, 'output_tokens': 14, 'total_tokens': 98, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}\nreturn_intermediate_steps\nâ€‹\nIn LangChain\nâ€‹\nSetting this parameter on AgentExecutor allows users to access intermediate_steps, which pairs agent actions (e.g., tool invocations) with their outcomes.\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n,\nreturn_intermediate_steps\n=\nTrue\n)\nresult\n=\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\nquery\n}\n)\nprint\n(\nresult\n[\n\"intermediate_steps\"\n]\n)\n[(ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log=\"\\nInvoking: `magic_function` with `{'input': 3}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-c9dfe3ab-2db6-4592-851e-89e056aeab32', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{\"input\":3}', 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_njTvl2RsVf4q1aMUxoYnJuK1'), 5)]\nIn LangGraph\nâ€‹\nBy default the\nreact agent executor\nin LangGraph appends all messages to the central state. Therefore, it is easy to see any intermediate steps by just looking at the full state.\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n=\ntools\n)\nmessages\n=\nlanggraph_agent_executor\n.\ninvoke\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n)\nmessages\nAPI Reference:\ncreate_react_agent\n{'messages': [HumanMessage(content='what is the value of magic_function(3)?', additional_kwargs={}, response_metadata={}, id='1abb52c2-4bc2-4d82-bd32-5a24c3976b0f'),\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XfQD6C7rAalcmicQubkhJVFq', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a20a4ee344', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-34f02786-5b5c-4bb1-bd9e-406c81944a24-0', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_XfQD6C7rAalcmicQubkhJVFq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\nToolMessage(content='5', name='magic_function', id='cbc9fadf-1962-4ed7-b476-348c774652be', tool_call_id='call_XfQD6C7rAalcmicQubkhJVFq'),\nAIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 78, 'total_tokens': 92, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None}, id='run-547e03d2-872d-4008-a38d-b7f739a77df5-0', usage_metadata={'input_tokens': 78, 'output_tokens': 14, 'total_tokens': 92, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}\nmax_iterations\nâ€‹\nIn LangChain\nâ€‹\nAgentExecutor\nimplements a\nmax_iterations\nparameter, allowing users to abort a run that exceeds a specified number of iterations.\n@tool\ndef\nmagic_function\n(\ninput\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Applies a magic function to an input.\"\"\"\nreturn\n\"Sorry, there was an error. Please try again.\"\ntools\n=\n[\nmagic_function\n]\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant. Respond only in Spanish.\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n# Placeholders fill up a **list** of messages\n(\n\"placeholder\"\n,\n\"{agent_scratchpad}\"\n)\n,\n]\n)\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n,\nverbose\n=\nTrue\n,\nmax_iterations\n=\n3\n,\n)\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\nquery\n}\n)\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3mLo siento, no puedo decirte directamente el valor de `magic_function(3)`. Si deseas, puedo usar la funciÃ³n mÃ¡gica para calcularlo. Â¿Te gustarÃ­a que lo hiciera?\u001b[0m\n\u001b[1m> Finished chain.\u001b[0m\n{'input': 'what is the value of magic_function(3)?',\n'output': 'Lo siento, no puedo decirte directamente el valor de `magic_function(3)`. Si deseas, puedo usar la funciÃ³n mÃ¡gica para calcularlo. Â¿Te gustarÃ­a que lo hiciera?'}\nIn LangGraph\nâ€‹\nIn LangGraph this is controlled via\nrecursion_limit\nconfiguration parameter.\nNote that in\nAgentExecutor\n, an \"iteration\" includes a full turn of tool invocation and execution. In LangGraph, each step contributes to the recursion limit, so we will need to multiply by two (and add one) to get equivalent results.\nIf the recursion limit is reached, LangGraph raises a specific exception type, that we can catch and manage similarly to AgentExecutor.\nfrom\nlanggraph\n.\nerrors\nimport\nGraphRecursionError\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nRECURSION_LIMIT\n=\n2\n*\n3\n+\n1\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n=\ntools\n)\ntry\n:\nfor\nchunk\nin\nlanggraph_agent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n,\n{\n\"recursion_limit\"\n:\nRECURSION_LIMIT\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nprint\n(\nchunk\n[\n\"messages\"\n]\n[\n-\n1\n]\n)\nexcept\nGraphRecursionError\n:\nprint\n(\n{\n\"input\"\n:\nquery\n,\n\"output\"\n:\n\"Agent stopped due to max iterations.\"\n}\n)\nAPI Reference:\ncreate_react_agent\ncontent='what is the value of magic_function(3)?' additional_kwargs={} response_metadata={} id='c2489fe8-e69c-4163-876d-3cce26b28521'\ncontent='' additional_kwargs={'tool_calls': [{'id': 'call_OyNTcO6SDAvZcBlIEknPRrTR', 'function': {'arguments': '{\"input\":\"3\"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-b65504bb-fa23-4f8a-8d6c-7edb6d16e7ff-0' tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_OyNTcO6SDAvZcBlIEknPRrTR', 'type': 'tool_call'}] usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\ncontent='Sorry, there was an error. Please try again.' name='magic_function' id='f00e0bff-54fe-4726-a1a7-127a59d8f7ed' tool_call_id='call_OyNTcO6SDAvZcBlIEknPRrTR'\ncontent=\"It seems there was an error when trying to compute the value of the magic function with input 3. Let's try again.\" additional_kwargs={'tool_calls': [{'id': 'call_Q020rQoJh4cnh8WglIMnDm4z', 'function': {'arguments': '{\"input\":\"3\"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 88, 'total_tokens': 128, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-556d8cb2-b47a-4826-b17d-b520982c2475-0' tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_Q020rQoJh4cnh8WglIMnDm4z', 'type': 'tool_call'}] usage_metadata={'input_tokens': 88, 'output_tokens': 40, 'total_tokens': 128, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\ncontent='Sorry, there was an error. Please try again.' name='magic_function' id='777212cd-8381-44db-9762-3f81951ea73e' tool_call_id='call_Q020rQoJh4cnh8WglIMnDm4z'\ncontent=\"It seems there is a persistent issue in computing the value of the magic function with the input 3. Unfortunately, I can't provide the value at this time. If you have any other questions or need further assistance, feel free to ask!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 150, 'total_tokens': 199, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None} id='run-92ec0b90-bc8e-4851-9139-f1d976145ab7-0' usage_metadata={'input_tokens': 150, 'output_tokens': 49, 'total_tokens': 199, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\nmax_execution_time\nâ€‹\nIn LangChain\nâ€‹\nAgentExecutor\nimplements a\nmax_execution_time\nparameter, allowing users to abort a run that exceeds a total time limit.\nimport\ntime\n@tool\ndef\nmagic_function\n(\ninput\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Applies a magic function to an input.\"\"\"\ntime\n.\nsleep\n(\n2.5\n)\nreturn\n\"Sorry, there was an error. Please try again.\"\ntools\n=\n[\nmagic_function\n]\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n,\nmax_execution_time\n=\n2\n,\nverbose\n=\nTrue\n,\n)\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\nquery\n}\n)\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3mLo siento, no tengo la capacidad de evaluar directamente una funciÃ³n llamada \"magic_function\" con el valor 3. Sin embargo, si me proporcionas mÃ¡s detalles sobre quÃ© hace la funciÃ³n o cÃ³mo estÃ¡ definida, podrÃ­a intentar ayudarte a comprender su comportamiento o resolverlo de otra manera.\u001b[0m\n\u001b[1m> Finished chain.\u001b[0m\n{'input': 'what is the value of magic_function(3)?',\n'output': 'Lo siento, no tengo la capacidad de evaluar directamente una funciÃ³n llamada \"magic_function\" con el valor 3. Sin embargo, si me proporcionas mÃ¡s detalles sobre quÃ© hace la funciÃ³n o cÃ³mo estÃ¡ definida, podrÃ­a intentar ayudarte a comprender su comportamiento o resolverlo de otra manera.'}\nIn LangGraph\nâ€‹\nWith LangGraph's react agent, you can control timeouts on two levels.\nYou can set a\nstep_timeout\nto bound each\nstep\n:\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n=\ntools\n)\n# Set the max timeout for each step here\nlanggraph_agent_executor\n.\nstep_timeout\n=\n2\ntry\n:\nfor\nchunk\nin\nlanggraph_agent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n)\n:\nprint\n(\nchunk\n)\nprint\n(\n\"------\"\n)\nexcept\nTimeoutError\n:\nprint\n(\n{\n\"input\"\n:\nquery\n,\n\"output\"\n:\n\"Agent stopped due to a step timeout.\"\n}\n)\nAPI Reference:\ncreate_react_agent\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UuxSgpGaqzX84sNlKzCVOiRO', 'function': {'arguments': '{\"input\":\"3\"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-24c94cbd-2962-48cf-a447-af888eb6ef86-0', tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_UuxSgpGaqzX84sNlKzCVOiRO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}\n------\n{'input': 'what is the value of magic_function(3)?', 'output': 'Agent stopped due to a step timeout.'}\nThe other way to set a single max timeout for an entire run is to directly use the python stdlib\nasyncio\nlibrary.\nimport\nasyncio\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n=\ntools\n)\nasync\ndef\nstream\n(\nlanggraph_agent_executor\n,\ninputs\n)\n:\nasync\nfor\nchunk\nin\nlanggraph_agent_executor\n.\nastream\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n)\n:\nprint\n(\nchunk\n)\nprint\n(\n\"------\"\n)\ntry\n:\ntask\n=\nasyncio\n.\ncreate_task\n(\nstream\n(\nlanggraph_agent_executor\n,\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n)\n)\nawait\nasyncio\n.\nwait_for\n(\ntask\n,\ntimeout\n=\n3\n)\nexcept\nasyncio\n.\nTimeoutError\n:\nprint\n(\n\"Task Cancelled.\"\n)\nAPI Reference:\ncreate_react_agent\n{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_km17xvoY7wJ5yNnXhb5V9D3I', 'function': {'arguments': '{\"input\":\"3\"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_45c6de4934', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b44a04e5-9b68-4020-be36-98de1593eefc-0', tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_km17xvoY7wJ5yNnXhb5V9D3I', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}\n------\nTask Cancelled.\nearly_stopping_method\nâ€‹\nIn LangChain\nâ€‹\nWith LangChain's\nAgentExecutor\n, you could configure an\nearly_stopping_method\nto either return a string saying \"Agent stopped due to iteration limit or time limit.\" (\n\"force\"\n) or prompt the LLM a final time to respond (\n\"generate\"\n).\nfrom\nlangchain\n.\nagents\nimport\nAgentExecutor\n,\ncreate_tool_calling_agent\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant.\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n# Placeholders fill up a **list** of messages\n(\n\"placeholder\"\n,\n\"{agent_scratchpad}\"\n)\n,\n]\n)\n@tool\ndef\nmagic_function\n(\ninput\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Applies a magic function to an input.\"\"\"\nreturn\n\"Sorry there was an error, please try again.\"\ntools\n=\n[\nmagic_function\n]\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\nprompt\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n,\nearly_stopping_method\n=\n\"force\"\n,\nmax_iterations\n=\n1\n)\nresult\n=\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\nquery\n}\n)\nprint\n(\n\"Output with early_stopping_method='force':\"\n)\nprint\n(\nresult\n[\n\"output\"\n]\n)\nAPI Reference:\nChatPromptTemplate\n|\ntool\nOutput with early_stopping_method='force':\nAgent stopped due to max iterations.\nIn LangGraph\nâ€‹\nIn LangGraph, you can explicitly handle the response behavior outside the agent, since the full state can be accessed.\nfrom\nlanggraph\n.\nerrors\nimport\nGraphRecursionError\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nRECURSION_LIMIT\n=\n2\n*\n1\n+\n1\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n=\ntools\n)\ntry\n:\nfor\nchunk\nin\nlanggraph_agent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n,\n{\n\"recursion_limit\"\n:\nRECURSION_LIMIT\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nprint\n(\nchunk\n[\n\"messages\"\n]\n[\n-\n1\n]\n)\nexcept\nGraphRecursionError\n:\nprint\n(\n{\n\"input\"\n:\nquery\n,\n\"output\"\n:\n\"Agent stopped due to max iterations.\"\n}\n)\nAPI Reference:\ncreate_react_agent\ncontent='what is the value of magic_function(3)?' additional_kwargs={} response_metadata={} id='81fd2e50-1e6a-4871-87aa-b7c1225913a4'\ncontent='' additional_kwargs={'tool_calls': [{'id': 'call_aaEzj3aO1RTnB0uoc9rYUIhi', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-476bc4b1-b7bf-4607-a31c-ddf09dc814c5-0' tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_aaEzj3aO1RTnB0uoc9rYUIhi', 'type': 'tool_call'}] usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\ncontent='Sorry there was an error, please try again.' name='magic_function' id='dcbe7e3e-0ed4-467d-a729-2f45916ff44f' tool_call_id='call_aaEzj3aO1RTnB0uoc9rYUIhi'\ncontent=\"It seems there was an error when trying to compute the value of `magic_function(3)`. Let's try that again.\" additional_kwargs={'tool_calls': [{'id': 'call_jr4R8uJn2pdXF5GZC2Dg3YWS', 'function': {'arguments': '{\"input\":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 87, 'total_tokens': 127, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-d94b8932-6e9e-4ab1-99f7-7dca89887ffe-0' tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_jr4R8uJn2pdXF5GZC2Dg3YWS', 'type': 'tool_call'}] usage_metadata={'input_tokens': 87, 'output_tokens': 40, 'total_tokens': 127, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n{'input': 'what is the value of magic_function(3)?', 'output': 'Agent stopped due to max iterations.'}\ntrim_intermediate_steps\nâ€‹\nIn LangChain\nâ€‹\nWith LangChain's\nAgentExecutor\n, you could trim the intermediate steps of long-running agents using\ntrim_intermediate_steps\n, which is either an integer (indicating the agent should keep the last N steps) or a custom function.\nFor instance, we could trim the value so the agent only sees the most recent intermediate step.\nfrom\nlangchain\n.\nagents\nimport\nAgentExecutor\n,\ncreate_tool_calling_agent\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant.\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n# Placeholders fill up a **list** of messages\n(\n\"placeholder\"\n,\n\"{agent_scratchpad}\"\n)\n,\n]\n)\nmagic_step_num\n=\n1\n@tool\ndef\nmagic_function\n(\ninput\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Applies a magic function to an input.\"\"\"\nglobal\nmagic_step_num\nprint\n(\nf\"Call number:\n{\nmagic_step_num\n}\n\"\n)\nmagic_step_num\n+=\n1\nreturn\ninput\n+\nmagic_step_num\ntools\n=\n[\nmagic_function\n]\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\nprompt\n)\ndef\ntrim_steps\n(\nsteps\n:\nlist\n)\n:\n# Let's give the agent amnesia\nreturn\n[\n]\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n,\ntrim_intermediate_steps\n=\ntrim_steps\n)\nquery\n=\n\"Call the magic function 4 times in sequence with the value 3. You cannot call it multiple times at once.\"\nfor\nstep\nin\nagent_executor\n.\nstream\n(\n{\n\"input\"\n:\nquery\n}\n)\n:\npass\nAPI Reference:\nChatPromptTemplate\n|\ntool\nCall number: 1\nCall number: 2\nCall number: 3\nCall number: 4\nCall number: 5\nCall number: 6\nCall number: 7\nCall number: 8\nCall number: 9\nCall number: 10\nCall number: 11\nCall number: 12\nCall number: 13\nCall number: 14\n``````output\nStopping agent prematurely due to triggering stop condition\n``````output\nCall number: 15\nIn LangGraph\nâ€‹\nWe can use the\nprompt\njust as before when passing in\nprompt templates\n.\nfrom\nlanggraph\n.\nerrors\nimport\nGraphRecursionError\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nfrom\nlanggraph\n.\nprebuilt\n.\nchat_agent_executor\nimport\nAgentState\nmagic_step_num\n=\n1\n@tool\ndef\nmagic_function\n(\ninput\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Applies a magic function to an input.\"\"\"\nglobal\nmagic_step_num\nprint\n(\nf\"Call number:\n{\nmagic_step_num\n}\n\"\n)\nmagic_step_num\n+=\n1\nreturn\ninput\n+\nmagic_step_num\ntools\n=\n[\nmagic_function\n]\ndef\n_modify_state_messages\n(\nstate\n:\nAgentState\n)\n:\n# Give the agent amnesia, only keeping the original user query\nreturn\n[\n(\n\"system\"\n,\n\"You are a helpful assistant\"\n)\n,\nstate\n[\n\"messages\"\n]\n[\n0\n]\n]\nlanggraph_agent_executor\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n,\nprompt\n=\n_modify_state_messages\n)\ntry\n:\nfor\nstep\nin\nlanggraph_agent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"human\"\n,\nquery\n)\n]\n}\n,\nstream_mode\n=\n\"updates\"\n)\n:\npass\nexcept\nGraphRecursionError\nas\ne\n:\nprint\n(\n\"Stopping agent prematurely due to triggering stop condition\"\n)\nAPI Reference:\ncreate_react_agent\nCall number: 1\nCall number: 2\nCall number: 3\nCall number: 4\nCall number: 5\nCall number: 6\nCall number: 7\nCall number: 8\nCall number: 9\nCall number: 10\nCall number: 11\nCall number: 12\nStopping agent prematurely due to triggering stop condition\nNext steps\nâ€‹\nYou've now learned how to migrate your LangChain agent executors to LangGraph.\nNext, check out other\nLangGraph how-to guides\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/multi_vector/",
    "How-to guides\nHow to retrieve using multiple vectors per document\nOn this page\nHow to retrieve using multiple vectors per document\nIt can often be useful to store multiple\nvectors\nper document. There are multiple use cases where this is beneficial. For example, we can\nembed\nmultiple chunks of a document and associate those embeddings with the parent document, allowing\nretriever\nhits on the chunks to return the larger document.\nLangChain implements a base\nMultiVectorRetriever\n, which simplifies this process. Much of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the\nMultiVectorRetriever\n.\nThe methods to create multiple vectors per document include:\nSmaller chunks: split a document into smaller chunks, and embed those (this is\nParentDocumentRetriever\n).\nSummary: create a summary for each document, embed that along with (or instead of) the document.\nHypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.\nNote that this also enables another method of adding embeddings - manually. This is useful because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control.\nBelow we walk through an example. First we instantiate some documents. We will index them in an (in-memory)\nChroma\nvector store using\nOpenAI\nembeddings, but any LangChain vector store or embeddings model will suffice.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain\n-\nchroma langchain langchain\n-\nopenai\n>\n/\ndev\n/\nnull\nfrom\nlangchain\n.\nstorage\nimport\nInMemoryByteStore\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nTextLoader\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nloaders\n=\n[\nTextLoader\n(\n\"paul_graham_essay.txt\"\n)\n,\nTextLoader\n(\n\"state_of_the_union.txt\"\n)\n,\n]\ndocs\n=\n[\n]\nfor\nloader\nin\nloaders\n:\ndocs\n.\nextend\n(\nloader\n.\nload\n(\n)\n)\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n10000\n)\ndocs\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\n# The vectorstore to use to index the child chunks\nvectorstore\n=\nChroma\n(\ncollection_name\n=\n\"full_documents\"\n,\nembedding_function\n=\nOpenAIEmbeddings\n(\n)\n)\nSmaller chunks\nâ€‹\nOften times it can be useful to retrieve larger chunks of information, but embed smaller chunks. This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the\nParentDocumentRetriever\ndoes. Here we show what is going on under the hood.\nWe will make a distinction between the vector store, which indexes embeddings of the (sub) documents, and the document store, which houses the \"parent\" documents and associates them with an identifier.\nimport\nuuid\nfrom\nlangchain\n.\nretrievers\n.\nmulti_vector\nimport\nMultiVectorRetriever\n# The storage layer for the parent documents\nstore\n=\nInMemoryByteStore\n(\n)\nid_key\n=\n\"doc_id\"\n# The retriever (empty to start)\nretriever\n=\nMultiVectorRetriever\n(\nvectorstore\n=\nvectorstore\n,\nbyte_store\n=\nstore\n,\nid_key\n=\nid_key\n,\n)\ndoc_ids\n=\n[\nstr\n(\nuuid\n.\nuuid4\n(\n)\n)\nfor\n_\nin\ndocs\n]\nWe next generate the \"sub\" documents by splitting the original documents. Note that we store the document identifier in the\nmetadata\nof the corresponding\nDocument\nobject.\n# The splitter to use to create smaller chunks\nchild_text_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n400\n)\nsub_docs\n=\n[\n]\nfor\ni\n,\ndoc\nin\nenumerate\n(\ndocs\n)\n:\n_id\n=\ndoc_ids\n[\ni\n]\n_sub_docs\n=\nchild_text_splitter\n.\nsplit_documents\n(\n[\ndoc\n]\n)\nfor\n_doc\nin\n_sub_docs\n:\n_doc\n.\nmetadata\n[\nid_key\n]\n=\n_id\nsub_docs\n.\nextend\n(\n_sub_docs\n)\nFinally, we index the documents in our vector store and document store:\nretriever\n.\nvectorstore\n.\nadd_documents\n(\nsub_docs\n)\nretriever\n.\ndocstore\n.\nmset\n(\nlist\n(\nzip\n(\ndoc_ids\n,\ndocs\n)\n)\n)\nThe vector store alone will retrieve small chunks:\nretriever\n.\nvectorstore\n.\nsimilarity_search\n(\n\"justice breyer\"\n)\n[\n0\n]\nDocument(page_content='Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '064eca46-a4c4-4789-8e3b-583f9597e54f', 'source': 'state_of_the_union.txt'})\nWhereas the retriever will return the larger parent document:\nlen\n(\nretriever\n.\ninvoke\n(\n\"justice breyer\"\n)\n[\n0\n]\n.\npage_content\n)\n9875\nThe default search type the retriever performs on the vector database is a similarity search. LangChain vector stores also support searching via\nMax Marginal Relevance\n. This can be controlled via the\nsearch_type\nparameter of the retriever:\nfrom\nlangchain\n.\nretrievers\n.\nmulti_vector\nimport\nSearchType\nretriever\n.\nsearch_type\n=\nSearchType\n.\nmmr\nlen\n(\nretriever\n.\ninvoke\n(\n\"justice breyer\"\n)\n[\n0\n]\n.\npage_content\n)\n9875\nAssociating summaries with a document for retrieval\nâ€‹\nA summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.\nWe construct a simple\nchain\nthat will receive an input\nDocument\nobject and generate a summary using a LLM.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nimport\nuuid\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nchain\n=\n(\n{\n\"doc\"\n:\nlambda\nx\n:\nx\n.\npage_content\n}\n|\nChatPromptTemplate\n.\nfrom_template\n(\n\"Summarize the following document:\\n\\n{doc}\"\n)\n|\nllm\n|\nStrOutputParser\n(\n)\n)\nAPI Reference:\nDocument\n|\nStrOutputParser\n|\nChatPromptTemplate\nNote that we can\nbatch\nthe chain across documents:\nsummaries\n=\nchain\n.\nbatch\n(\ndocs\n,\n{\n\"max_concurrency\"\n:\n5\n}\n)\nWe can then initialize a\nMultiVectorRetriever\nas before, indexing the summaries in our vector store, and retaining the original documents in our document store:\n# The vectorstore to use to index the child chunks\nvectorstore\n=\nChroma\n(\ncollection_name\n=\n\"summaries\"\n,\nembedding_function\n=\nOpenAIEmbeddings\n(\n)\n)\n# The storage layer for the parent documents\nstore\n=\nInMemoryByteStore\n(\n)\nid_key\n=\n\"doc_id\"\n# The retriever (empty to start)\nretriever\n=\nMultiVectorRetriever\n(\nvectorstore\n=\nvectorstore\n,\nbyte_store\n=\nstore\n,\nid_key\n=\nid_key\n,\n)\ndoc_ids\n=\n[\nstr\n(\nuuid\n.\nuuid4\n(\n)\n)\nfor\n_\nin\ndocs\n]\nsummary_docs\n=\n[\nDocument\n(\npage_content\n=\ns\n,\nmetadata\n=\n{\nid_key\n:\ndoc_ids\n[\ni\n]\n}\n)\nfor\ni\n,\ns\nin\nenumerate\n(\nsummaries\n)\n]\nretriever\n.\nvectorstore\n.\nadd_documents\n(\nsummary_docs\n)\nretriever\n.\ndocstore\n.\nmset\n(\nlist\n(\nzip\n(\ndoc_ids\n,\ndocs\n)\n)\n)\n# # We can also add the original chunks to the vectorstore if we so want\n# for i, doc in enumerate(docs):\n#     doc.metadata[id_key] = doc_ids[i]\n# retriever.vectorstore.add_documents(docs)\nQuerying the vector store will return summaries:\nsub_docs\n=\nretriever\n.\nvectorstore\n.\nsimilarity_search\n(\n\"justice breyer\"\n)\nsub_docs\n[\n0\n]\nDocument(page_content=\"President Biden recently nominated Judge Ketanji Brown Jackson to serve on the United States Supreme Court, emphasizing her qualifications and broad support. The President also outlined a plan to secure the border, fix the immigration system, protect women's rights, support LGBTQ+ Americans, and advance mental health services. He highlighted the importance of bipartisan unity in passing legislation, such as the Violence Against Women Act. The President also addressed supporting veterans, particularly those impacted by exposure to burn pits, and announced plans to expand benefits for veterans with respiratory cancers. Additionally, he proposed a plan to end cancer as we know it through the Cancer Moonshot initiative. President Biden expressed optimism about the future of America and emphasized the strength of the American people in overcoming challenges.\", metadata={'doc_id': '84015b1b-980e-400a-94d8-cf95d7e079bd'})\nWhereas the retriever will return the larger source document:\nretrieved_docs\n=\nretriever\n.\ninvoke\n(\n\"justice breyer\"\n)\nlen\n(\nretrieved_docs\n[\n0\n]\n.\npage_content\n)\n9194\nHypothetical Queries\nâ€‹\nAn LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document, which might bear close semantic similarity to relevant queries in a\nRAG\napplication. These questions can then be embedded and associated with the documents to improve retrieval.\nBelow, we use the\nwith_structured_output\nmethod to structure the LLM output into a list of strings.\nfrom\ntyping\nimport\nList\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nHypotheticalQuestions\n(\nBaseModel\n)\n:\n\"\"\"Generate hypothetical questions.\"\"\"\nquestions\n:\nList\n[\nstr\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"List of questions\"\n)\nchain\n=\n(\n{\n\"doc\"\n:\nlambda\nx\n:\nx\n.\npage_content\n}\n# Only asking for 3 hypothetical questions, but this could be adjusted\n|\nChatPromptTemplate\n.\nfrom_template\n(\n\"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n)\n|\nChatOpenAI\n(\nmax_retries\n=\n0\n,\nmodel\n=\n\"gpt-4o\"\n)\n.\nwith_structured_output\n(\nHypotheticalQuestions\n)\n|\n(\nlambda\nx\n:\nx\n.\nquestions\n)\n)\nInvoking the chain on a single document demonstrates that it outputs a list of questions:\nchain\n.\ninvoke\n(\ndocs\n[\n0\n]\n)\n[\"What impact did the IBM 1401 have on the author's early programming experiences?\",\n\"How did the transition from using the IBM 1401 to microcomputers influence the author's programming journey?\",\n\"What role did Lisp play in shaping the author's understanding and approach to AI?\"]\nWe can batch then batch the chain over all documents and assemble our vector store and document store as before:\n# Batch chain over documents to generate hypothetical questions\nhypothetical_questions\n=\nchain\n.\nbatch\n(\ndocs\n,\n{\n\"max_concurrency\"\n:\n5\n}\n)\n# The vectorstore to use to index the child chunks\nvectorstore\n=\nChroma\n(\ncollection_name\n=\n\"hypo-questions\"\n,\nembedding_function\n=\nOpenAIEmbeddings\n(\n)\n)\n# The storage layer for the parent documents\nstore\n=\nInMemoryByteStore\n(\n)\nid_key\n=\n\"doc_id\"\n# The retriever (empty to start)\nretriever\n=\nMultiVectorRetriever\n(\nvectorstore\n=\nvectorstore\n,\nbyte_store\n=\nstore\n,\nid_key\n=\nid_key\n,\n)\ndoc_ids\n=\n[\nstr\n(\nuuid\n.\nuuid4\n(\n)\n)\nfor\n_\nin\ndocs\n]\n# Generate Document objects from hypothetical questions\nquestion_docs\n=\n[\n]\nfor\ni\n,\nquestion_list\nin\nenumerate\n(\nhypothetical_questions\n)\n:\nquestion_docs\n.\nextend\n(\n[\nDocument\n(\npage_content\n=\ns\n,\nmetadata\n=\n{\nid_key\n:\ndoc_ids\n[\ni\n]\n}\n)\nfor\ns\nin\nquestion_list\n]\n)\nretriever\n.\nvectorstore\n.\nadd_documents\n(\nquestion_docs\n)\nretriever\n.\ndocstore\n.\nmset\n(\nlist\n(\nzip\n(\ndoc_ids\n,\ndocs\n)\n)\n)\nNote that querying the underlying vector store will retrieve hypothetical questions that are semantically similar to the input query:\nsub_docs\n=\nretriever\n.\nvectorstore\n.\nsimilarity_search\n(\n\"justice breyer\"\n)\nsub_docs\n[Document(page_content='What might be the potential benefits of nominating Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court?', metadata={'doc_id': '43292b74-d1b8-4200-8a8b-ea0cb57fbcdb'}),\nDocument(page_content='How might the Bipartisan Infrastructure Law impact the economic competition between the U.S. and China?', metadata={'doc_id': '66174780-d00c-4166-9791-f0069846e734'}),\nDocument(page_content='What factors led to the creation of Y Combinator?', metadata={'doc_id': '72003c4e-4cc9-4f09-a787-0b541a65b38c'}),\nDocument(page_content='How did the ability to publish essays online change the landscape for writers and thinkers?', metadata={'doc_id': 'e8d2c648-f245-4bcc-b8d3-14e64a164b64'})]\nAnd invoking the retriever will return the corresponding document:\nretrieved_docs\n=\nretriever\n.\ninvoke\n(\n\"justice breyer\"\n)\nlen\n(\nretrieved_docs\n[\n0\n]\n.\npage_content\n)\n9194\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/multimodal_inputs/",
    "How-to guides\nHow to pass multimodal data to models\nOn this page\nHow to pass multimodal data to models\nHere we demonstrate how to pass\nmultimodal\ninput directly to models.\nLangChain supports multimodal data as input to chat models:\nFollowing provider-specific formats\nAdhering to a cross-provider standard\nBelow, we demonstrate the cross-provider standard. See\nchat model integrations\nfor detail\non native formats for specific providers.\nnote\nMost chat models that support multimodal\nimage\ninputs also accept those values in\nOpenAI's\nChat Completions format\n:\n{\n\"type\"\n:\n\"image_url\"\n,\n\"image_url\"\n:\n{\n\"url\"\n:\nimage_url\n}\n,\n}\nImages\nâ€‹\nMany providers will accept images passed in-line as base64 data. Some will additionally accept an image from a URL directly.\nImages from base64 data\nâ€‹\nTo pass images in-line, format them as content blocks of the following form:\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"mime_type\"\n:\n\"image/jpeg\"\n,\n# or image/png, etc.\n\"data\"\n:\n\"<base64 data string>\"\n,\n}\nExample:\nimport\nbase64\nimport\nhttpx\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\n# Fetch image data\nimage_url\n=\n\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\nimage_data\n=\nbase64\n.\nb64encode\n(\nhttpx\n.\nget\n(\nimage_url\n)\n.\ncontent\n)\n.\ndecode\n(\n\"utf-8\"\n)\n# Pass to LLM\nllm\n=\ninit_chat_model\n(\n\"anthropic:claude-3-5-sonnet-latest\"\n)\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the weather in this image:\"\n,\n}\n,\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"data\"\n:\nimage_data\n,\n\"mime_type\"\n:\n\"image/jpeg\"\n,\n}\n,\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nThe image shows a beautiful clear day with bright blue skies and wispy cirrus clouds stretching across the horizon. The clouds are thin and streaky, creating elegant patterns against the blue backdrop. The lighting suggests it's during the day, possibly late afternoon given the warm, golden quality of the light on the grass. The weather appears calm with no signs of wind (the grass looks relatively still) and no indication of rain. It's the kind of perfect, mild weather that's ideal for walking along the wooden boardwalk through the marsh grass.\nSee\nLangSmith trace\nfor more detail.\nImages from a URL\nâ€‹\nSome providers (including\nOpenAI\n,\nAnthropic\n, and\nGoogle Gemini\n) will also accept images from URLs directly.\nTo pass images as URLs, format them as content blocks of the following form:\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\n\"https://...\"\n,\n}\nExample:\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the weather in this image:\"\n,\n}\n,\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\nimage_url\n,\n}\n,\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nThe weather in this image appears to be pleasant and clear. The sky is mostly blue with a few scattered, light clouds, and there is bright sunlight illuminating the green grass and plants. There are no signs of rain or stormy conditions, suggesting it is a calm, likely warm dayâ€”typical of spring or summer.\nWe can also pass in multiple images:\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Are these two images the same?\"\n}\n,\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\nimage_url\n}\n,\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\nimage_url\n}\n,\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nYes, these two images are the same. They depict a wooden boardwalk going through a grassy field under a blue sky with some clouds. The colors, composition, and elements in both images are identical.\nDocuments (PDF)\nâ€‹\nSome providers (including\nOpenAI\n,\nAnthropic\n, and\nGoogle Gemini\n) will accept PDF documents.\nnote\nOpenAI requires file-names be specified for PDF inputs. When using LangChain's format, include the\nfilename\nkey. See\nexample below\n.\nDocuments from base64 data\nâ€‹\nTo pass documents in-line, format them as content blocks of the following form:\n{\n\"type\"\n:\n\"file\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"mime_type\"\n:\n\"application/pdf\"\n,\n\"data\"\n:\n\"<base64 data string>\"\n,\n}\nExample:\nimport\nbase64\nimport\nhttpx\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\n# Fetch PDF data\npdf_url\n=\n\"https://pdfobject.com/pdf/sample.pdf\"\npdf_data\n=\nbase64\n.\nb64encode\n(\nhttpx\n.\nget\n(\npdf_url\n)\n.\ncontent\n)\n.\ndecode\n(\n\"utf-8\"\n)\n# Pass to LLM\nllm\n=\ninit_chat_model\n(\n\"anthropic:claude-3-5-sonnet-latest\"\n)\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the document:\"\n,\n}\n,\n{\n\"type\"\n:\n\"file\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"data\"\n:\npdf_data\n,\n\"mime_type\"\n:\n\"application/pdf\"\n,\n}\n,\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nThis document appears to be a sample PDF file that contains Lorem ipsum placeholder text. It begins with a title \"Sample PDF\" followed by the subtitle \"This is a simple PDF file. Fun fun fun.\"\nThe rest of the document consists of several paragraphs of Lorem ipsum text, which is a commonly used placeholder text in design and publishing. The text is formatted in a clean, readable layout with consistent paragraph spacing. The document appears to be a single page containing four main paragraphs of this placeholder text.\nThe Lorem ipsum text, while appearing to be Latin, is actually scrambled Latin-like text that is used primarily to demonstrate the visual form of a document or typeface without the distraction of meaningful content. It's commonly used in publishing and graphic design when the actual content is not yet available but the layout needs to be demonstrated.\nThe document has a professional, simple layout with generous margins and clear paragraph separation, making it an effective example of basic PDF formatting and structure.\nDocuments from a URL\nâ€‹\nSome providers (specifically\nAnthropic\n)\nwill also accept documents from URLs directly.\nTo pass documents as URLs, format them as content blocks of the following form:\n{\n\"type\"\n:\n\"file\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\n\"https://...\"\n,\n}\nExample:\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the document:\"\n,\n}\n,\n{\n\"type\"\n:\n\"file\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\npdf_url\n,\n}\n,\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nThis document appears to be a sample PDF file with both text and an image. It begins with a title \"Sample PDF\" followed by the text \"This is a simple PDF file. Fun fun fun.\" The rest of the document contains Lorem ipsum placeholder text arranged in several paragraphs. The content is shown both as text and as an image of the formatted PDF, with the same content displayed in a clean, formatted layout with consistent spacing and typography. The document consists of a single page containing this sample text.\nAudio\nâ€‹\nSome providers (including\nOpenAI\nand\nGoogle Gemini\n) will accept audio inputs.\nAudio from base64 data\nâ€‹\nTo pass audio in-line, format them as content blocks of the following form:\n{\n\"type\"\n:\n\"audio\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"mime_type\"\n:\n\"audio/wav\"\n,\n# or appropriate mime-type\n\"data\"\n:\n\"<base64 data string>\"\n,\n}\nExample:\nimport\nbase64\nimport\nhttpx\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\n# Fetch audio data\naudio_url\n=\n\"https://upload.wikimedia.org/wikipedia/commons/3/3d/Alcal%C3%A1_de_Henares_%28RPS_13-04-2024%29_canto_de_ruise%C3%B1or_%28Luscinia_megarhynchos%29_en_el_Soto_del_Henares.wav\"\naudio_data\n=\nbase64\n.\nb64encode\n(\nhttpx\n.\nget\n(\naudio_url\n)\n.\ncontent\n)\n.\ndecode\n(\n\"utf-8\"\n)\n# Pass to LLM\nllm\n=\ninit_chat_model\n(\n\"google_genai:gemini-2.5-flash\"\n)\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe this audio:\"\n,\n}\n,\n{\n\"type\"\n:\n\"audio\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"data\"\n:\naudio_data\n,\n\"mime_type\"\n:\n\"audio/wav\"\n,\n}\n,\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nThe audio appears to consist primarily of bird sounds, specifically bird vocalizations like chirping and possibly other bird songs.\nProvider-specific parameters\nâ€‹\nSome providers will support or require additional fields on content blocks containing multimodal data.\nFor example, Anthropic lets you specify\ncaching\nof\nspecific content to reduce token consumption.\nTo use these fields, you can:\nStore them on directly on the content block; or\nUse the native format supported by each provider (see\nchat model integrations\nfor detail).\nWe show three examples below.\nExample: Anthropic prompt caching\nâ€‹\nllm\n=\ninit_chat_model\n(\n\"anthropic:claude-3-5-sonnet-latest\"\n)\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the weather in this image:\"\n,\n}\n,\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\nimage_url\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n,\n}\n,\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nresponse\n.\nusage_metadata\nThe image shows a beautiful, clear day with partly cloudy skies. The sky is a vibrant blue with wispy, white cirrus clouds stretching across it. The lighting suggests it's during daylight hours, possibly late afternoon or early evening given the warm, golden quality of the light on the grass. The weather appears calm with no signs of wind (the grass looks relatively still) and no threatening weather conditions. It's the kind of perfect weather you'd want for a walk along this wooden boardwalk through the marshland or grassland area.\n{'input_tokens': 1586,\n'output_tokens': 117,\n'total_tokens': 1703,\n'input_token_details': {'cache_read': 0, 'cache_creation': 1582}}\nnext_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Summarize that in 5 words.\"\n,\n}\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n,\nresponse\n,\nnext_message\n]\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nresponse\n.\nusage_metadata\nClear blue skies, wispy clouds.\n{'input_tokens': 1716,\n'output_tokens': 12,\n'total_tokens': 1728,\n'input_token_details': {'cache_read': 1582, 'cache_creation': 0}}\nExample: Anthropic citations\nâ€‹\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Generate a 5 word summary of this document.\"\n,\n}\n,\n{\n\"type\"\n:\n\"file\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"data\"\n:\npdf_data\n,\n\"mime_type\"\n:\n\"application/pdf\"\n,\n\"citations\"\n:\n{\n\"enabled\"\n:\nTrue\n}\n,\n}\n,\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nresponse\n.\ncontent\n[{'citations': [{'cited_text': 'Sample PDF\\r\\nThis is a simple PDF file. Fun fun fun.\\r\\n',\n'document_index': 0,\n'document_title': None,\n'end_page_number': 2,\n'start_page_number': 1,\n'type': 'page_location'}],\n'text': 'Simple PDF file: fun fun',\n'type': 'text'}]\nExample: OpenAI file names\nâ€‹\nOpenAI requires that PDF documents be associated with file names:\nllm\n=\ninit_chat_model\n(\n\"openai:gpt-4.1\"\n)\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the document:\"\n,\n}\n,\n{\n\"type\"\n:\n\"file\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"data\"\n:\npdf_data\n,\n\"mime_type\"\n:\n\"application/pdf\"\n,\n\"filename\"\n:\n\"my-file\"\n,\n}\n,\n]\n,\n}\nresponse\n=\nllm\n.\ninvoke\n(\n[\nmessage\n]\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nThe document is a sample PDF file containing placeholder text. It consists of one page, titled \"Sample PDF\". The content is a mixture of English and the commonly used filler text \"Lorem ipsum dolor sit amet...\" and its extensions, which are often used in publishing and web design as generic text to demonstrate font, layout, and other visual elements.\n**Key points about the document:**\n- Length: 1 page\n- Purpose: Demonstrative/sample content\n- Content: No substantive or meaningful information, just demonstration text in paragraph form\n- Language: English (with the Latin-like \"Lorem Ipsum\" text used for layout purposes)\nThere are no charts, tables, diagrams, or images on the pageâ€”only plain text. The document serves as an example of what a PDF file looks like rather than providing actual, useful content.\nTool calls\nâ€‹\nSome multimodal models support\ntool calling\nfeatures as well. To call tools using such models, simply bind tools to them in the\nusual way\n, and invoke the model using content blocks of the desired type (e.g., containing image data).\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nweather_tool\n(\nweather\n:\nLiteral\n[\n\"sunny\"\n,\n\"cloudy\"\n,\n\"rainy\"\n]\n)\n-\n>\nNone\n:\n\"\"\"Describe the weather\"\"\"\npass\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nweather_tool\n]\n)\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the weather in this image:\"\n}\n,\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\nimage_url\n}\n,\n]\n,\n}\nresponse\n=\nllm_with_tools\n.\ninvoke\n(\n[\nmessage\n]\n)\nresponse\n.\ntool_calls\nAPI Reference:\ntool\n[{'name': 'weather_tool',\n'args': {'weather': 'sunny'},\n'id': 'toolu_01G6JgdkhwggKcQKfhXZQPjf',\n'type': 'tool_call'}]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/multimodal_prompts/",
    "How-to guides\nHow to use multimodal prompts\nHow to use multimodal prompts\nHere we demonstrate how to use prompt templates to format\nmultimodal\ninputs to models.\nTo use prompt templates in the context of multimodal data, we can templatize elements of the corresponding content block.\nFor example, below we define a prompt that takes a URL for an image as a parameter:\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n# Define prompt\nprompt\n=\nChatPromptTemplate\n(\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"Describe the image provided.\"\n,\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\n\"{image_url}\"\n,\n}\n,\n]\n,\n}\n,\n]\n)\nAPI Reference:\nChatPromptTemplate\nLet's use this prompt to pass an image to a\nchat model\n:\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"anthropic:claude-3-5-sonnet-latest\"\n)\nurl\n=\n\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\nchain\n=\nprompt\n|\nllm\nresponse\n=\nchain\n.\ninvoke\n(\n{\n\"image_url\"\n:\nurl\n}\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nThis image shows a beautiful wooden boardwalk cutting through a lush green wetland or marsh area. The boardwalk extends straight ahead toward the horizon, creating a strong leading line through the composition. On either side, tall green grasses sway in what appears to be a summer or late spring setting. The sky is particularly striking, with wispy cirrus clouds streaking across a vibrant blue background. In the distance, you can see a tree line bordering the wetland area. The lighting suggests this may be during \"golden hour\" - either early morning or late afternoon - as there's a warm, gentle quality to the light that's illuminating the scene. The wooden planks of the boardwalk appear well-maintained and provide safe passage through what would otherwise be difficult terrain to traverse. It's the kind of scene you might find in a nature preserve or wildlife refuge designed to give visitors access to observe wetland ecosystems while protecting the natural environment.\nNote that we can templatize arbitrary elements of the content block:\nprompt\n=\nChatPromptTemplate\n(\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"Describe the image provided.\"\n,\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"mime_type\"\n:\n\"{image_mime_type}\"\n,\n\"data\"\n:\n\"{image_data}\"\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"{cache_type}\"\n}\n,\n}\n,\n]\n,\n}\n,\n]\n)\nimport\nbase64\nimport\nhttpx\nimage_data\n=\nbase64\n.\nb64encode\n(\nhttpx\n.\nget\n(\nurl\n)\n.\ncontent\n)\n.\ndecode\n(\n\"utf-8\"\n)\nchain\n=\nprompt\n|\nllm\nresponse\n=\nchain\n.\ninvoke\n(\n{\n\"image_data\"\n:\nimage_data\n,\n\"image_mime_type\"\n:\n\"image/jpeg\"\n,\n\"cache_type\"\n:\n\"ephemeral\"\n,\n}\n)\nprint\n(\nresponse\n.\ntext\n(\n)\n)\nThis image shows a beautiful wooden boardwalk cutting through a lush green marsh or wetland area. The boardwalk extends straight ahead toward the horizon, creating a strong leading line in the composition. The surrounding vegetation consists of tall grass and reeds in vibrant green hues, with some bushes and trees visible in the background. The sky is particularly striking, featuring a bright blue color with wispy white clouds streaked across it. The lighting suggests this photo was taken during the \"golden hour\" - either early morning or late afternoon - giving the scene a warm, peaceful quality. The raised wooden path provides accessible access through what would otherwise be difficult terrain to traverse, allowing visitors to experience and appreciate this natural environment.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/output_parser_custom/",
    "How-to guides\nHow to create a custom Output Parser\nOn this page\nHow to create a custom Output Parser\nIn some situations you may want to implement a custom\nparser\nto structure the model output into a custom format.\nThere are two ways to implement a custom parser:\nUsing\nRunnableLambda\nor\nRunnableGenerator\nin\nLCEL\n-- we strongly recommend this for most use cases\nBy inheriting from one of the base classes for out parsing -- this is the hard way of doing things\nThe difference between the two approaches are mostly superficial and are mainly in terms of which callbacks are triggered (e.g.,\non_chain_start\nvs.\non_parser_start\n), and how a runnable lambda vs. a parser might be visualized in a tracing platform like LangSmith.\nRunnable Lambdas and Generators\nâ€‹\nThe recommended way to parse is using\nrunnable lambdas\nand\nrunnable generators\n!\nHere, we will make a simple parse that inverts the case of the output from the model.\nFor example, if the model outputs: \"Meow\", the parser will produce \"mEOW\".\nfrom\ntyping\nimport\nIterable\nfrom\nlangchain_anthropic\n.\nchat_models\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nAIMessageChunk\nmodel\n=\nChatAnthropic\n(\nmodel_name\n=\n\"claude-2.1\"\n)\ndef\nparse\n(\nai_message\n:\nAIMessage\n)\n-\n>\nstr\n:\n\"\"\"Parse the AI message.\"\"\"\nreturn\nai_message\n.\ncontent\n.\nswapcase\n(\n)\nchain\n=\nmodel\n|\nparse\nchain\n.\ninvoke\n(\n\"hello\"\n)\nAPI Reference:\nAIMessage\n|\nAIMessageChunk\n'hELLO!'\ntip\nLCEL automatically upgrades the function\nparse\nto\nRunnableLambda(parse)\nwhen composed using a\n|\nsyntax.\nIf you don't like that you can manually import\nRunnableLambda\nand then run\nparse = RunnableLambda(parse)\n.\nDoes streaming work?\nfor\nchunk\nin\nchain\n.\nstream\n(\n\"tell me about yourself in one sentence\"\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\ni'M cLAUDE, AN ai ASSISTANT CREATED BY aNTHROPIC TO BE HELPFUL, HARMLESS, AND HONEST.|\nNo, it doesn't because the parser aggregates the input before parsing the output.\nIf we want to implement a streaming parser, we can have the parser accept an iterable over the input instead and yield\nthe results as they're available.\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableGenerator\ndef\nstreaming_parse\n(\nchunks\n:\nIterable\n[\nAIMessageChunk\n]\n)\n-\n>\nIterable\n[\nstr\n]\n:\nfor\nchunk\nin\nchunks\n:\nyield\nchunk\n.\ncontent\n.\nswapcase\n(\n)\nstreaming_parse\n=\nRunnableGenerator\n(\nstreaming_parse\n)\nAPI Reference:\nRunnableGenerator\nimportant\nPlease wrap the streaming parser in\nRunnableGenerator\nas we may stop automatically upgrading it with the\n|\nsyntax.\nchain\n=\nmodel\n|\nstreaming_parse\nchain\n.\ninvoke\n(\n\"hello\"\n)\n'hELLO!'\nLet's confirm that streaming works!\nfor\nchunk\nin\nchain\n.\nstream\n(\n\"tell me about yourself in one sentence\"\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\ni|'M| cLAUDE|,| AN| ai| ASSISTANT| CREATED| BY| aN|THROP|IC| TO| BE| HELPFUL|,| HARMLESS|,| AND| HONEST|.|\nInheriting from Parsing Base Classes\nâ€‹\nAnother approach to implement a parser is by inheriting from\nBaseOutputParser\n,\nBaseGenerationOutputParser\nor another one of the base parsers depending on what you need to do.\nIn general, we\ndo not\nrecommend this approach for most use cases as it results in more code to write without significant benefits.\nThe simplest kind of output parser extends the\nBaseOutputParser\nclass and must implement the following methods:\nparse\n: takes the string output from the model and parses it\n(optional)\n_type\n: identifies the name of the parser.\nWhen the output from the chat model or LLM is malformed, the can throw an\nOutputParserException\nto indicate that parsing fails because of bad input. Using this exception allows code that utilizes the parser to handle the exceptions in a consistent manner.\nParsers are Runnables! ðŸƒ\nBecause\nBaseOutputParser\nimplements the\nRunnable\ninterface, any custom parser you will create this way will become valid LangChain Runnables and will benefit from automatic async support, batch interface, logging support etc.\nSimple Parser\nâ€‹\nHere's a simple parser that can parse a\nstring\nrepresentation of a boolean (e.g.,\nYES\nor\nNO\n) and convert it into the corresponding\nboolean\ntype.\nfrom\nlangchain_core\n.\nexceptions\nimport\nOutputParserException\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nBaseOutputParser\n# The [bool] desribes a parameterization of a generic.\n# It's basically indicating what the return type of parse is\n# in this case the return type is either True or False\nclass\nBooleanOutputParser\n(\nBaseOutputParser\n[\nbool\n]\n)\n:\n\"\"\"Custom boolean parser.\"\"\"\ntrue_val\n:\nstr\n=\n\"YES\"\nfalse_val\n:\nstr\n=\n\"NO\"\ndef\nparse\n(\nself\n,\ntext\n:\nstr\n)\n-\n>\nbool\n:\ncleaned_text\n=\ntext\n.\nstrip\n(\n)\n.\nupper\n(\n)\nif\ncleaned_text\nnot\nin\n(\nself\n.\ntrue_val\n.\nupper\n(\n)\n,\nself\n.\nfalse_val\n.\nupper\n(\n)\n)\n:\nraise\nOutputParserException\n(\nf\"BooleanOutputParser expected output value to either be \"\nf\"\n{\nself\n.\ntrue_val\n}\nor\n{\nself\n.\nfalse_val\n}\n(case-insensitive). \"\nf\"Received\n{\ncleaned_text\n}\n.\"\n)\nreturn\ncleaned_text\n==\nself\n.\ntrue_val\n.\nupper\n(\n)\n@property\ndef\n_type\n(\nself\n)\n-\n>\nstr\n:\nreturn\n\"boolean_output_parser\"\nAPI Reference:\nOutputParserException\n|\nBaseOutputParser\nparser\n=\nBooleanOutputParser\n(\n)\nparser\n.\ninvoke\n(\n\"YES\"\n)\nTrue\ntry\n:\nparser\n.\ninvoke\n(\n\"MEOW\"\n)\nexcept\nException\nas\ne\n:\nprint\n(\nf\"Triggered an exception of type:\n{\ntype\n(\ne\n)\n}\n\"\n)\nTriggered an exception of type: <class 'langchain_core.exceptions.OutputParserException'>\nLet's test changing the parameterization\nparser\n=\nBooleanOutputParser\n(\ntrue_val\n=\n\"OKAY\"\n)\nparser\n.\ninvoke\n(\n\"OKAY\"\n)\nTrue\nLet's confirm that other LCEL methods are present\nparser\n.\nbatch\n(\n[\n\"OKAY\"\n,\n\"NO\"\n]\n)\n[True, False]\nawait\nparser\n.\nabatch\n(\n[\n\"OKAY\"\n,\n\"NO\"\n]\n)\n[True, False]\nfrom\nlangchain_anthropic\n.\nchat_models\nimport\nChatAnthropic\nanthropic\n=\nChatAnthropic\n(\nmodel_name\n=\n\"claude-2.1\"\n)\nanthropic\n.\ninvoke\n(\n\"say OKAY or NO\"\n)\nAIMessage(content='OKAY')\nLet's test that our parser works!\nchain\n=\nanthropic\n|\nparser\nchain\n.\ninvoke\n(\n\"say OKAY or NO\"\n)\nTrue\nnote\nThe parser will work with either the output from an LLM (a string) or the output from a chat model (an\nAIMessage\n)!\nParsing Raw Model Outputs\nâ€‹\nSometimes there is additional metadata on the model output that is important besides the raw text. One example of this is tool calling, where arguments intended to be passed to called functions are returned in a separate property. If you need this finer-grained control, you can instead subclass the\nBaseGenerationOutputParser\nclass.\nThis class requires a single method\nparse_result\n. This method takes raw model output (e.g., list of\nGeneration\nor\nChatGeneration\n) and returns the parsed output.\nSupporting both\nGeneration\nand\nChatGeneration\nallows the parser to work with both regular LLMs as well as with Chat Models.\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\nexceptions\nimport\nOutputParserException\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nBaseGenerationOutputParser\nfrom\nlangchain_core\n.\noutputs\nimport\nChatGeneration\n,\nGeneration\nclass\nStrInvertCase\n(\nBaseGenerationOutputParser\n[\nstr\n]\n)\n:\n\"\"\"An example parser that inverts the case of the characters in the message.\nThis is an example parse shown just for demonstration purposes and to keep\nthe example as simple as possible.\n\"\"\"\ndef\nparse_result\n(\nself\n,\nresult\n:\nList\n[\nGeneration\n]\n,\n*\n,\npartial\n:\nbool\n=\nFalse\n)\n-\n>\nstr\n:\n\"\"\"Parse a list of model Generations into a specific format.\nArgs:\nresult: A list of Generations to be parsed. The Generations are assumed\nto be different candidate outputs for a single model input.\nMany parsers assume that only a single generation is passed it in.\nWe will assert for that\npartial: Whether to allow partial results. This is used for parsers\nthat support streaming\n\"\"\"\nif\nlen\n(\nresult\n)\n!=\n1\n:\nraise\nNotImplementedError\n(\n\"This output parser can only be used with a single generation.\"\n)\ngeneration\n=\nresult\n[\n0\n]\nif\nnot\nisinstance\n(\ngeneration\n,\nChatGeneration\n)\n:\n# Say that this one only works with chat generations\nraise\nOutputParserException\n(\n\"This output parser can only be used with a chat generation.\"\n)\nreturn\ngeneration\n.\nmessage\n.\ncontent\n.\nswapcase\n(\n)\nchain\n=\nanthropic\n|\nStrInvertCase\n(\n)\nAPI Reference:\nOutputParserException\n|\nAIMessage\n|\nBaseGenerationOutputParser\n|\nChatGeneration\n|\nGeneration\nLet's the new parser! It should be inverting the output from the model.\nchain\n.\ninvoke\n(\n\"Tell me a short sentence about yourself\"\n)\n'hELLO! mY NAME IS cLAUDE.'\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/output_parser_fixing/",
    "How-to guides\nHow to use the output-fixing parser\nHow to use the output-fixing parser\nThis\noutput parser\nwraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.\nBut we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it.\nFor this example, we'll use the above Pydantic output parser. Here's what happens if we pass it a result that does not comply with the schema:\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\nexceptions\nimport\nOutputParserException\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nPydanticOutputParser\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\npydantic\nimport\nBaseModel\n,\nField\nAPI Reference:\nOutputParserException\n|\nPydanticOutputParser\nclass\nActor\n(\nBaseModel\n)\n:\nname\n:\nstr\n=\nField\n(\ndescription\n=\n\"name of an actor\"\n)\nfilm_names\n:\nList\n[\nstr\n]\n=\nField\n(\ndescription\n=\n\"list of names of films they starred in\"\n)\nactor_query\n=\n\"Generate the filmography for a random actor.\"\nparser\n=\nPydanticOutputParser\n(\npydantic_object\n=\nActor\n)\nmisformatted\n=\n\"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\"\ntry\n:\nparser\n.\nparse\n(\nmisformatted\n)\nexcept\nOutputParserException\nas\ne\n:\nprint\n(\ne\n)\nInvalid json output: {'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\nNow we can construct and use a\nOutputFixingParser\n. This output parser takes as an argument another output parser but also an LLM with which to try to correct any formatting mistakes.\nfrom\nlangchain\n.\noutput_parsers\nimport\nOutputFixingParser\nnew_parser\n=\nOutputFixingParser\n.\nfrom_llm\n(\nparser\n=\nparser\n,\nllm\n=\nChatOpenAI\n(\n)\n)\nnew_parser\n.\nparse\n(\nmisformatted\n)\nActor(name='Tom Hanks', film_names=['Forrest Gump'])\nFind out api documentation for\nOutputFixingParser\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/output_parser_json/",
    "How-to guides\nHow to parse JSON output\nOn this page\nHow to parse JSON output\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nOutput parsers\nPrompt templates\nStructured output\nChaining runnables together\nWhile some model providers support\nbuilt-in ways to return structured output\n, not all do. We can use an output parser to help users to specify an arbitrary JSON schema via the prompt, query a model for outputs that conform to that schema, and finally parse that schema as JSON.\nnote\nKeep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed JSON.\nThe\nJsonOutputParser\nis one built-in option for prompting for and then parsing JSON output. While it is similar in functionality to the\nPydanticOutputParser\n, it also supports streaming back partial JSON objects.\nHere's an example of how it can be used alongside\nPydantic\nto conveniently declare the expected schema:\n%\npip install\n-\nqU langchain langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nJsonOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\npydantic\nimport\nBaseModel\n,\nField\nmodel\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\n# Define your desired data structure.\nclass\nJoke\n(\nBaseModel\n)\n:\nsetup\n:\nstr\n=\nField\n(\ndescription\n=\n\"question to set up a joke\"\n)\npunchline\n:\nstr\n=\nField\n(\ndescription\n=\n\"answer to resolve the joke\"\n)\n# And a query intented to prompt a language model to populate the data structure.\njoke_query\n=\n\"Tell me a joke.\"\n# Set up a parser + inject instructions into the prompt template.\nparser\n=\nJsonOutputParser\n(\npydantic_object\n=\nJoke\n)\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n,\ninput_variables\n=\n[\n\"query\"\n]\n,\npartial_variables\n=\n{\n\"format_instructions\"\n:\nparser\n.\nget_format_instructions\n(\n)\n}\n,\n)\nchain\n=\nprompt\n|\nmodel\n|\nparser\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\njoke_query\n}\n)\nAPI Reference:\nJsonOutputParser\n|\nPromptTemplate\n{'setup': \"Why couldn't the bicycle stand up by itself?\",\n'punchline': 'Because it was two tired!'}\nNote that we are passing\nformat_instructions\nfrom the parser directly into the prompt. You can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions:\nparser\n.\nget_format_instructions\n(\n)\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\nStreaming\nâ€‹\nAs mentioned above, a key difference between the\nJsonOutputParser\nand the\nPydanticOutputParser\nis that the\nJsonOutputParser\noutput parser supports streaming partial chunks. Here's what that looks like:\nfor\ns\nin\nchain\n.\nstream\n(\n{\n\"query\"\n:\njoke_query\n}\n)\n:\nprint\n(\ns\n)\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\nWithout Pydantic\nâ€‹\nYou can also use the\nJsonOutputParser\nwithout Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\njoke_query\n=\n\"Tell me a joke.\"\nparser\n=\nJsonOutputParser\n(\n)\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n,\ninput_variables\n=\n[\n\"query\"\n]\n,\npartial_variables\n=\n{\n\"format_instructions\"\n:\nparser\n.\nget_format_instructions\n(\n)\n}\n,\n)\nchain\n=\nprompt\n|\nmodel\n|\nparser\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\njoke_query\n}\n)\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\nNext steps\nâ€‹\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the\nbroader guide on obtaining structured output\nfor other techniques.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/output_parser_retry/",
    "How-to guides\nHow to retry when a parsing error occurs\nHow to retry when a parsing error occurs\nWhile in some cases it is possible to fix any parsing mistakes by only looking at the output, in other cases it isn't. An example of this is when the output is not just in the incorrect format, but is partially complete. Consider the below example.\nfrom\nlangchain\n.\noutput_parsers\nimport\nOutputFixingParser\nfrom\nlangchain_core\n.\nexceptions\nimport\nOutputParserException\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nPydanticOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\n,\nOpenAI\nfrom\npydantic\nimport\nBaseModel\n,\nField\nAPI Reference:\nOutputParserException\n|\nPydanticOutputParser\n|\nPromptTemplate\ntemplate\n=\n\"\"\"Based on the user question, provide an Action and Action Input for what step should be taken.\n{format_instructions}\nQuestion: {query}\nResponse:\"\"\"\nclass\nAction\n(\nBaseModel\n)\n:\naction\n:\nstr\n=\nField\n(\ndescription\n=\n\"action to take\"\n)\naction_input\n:\nstr\n=\nField\n(\ndescription\n=\n\"input to the action\"\n)\nparser\n=\nPydanticOutputParser\n(\npydantic_object\n=\nAction\n)\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n,\ninput_variables\n=\n[\n\"query\"\n]\n,\npartial_variables\n=\n{\n\"format_instructions\"\n:\nparser\n.\nget_format_instructions\n(\n)\n}\n,\n)\nprompt_value\n=\nprompt\n.\nformat_prompt\n(\nquery\n=\n\"who is leo di caprios gf?\"\n)\nbad_response\n=\n'{\"action\": \"search\"}'\nIf we try to parse this response as is, we will get an error:\ntry\n:\nparser\n.\nparse\n(\nbad_response\n)\nexcept\nOutputParserException\nas\ne\n:\nprint\n(\ne\n)\nFailed to parse Action from completion {\"action\": \"search\"}. Got: 1 validation error for Action\naction_input\nField required [type=missing, input_value={'action': 'search'}, input_type=dict]\nFor further information visit https://errors.pydantic.dev/2.9/v/missing\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\nIf we try to use the\nOutputFixingParser\nto fix this error, it will be confused - namely, it doesn't know what to actually put for action input.\nfix_parser\n=\nOutputFixingParser\n.\nfrom_llm\n(\nparser\n=\nparser\n,\nllm\n=\nChatOpenAI\n(\n)\n)\nfix_parser\n.\nparse\n(\nbad_response\n)\nAction(action='search', action_input='input')\nInstead, we can use the RetryOutputParser, which passes in the prompt (as well as the original output) to try again to get a better response.\nfrom\nlangchain\n.\noutput_parsers\nimport\nRetryOutputParser\nretry_parser\n=\nRetryOutputParser\n.\nfrom_llm\n(\nparser\n=\nparser\n,\nllm\n=\nOpenAI\n(\ntemperature\n=\n0\n)\n)\nretry_parser\n.\nparse_with_prompt\n(\nbad_response\n,\nprompt_value\n)\nAction(action='search', action_input='leo di caprio girlfriend')\nWe can also add the RetryOutputParser easily with a custom chain which transform the raw LLM/ChatModel output into a more workable format.\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\n,\nRunnableParallel\ncompletion_chain\n=\nprompt\n|\nOpenAI\n(\ntemperature\n=\n0\n)\nmain_chain\n=\nRunnableParallel\n(\ncompletion\n=\ncompletion_chain\n,\nprompt_value\n=\nprompt\n)\n|\nRunnableLambda\n(\nlambda\nx\n:\nretry_parser\n.\nparse_with_prompt\n(\n**\nx\n)\n)\nmain_chain\n.\ninvoke\n(\n{\n\"query\"\n:\n\"who is leo di caprios gf?\"\n}\n)\nAPI Reference:\nRunnableLambda\n|\nRunnableParallel\nAction(action='search', action_input='leo di caprio girlfriend')\nFind out api documentation for\nRetryOutputParser\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/output_parser_string/",
    "How-to guides\nHow to parse text from message objects\nHow to parse text from message objects\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nMessages\nOutput parsers\nLangChain Expression Language (LCEL)\nLangChain\nmessage\nobjects support content in a\nvariety of formats\n, including text,\nmultimodal data\n, and a list of\ncontent block\ndicts.\nThe format of\nChat model\nresponse content may depend on the provider. For example, the chat model for\nAnthropic\nwill return string content for typical string input:\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-5-haiku-latest\"\n)\nresponse\n=\nllm\n.\ninvoke\n(\n\"Hello\"\n)\nresponse\n.\ncontent\n'Hi there! How are you doing today? Is there anything I can help you with?'\nBut when tool calls are generated, the response content is structured into content blocks that convey the model's reasoning process:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nget_weather\n(\nlocation\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Get the weather from a location.\"\"\"\nreturn\n\"Sunny.\"\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nget_weather\n]\n)\nresponse\n=\nllm_with_tools\n.\ninvoke\n(\n\"What's the weather in San Francisco, CA?\"\n)\nresponse\n.\ncontent\nAPI Reference:\ntool\n[{'text': \"I'll help you get the current weather for San Francisco, California. Let me check that for you right away.\",\n'type': 'text'},\n{'id': 'toolu_015PwwcKxWYctKfY3pruHFyy',\n'input': {'location': 'San Francisco, CA'},\n'name': 'get_weather',\n'type': 'tool_use'}]\nTo automatically parse text from message objects irrespective of the format of the underlying content, we can use\nStrOutputParser\n. We can compose it with a chat model as follows:\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nchain\n=\nllm_with_tools\n|\nStrOutputParser\n(\n)\nAPI Reference:\nStrOutputParser\nStrOutputParser\nsimplifies the extraction of text from message objects:\nresponse\n=\nchain\n.\ninvoke\n(\n\"What's the weather in San Francisco, CA?\"\n)\nprint\n(\nresponse\n)\nI'll help you check the weather in San Francisco, CA right away.\nThis is particularly useful in streaming contexts:\nfor\nchunk\nin\nchain\n.\nstream\n(\n\"What's the weather in San Francisco, CA?\"\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"|\"\n)\n|I'll| help| you get| the current| weather for| San Francisco, California|. Let| me retrieve| that| information for you.||||||||||\nSee the\nAPI Reference\nfor more information.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/output_parser_xml/",
    "How-to guides\nHow to parse XML output\nOn this page\nHow to parse XML output\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nOutput parsers\nPrompt templates\nStructured output\nChaining runnables together\nLLMs from different providers often have different strengths depending on the specific data they are trained on. This also means that some may be \"better\" and more reliable at generating output in formats other than JSON.\nThis guide shows you how to use the\nXMLOutputParser\nto prompt models for XML output, then and\nparse\nthat output into a usable format.\nnote\nKeep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed XML.\nIn the following examples, we use Anthropic's Claude-2 model (\nhttps://docs.anthropic.com/claude/docs\n), which is one such model that is optimized for XML tags.\n%\npip install\n-\nqU langchain langchain\n-\nanthropic\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"ANTHROPIC_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"ANTHROPIC_API_KEY\"\n]\n=\ngetpass\n(\n)\nLet's start with a simple request to the model.\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nXMLOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nmodel\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-2.1\"\n,\nmax_tokens_to_sample\n=\n512\n,\ntemperature\n=\n0.1\n)\nactor_query\n=\n\"Generate the shortened filmography for Tom Hanks.\"\noutput\n=\nmodel\n.\ninvoke\n(\nf\"\"\"\n{\nactor_query\n}\nPlease enclose the movies in <movie></movie> tags\"\"\"\n)\nprint\n(\noutput\n.\ncontent\n)\nAPI Reference:\nXMLOutputParser\n|\nPromptTemplate\nHere is the shortened filmography for Tom Hanks, with movies enclosed in XML tags:\n<movie>Splash</movie>\n<movie>Big</movie>\n<movie>A League of Their Own</movie>\n<movie>Sleepless in Seattle</movie>\n<movie>Forrest Gump</movie>\n<movie>Toy Story</movie>\n<movie>Apollo 13</movie>\n<movie>Saving Private Ryan</movie>\n<movie>Cast Away</movie>\n<movie>The Da Vinci Code</movie>\nThis actually worked pretty well! But it would be nice to parse that XML into a more easily usable format. We can use the\nXMLOutputParser\nto both add default format instructions to the prompt and parse outputted XML into a dict:\nparser\n=\nXMLOutputParser\n(\n)\n# We will add these instructions to the prompt below\nparser\n.\nget_format_instructions\n(\n)\n'The output should be formatted as a XML file.\\n1. Output should conform to the tags below. \\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema. \\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n\\`\\`\\`\\nNone\\n\\`\\`\\`'\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"\"\"{query}\\n{format_instructions}\"\"\"\n,\ninput_variables\n=\n[\n\"query\"\n]\n,\npartial_variables\n=\n{\n\"format_instructions\"\n:\nparser\n.\nget_format_instructions\n(\n)\n}\n,\n)\nchain\n=\nprompt\n|\nmodel\n|\nparser\noutput\n=\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\nactor_query\n}\n)\nprint\n(\noutput\n)\n{'filmography': [{'movie': [{'title': 'Big'}, {'year': '1988'}]}, {'movie': [{'title': 'Forrest Gump'}, {'year': '1994'}]}, {'movie': [{'title': 'Toy Story'}, {'year': '1995'}]}, {'movie': [{'title': 'Saving Private Ryan'}, {'year': '1998'}]}, {'movie': [{'title': 'Cast Away'}, {'year': '2000'}]}]}\nWe can also add some tags to tailor the output to our needs. You can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions:\nparser\n=\nXMLOutputParser\n(\ntags\n=\n[\n\"movies\"\n,\n\"actor\"\n,\n\"film\"\n,\n\"name\"\n,\n\"genre\"\n]\n)\n# We will add these instructions to the prompt below\nparser\n.\nget_format_instructions\n(\n)\n'The output should be formatted as a XML file.\\n1. Output should conform to the tags below. \\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema. \\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n\\`\\`\\`\\n[\\'movies\\', \\'actor\\', \\'film\\', \\'name\\', \\'genre\\']\\n\\`\\`\\`'\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"\"\"{query}\\n{format_instructions}\"\"\"\n,\ninput_variables\n=\n[\n\"query\"\n]\n,\npartial_variables\n=\n{\n\"format_instructions\"\n:\nparser\n.\nget_format_instructions\n(\n)\n}\n,\n)\nchain\n=\nprompt\n|\nmodel\n|\nparser\noutput\n=\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\nactor_query\n}\n)\nprint\n(\noutput\n)\n{'movies': [{'actor': [{'name': 'Tom Hanks'}, {'film': [{'name': 'Forrest Gump'}, {'genre': 'Drama'}]}, {'film': [{'name': 'Cast Away'}, {'genre': 'Adventure'}]}, {'film': [{'name': 'Saving Private Ryan'}, {'genre': 'War'}]}]}]}\nThis output parser also supports streaming of partial chunks. Here's an example:\nfor\ns\nin\nchain\n.\nstream\n(\n{\n\"query\"\n:\nactor_query\n}\n)\n:\nprint\n(\ns\n)\n{'movies': [{'actor': [{'name': 'Tom Hanks'}]}]}\n{'movies': [{'actor': [{'film': [{'name': 'Forrest Gump'}]}]}]}\n{'movies': [{'actor': [{'film': [{'genre': 'Drama'}]}]}]}\n{'movies': [{'actor': [{'film': [{'name': 'Cast Away'}]}]}]}\n{'movies': [{'actor': [{'film': [{'genre': 'Adventure'}]}]}]}\n{'movies': [{'actor': [{'film': [{'name': 'Saving Private Ryan'}]}]}]}\n{'movies': [{'actor': [{'film': [{'genre': 'War'}]}]}]}\nNext steps\nâ€‹\nYou've now learned how to prompt a model to return XML. Next, check out the\nbroader guide on obtaining structured output\nfor other related techniques.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/output_parser_yaml/",
    "How-to guides\nHow to parse YAML output\nOn this page\nHow to parse YAML output\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nOutput parsers\nPrompt templates\nStructured output\nChaining runnables together\nLLMs from different providers often have different strengths depending on the specific data they are trained on. This also means that some may be \"better\" and more reliable at generating output in formats other than JSON.\nThis output parser allows users to specify an arbitrary schema and query LLMs for outputs that conform to that schema, using YAML to format their response.\nnote\nKeep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed YAML.\n%\npip install\n-\nqU langchain langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nWe use\nPydantic\nwith the\nYamlOutputParser\nto declare our data model and give the model more context as to what type of YAML it should generate:\nfrom\nlangchain\n.\noutput_parsers\nimport\nYamlOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\npydantic\nimport\nBaseModel\n,\nField\n# Define your desired data structure.\nclass\nJoke\n(\nBaseModel\n)\n:\nsetup\n:\nstr\n=\nField\n(\ndescription\n=\n\"question to set up a joke\"\n)\npunchline\n:\nstr\n=\nField\n(\ndescription\n=\n\"answer to resolve the joke\"\n)\nmodel\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\n# And a query intented to prompt a language model to populate the data structure.\njoke_query\n=\n\"Tell me a joke.\"\n# Set up a parser + inject instructions into the prompt template.\nparser\n=\nYamlOutputParser\n(\npydantic_object\n=\nJoke\n)\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n,\ninput_variables\n=\n[\n\"query\"\n]\n,\npartial_variables\n=\n{\n\"format_instructions\"\n:\nparser\n.\nget_format_instructions\n(\n)\n}\n,\n)\nchain\n=\nprompt\n|\nmodel\n|\nparser\nchain\n.\ninvoke\n(\n{\n\"query\"\n:\njoke_query\n}\n)\nAPI Reference:\nPromptTemplate\nJoke(setup=\"Why couldn't the bicycle find its way home?\", punchline='Because it lost its bearings!')\nThe parser will automatically parse the output YAML and create a Pydantic model with the data. We can see the parser's\nformat_instructions\n, which get added to the prompt:\nparser\n.\nget_format_instructions\n(\n)\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\nNext steps\nâ€‹\nYou've now learned how to prompt a model to return YAML. Next, check out the\nbroader guide on obtaining structured output\nfor other related techniques.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/parent_document_retriever/",
    "How-to guides\nHow to use the Parent Document Retriever\nOn this page\nHow to use the Parent Document Retriever\nWhen splitting documents for\nretrieval\n, there are often conflicting desires:\nYou may want to have small documents, so that their embeddings can most\naccurately reflect their meaning. If too long, then the embeddings can\nlose meaning.\nYou want to have long enough documents that the context of each chunk is\nretained.\nThe\nParentDocumentRetriever\nstrikes that balance by splitting and storing\nsmall chunks of data. During retrieval, it first fetches the small chunks\nbut then looks up the parent ids for those chunks and returns those larger\ndocuments.\nNote that \"parent document\" refers to the document that a small chunk\noriginated from. This can either be the whole raw document OR a larger\nchunk.\nfrom\nlangchain\n.\nretrievers\nimport\nParentDocumentRetriever\nfrom\nlangchain\n.\nstorage\nimport\nInMemoryStore\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nTextLoader\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nloaders\n=\n[\nTextLoader\n(\n\"paul_graham_essay.txt\"\n)\n,\nTextLoader\n(\n\"state_of_the_union.txt\"\n)\n,\n]\ndocs\n=\n[\n]\nfor\nloader\nin\nloaders\n:\ndocs\n.\nextend\n(\nloader\n.\nload\n(\n)\n)\nRetrieving full documents\nâ€‹\nIn this mode, we want to retrieve the full documents. Therefore, we only specify a child\nsplitter\n.\n# This text splitter is used to create the child documents\nchild_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n400\n)\n# The vectorstore to use to index the child chunks\nvectorstore\n=\nChroma\n(\ncollection_name\n=\n\"full_documents\"\n,\nembedding_function\n=\nOpenAIEmbeddings\n(\n)\n)\n# The storage layer for the parent documents\nstore\n=\nInMemoryStore\n(\n)\nretriever\n=\nParentDocumentRetriever\n(\nvectorstore\n=\nvectorstore\n,\ndocstore\n=\nstore\n,\nchild_splitter\n=\nchild_splitter\n,\n)\nretriever\n.\nadd_documents\n(\ndocs\n,\nids\n=\nNone\n)\nThis should yield two keys, because we added two documents.\nlist\n(\nstore\n.\nyield_keys\n(\n)\n)\n['9a63376c-58cc-42c9-b0f7-61f0e1a3a688',\n'40091598-e918-4a18-9be0-f46413a95ae4']\nLet's now call the vector store search functionality - we should see that it returns small chunks (since we're storing the small chunks).\nsub_docs\n=\nvectorstore\n.\nsimilarity_search\n(\n\"justice breyer\"\n)\nprint\n(\nsub_docs\n[\n0\n]\n.\npage_content\n)\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nLet's now retrieve from the overall retriever. This should return large documents - since it returns the documents where the smaller chunks are located.\nretrieved_docs\n=\nretriever\n.\ninvoke\n(\n\"justice breyer\"\n)\nlen\n(\nretrieved_docs\n[\n0\n]\n.\npage_content\n)\n38540\nRetrieving larger chunks\nâ€‹\nSometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents).\n# This text splitter is used to create the parent documents\nparent_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n2000\n)\n# This text splitter is used to create the child documents\n# It should create documents smaller than the parent\nchild_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n400\n)\n# The vectorstore to use to index the child chunks\nvectorstore\n=\nChroma\n(\ncollection_name\n=\n\"split_parents\"\n,\nembedding_function\n=\nOpenAIEmbeddings\n(\n)\n)\n# The storage layer for the parent documents\nstore\n=\nInMemoryStore\n(\n)\nretriever\n=\nParentDocumentRetriever\n(\nvectorstore\n=\nvectorstore\n,\ndocstore\n=\nstore\n,\nchild_splitter\n=\nchild_splitter\n,\nparent_splitter\n=\nparent_splitter\n,\n)\nretriever\n.\nadd_documents\n(\ndocs\n)\nWe can see that there are much more than two documents now - these are the larger chunks.\nlen\n(\nlist\n(\nstore\n.\nyield_keys\n(\n)\n)\n)\n66\nLet's make sure the underlying vector store still retrieves the small chunks.\nsub_docs\n=\nvectorstore\n.\nsimilarity_search\n(\n\"justice breyer\"\n)\nprint\n(\nsub_docs\n[\n0\n]\n.\npage_content\n)\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nretrieved_docs\n=\nretriever\n.\ninvoke\n(\n\"justice breyer\"\n)\nlen\n(\nretrieved_docs\n[\n0\n]\n.\npage_content\n)\n1849\nprint\n(\nretrieved_docs\n[\n0\n]\n.\npage_content\n)\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections.\nWe cannot let this happen.\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.\nWe can do both. At our border, weâ€™ve installed new technology like cutting-edge scanners to better detect drug smuggling.\nWeâ€™ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\nWeâ€™re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.\nWeâ€™re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/pydantic_compatibility/",
    "How-to guides\nHow to use LangChain with different Pydantic versions\nHow to use LangChain with different Pydantic versions\nAs of the\n0.3\nrelease, LangChain uses Pydantic 2 internally.\nUsers should install Pydantic 2 and are advised to\navoid\nusing the\npydantic.v1\nnamespace of Pydantic 2 with\nLangChain APIs.\nIf you're working with prior versions of LangChain, please see the following guide\non\nPydantic compatibility\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/",
    "How-to guides\nHow to add chat history\nOn this page\nHow to add chat history\nnote\nThis guide previously used the\nRunnableWithMessageHistory\nabstraction. You can access this version of the documentation in the\nv0.2 docs\n.\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of\nLangGraph persistence\nto incorporate\nmemory\ninto new LangChain applications.\nIf your code is already relying on\nRunnableWithMessageHistory\nor\nBaseChatMessageHistory\n, you do\nnot\nneed to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses\nRunnableWithMessageHistory\nwill continue to work as expected.\nPlease see\nHow to migrate to LangGraph Memory\nfor more details.\nIn many\nQ&A applications\nwe want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\nIn this guide we focus on\nadding logic for incorporating historical messages.\nThis is largely a condensed version of the\nConversational RAG tutorial\n.\nWe will cover two approaches:\nChains\n, in which we always execute a retrieval step;\nAgents\n, in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).\nFor the external knowledge source, we will use the same\nLLM Powered Autonomous Agents\nblog post by Lilian Weng from the\nRAG tutorial\n.\nBoth approaches leverage\nLangGraph\nas an orchestration framework. LangGraph implements a built-in\npersistence layer\n, making it ideal for chat applications that support multiple conversational turns.\nSetup\nâ€‹\nDependencies\nâ€‹\nWe'll use OpenAI embeddings and an InMemory vector store in this walkthrough, but everything shown here works with any\nEmbeddings\n, and\nVectorStore\nor\nRetriever\n.\nWe'll use the following packages:\n%\n%\ncapture\n-\n-\nno\n-\nstderr\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langgraph langchain\n-\ncommunity beautifulsoup4\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"LANGSMITH_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nComponents\nâ€‹\nWe will need to select three components from LangChain's suite of integrations.\nA\nchat model\n:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nAn\nembedding model\n:\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nAnd a\nvector store\n:\nSelect\nvector store\n:\nIn-memory\nâ–¾\nIn-memory\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\npip install -qU langchain-core\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\nChains\nâ€‹\nThe\nRAG Tutorial\nindexes an\nLLM Powered Autonomous Agents\nblog post by Lilian Weng. We will repeat that here. Below we load the content of the page, split it into sub-documents, and embed the documents into our\nvector store\n:\nimport\nbs4\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader\n(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,\n)\n,\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4\n.\nSoupStrainer\n(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n)\n,\n)\ndocs\n=\nloader\n.\nload\n(\n)\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\nAPI Reference:\nDocument\n# Index chunks\n_\n=\nvector_store\n.\nadd_documents\n(\ndocuments\n=\nall_splits\n)\nAs detailed in\nPart 2\nof the RAG tutorial, we can naturally support a conversational experience by representing the flow of the RAG application as a sequence of\nmessages\n:\nUser input as a\nHumanMessage\n;\nVector store query as an\nAIMessage\nwith tool calls;\nRetrieved documents as a\nToolMessage\n;\nFinal response as a\nAIMessage\n.\nWe will use\ntool-calling\nto facilitate this, which additionally allows the query to be generated by the LLM. We can build a\ntool\nto execute the retrieval step:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve\n(\nquery\n:\nstr\n)\n:\n\"\"\"Retrieve information related to a query.\"\"\"\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nquery\n,\nk\n=\n2\n)\nserialized\n=\n\"\\n\\n\"\n.\njoin\n(\n(\nf\"Source:\n{\ndoc\n.\nmetadata\n}\n\\nContent:\n{\ndoc\n.\npage_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized\n,\nretrieved_docs\nAPI Reference:\ntool\nWe can now build our LangGraph application.\nNote that we compile it with a\ncheckpointer\nto support a back-and-forth conversation. LangGraph comes with a simple\nin-memory checkpointer\n, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\nfrom\nlangchain_core\n.\nmessages\nimport\nSystemMessage\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nMessagesState\n,\nStateGraph\nfrom\nlanggraph\n.\nprebuilt\nimport\nToolNode\n,\ntools_condition\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef\nquery_or_respond\n(\nstate\n:\nMessagesState\n)\n:\n\"\"\"Generate tool call for retrieval or respond.\"\"\"\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nretrieve\n]\n)\nresponse\n=\nllm_with_tools\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\n# MessagesState appends messages to state instead of overwriting\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\n# Step 2: Execute the retrieval.\ntools\n=\nToolNode\n(\n[\nretrieve\n]\n)\n# Step 3: Generate a response using the retrieved content.\ndef\ngenerate\n(\nstate\n:\nMessagesState\n)\n:\n\"\"\"Generate answer.\"\"\"\n# Get generated ToolMessages\nrecent_tool_messages\n=\n[\n]\nfor\nmessage\nin\nreversed\n(\nstate\n[\n\"messages\"\n]\n)\n:\nif\nmessage\n.\ntype\n==\n\"tool\"\n:\nrecent_tool_messages\n.\nappend\n(\nmessage\n)\nelse\n:\nbreak\ntool_messages\n=\nrecent_tool_messages\n[\n:\n:\n-\n1\n]\n# Format into prompt\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\ncontent\nfor\ndoc\nin\ntool_messages\n)\nsystem_message_content\n=\n(\n\"You are an assistant for question-answering tasks. \"\n\"Use the following pieces of retrieved context to answer \"\n\"the question. If you don't know the answer, say that you \"\n\"don't know. Use three sentences maximum and keep the \"\n\"answer concise.\"\n\"\\n\\n\"\nf\"\n{\ndocs_content\n}\n\"\n)\nconversation_messages\n=\n[\nmessage\nfor\nmessage\nin\nstate\n[\n\"messages\"\n]\nif\nmessage\n.\ntype\nin\n(\n\"human\"\n,\n\"system\"\n)\nor\n(\nmessage\n.\ntype\n==\n\"ai\"\nand\nnot\nmessage\n.\ntool_calls\n)\n]\nprompt\n=\n[\nSystemMessage\n(\nsystem_message_content\n)\n]\n+\nconversation_messages\n# Run\nresponse\n=\nllm\n.\ninvoke\n(\nprompt\n)\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\n# Build graph\ngraph_builder\n=\nStateGraph\n(\nMessagesState\n)\ngraph_builder\n.\nadd_node\n(\nquery_or_respond\n)\ngraph_builder\n.\nadd_node\n(\ntools\n)\ngraph_builder\n.\nadd_node\n(\ngenerate\n)\ngraph_builder\n.\nset_entry_point\n(\n\"query_or_respond\"\n)\ngraph_builder\n.\nadd_conditional_edges\n(\n\"query_or_respond\"\n,\ntools_condition\n,\n{\nEND\n:\nEND\n,\n\"tools\"\n:\n\"tools\"\n}\n,\n)\ngraph_builder\n.\nadd_edge\n(\n\"tools\"\n,\n\"generate\"\n)\ngraph_builder\n.\nadd_edge\n(\n\"generate\"\n,\nEND\n)\nmemory\n=\nMemorySaver\n(\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nAPI Reference:\nSystemMessage\n|\nMemorySaver\n|\nStateGraph\n|\nToolNode\n|\ntools_condition\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nLet's test our application.\nNote that it responds appropriately to messages that do not require an additional retrieval step:\n# Specify an ID for the thread\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"abc123\"\n}\n}\ninput_message\n=\n\"Hello\"\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\nconfig\n=\nconfig\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nHello\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello! How can I assist you today?\nAnd when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:\ninput_message\n=\n\"What is Task Decomposition?\"\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\nconfig\n=\nconfig\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat is Task Decomposition?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_RntwX5GMt531biEE9MqSbgLV)\nCall ID: call_RntwX5GMt531biEE9MqSbgLV\nArgs:\nquery: Task Decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTask Decomposition is the process of breaking down a complicated task into smaller, more manageable steps. It often involves techniques like Chain of Thought (CoT), where the model is prompted to \"think step by step,\" allowing for better handling of complex tasks. This approach enhances model performance and provides insight into the model's reasoning process.\nFinally, because we have compiled our application with a\ncheckpointer\n, historical messages are maintained in the state. This allows the model to contextualize user queries:\ninput_message\n=\n\"Can you look up some common ways of doing it?\"\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\nconfig\n=\nconfig\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nCan you look up some common ways of doing it?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_kwO5rYPyJ0MftYKoKRFjKpZM)\nCall ID: call_kwO5rYPyJ0MftYKoKRFjKpZM\nArgs:\nquery: common methods for task decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nCommon ways of Task Decomposition include: (1) using large language models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"; (2) utilizing task-specific instructions, such as \"Write a story outline\" for creative tasks; and (3) incorporating human inputs to guide the decomposition process.\nNote that we can observe the full sequence of messages sent to the chat model-- including tool calls and retrieved context-- in the\nLangSmith trace\n.\nThe conversation history can also be inspected via the state of the application:\nchat_history\n=\ngraph\n.\nget_state\n(\nconfig\n)\n.\nvalues\n[\n\"messages\"\n]\nfor\nmessage\nin\nchat_history\n:\nmessage\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nHello\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello! How can I assist you today?\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat is Task Decomposition?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_RntwX5GMt531biEE9MqSbgLV)\nCall ID: call_RntwX5GMt531biEE9MqSbgLV\nArgs:\nquery: Task Decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTask Decomposition is the process of breaking down a complicated task into smaller, more manageable steps. It often involves techniques like Chain of Thought (CoT), where the model is prompted to \"think step by step,\" allowing for better handling of complex tasks. This approach enhances model performance and provides insight into the model's reasoning process.\n================================\u001b[1m Human Message \u001b[0m=================================\nCan you look up some common ways of doing it?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_kwO5rYPyJ0MftYKoKRFjKpZM)\nCall ID: call_kwO5rYPyJ0MftYKoKRFjKpZM\nArgs:\nquery: common methods for task decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nCommon ways of Task Decomposition include: (1) using large language models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"; (2) utilizing task-specific instructions, such as \"Write a story outline\" for creative tasks; and (3) incorporating human inputs to guide the decomposition process.\nAgents\nâ€‹\nAgents\nleverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\nBelow we assemble a minimal RAG agent. Using LangGraph's\npre-built ReAct agent constructor\n, we can do this in one line.\ntip\nCheck out LangGraph's\nAgentic RAG\ntutorial for more advanced formulations.\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\nagent_executor\n=\ncreate_react_agent\n(\nllm\n,\n[\nretrieve\n]\n,\ncheckpointer\n=\nmemory\n)\nAPI Reference:\ncreate_react_agent\nLet's inspect the graph:\ndisplay\n(\nImage\n(\nagent_executor\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nThe key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.\nLet's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"def234\"\n}\n}\ninput_message\n=\n(\n\"What is the standard method for Task Decomposition?\\n\\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent_executor\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\nconfig\n=\nconfig\n,\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_rxBqio7dxthnMuzjr4AIquSZ)\nCall ID: call_rxBqio7dxthnMuzjr4AIquSZ\nArgs:\nquery: standard method for Task Decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_kmQMRWCKeBdtXdlJi8yZD9CO)\nCall ID: call_kmQMRWCKeBdtXdlJi8yZD9CO\nArgs:\nquery: common extensions of Task Decomposition methods\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe standard method for Task Decomposition involves breaking down complex tasks into smaller, manageable steps. Here are the main techniques:\n1. **Chain of Thought (CoT)**: This prompting technique encourages a model to \"think step by step,\" allowing it to utilize more computational resources during testing to decompose challenging tasks into simpler parts. CoT not only simplifies tasks but also provides insights into the model's reasoning process.\n2. **Simple Prompting**: This can involve straightforward queries like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to guide the model in identifying the necessary steps.\n3. **Task-specific Instructions**: Using specific prompts tailored to the task at hand, such as \"Write a story outline\" for creative writing, allows for more directed decomposition.\n4. **Human Inputs**: Involving human expertise can also aid in breaking down tasks effectively.\n### Common Extensions of Task Decomposition Methods\n1. **Tree of Thoughts**: This method extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into various thought steps and generates multiple thoughts per step, forming a tree structure. This can utilize search processes like breadth-first search (BFS) or depth-first search (DFS) to evaluate states through classifiers or majority voting.\nThese extensions build on the basic principles of task decomposition, enhancing the depth and breadth of reasoning applied to complex tasks.\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nNext steps\nâ€‹\nWe've covered the steps to build a basic conversational Q&A application:\nWe used chains to build a predictable application that generates search queries for each user input;\nWe used agents to build an application that \"decides\" when and how to generate search queries.\nTo explore different types of retrievers and retrieval strategies, visit the\nretrievers\nsection of the how-to guides.\nFor a detailed walkthrough of LangChain's conversation memory abstractions, visit the\nHow to add message history (memory)\nLCEL page.\nTo learn more about agents, head to the\nAgents Modules\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/qa_citations/",
    "How-to guides\nHow to get a RAG application to add citations\nOn this page\nHow to get a RAG application to add citations\nThis guide reviews methods to get a model to cite which parts of the source documents it referenced in generating its response.\nWe will cover five methods:\nUsing tool-calling to cite document IDs;\nUsing tool-calling to cite documents IDs and provide text snippets;\nDirect prompting;\nRetrieval post-processing (i.e., compressing the retrieved context to make it more relevant);\nGeneration post-processing (i.e., issuing a second LLM call to annotate a generated answer with citations).\nWe generally suggest using the first item of the list that works for your use-case. That is, if your model supports tool-calling, try methods 1 or 2; otherwise, or if those fail, advance down the list.\nLet's first create a simple\nRAG\nchain. To start we'll just retrieve from Wikipedia using the\nWikipediaRetriever\n. We will use the same\nLangGraph\nimplementation from the\nRAG Tutorial\n.\nSetup\nâ€‹\nFirst we'll need to install some dependencies:\n%\npip install\n-\nqU langchain\n-\ncommunity wikipedia\nLet's first select a LLM:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nWe can now load a\nretriever\nand construct our\nprompt\n:\nfrom\nlangchain_community\n.\nretrievers\nimport\nWikipediaRetriever\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nsystem_prompt\n=\n(\n\"You're a helpful AI assistant. Given a user question \"\n\"and some Wikipedia article snippets, answer the user \"\n\"question. If none of the articles answer the question, \"\n\"just say you don't know.\"\n\"\\n\\nHere are the Wikipedia articles: \"\n\"{context}\"\n)\nretriever\n=\nWikipediaRetriever\n(\ntop_k_results\n=\n6\n,\ndoc_content_chars_max\n=\n2000\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem_prompt\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n]\n)\nprompt\n.\npretty_print\n(\n)\nAPI Reference:\nChatPromptTemplate\n================================\u001b[1m System Message \u001b[0m================================\nYou're a helpful AI assistant. Given a user question and some Wikipedia article snippets, answer the user question. If none of the articles answer the question, just say you don't know.\nHere are the Wikipedia articles: \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n================================\u001b[1m Human Message \u001b[0m=================================\n\u001b[33;1m\u001b[1;3m{question}\u001b[0m\nNow that we've got a\nmodel\n,\nretriever\nand\nprompt\n, let's chain them all together. Following the how-to guide on\nadding citations\nto a RAG application, we'll make it so our chain returns both the answer and the retrieved Documents. This uses the same\nLangGraph\nimplementation as in the\nRAG Tutorial\n.\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nStateGraph\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\n# Define state for application\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\n# Define application steps\ndef\nretrieve\n(\nstate\n:\nState\n)\n:\nretrieved_docs\n=\nretriever\n.\ninvoke\n(\nstate\n[\n\"question\"\n]\n)\nreturn\n{\n\"context\"\n:\nretrieved_docs\n}\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\n# Compile application and test\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nDocument\n|\nStateGraph\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"How fast are cheetahs?\"\n}\n)\nsources\n=\n[\ndoc\n.\nmetadata\n[\n\"source\"\n]\nfor\ndoc\nin\nresult\n[\n\"context\"\n]\n]\nprint\n(\nf\"Sources:\n{\nsources\n}\n\\n\\n\"\n)\nprint\n(\nf\"Answer:\n{\nresult\n[\n'answer'\n]\n}\n\"\n)\nSources: ['https://en.wikipedia.org/wiki/Cheetah', 'https://en.wikipedia.org/wiki/Southeast_African_cheetah', 'https://en.wikipedia.org/wiki/Footspeed', 'https://en.wikipedia.org/wiki/Fastest_animals', 'https://en.wikipedia.org/wiki/Pursuit_predation', 'https://en.wikipedia.org/wiki/Gepard-class_fast_attack_craft']\nAnswer: Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).\nCheck out the\nLangSmith trace\n.\nTool-calling\nâ€‹\nIf your LLM of choice implements a\ntool-calling\nfeature, you can use it to make the model specify which of the provided documents it's referencing when generating its answer. LangChain tool-calling models implement a\n.with_structured_output\nmethod which will force generation adhering to a desired schema (see details\nhere\n).\nCite documents\nâ€‹\nTo cite documents using an identifier, we format the identifiers into the prompt, then use\n.with_structured_output\nto coerce the LLM to reference these identifiers in its output.\nFirst we define a schema for the output. The\n.with_structured_output\nsupports multiple formats, including JSON schema and Pydantic. Here we will use Pydantic:\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nCitedAnswer\n(\nBaseModel\n)\n:\n\"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\nanswer\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The answer to the user question, which is based only on the given sources.\"\n,\n)\ncitations\n:\nList\n[\nint\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The integer IDs of the SPECIFIC sources which justify the answer.\"\n,\n)\nLet's see what the model output is like when we pass in our functions and a user input:\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nCitedAnswer\n)\nexample_q\n=\n\"\"\"What Brian's height?\nSource: 1\nInformation: Suzy is 6'2\"\nSource: 2\nInformation: Jeremiah is blonde\nSource: 3\nInformation: Brian is 3 inches shorter than Suzy\"\"\"\nresult\n=\nstructured_llm\n.\ninvoke\n(\nexample_q\n)\nresult\nCitedAnswer(answer='Brian is 5\\'11\".', citations=[1, 3])\nOr as a dict:\nresult\n.\ndict\n(\n)\n{'answer': 'Brian is 5\\'11\".', 'citations': [1, 3]}\nNow we structure the source identifiers into the prompt to replicate with our chain. We will make three changes:\nUpdate the prompt to include source identifiers;\nUse the\nstructured_llm\n(i.e.,\nllm.with_structured_output(CitedAnswer)\n);\nReturn the Pydantic object in the output.\ndef\nformat_docs_with_id\n(\ndocs\n:\nList\n[\nDocument\n]\n)\n-\n>\nstr\n:\nformatted\n=\n[\nf\"Source ID:\n{\ni\n}\n\\nArticle Title:\n{\ndoc\n.\nmetadata\n[\n'title'\n]\n}\n\\nArticle Snippet:\n{\ndoc\n.\npage_content\n}\n\"\nfor\ni\n,\ndoc\nin\nenumerate\n(\ndocs\n)\n]\nreturn\n\"\\n\\n\"\n+\n\"\\n\\n\"\n.\njoin\n(\nformatted\n)\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nCitedAnswer\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\nformatted_docs\n=\nformat_docs_with_id\n(\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\nformatted_docs\n}\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nCitedAnswer\n)\nresponse\n=\nstructured_llm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n}\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"How fast are cheetahs?\"\n}\n)\nresult\n[\n\"answer\"\n]\nCitedAnswer(answer='Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).', citations=[0, 3])\nWe can inspect the document at index 0, which the model cited:\nprint\n(\nresult\n[\n\"context\"\n]\n[\n0\n]\n)\npage_content='The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67â€“94 cm (26â€“37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.\nThe cheetah lives in three main social groups: females and their cubs, male \"coalitions\", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson's gazelles. The cheetah typically stalks its prey within 60â€“100 m (200â€“330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned a' metadata={'title': 'Cheetah', 'summary': 'The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67â€“94 cm (26â€“37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.\\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.\\nThe cheetah lives in three main social groups: females and their cubs, male \"coalitions\", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson\\'s gazelles. The cheetah typically stalks its prey within 60â€“100 m (200â€“330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned at around four months and are independent by around 20 months of age.\\nThe cheetah is threatened by habitat loss, conflict with humans, poaching and high susceptibility to diseases. The global cheetah population was estimated in 2021 at 6,517; it is listed as Vulnerable on the IUCN Red List. It has been widely depicted in art, literature, advertising, and animation. It was tamed in ancient Egypt and trained for hunting ungulates in the Arabian Peninsula and India. It has been kept in zoos since the early 19th century.', 'source': 'https://en.wikipedia.org/wiki/Cheetah'}\nLangSmith trace:\nhttps://smith.langchain.com/public/6f34d136-451d-4625-90c8-2d8decebc21a/r\nCite snippets\nâ€‹\nTo return text spans (perhaps in addition to source identifiers), we can use the same approach. The only change will be to build a more complex output schema, here using Pydantic, that includes a \"quote\" alongside a source identifier.\nAside: Note that if we break up our documents so that we have many documents with only a sentence or two instead of a few long documents, citing documents becomes roughly equivalent to citing snippets, and may be easier for the model because the model just needs to return an identifier for each snippet instead of the actual text. Probably worth trying both approaches and evaluating.\nclass\nCitation\n(\nBaseModel\n)\n:\nsource_id\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The integer ID of a SPECIFIC source which justifies the answer.\"\n,\n)\nquote\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The VERBATIM quote from the specified source that justifies the answer.\"\n,\n)\nclass\nQuotedAnswer\n(\nBaseModel\n)\n:\n\"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\nanswer\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The answer to the user question, which is based only on the given sources.\"\n,\n)\ncitations\n:\nList\n[\nCitation\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Citations from the given sources that justify the answer.\"\n)\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nQuotedAnswer\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\nformatted_docs\n=\nformat_docs_with_id\n(\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\nformatted_docs\n}\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nQuotedAnswer\n)\nresponse\n=\nstructured_llm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n}\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nHere we see that the model has extracted a relevant snippet of text from source 0:\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"How fast are cheetahs?\"\n}\n)\nresult\n[\n\"answer\"\n]\nQuotedAnswer(answer='Cheetahs are capable of running at speeds of 93 to 104 km/h (58 to 65 mph).', citations=[Citation(source_id=0, quote='The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed.')])\nLangSmith trace:\nhttps://smith.langchain.com/public/e16dc72f-4261-4f25-a9a7-906238737283/r\nDirect prompting\nâ€‹\nSome models don't support function-calling. We can achieve similar results with direct prompting. Let's try instructing a model to generate structured XML for its output:\nxml_system\n=\n\"\"\"You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, \\\nanswer the user question and provide citations. If none of the articles answer the question, just say you don't know.\nRemember, you must return both an answer and citations. A citation consists of a VERBATIM quote that \\\njustifies the answer and the ID of the quote article. Return a citation for every quote across all articles \\\nthat justify the answer. Use the following format for your final output:\n<cited_answer>\n<answer></answer>\n<citations>\n<citation><source_id></source_id><quote></quote></citation>\n<citation><source_id></source_id><quote></quote></citation>\n...\n</citations>\n</cited_answer>\nHere are the Wikipedia articles:{context}\"\"\"\nxml_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nxml_system\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n]\n)\nWe now make similar small updates to our chain:\nWe update the formatting function to wrap the retrieved context in XML tags;\nWe do not use\n.with_structured_output\n(e.g., because it does not exist for a model);\nWe use\nXMLOutputParser\nto parse the answer into a dict.\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nXMLOutputParser\ndef\nformat_docs_xml\n(\ndocs\n:\nList\n[\nDocument\n]\n)\n-\n>\nstr\n:\nformatted\n=\n[\n]\nfor\ni\n,\ndoc\nin\nenumerate\n(\ndocs\n)\n:\ndoc_str\n=\nf\"\"\"\\\n<source id=\\\"\n{\ni\n}\n\\\">\n<title>\n{\ndoc\n.\nmetadata\n[\n\"title\"\n]\n}\n</title>\n<article_snippet>\n{\ndoc\n.\npage_content\n}\n</article_snippet>\n</source>\"\"\"\nformatted\n.\nappend\n(\ndoc_str\n)\nreturn\n\"\\n\\n<sources>\"\n+\n\"\\n\"\n.\njoin\n(\nformatted\n)\n+\n\"</sources>\"\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\ndict\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\nformatted_docs\n=\nformat_docs_xml\n(\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nxml_prompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\nformatted_docs\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nparsed_response\n=\nXMLOutputParser\n(\n)\n.\ninvoke\n(\nresponse\n)\nreturn\n{\n\"answer\"\n:\nparsed_response\n}\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nXMLOutputParser\nNote that citations are again structured into the answer:\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"How fast are cheetahs?\"\n}\n)\nresult\n[\n\"answer\"\n]\n{'cited_answer': [{'answer': 'Cheetahs can run at speeds of 93 to 104 km/h (58 to 65 mph).'},\n{'citations': [{'citation': [{'source_id': '0'},\n{'quote': 'The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph);'}]},\n{'citation': [{'source_id': '3'},\n{'quote': 'The fastest land animal is the cheetah.'}]}]}]}\nLangSmith trace:\nhttps://smith.langchain.com/public/0c45f847-c640-4b9a-a5fa-63559e413527/r\nRetrieval post-processing\nâ€‹\nAnother approach is to post-process our retrieved documents to compress the content, so that the source content is already minimal enough that we don't need the model to cite specific sources or spans. For example, we could break up each document into a sentence or two, embed those and keep only the most relevant ones. LangChain has some built-in components for this. Here we'll use a\nRecursiveCharacterTextSplitter\n, which creates chunks of a specified size by splitting on separator substrings, and an\nEmbeddingsFilter\n, which keeps only the texts with the most relevant embeddings.\nThis approach effectively updates our\nretrieve\nstep to compress the documents. Let's first select an\nembedding model\n:\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nWe can now rewrite the\nretrieve\nstep:\nfrom\nlangchain\n.\nretrievers\n.\ndocument_compressors\nimport\nEmbeddingsFilter\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableParallel\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nsplitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n400\n,\nchunk_overlap\n=\n0\n,\nseparators\n=\n[\n\"\\n\\n\"\n,\n\"\\n\"\n,\n\".\"\n,\n\" \"\n]\n,\nkeep_separator\n=\nFalse\n,\n)\ncompressor\n=\nEmbeddingsFilter\n(\nembeddings\n=\nembeddings\n,\nk\n=\n10\n)\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\ndef\nretrieve\n(\nstate\n:\nState\n)\n:\nretrieved_docs\n=\nretriever\n.\ninvoke\n(\nstate\n[\n\"question\"\n]\n)\nsplit_docs\n=\nsplitter\n.\nsplit_documents\n(\nretrieved_docs\n)\nstateful_docs\n=\ncompressor\n.\ncompress_documents\n(\nsplit_docs\n,\nstate\n[\n\"question\"\n]\n)\nreturn\n{\n\"context\"\n:\nstateful_docs\n}\nAPI Reference:\nRunnableParallel\nLet's test this out:\nretrieval_result\n=\nretrieve\n(\n{\n\"question\"\n:\n\"How fast are cheetahs?\"\n}\n)\nfor\ndoc\nin\nretrieval_result\n[\n\"context\"\n]\n:\nprint\n(\nf\"\n{\ndoc\n.\npage_content\n}\n\\n\\n\"\n)\nAdults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail\nThe cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67â€“94 cm (26â€“37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in)\n2 mph), or 171 body lengths per second. The cheetah, the fastest land mammal, scores at only 16 body lengths per second\nIt feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson's gazelles. The cheetah typically stalks its prey within 60â€“100 m (200â€“330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran\nThe cheetah lives in three main social groups: females and their cubs, male \"coalitions\", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk\nThe Southeast African cheetah (Acinonyx jubatus jubatus) is the nominate cheetah subspecies native to East and Southern Africa. The Southern African cheetah lives mainly in the lowland areas and deserts of the Kalahari, the savannahs of Okavango Delta, and the grasslands of the Transvaal region in South Africa. In Namibia, cheetahs are mostly found in farmlands\nSubpopulations have been called \"South African cheetah\" and \"Namibian cheetah.\"\nIn India, four cheetahs of the subspecies are living in Kuno National Park in Madhya Pradesh after having been introduced there\nAcinonyx jubatus velox proposed in 1913 by Edmund Heller on basis of a cheetah that was shot by Kermit Roosevelt in June 1909 in the Kenyan highlands.\nAcinonyx rex proposed in 1927 by Reginald Innes Pocock on basis of a specimen from the Umvukwe Range in Rhodesia.\nNext, we assemble it into our chain as before:\n# This step is unchanged from our original RAG implementation\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"How fast are cheetahs?\"\n}\n)\nprint\n(\nresult\n[\n\"answer\"\n]\n)\nCheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph). They are known as the fastest land animals.\nNote that the document content is now compressed, although the document objects retain the original content in a \"summary\" key in their metadata. These summaries are not passed to the model; only the condensed content is.\nresult\n[\n\"context\"\n]\n[\n0\n]\n.\npage_content\n# passed to model\n'Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail'\nresult\n[\n\"context\"\n]\n[\n0\n]\n.\nmetadata\n[\n\"summary\"\n]\n# original document  # original document\n'The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67â€“94 cm (26â€“37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.\\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.\\nThe cheetah lives in three main social groups: females and their cubs, male \"coalitions\", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson\\'s gazelles. The cheetah typically stalks its prey within 60â€“100 m (200â€“330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned at around four months and are independent by around 20 months of age.\\nThe cheetah is threatened by habitat loss, conflict with humans, poaching and high susceptibility to diseases. The global cheetah population was estimated in 2021 at 6,517; it is listed as Vulnerable on the IUCN Red List. It has been widely depicted in art, literature, advertising, and animation. It was tamed in ancient Egypt and trained for hunting ungulates in the Arabian Peninsula and India. It has been kept in zoos since the early 19th century.'\nLangSmith trace:\nhttps://smith.langchain.com/public/21b0dc15-d70a-4293-9402-9c70f9178e66/r\nGeneration post-processing\nâ€‹\nAnother approach is to post-process our model generation. In this example we'll first generate just an answer, and then we'll ask the model to annotate it's own answer with citations. The downside of this approach is of course that it is slower and more expensive, because two model calls need to be made.\nLet's apply this to our initial chain. If desired, we can implement this via a third step in our application.\nclass\nCitation\n(\nBaseModel\n)\n:\nsource_id\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The integer ID of a SPECIFIC source which justifies the answer.\"\n,\n)\nquote\n:\nstr\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"The VERBATIM quote from the specified source that justifies the answer.\"\n,\n)\nclass\nAnnotatedAnswer\n(\nBaseModel\n)\n:\n\"\"\"Annotate the answer to the user question with quote citations that justify the answer.\"\"\"\ncitations\n:\nList\n[\nCitation\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Citations from the given sources that justify the answer.\"\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nAnnotatedAnswer\n)\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\nannotations\n:\nAnnotatedAnswer\ndef\nretrieve\n(\nstate\n:\nState\n)\n:\nretrieved_docs\n=\nretriever\n.\ninvoke\n(\nstate\n[\n\"question\"\n]\n)\nreturn\n{\n\"context\"\n:\nretrieved_docs\n}\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\ndef\nannotate\n(\nstate\n:\nState\n)\n:\nformatted_docs\n=\nformat_docs_with_id\n(\nstate\n[\n\"context\"\n]\n)\nmessages\n=\n[\n(\n\"system\"\n,\nsystem_prompt\n.\nformat\n(\ncontext\n=\nformatted_docs\n)\n)\n,\n(\n\"human\"\n,\nstate\n[\n\"question\"\n]\n)\n,\n(\n\"ai\"\n,\nstate\n[\n\"answer\"\n]\n)\n,\n(\n\"human\"\n,\n\"Annotate your answer with citations.\"\n)\n,\n]\nresponse\n=\nstructured_llm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"annotations\"\n:\nresponse\n}\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n,\nannotate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"How fast are cheetahs?\"\n}\n)\nprint\n(\nresult\n[\n\"answer\"\n]\n)\nCheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).\nresult\n[\n\"annotations\"\n]\nAnnotatedAnswer(citations=[Citation(source_id=0, quote='The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph)')])\nLangSmith trace:\nhttps://smith.langchain.com/public/b8257417-573b-47c4-a750-74e542035f19/r\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/qa_per_user/",
    "How-to guides\nHow to do per-user retrieval\nOn this page\nHow to do per-user retrieval\nThis guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a\nretriever\nbased on the user.\nWhen building a\nretrieval app\n, you often have to build it with multiple users in mind. This means that you may be storing data not just for one user, but for many different users, and they should not be able to see eachother's data. This means that you need to be able to configure your retrieval chain to only retrieve certain information. This generally involves two steps.\nStep 1: Make sure the retriever you are using supports multiple users\nAt the moment, there is no unified flag or filter for this in LangChain. Rather, each vectorstore and retriever may have their own, and may be called different things (namespaces, multi-tenancy, etc). For vectorstores, this is generally exposed as a keyword argument that is passed in during\nsimilarity_search\n. By reading the documentation or source code, figure out whether the retriever you are using supports multiple users, and, if so, how to use it.\nNote: adding documentation and/or support for multiple users for retrievers that do not support it (or document it) is a GREAT way to contribute to LangChain\nStep 2: Add that parameter as a configurable field for the chain\nThis will let you easily call the chain and configure any relevant flags at runtime. See\nthis documentation\nfor more information on configuration.\nNow, at runtime you can call this chain with configurable field.\nCode Example\nâ€‹\nLet's see a concrete example of what this looks like in code. We will use Pinecone for this example.\nTo configure Pinecone, set the following environment variable:\nPINECONE_API_KEY\n: Your Pinecone API key\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nembeddings\n=\nOpenAIEmbeddings\n(\n)\nvectorstore\n=\nPineconeVectorStore\n(\nindex_name\n=\n\"test-example\"\n,\nembedding\n=\nembeddings\n)\nvectorstore\n.\nadd_texts\n(\n[\n\"I worked at Kensho\"\n]\n,\nnamespace\n=\n\"harrison\"\n)\nvectorstore\n.\nadd_texts\n(\n[\n\"I worked at Facebook\"\n]\n,\nnamespace\n=\n\"ankush\"\n)\n['f907aab7-77c7-4347-acc2-6859f8142f92']\nThe pinecone kwarg for\nnamespace\ncan be used to separate documents\n# This will only get documents for Ankush\nvectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"namespace\"\n:\n\"ankush\"\n}\n)\n.\ninvoke\n(\n\"where did i work?\"\n)\n[Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')]\n# This will only get documents for Harrison\nvectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"namespace\"\n:\n\"harrison\"\n}\n)\n.\ninvoke\n(\n\"where did i work?\"\n)\n[Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')]\nWe can now create the chain that we will use to do question-answering over.\nLet's first select a LLM.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nThis will follow the basic implementation from the\nRAG tutorial\n, but we will allow the retrieval step to be configurable.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nConfigurableField\ntemplate\n=\n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\nAPI Reference:\nChatPromptTemplate\n|\nConfigurableField\nHere we mark the retriever as having a configurable field. All vectorstore retrievers have\nsearch_kwargs\nas a field. This is just a dictionary, with vectorstore specific fields.\nThis will let us pass in a value for\nsearch_kwargs\nwhen invoking the chain.\nconfigurable_retriever\n=\nretriever\n.\nconfigurable_fields\n(\nsearch_kwargs\n=\nConfigurableField\n(\nid\n=\n\"search_kwargs\"\n,\nname\n=\n\"Search Kwargs\"\n,\ndescription\n=\n\"The search kwargs to use\"\n,\n)\n)\nWe can now create the chain using our configurable retriever.\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nStateGraph\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\ndef\nretrieve\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n:\nretrieved_docs\n=\nconfigurable_retriever\n.\ninvoke\n(\nstate\n[\n\"question\"\n]\n,\nconfig\n)\nreturn\n{\n\"context\"\n:\nretrieved_docs\n}\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nDocument\n|\nRunnableConfig\n|\nStateGraph\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nWe can now invoke the chain with configurable options.\nsearch_kwargs\nis the id of the configurable field. The value is the search kwargs to use for Pinecone.\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"Where did the user work?\"\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"search_kwargs\"\n:\n{\n\"namespace\"\n:\n\"harrison\"\n}\n}\n}\n,\n)\nresult\n{'question': 'Where did the user work?',\n'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],\n'answer': 'The user worked at Kensho.'}\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"Where did the user work?\"\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"search_kwargs\"\n:\n{\n\"namespace\"\n:\n\"ankush\"\n}\n}\n}\n,\n)\nresult\n{'question': 'Where did the user work?',\n'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],\n'answer': 'The user worked at Facebook.'}\nFor details operating your specific vector store, see the\nintegration pages\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/qa_sources/",
    "How-to guides\nHow to get your RAG application to return sources\nOn this page\nHow to get your RAG application to return sources\nOften in\nQ&A\napplications it's important to show users the sources that were used to generate the answer. The simplest way to do this is for the chain to return the Documents that were retrieved in each generation.\nWe'll work off of the Q&A app we built over the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng in the\nRAG tutorial\n.\nWe will cover two approaches:\nUsing the basic RAG chain covered in\nPart 1\nof the RAG tutorial;\nUsing a conversational RAG chain as convered in\nPart 2\nof the tutorial.\nWe will also show how to structure sources into the model response, such that a model can report what specific sources it used in generating its answer.\nSetup\nâ€‹\nDependencies\nâ€‹\nWe'll use the following packages:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain langchain\n-\ncommunity langchainhub beautifulsoup4\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nComponents\nâ€‹\nWe will need to select three components from LangChain's suite of integrations.\nA\nchat model\n:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nAn\nembedding model\n:\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nAnd a\nvector store\n:\nSelect\nvector store\n:\nIn-memory\nâ–¾\nIn-memory\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\npip install -qU langchain-core\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\nRAG application\nâ€‹\nLet's reconstruct the Q&A app with sources we built over the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng in the\nRAG tutorial\n.\nFirst we index our documents:\nimport\nbs4\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader\n(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,\n)\n,\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4\n.\nSoupStrainer\n(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n)\n,\n)\ndocs\n=\nloader\n.\nload\n(\n)\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\nAPI Reference:\nDocument\n# Index chunks\n_\n=\nvector_store\n.\nadd_documents\n(\ndocuments\n=\nall_splits\n)\nNext we build the application:\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nStateGraph\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\n# Define prompt for question-answering\nprompt\n=\nhub\n.\npull\n(\n\"rlm/rag-prompt\"\n)\n# Define state for application\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\n# Define application steps\ndef\nretrieve\n(\nstate\n:\nState\n)\n:\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nstate\n[\n\"question\"\n]\n)\nreturn\n{\n\"context\"\n:\nretrieved_docs\n}\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\n# Compile application and test\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nDocument\n|\nStateGraph\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nBecause we're tracking the retrieved context in our application's state, it is accessible after invoking the application:\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What is Task Decomposition?\"\n}\n)\nprint\n(\nf\"Context:\n{\nresult\n[\n'context'\n]\n}\n\\n\\n\"\n)\nprint\n(\nf\"Answer:\n{\nresult\n[\n'answer'\n]\n}\n\"\n)\nContext: [Document(id='c8471b37-07d8-4d51-856e-4b2c22bca88d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='acb7eb6f-f252-4353-aec2-f459135354ba', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4fae6668-7fec-4237-9b2d-78132f4f3f3f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='3c79dd86-595e-42e8-b64d-404780f9e2d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]\nAnswer: Task Decomposition is the process of breaking down a complex task into smaller, manageable steps to facilitate execution. This can be achieved through techniques like Chain of Thought, which encourages step-by-step reasoning, or Tree of Thoughts, which explores multiple reasoning paths for each step. It can be implemented using simple prompts, specific instructions, or human input to effectively tackle the original task.\nHere,\n\"context\"\ncontains the sources that the LLM used in generating the response in\n\"answer\"\n.\nStructure sources in model response\nâ€‹\nUp to this point, we've simply propagated the documents returned from the retrieval step through to the final response. But this may not illustrate what subset of information the model relied on when generating its answer. Below, we show how to structure sources into the model response, allowing the model to report what specific context it relied on for its answer.\nIt is straightforward to extend the above LangGraph implementation. Below, we make a simple change: we use the model's tool-calling features to generate\nstructured output\n, consisting of an answer and list of sources. The schema for the response is represented in the\nAnswerWithSources\nTypedDict, below.\nfrom\ntyping\nimport\nList\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\n# Desired schema for response\nclass\nAnswerWithSources\n(\nTypedDict\n)\n:\n\"\"\"An answer to the question, with sources.\"\"\"\nanswer\n:\nstr\nsources\n:\nAnnotated\n[\nList\n[\nstr\n]\n,\n.\n.\n.\n,\n\"List of sources (author + year) used to answer the question\"\n,\n]\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nAnswerWithSources\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nstructured_llm\n=\nllm\n.\nwith_structured_output\n(\nAnswerWithSources\n)\nresponse\n=\nstructured_llm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n}\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nimport\njson\nresult\n=\ngraph\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What is Chain of Thought?\"\n}\n)\nprint\n(\njson\n.\ndumps\n(\nresult\n[\n\"answer\"\n]\n,\nindent\n=\n2\n)\n)\n{\n\"answer\": \"Chain of Thought (CoT) is a prompting technique that enhances model performance by instructing it to think step by step, allowing the decomposition of complex tasks into smaller, manageable steps. This method not only aids in task execution but also provides insights into the model's reasoning process. CoT has become a standard approach in improving how language models handle intricate problem-solving tasks.\",\n\"sources\": [\n\"Wei et al. 2022\"\n]\n}\ntip\nView\nLangSmith trace\n.\nConversational RAG\nâ€‹\nPart 2\nof the RAG tutorial implements a different architecture, in which steps in the RAG flow are represented via successive\nmessage\nobjects. This leverages additional\ntool-calling\nfeatures of chat models, and more naturally accommodates a \"back-and-forth\" conversational user experience.\nIn that tutorial (and below), we propagate the retrieved documents as\nartifacts\non the tool messages. That makes it easy to pluck out the retrieved documents. Below, we add them as an additional key in the state, for convenience.\nNote that we define the response format of the tool as\n\"content_and_artifact\"\n:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve\n(\nquery\n:\nstr\n)\n:\n\"\"\"Retrieve information related to a query.\"\"\"\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nquery\n,\nk\n=\n2\n)\nserialized\n=\n\"\\n\\n\"\n.\njoin\n(\n(\nf\"Source:\n{\ndoc\n.\nmetadata\n}\n\\nContent:\n{\ndoc\n.\npage_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized\n,\nretrieved_docs\nAPI Reference:\ntool\nWe can now build and compile the exact same application as in\nPart 2\nof the RAG tutorial, with two changes:\nWe add a\ncontext\nkey of the state to store retrieved documents;\nIn the\ngenerate\nstep, we pluck out the retrieved documents and populate them in the state.\nThese changes are highlighted below.\nfrom\nlangchain_core\n.\nmessages\nimport\nSystemMessage\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nMessagesState\n,\nStateGraph\nfrom\nlanggraph\n.\nprebuilt\nimport\nToolNode\n,\ntools_condition\nclass\nState\n(\nMessagesState\n)\n:\ncontext\n:\nList\n[\nDocument\n]\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef\nquery_or_respond\n(\nstate\n:\nState\n)\n:\n\"\"\"Generate tool call for retrieval or respond.\"\"\"\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nretrieve\n]\n)\nresponse\n=\nllm_with_tools\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\n# MessagesState appends messages to state instead of overwriting\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\n# Step 2: Execute the retrieval.\ntools\n=\nToolNode\n(\n[\nretrieve\n]\n)\n# Step 3: Generate a response using the retrieved content.\ndef\ngenerate\n(\nstate\n:\nMessagesState\n)\n:\n\"\"\"Generate answer.\"\"\"\n# Get generated ToolMessages\nrecent_tool_messages\n=\n[\n]\nfor\nmessage\nin\nreversed\n(\nstate\n[\n\"messages\"\n]\n)\n:\nif\nmessage\n.\ntype\n==\n\"tool\"\n:\nrecent_tool_messages\n.\nappend\n(\nmessage\n)\nelse\n:\nbreak\ntool_messages\n=\nrecent_tool_messages\n[\n:\n:\n-\n1\n]\n# Format into prompt\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\ncontent\nfor\ndoc\nin\ntool_messages\n)\nsystem_message_content\n=\n(\n\"You are an assistant for question-answering tasks. \"\n\"Use the following pieces of retrieved context to answer \"\n\"the question. If you don't know the answer, say that you \"\n\"don't know. Use three sentences maximum and keep the \"\n\"answer concise.\"\n\"\\n\\n\"\nf\"\n{\ndocs_content\n}\n\"\n)\nconversation_messages\n=\n[\nmessage\nfor\nmessage\nin\nstate\n[\n\"messages\"\n]\nif\nmessage\n.\ntype\nin\n(\n\"human\"\n,\n\"system\"\n)\nor\n(\nmessage\n.\ntype\n==\n\"ai\"\nand\nnot\nmessage\n.\ntool_calls\n)\n]\nprompt\n=\n[\nSystemMessage\n(\nsystem_message_content\n)\n]\n+\nconversation_messages\n# Run\nresponse\n=\nllm\n.\ninvoke\n(\nprompt\n)\ncontext\n=\n[\n]\nfor\ntool_message\nin\ntool_messages\n:\ncontext\n.\nextend\n(\ntool_message\n.\nartifact\n)\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n,\n\"context\"\n:\ncontext\n}\nAPI Reference:\nSystemMessage\n|\nStateGraph\n|\nToolNode\n|\ntools_condition\nWe can compile the application as before:\ngraph_builder\n=\nStateGraph\n(\nMessagesState\n)\ngraph_builder\n.\nadd_node\n(\nquery_or_respond\n)\ngraph_builder\n.\nadd_node\n(\ntools\n)\ngraph_builder\n.\nadd_node\n(\ngenerate\n)\ngraph_builder\n.\nset_entry_point\n(\n\"query_or_respond\"\n)\ngraph_builder\n.\nadd_conditional_edges\n(\n\"query_or_respond\"\n,\ntools_condition\n,\n{\nEND\n:\nEND\n,\n\"tools\"\n:\n\"tools\"\n}\n,\n)\ngraph_builder\n.\nadd_edge\n(\n\"tools\"\n,\n\"generate\"\n)\ngraph_builder\n.\nadd_edge\n(\n\"generate\"\n,\nEND\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nInvoking our application, we see that the retrieved\nDocument\nobjects are accessible from the application state.\ninput_message\n=\n\"What is Task Decomposition?\"\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ninput_message\n}\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nstep\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat is Task Decomposition?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nretrieve (call_oA0XZ5hF70X0oW4ccNUFCFxX)\nCall ID: call_oA0XZ5hF70X0oW4ccNUFCFxX\nArgs:\nquery: Task Decomposition\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: retrieve\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTask Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often utilizes techniques like Chain of Thought (CoT) prompting, which encourages models to think step by step, enhancing performance on complex tasks. This approach helps clarify the model's reasoning and makes it easier to tackle difficult problems.\nstep\n[\n\"context\"\n]\n[Document(id='c8471b37-07d8-4d51-856e-4b2c22bca88d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'),\nDocument(id='acb7eb6f-f252-4353-aec2-f459135354ba', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]\ntip\nCheck out the\nLangSmith trace\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/qa_streaming/",
    "How-to guides\nHow to stream results from your RAG application\nOn this page\nHow to stream results from your RAG application\nThis guide explains how to stream results from a\nRAG\napplication. It covers streaming tokens from the final output as well as intermediate steps of a chain (e.g., from query re-writing).\nWe'll work off of the Q&A app with sources we built over the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng in the\nRAG tutorial\n.\nSetup\nâ€‹\nDependencies\nâ€‹\nWe'll use the following packages:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain langchain\n-\ncommunity langchainhub beautifulsoup4\nLangSmith\nâ€‹\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\nos\n.\nenviron\n[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos\n.\nenviron\n[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n)\nComponents\nâ€‹\nWe will need to select three components from LangChain's suite of integrations.\nA\nchat model\n:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nAn\nembedding model\n:\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nAnd a\nvector store\n:\nSelect\nvector store\n:\nIn-memory\nâ–¾\nIn-memory\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\npip install -qU langchain-core\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\nRAG application\nâ€‹\nLet's reconstruct the Q&A app with sources we built over the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng in the\nRAG tutorial\n.\nFirst we index our documents:\nimport\nbs4\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader\n(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,\n)\n,\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4\n.\nSoupStrainer\n(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n)\n,\n)\ndocs\n=\nloader\n.\nload\n(\n)\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndocs\n)\nAPI Reference:\nDocument\n# Index chunks\n_\n=\nvector_store\n.\nadd_documents\n(\ndocuments\n=\nall_splits\n)\nNext we build the application:\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nStateGraph\nfrom\ntyping_extensions\nimport\nList\n,\nTypedDict\n# Define prompt for question-answering\nprompt\n=\nhub\n.\npull\n(\n\"rlm/rag-prompt\"\n)\n# Define state for application\nclass\nState\n(\nTypedDict\n)\n:\nquestion\n:\nstr\ncontext\n:\nList\n[\nDocument\n]\nanswer\n:\nstr\n# Define application steps\ndef\nretrieve\n(\nstate\n:\nState\n)\n:\nretrieved_docs\n=\nvector_store\n.\nsimilarity_search\n(\nstate\n[\n\"question\"\n]\n)\nreturn\n{\n\"context\"\n:\nretrieved_docs\n}\ndef\ngenerate\n(\nstate\n:\nState\n)\n:\ndocs_content\n=\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\nstate\n[\n\"context\"\n]\n)\nmessages\n=\nprompt\n.\ninvoke\n(\n{\n\"question\"\n:\nstate\n[\n\"question\"\n]\n,\n\"context\"\n:\ndocs_content\n}\n)\nresponse\n=\nllm\n.\ninvoke\n(\nmessages\n)\nreturn\n{\n\"answer\"\n:\nresponse\n.\ncontent\n}\n# Compile application and test\ngraph_builder\n=\nStateGraph\n(\nState\n)\n.\nadd_sequence\n(\n[\nretrieve\n,\ngenerate\n]\n)\ngraph_builder\n.\nadd_edge\n(\nSTART\n,\n\"retrieve\"\n)\ngraph\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nDocument\n|\nStateGraph\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nStreaming final outputs\nâ€‹\nLangGraph supports several\nstreaming modes\n, which can be controlled by specifying the\nstream_mode\nparameter. Setting\nstream_mode=\"messages\"\nallows us to stream tokens from chat model invocations.\nIn general there can be multiple chat model invocations in an application (although here there is just one). Below, we filter to only the last step using the name of the corresponding node:\ninput_message\n=\n\"What is Task Decomposition?\"\nfor\nmessage\n,\nmetadata\nin\ngraph\n.\nstream\n(\n{\n\"question\"\n:\n\"What is Task Decomposition?\"\n}\n,\nstream_mode\n=\n\"messages\"\n,\n)\n:\nif\nmetadata\n[\n\"langgraph_node\"\n]\n==\n\"generate\"\n:\nprint\n(\nmessage\n.\ncontent\n,\nend\n=\n\"|\"\n)\n|Task| De|composition| is| a| technique| used| to| break| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| often| involves| prompting| models| to| \"|think| step| by| step|,\"| allowing| for| clearer| reasoning| and| better| performance| on| intricate| problems|.| This| can| be| achieved| through| various| methods|,| including| simple| prompts|,| task|-specific| instructions|,| or| human| input|.||\nStreaming intermediate steps\nâ€‹\nOther streaming modes will generally stream steps from our invocation-- i.e., state updates from individual nodes. In this case, each node is just appending a new key to the state:\nfor\nstep\nin\ngraph\n.\nstream\n(\n{\n\"question\"\n:\n\"What is Task Decomposition?\"\n}\n,\nstream_mode\n=\n\"updates\"\n,\n)\n:\nprint\n(\nf\"\n{\nstep\n}\n\\n\\n----------------\\n\"\n)\n{'retrieve': {'context': [Document(id='5bf5e308-6ccb-4f09-94d2-d0c36b8c9980', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='d8aed221-7943-414d-8ed7-63c2b0e7523b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='bfa87007-02ef-4f81-a008-4522ecea1025', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='6aff7fc0-5c21-4986-9f1e-91e89715d934', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}\n----------------\n{'generate': {'answer': 'Task Decomposition is the process of breaking down a complex task into smaller, manageable steps to enhance understanding and execution. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think through steps systematically, allowing for better problem-solving. It can be achieved through simple prompting, task-specific instructions, or human input.'}}\n----------------\nFor more on streaming with LangGraph, check out its\nstreaming documentation\n. For more information on streaming individual LangChain\nRunnables\n, refer to\nthis guide\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/recursive_json_splitter/",
    "How-to guides\nHow to split JSON data\nOn this page\nHow to split JSON data\nThis json splitter\nsplits\njson data while allowing control over chunk sizes. It traverses json data depth first and builds smaller json chunks. It attempts to keep nested json objects whole but will split them if needed to keep chunks between a min_chunk_size and the max_chunk_size.\nIf the value is not a nested json, but rather a very large string the string will not be split. If you need a hard cap on the chunk size consider composing this with a Recursive Text splitter on those chunks. There is an optional pre-processing step to split lists, by first converting them to json (dict) and then splitting them as such.\nHow the text is split: json value.\nHow the chunk size is measured: by number of characters.\n%\npip install\n-\nqU langchain\n-\ntext\n-\nsplitters\nFirst we load some json data:\nimport\njson\nimport\nrequests\n# This is a large nested json object and will be loaded as a python dict\njson_data\n=\nrequests\n.\nget\n(\n\"https://api.smith.langchain.com/openapi.json\"\n)\n.\njson\n(\n)\nBasic usage\nâ€‹\nSpecify\nmax_chunk_size\nto constrain chunk sizes:\nfrom\nlangchain_text_splitters\nimport\nRecursiveJsonSplitter\nsplitter\n=\nRecursiveJsonSplitter\n(\nmax_chunk_size\n=\n300\n)\nTo obtain json chunks, use the\n.split_json\nmethod:\n# Recursively split json data - If you need to access/manipulate the smaller json chunks\njson_chunks\n=\nsplitter\n.\nsplit_json\n(\njson_data\n=\njson_data\n)\nfor\nchunk\nin\njson_chunks\n[\n:\n3\n]\n:\nprint\n(\nchunk\n)\n{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'servers': [{'url': 'https://api.smith.langchain.com', 'description': 'LangSmith API endpoint.'}]}\n{'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.', 'operationId': 'read_tracer_session_api_v1_sessions__session_id__get'}}}}\n{'paths': {'/api/v1/sessions/{session_id}': {'get': {'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\nTo obtain LangChain\nDocument\nobjects, use the\n.create_documents\nmethod:\n# The splitter can also output documents\ndocs\n=\nsplitter\n.\ncreate_documents\n(\ntexts\n=\n[\njson_data\n]\n)\nfor\ndoc\nin\ndocs\n[\n:\n3\n]\n:\nprint\n(\ndoc\n)\npage_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"servers\": [{\"url\": \"https://api.smith.langchain.com\", \"description\": \"LangSmith API endpoint.\"}]}'\npage_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}'\npage_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\nOr use\n.split_text\nto obtain string content directly:\ntexts\n=\nsplitter\n.\nsplit_text\n(\njson_data\n=\njson_data\n)\nprint\n(\ntexts\n[\n0\n]\n)\nprint\n(\ntexts\n[\n1\n]\n)\n{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"servers\": [{\"url\": \"https://api.smith.langchain.com\", \"description\": \"LangSmith API endpoint.\"}]}\n{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}\nHow to manage chunk sizes from list content\nâ€‹\nNote that one of the chunks in this example is larger than the specified\nmax_chunk_size\nof 300. Reviewing one of these chunks that was bigger we see there is a list object there:\nprint\n(\n[\nlen\n(\ntext\n)\nfor\ntext\nin\ntexts\n]\n[\n:\n10\n]\n)\nprint\n(\n)\nprint\n(\ntexts\n[\n3\n]\n)\n[171, 231, 126, 469, 210, 213, 237, 271, 191, 232]\n{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"include_stats\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"boolean\", \"default\": false, \"title\": \"Include Stats\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}\nThe json splitter by default does not split lists.\nSpecify\nconvert_lists=True\nto preprocess the json, converting list content to dicts with\nindex:item\nas\nkey:val\npairs:\ntexts\n=\nsplitter\n.\nsplit_text\n(\njson_data\n=\njson_data\n,\nconvert_lists\n=\nTrue\n)\nLet's look at the size of the chunks. Now they are all under the max\nprint\n(\n[\nlen\n(\ntext\n)\nfor\ntext\nin\ntexts\n]\n[\n:\n10\n]\n)\n[176, 236, 141, 203, 212, 221, 210, 213, 242, 291]\nThe list has been converted to a dict, but retains all the needed contextual information even if split into many chunks:\nprint\n(\ntexts\n[\n1\n]\n)\n{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": {\"0\": \"tracer-sessions\"}, \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}\n# We can also look at the documents\ndocs\n[\n1\n]\nDocument(page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}')\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/recursive_text_splitter/",
    "How-to guides\nHow to recursively split text by characters\nOn this page\nHow to recursively split text by characters\nThis\ntext splitter\nis the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is\n[\"\\n\\n\", \"\\n\", \" \", \"\"]\n. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\nHow the text is split: by list of characters.\nHow the chunk size is measured: by number of characters.\nBelow we show example usage.\nTo obtain the string content directly, use\n.split_text\n.\nTo create LangChain\nDocument\nobjects (e.g., for use in downstream tasks), use\n.create_documents\n.\n%\npip install\n-\nqU langchain\n-\ntext\n-\nsplitters\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load example document\nwith\nopen\n(\n\"state_of_the_union.txt\"\n)\nas\nf\n:\nstate_of_the_union\n=\nf\n.\nread\n(\n)\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\n# Set a really small chunk size, just to show.\nchunk_size\n=\n100\n,\nchunk_overlap\n=\n20\n,\nlength_function\n=\nlen\n,\nis_separator_regex\n=\nFalse\n,\n)\ntexts\n=\ntext_splitter\n.\ncreate_documents\n(\n[\nstate_of_the_union\n]\n)\nprint\n(\ntexts\n[\n0\n]\n)\nprint\n(\ntexts\n[\n1\n]\n)\npage_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and'\npage_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.'\ntext_splitter\n.\nsplit_text\n(\nstate_of_the_union\n)\n[\n:\n2\n]\n['Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and',\n'of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.']\nLet's go through the parameters set above for\nRecursiveCharacterTextSplitter\n:\nchunk_size\n: The maximum size of a chunk, where size is determined by the\nlength_function\n.\nchunk_overlap\n: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.\nlength_function\n: Function determining the chunk size.\nis_separator_regex\n: Whether the separator list (defaulting to\n[\"\\n\\n\", \"\\n\", \" \", \"\"]\n) should be interpreted as regex.\nSplitting text from languages without word boundaries\nâ€‹\nSome writing systems do not have\nword boundaries\n, for example Chinese, Japanese, and Thai. Splitting text with the default separator list of\n[\"\\n\\n\", \"\\n\", \" \", \"\"]\ncan cause words to be split between chunks. To keep words together, you can override the list of separators to include additional punctuation:\nAdd ASCII full-stop \"\n.\n\",\nUnicode fullwidth\nfull stop \"\nï¼Ž\n\" (used in Chinese text), and\nideographic full stop\n\"\nã€‚\n\" (used in Japanese and Chinese)\nAdd\nZero-width space\nused in Thai, Myanmar, Kmer, and Japanese.\nAdd ASCII comma \"\n,\n\", Unicode fullwidth comma \"\nï¼Œ\n\", and Unicode ideographic comma \"\nã€\n\"\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nseparators\n=\n[\n\"\\n\\n\"\n,\n\"\\n\"\n,\n\" \"\n,\n\".\"\n,\n\",\"\n,\n\"\\u200b\"\n,\n# Zero-width space\n\"\\uff0c\"\n,\n# Fullwidth comma\n\"\\u3001\"\n,\n# Ideographic comma\n\"\\uff0e\"\n,\n# Fullwidth full stop\n\"\\u3002\"\n,\n# Ideographic full stop\n\"\"\n,\n]\n,\n# Existing args\n)\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/response_metadata/",
    "How-to guides\nResponse metadata\nOn this page\nResponse metadata\nMany model providers include some metadata in their chat generation\nresponses\n. This metadata can be accessed via the\nAIMessage.response_metadata: Dict\nattribute. Depending on the model provider and model configuration, this can contain information like\ntoken counts\n,\nlogprobs\n, and more.\nHere's what the response metadata looks like for a few different providers:\nOpenAI\nâ€‹\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nmsg\n=\nllm\n.\ninvoke\n(\n\"What's the oldest known example of cuneiform\"\n)\nmsg\n.\nresponse_metadata\n{'token_usage': {'completion_tokens': 88,\n'prompt_tokens': 16,\n'total_tokens': 104,\n'completion_tokens_details': {'accepted_prediction_tokens': 0,\n'audio_tokens': 0,\n'reasoning_tokens': 0,\n'rejected_prediction_tokens': 0},\n'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n'model_name': 'gpt-4o-mini-2024-07-18',\n'system_fingerprint': 'fp_34a54ae93c',\n'id': 'chatcmpl-ByN1Qkvqb5fAGKKzXXxZ3rBlnqkWs',\n'service_tier': 'default',\n'finish_reason': 'stop',\n'logprobs': None}\nAnthropic\nâ€‹\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n)\nmsg\n=\nllm\n.\ninvoke\n(\n\"What's the oldest known example of cuneiform\"\n)\nmsg\n.\nresponse_metadata\n{'id': 'msg_01NTWnqvbNKSjGfqQL7xikau',\n'model': 'claude-3-7-sonnet-20250219',\n'stop_reason': 'end_turn',\n'stop_sequence': None,\n'usage': {'cache_creation_input_tokens': 0,\n'cache_read_input_tokens': 0,\n'input_tokens': 17,\n'output_tokens': 197,\n'server_tool_use': None,\n'service_tier': 'standard'},\n'model_name': 'claude-3-7-sonnet-20250219'}\nGoogle Generative AI\nâ€‹\nfrom\nlangchain_google_genai\nimport\nChatGoogleGenerativeAI\nllm\n=\nChatGoogleGenerativeAI\n(\nmodel\n=\n\"gemini-2.5-flash\"\n)\nmsg\n=\nllm\n.\ninvoke\n(\n\"What's the oldest known example of cuneiform\"\n)\nmsg\n.\nresponse_metadata\n{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n'finish_reason': 'STOP',\n'model_name': 'gemini-2.5-flash',\n'safety_ratings': []}\nBedrock (Anthropic)\nâ€‹\nfrom\nlangchain_aws\nimport\nChatBedrockConverse\nllm\n=\nChatBedrockConverse\n(\nmodel\n=\n\"anthropic.claude-3-7-sonnet-20250219-v1:0\"\n)\nmsg\n=\nllm\n.\ninvoke\n(\n\"What's the oldest known example of cuneiform\"\n)\nmsg\n.\nresponse_metadata\n{'ResponseMetadata': {'RequestId': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31',\n'HTTPStatusCode': 200,\n'HTTPHeaders': {'date': 'Sat, 22 Mar 2025 11:27:46 GMT',\n'content-type': 'application/json',\n'content-length': '1660',\n'connection': 'keep-alive',\n'x-amzn-requestid': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31'},\n'RetryAttempts': 0},\n'stopReason': 'end_turn',\n'metrics': {'latencyMs': [11044]}}\nMistralAI\nâ€‹\nfrom\nlangchain_mistralai\nimport\nChatMistralAI\nllm\n=\nChatMistralAI\n(\nmodel\n=\n\"mistral-small-latest\"\n)\nmsg\n=\nllm\n.\ninvoke\n(\n[\n(\n\"human\"\n,\n\"What's the oldest known example of cuneiform\"\n)\n]\n)\nmsg\n.\nresponse_metadata\n{'token_usage': {'prompt_tokens': 13,\n'total_tokens': 306,\n'completion_tokens': 293},\n'model_name': 'mistral-small-latest',\n'model': 'mistral-small-latest',\n'finish_reason': 'stop'}\nGroq\nâ€‹\nfrom\nlangchain_groq\nimport\nChatGroq\nllm\n=\nChatGroq\n(\nmodel\n=\n\"llama-3.1-8b-instant\"\n)\nmsg\n=\nllm\n.\ninvoke\n(\n\"What's the oldest known example of cuneiform\"\n)\nmsg\n.\nresponse_metadata\n{'token_usage': {'completion_tokens': 184,\n'prompt_tokens': 45,\n'total_tokens': 229,\n'completion_time': 0.245333333,\n'prompt_time': 0.002262803,\n'queue_time': 0.19315161,\n'total_time': 0.247596136},\n'model_name': 'llama-3.1-8b-instant',\n'system_fingerprint': 'fp_a56f6eea01',\n'finish_reason': 'stop',\n'logprobs': None}\nFireworksAI\nâ€‹\nfrom\nlangchain_fireworks\nimport\nChatFireworks\nllm\n=\nChatFireworks\n(\nmodel\n=\n\"accounts/fireworks/models/llama-v3p1-70b-instruct\"\n)\nmsg\n=\nllm\n.\ninvoke\n(\n\"What's the oldest known example of cuneiform\"\n)\nmsg\n.\nresponse_metadata\n{'token_usage': {'prompt_tokens': 25,\n'total_tokens': 352,\n'completion_tokens': 327},\n'model_name': 'accounts/fireworks/models/llama-v3p1-70b-instruct',\n'system_fingerprint': '',\n'finish_reason': 'stop',\n'logprobs': None}\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/runnable_runtime_secrets/",
    "How-to guides\nHow to pass runtime secrets to runnables\nHow to pass runtime secrets to runnables\nRequires\nlangchain-core >= 0.2.22\nWe can pass in secrets to our\nrunnables\nat runtime using the\nRunnableConfig\n. Specifically we can pass in secrets with a\n__\nprefix to the\nconfigurable\nfield. This will ensure that these secrets aren't traced as part of the invocation:\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nfoo\n(\nx\n:\nint\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nint\n:\n\"\"\"Sum x and a secret int\"\"\"\nreturn\nx\n+\nconfig\n[\n\"configurable\"\n]\n[\n\"__top_secret_int\"\n]\nfoo\n.\ninvoke\n(\n{\n\"x\"\n:\n5\n}\n,\n{\n\"configurable\"\n:\n{\n\"__top_secret_int\"\n:\n2\n,\n\"traced_key\"\n:\n\"bar\"\n}\n}\n)\nAPI Reference:\nRunnableConfig\n|\ntool\n7\nLooking at the LangSmith trace for this run, we can see that \"traced_key\" was recorded (as part of Metadata) while our secret int was not:\nhttps://smith.langchain.com/public/aa7e3289-49ca-422d-a408-f6b927210170/r\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/self_query/",
    "How-to guides\nHow to do \"self-querying\" retrieval\nOn this page\nHow to do \"self-querying\" retrieval\ninfo\nHead to\nIntegrations\nfor documentation on vector stores with built-in support for self-querying.\nA self-querying\nretriever\nis one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying\nvector store\n. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\nGet started\nâ€‹\nFor demonstration purposes we'll use a\nChroma\nvector store. We've created a small demo set of documents that contain summaries of movies.\nNote:\nThe self-query retriever requires you to have\nlark\npackage installed.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  lark langchain\n-\nchroma\nfrom\nlangchain_chroma\nimport\nChroma\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\ndocs\n=\n[\nDocument\n(\npage_content\n=\n\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\"\n,\nmetadata\n=\n{\n\"year\"\n:\n1993\n,\n\"rating\"\n:\n7.7\n,\n\"genre\"\n:\n\"science fiction\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\"\n,\nmetadata\n=\n{\n\"year\"\n:\n2010\n,\n\"director\"\n:\n\"Christopher Nolan\"\n,\n\"rating\"\n:\n8.2\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\"\n,\nmetadata\n=\n{\n\"year\"\n:\n2006\n,\n\"director\"\n:\n\"Satoshi Kon\"\n,\n\"rating\"\n:\n8.6\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"A bunch of normal-sized women are supremely wholesome and some men pine after them\"\n,\nmetadata\n=\n{\n\"year\"\n:\n2019\n,\n\"director\"\n:\n\"Greta Gerwig\"\n,\n\"rating\"\n:\n8.3\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Toys come alive and have a blast doing so\"\n,\nmetadata\n=\n{\n\"year\"\n:\n1995\n,\n\"genre\"\n:\n\"animated\"\n}\n,\n)\n,\nDocument\n(\npage_content\n=\n\"Three men walk into the Zone, three men walk out of the Zone\"\n,\nmetadata\n=\n{\n\"year\"\n:\n1979\n,\n\"director\"\n:\n\"Andrei Tarkovsky\"\n,\n\"genre\"\n:\n\"thriller\"\n,\n\"rating\"\n:\n9.9\n,\n}\n,\n)\n,\n]\nvectorstore\n=\nChroma\n.\nfrom_documents\n(\ndocs\n,\nOpenAIEmbeddings\n(\n)\n)\nAPI Reference:\nDocument\nCreating our self-querying retriever\nâ€‹\nNow we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\nfrom\nlangchain\n.\nchains\n.\nquery_constructor\n.\nschema\nimport\nAttributeInfo\nfrom\nlangchain\n.\nretrievers\n.\nself_query\n.\nbase\nimport\nSelfQueryRetriever\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmetadata_field_info\n=\n[\nAttributeInfo\n(\nname\n=\n\"genre\"\n,\ndescription\n=\n\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\"\n,\ntype\n=\n\"string\"\n,\n)\n,\nAttributeInfo\n(\nname\n=\n\"year\"\n,\ndescription\n=\n\"The year the movie was released\"\n,\ntype\n=\n\"integer\"\n,\n)\n,\nAttributeInfo\n(\nname\n=\n\"director\"\n,\ndescription\n=\n\"The name of the movie director\"\n,\ntype\n=\n\"string\"\n,\n)\n,\nAttributeInfo\n(\nname\n=\n\"rating\"\n,\ndescription\n=\n\"A 1-10 rating for the movie\"\n,\ntype\n=\n\"float\"\n)\n,\n]\ndocument_content_description\n=\n\"Brief summary of a movie\"\nllm\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\nretriever\n=\nSelfQueryRetriever\n.\nfrom_llm\n(\nllm\n,\nvectorstore\n,\ndocument_content_description\n,\nmetadata_field_info\n,\n)\nTesting it out\nâ€‹\nAnd now we can actually try using our retriever!\n# This example only specifies a filter\nretriever\n.\ninvoke\n(\n\"I want to watch a movie rated higher than 8.5\"\n)\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979}),\nDocument(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006})]\n# This example specifies a query and a filter\nretriever\n.\ninvoke\n(\n\"Has Greta Gerwig directed any movies about women\"\n)\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'director': 'Greta Gerwig', 'rating': 8.3, 'year': 2019})]\n# This example specifies a composite filter\nretriever\n.\ninvoke\n(\n\"What's a highly rated (above 8.5) science fiction film?\"\n)\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}),\nDocument(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979})]\n# This example specifies a query and composite filter\nretriever\n.\ninvoke\n(\n\"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]\nFilter k\nâ€‹\nWe can also use the self query retriever to specify\nk\n: the number of documents to fetch.\nWe can do this by passing\nenable_limit=True\nto the constructor.\nretriever\n=\nSelfQueryRetriever\n.\nfrom_llm\n(\nllm\n,\nvectorstore\n,\ndocument_content_description\n,\nmetadata_field_info\n,\nenable_limit\n=\nTrue\n,\n)\n# This example only specifies a relevant query\nretriever\n.\ninvoke\n(\n\"What are two movies about dinosaurs\"\n)\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993}),\nDocument(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]\nConstructing from scratch with LCEL\nâ€‹\nTo see what's going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.\nFirst, we need to create a query-construction chain. This chain will take a user query and generated a\nStructuredQuery\nobject which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we'll ignore here for simplicity.\nfrom\nlangchain\n.\nchains\n.\nquery_constructor\n.\nbase\nimport\n(\nStructuredQueryOutputParser\n,\nget_query_constructor_prompt\n,\n)\nprompt\n=\nget_query_constructor_prompt\n(\ndocument_content_description\n,\nmetadata_field_info\n,\n)\noutput_parser\n=\nStructuredQueryOutputParser\n.\nfrom_components\n(\n)\nquery_constructor\n=\nprompt\n|\nllm\n|\noutput_parser\nLet's look at our prompt:\nprint\n(\nprompt\n.\nformat\n(\nquery\n=\n\"dummy question\"\n)\n)\nYour goal is to structure the user's query to match the request schema provided below.\n<< Structured Request Schema >>\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\n\\`\\`\\`json\n{\n\"query\": string \\ text string to compare to document contents\n\"filter\": string \\ logical condition statement for filtering documents\n}\n\\`\\`\\`\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\nA logical condition statement is composed of one or more comparison and logical operation statements.\nA comparison statement takes the form: `comp(attr, val)`:\n- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator\n- `attr` (string):  name of attribute to apply the comparison to\n- `val` (string): is the comparison value\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\n- `op` (and | or | not): logical operator\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\nMake sure that you only use the comparators and logical operators listed above and no others.\nMake sure that filters only refer to attributes that exist in the data source.\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.\n<< Example 1. >>\nData Source:\n\\`\\`\\`json\n{\n\"content\": \"Lyrics of a song\",\n\"attributes\": {\n\"artist\": {\n\"type\": \"string\",\n\"description\": \"Name of the song artist\"\n},\n\"length\": {\n\"type\": \"integer\",\n\"description\": \"Length of the song in seconds\"\n},\n\"genre\": {\n\"type\": \"string\",\n\"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\n}\n}\n}\n\\`\\`\\`\nUser Query:\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre\nStructured Request:\n\\`\\`\\`json\n{\n\"query\": \"teenager love\",\n\"filter\": \"and(or(eq(\\\"artist\\\", \\\"Taylor Swift\\\"), eq(\\\"artist\\\", \\\"Katy Perry\\\")), lt(\\\"length\\\", 180), eq(\\\"genre\\\", \\\"pop\\\"))\"\n}\n\\`\\`\\`\n<< Example 2. >>\nData Source:\n\\`\\`\\`json\n{\n\"content\": \"Lyrics of a song\",\n\"attributes\": {\n\"artist\": {\n\"type\": \"string\",\n\"description\": \"Name of the song artist\"\n},\n\"length\": {\n\"type\": \"integer\",\n\"description\": \"Length of the song in seconds\"\n},\n\"genre\": {\n\"type\": \"string\",\n\"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\n}\n}\n}\n\\`\\`\\`\nUser Query:\nWhat are songs that were not published on Spotify\nStructured Request:\n\\`\\`\\`json\n{\n\"query\": \"\",\n\"filter\": \"NO_FILTER\"\n}\n\\`\\`\\`\n<< Example 3. >>\nData Source:\n\\`\\`\\`json\n{\n\"content\": \"Brief summary of a movie\",\n\"attributes\": {\n\"genre\": {\n\"description\": \"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n\"type\": \"string\"\n},\n\"year\": {\n\"description\": \"The year the movie was released\",\n\"type\": \"integer\"\n},\n\"director\": {\n\"description\": \"The name of the movie director\",\n\"type\": \"string\"\n},\n\"rating\": {\n\"description\": \"A 1-10 rating for the movie\",\n\"type\": \"float\"\n}\n}\n}\n\\`\\`\\`\nUser Query:\ndummy question\nStructured Request:\nAnd what our full chain produces:\nquery_constructor\n.\ninvoke\n(\n{\n\"query\"\n:\n\"What are some sci-fi movies from the 90's directed by Luc Besson about taxi drivers\"\n}\n)\nStructuredQuery(query='taxi driver', filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='science fiction'), Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GTE: 'gte'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2000)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Luc Besson')]), limit=None)\nThe query constructor is the key element of the self-query retriever. To make a great retrieval system you'll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data,\ncheck out this cookbook\n.\nThe next key element is the structured query translator. This is the object responsible for translating the generic\nStructuredQuery\nobject into a metadata filter in the syntax of the vector store you're using. LangChain comes with a number of built-in translators. To see them all head to the\nIntegrations section\n.\nfrom\nlangchain_community\n.\nquery_constructors\n.\nchroma\nimport\nChromaTranslator\nretriever\n=\nSelfQueryRetriever\n(\nquery_constructor\n=\nquery_constructor\n,\nvectorstore\n=\nvectorstore\n,\nstructured_query_translator\n=\nChromaTranslator\n(\n)\n,\n)\nretriever\n.\ninvoke\n(\n\"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/semantic-chunker/",
    "How-to guides\nHow to split text based on semantic similarity\nOn this page\nHow to split text based on semantic similarity\nTaken from Greg Kamradt's wonderful notebook:\n5_Levels_Of_Text_Splitting\nAll credit to him.\nThis guide covers how to split chunks based on their semantic similarity. If embeddings are sufficiently far apart, chunks are split.\nAt a high level, this splits into sentences, then groups into groups of 3\nsentences, and then merges one that are similar in the embedding space.\nInstall Dependencies\nâ€‹\n!pip install\n-\n-\nquiet langchain_experimental langchain_openai\nLoad Example Data\nâ€‹\n# This is a long document we can split up.\nwith\nopen\n(\n\"state_of_the_union.txt\"\n)\nas\nf\n:\nstate_of_the_union\n=\nf\n.\nread\n(\n)\nCreate Text Splitter\nâ€‹\nTo instantiate a\nSemanticChunker\n, we must specify an embedding model. Below we will use\nOpenAIEmbeddings\n.\nfrom\nlangchain_experimental\n.\ntext_splitter\nimport\nSemanticChunker\nfrom\nlangchain_openai\n.\nembeddings\nimport\nOpenAIEmbeddings\ntext_splitter\n=\nSemanticChunker\n(\nOpenAIEmbeddings\n(\n)\n)\nSplit Text\nâ€‹\nWe split text in the usual way, e.g., by invoking\n.create_documents\nto create LangChain\nDocument\nobjects:\ndocs\n=\ntext_splitter\n.\ncreate_documents\n(\n[\nstate_of_the_union\n]\n)\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n)\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament â€œLight will win over darkness.â€ The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history weâ€™ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.\nBreakpoints\nâ€‹\nThis chunker works by determining when to \"break\" apart sentences. This is done by looking for differences in embeddings between any two sentences. When that difference is past some threshold, then they are split.\nThere are a few ways to determine what that threshold is, which are controlled by the\nbreakpoint_threshold_type\nkwarg.\nNote: if the resulting chunk sizes are too small/big, the additional kwargs\nbreakpoint_threshold_amount\nand\nmin_chunk_size\ncan be used for adjustments.\nPercentile\nâ€‹\nThe default way to split is based on percentile. In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split. The default value for X is 95.0 and can be adjusted by the keyword argument\nbreakpoint_threshold_amount\nwhich expects a number between 0.0 and 100.0.\ntext_splitter\n=\nSemanticChunker\n(\nOpenAIEmbeddings\n(\n)\n,\nbreakpoint_threshold_type\n=\n\"percentile\"\n)\ndocs\n=\ntext_splitter\n.\ncreate_documents\n(\n[\nstate_of_the_union\n]\n)\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n)\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament â€œLight will win over darkness.â€ The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history weâ€™ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.\nprint\n(\nlen\n(\ndocs\n)\n)\n26\nStandard Deviation\nâ€‹\nIn this method, any difference greater than X standard deviations is split. The default value for X is 3.0 and can be adjusted by the keyword argument\nbreakpoint_threshold_amount\n.\ntext_splitter\n=\nSemanticChunker\n(\nOpenAIEmbeddings\n(\n)\n,\nbreakpoint_threshold_type\n=\n\"standard_deviation\"\n)\ndocs\n=\ntext_splitter\n.\ncreate_documents\n(\n[\nstate_of_the_union\n]\n)\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n)\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament â€œLight will win over darkness.â€ The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history weâ€™ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving. And the costs and the threats to America and the world keep rising. Thatâ€™s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. The United States is a member along with 29 other nations. It matters. American diplomacy matters. American resolve matters. Putinâ€™s latest attack on Ukraine was premeditated and unprovoked. He rejected repeated efforts at diplomacy. He thought the West and NATO wouldnâ€™t respond. And he thought he could divide us at home. Putin was wrong. We were ready. Here is what we did. We prepared extensively and carefully. We spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. I spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression. We countered Russiaâ€™s lies with truth. And now that he has acted the free world is holding him accountable. Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. Together with our allies â€“we are right now enforcing powerful economic sanctions. We are cutting off Russiaâ€™s largest banks from the international financial system. Preventing Russiaâ€™s central bank from defending the Russian Ruble making Putinâ€™s $630 Billion â€œwar fundâ€ worthless. We are choking off Russiaâ€™s access to technology that will sap its economic strength and weaken its military for years to come. Tonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs. We are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. And tonight I am announcing that we will join our allies in closing off American air space to all Russian flights â€“ further isolating Russia â€“ and adding an additional squeeze â€“on their economy. The Ruble has lost 30% of its value. The Russian stock market has lost 40% of its value and trading remains suspended. Russiaâ€™s economy is reeling and Putin alone is to blame. Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. We are giving more than $1 Billion in direct assistance to Ukraine. And we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering. Let me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine. Our forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies â€“ in the event that Putin decides to keep moving west. For that purpose weâ€™ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. As I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power. And we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them. Putin has unleashed violence and chaos. But while he may make gains on the battlefield â€“ he will pay a continuing high price over the long run. And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards. To all Americans, I will be honest with you, as Iâ€™ve always promised. A Russian dictator, invading a foreign country, has costs around the world. And Iâ€™m taking robust action to make sure the pain of our sanctions  is targeted at Russiaâ€™s economy. And I will use every tool at our disposal to protect American businesses and consumers. Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world. America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies. These steps will help blunt gas prices here at home. And I know the news about whatâ€™s happening can seem alarming.\nprint\n(\nlen\n(\ndocs\n)\n)\n4\nInterquartile\nâ€‹\nIn this method, the interquartile distance is used to split chunks. The interquartile range can be scaled by the keyword argument\nbreakpoint_threshold_amount\n, the default value is 1.5.\ntext_splitter\n=\nSemanticChunker\n(\nOpenAIEmbeddings\n(\n)\n,\nbreakpoint_threshold_type\n=\n\"interquartile\"\n)\ndocs\n=\ntext_splitter\n.\ncreate_documents\n(\n[\nstate_of_the_union\n]\n)\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n)\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament â€œLight will win over darkness.â€ The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history weâ€™ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.\nprint\n(\nlen\n(\ndocs\n)\n)\n25\nGradient\nâ€‹\nIn this method, the gradient of distance is used to split chunks along with the percentile method. This method is useful when chunks are highly correlated with each other or specific to a domain e.g. legal or medical. The idea is to apply anomaly detection on gradient array so that the distribution become wider and easy to identify boundaries in highly semantic data.\nSimilar to the percentile method, the split can be adjusted by the keyword argument\nbreakpoint_threshold_amount\nwhich expects a number between 0.0 and 100.0, the default value is 95.0.\ntext_splitter\n=\nSemanticChunker\n(\nOpenAIEmbeddings\n(\n)\n,\nbreakpoint_threshold_type\n=\n\"gradient\"\n)\ndocs\n=\ntext_splitter\n.\ncreate_documents\n(\n[\nstate_of_the_union\n]\n)\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n)\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman.\nprint\n(\nlen\n(\ndocs\n)\n)\n26\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/sequence/",
    "How-to guides\nHow to chain runnables\nOn this page\nHow to chain runnables\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Expression Language (LCEL)\nPrompt templates\nChat models\nOutput parser\nOne point about\nLangChain Expression Language\nis that any two runnables can be \"chained\" together into sequences. The output of the previous runnable's\n.invoke()\ncall is passed as input to the next runnable. This can be done using the pipe operator (\n|\n), or the more explicit\n.pipe()\nmethod, which does the same thing.\nThe resulting\nRunnableSequence\nis itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like\nLangSmith\n.\nThe pipe operator:\n|\nâ€‹\nTo show off how this works, let's go through an example. We'll walk through a common pattern in LangChain: using a\nprompt template\nto format input into a\nchat model\n, and finally converting the chat message output into a string with an\noutput parser\n.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"tell me a joke about {topic}\"\n)\nchain\n=\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\nPrompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:\nchain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\n\"Why don't bears wear shoes?\\n\\nBecause they prefer to go bear-foot!\"\nCoercion\nâ€‹\nWe can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.\nFor example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.\nWe would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a\nRunnableParallel\n, which runs all of its values in parallel and returns a dict with the results.\nThis happens to be the same format the next prompt template expects. Here it is in action:\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nanalysis_prompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"is this a funny joke? {joke}\"\n)\ncomposed_chain\n=\n{\n\"joke\"\n:\nchain\n}\n|\nanalysis_prompt\n|\nmodel\n|\nStrOutputParser\n(\n)\ncomposed_chain\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"bears\"\n}\n)\nAPI Reference:\nStrOutputParser\n'Yes, that\\'s a funny joke! It\\'s a classic pun that plays on the homophone pair \"bare-foot\" and \"bear-foot.\" The humor comes from:\\n\\n1. The wordplay between \"barefoot\" (not wearing shoes) and \"bear-foot\" (the foot of a bear)\\n2. The logical connection to the setup (bears don\\'t wear shoes)\\n3. It\\'s family-friendly and accessible\\n4. It\\'s a simple, clean pun that creates an unexpected but satisfying punchline\\n\\nIt\\'s the kind of joke that might make you groan and smile at the same time - what people often call a \"dad joke.\"'\nFunctions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:\ncomposed_chain_with_lambda\n=\n(\nchain\n|\n(\nlambda\ninput\n:\n{\n\"joke\"\n:\ninput\n}\n)\n|\nanalysis_prompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n)\ncomposed_chain_with_lambda\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"beets\"\n}\n)\n'Yes, that\\'s a cute and funny joke! It works well because:\\n\\n1. It plays on the double meaning of \"roots\" - both the literal roots of the beet plant and the metaphorical sense of knowing one\\'s origins or foundation\\n2. It\\'s a simple, clean pun that doesn\\'t rely on offensive content\\n3. It has a satisfying logical connection (beets are root vegetables)\\n\\nIt\\'s the kind of wholesome food pun that might make people groan a little but also smile. Perfect for sharing in casual conversation or with kids!'\nHowever, keep in mind that using functions like this may interfere with operations like streaming. See\nthis section\nfor more information.\nThe\n.pipe()\nmethod\nâ€‹\nWe could also compose the same sequence using the\n.pipe()\nmethod. Here's what that looks like:\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableParallel\ncomposed_chain_with_pipe\n=\n(\nRunnableParallel\n(\n{\n\"joke\"\n:\nchain\n}\n)\n.\npipe\n(\nanalysis_prompt\n)\n.\npipe\n(\nmodel\n)\n.\npipe\n(\nStrOutputParser\n(\n)\n)\n)\ncomposed_chain_with_pipe\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"battlestar galactica\"\n}\n)\nAPI Reference:\nRunnableParallel\n\"This joke is moderately funny! It plays on Battlestar Galactica lore where Cylons are robots with 12 different models trying to infiltrate human society. The humor comes from the idea of a Cylon accidentally revealing their non-human nature through a pickup line that references their artificial origins. It's a decent nerd-culture joke that would land well with fans of the show, though someone unfamiliar with Battlestar Galactica might not get the reference. The punchline effectively highlights the contradiction in a Cylon trying to blend in while simultaneously revealing their true identity.\"\nOr the abbreviated:\ncomposed_chain_with_pipe\n=\nRunnableParallel\n(\n{\n\"joke\"\n:\nchain\n}\n)\n.\npipe\n(\nanalysis_prompt\n,\nmodel\n,\nStrOutputParser\n(\n)\n)\nRelated\nâ€‹\nStreaming\n: Check out the streaming guide to understand the streaming behavior of a chain\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/serialization/",
    "How-to guides\nHow to save and load LangChain objects\nOn this page\nHow to save and load LangChain objects\nLangChain classes implement standard methods for serialization. Serializing LangChain objects using these methods confer some advantages:\nSecrets, such as API keys, are separated from other parameters and can be loaded back to the object on de-serialization;\nDe-serialization is kept compatible across package versions, so objects that were serialized with one version of LangChain can be properly de-serialized with another.\nTo save and load LangChain objects using this system, use the\ndumpd\n,\ndumps\n,\nload\n, and\nloads\nfunctions in the\nload module\nof\nlangchain-core\n. These functions support JSON and JSON-serializable objects.\nAll LangChain objects that inherit from\nSerializable\nare JSON-serializable. Examples include\nmessages\n,\ndocument objects\n(e.g., as returned from\nretrievers\n), and most\nRunnables\n, such as chat models, retrievers, and\nchains\nimplemented with the LangChain Expression Language.\nBelow we walk through an example with a simple\nLLM chain\n.\ncaution\nDe-serialization using\nload\nand\nloads\ncan instantiate any serializable LangChain object. Only use this feature with trusted inputs!\nDe-serialization is a beta feature and is subject to change.\nfrom\nlangchain_core\n.\nload\nimport\ndumpd\n,\ndumps\n,\nload\n,\nloads\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"Translate the following into {language}:\"\n)\n,\n(\n\"user\"\n,\n\"{text}\"\n)\n,\n]\n,\n)\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\napi_key\n=\n\"llm-api-key\"\n)\nchain\n=\nprompt\n|\nllm\nAPI Reference:\ndumpd\n|\ndumps\n|\nload\n|\nloads\n|\nChatPromptTemplate\nSaving objects\nâ€‹\nTo json\nâ€‹\nstring_representation\n=\ndumps\n(\nchain\n,\npretty\n=\nTrue\n)\nprint\n(\nstring_representation\n[\n:\n500\n]\n)\n{\n\"lc\": 1,\n\"type\": \"constructor\",\n\"id\": [\n\"langchain\",\n\"schema\",\n\"runnable\",\n\"RunnableSequence\"\n],\n\"kwargs\": {\n\"first\": {\n\"lc\": 1,\n\"type\": \"constructor\",\n\"id\": [\n\"langchain\",\n\"prompts\",\n\"chat\",\n\"ChatPromptTemplate\"\n],\n\"kwargs\": {\n\"input_variables\": [\n\"language\",\n\"text\"\n],\n\"messages\": [\n{\n\"lc\": 1,\n\"type\": \"constructor\",\nTo a json-serializable Python dict\nâ€‹\ndict_representation\n=\ndumpd\n(\nchain\n)\nprint\n(\ntype\n(\ndict_representation\n)\n)\n<class 'dict'>\nTo disk\nâ€‹\nimport\njson\nwith\nopen\n(\n\"/tmp/chain.json\"\n,\n\"w\"\n)\nas\nfp\n:\njson\n.\ndump\n(\nstring_representation\n,\nfp\n)\nNote that the API key is withheld from the serialized representations. Parameters that are considered secret are specified by the\n.lc_secrets\nattribute of the LangChain object:\nchain\n.\nlast\n.\nlc_secrets\n{'openai_api_key': 'OPENAI_API_KEY'}\nLoading objects\nâ€‹\nSpecifying\nsecrets_map\nin\nload\nand\nloads\nwill load the corresponding secrets onto the de-serialized LangChain object.\nFrom string\nâ€‹\nchain\n=\nloads\n(\nstring_representation\n,\nsecrets_map\n=\n{\n\"OPENAI_API_KEY\"\n:\n\"llm-api-key\"\n}\n)\nFrom dict\nâ€‹\nchain\n=\nload\n(\ndict_representation\n,\nsecrets_map\n=\n{\n\"OPENAI_API_KEY\"\n:\n\"llm-api-key\"\n}\n)\nFrom disk\nâ€‹\nwith\nopen\n(\n\"/tmp/chain.json\"\n,\n\"r\"\n)\nas\nfp\n:\nchain\n=\nloads\n(\njson\n.\nload\n(\nfp\n)\n,\nsecrets_map\n=\n{\n\"OPENAI_API_KEY\"\n:\n\"llm-api-key\"\n}\n)\nNote that we recover the API key specified at the start of the guide:\nchain\n.\nlast\n.\nopenai_api_key\n.\nget_secret_value\n(\n)\n'llm-api-key'\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/split_by_token/",
    "How-to guides\nHow to split text by tokens\nOn this page\nHow to split text by tokens\nLanguage models have a\ntoken\nlimit. You should not exceed the token limit. When you\nsplit your text\ninto chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.\ntiktoken\nâ€‹\nnote\ntiktoken\nis a fast\nBPE\ntokenizer created by\nOpenAI\n.\nWe can use\ntiktoken\nto estimate tokens used. It will probably be more accurate for the OpenAI models.\nHow the text is split: by character passed in.\nHow the chunk size is measured: by\ntiktoken\ntokenizer.\nCharacterTextSplitter\n,\nRecursiveCharacterTextSplitter\n, and\nTokenTextSplitter\ncan be used with\ntiktoken\ndirectly.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\ntext\n-\nsplitters tiktoken\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\n# This is a long document we can split up.\nwith\nopen\n(\n\"state_of_the_union.txt\"\n)\nas\nf\n:\nstate_of_the_union\n=\nf\n.\nread\n(\n)\nTo split with a\nCharacterTextSplitter\nand then merge chunks with\ntiktoken\n, use its\n.from_tiktoken_encoder()\nmethod. Note that splits from this method can be larger than the chunk size measured by the\ntiktoken\ntokenizer.\nThe\n.from_tiktoken_encoder()\nmethod takes either\nencoding_name\nas an argument (e.g.\ncl100k_base\n), or the\nmodel_name\n(e.g.\ngpt-4\n). All additional arguments like\nchunk_size\n,\nchunk_overlap\n, and\nseparators\nare used to instantiate\nCharacterTextSplitter\n:\ntext_splitter\n=\nCharacterTextSplitter\n.\nfrom_tiktoken_encoder\n(\nencoding_name\n=\n\"cl100k_base\"\n,\nchunk_size\n=\n100\n,\nchunk_overlap\n=\n0\n)\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\nstate_of_the_union\n)\nprint\n(\ntexts\n[\n0\n]\n)\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\nLast year COVID-19 kept us apart. This year we are finally together again.\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.\nWith a duty to one another to the American people to the Constitution.\nTo implement a hard constraint on the chunk size, we can use\nRecursiveCharacterTextSplitter.from_tiktoken_encoder\n, where each split will be recursively split if it has a larger size:\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n.\nfrom_tiktoken_encoder\n(\nmodel_name\n=\n\"gpt-4\"\n,\nchunk_size\n=\n100\n,\nchunk_overlap\n=\n0\n,\n)\nWe can also load a\nTokenTextSplitter\nsplitter, which works with\ntiktoken\ndirectly and will ensure each split is smaller than chunk size.\nfrom\nlangchain_text_splitters\nimport\nTokenTextSplitter\ntext_splitter\n=\nTokenTextSplitter\n(\nchunk_size\n=\n10\n,\nchunk_overlap\n=\n0\n)\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\nstate_of_the_union\n)\nprint\n(\ntexts\n[\n0\n]\n)\nMadam Speaker, Madam Vice President, our\nSome written languages (e.g. Chinese and Japanese) have characters which encode to 2 or more tokens. Using the\nTokenTextSplitter\ndirectly can split the tokens for a character between two chunks causing malformed Unicode characters. Use\nRecursiveCharacterTextSplitter.from_tiktoken_encoder\nor\nCharacterTextSplitter.from_tiktoken_encoder\nto ensure chunks contain valid Unicode strings.\nspaCy\nâ€‹\nnote\nspaCy\nis an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\nLangChain implements splitters based on the\nspaCy tokenizer\n.\nHow the text is split: by\nspaCy\ntokenizer.\nHow the chunk size is measured: by number of characters.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  spacy\n# This is a long document we can split up.\nwith\nopen\n(\n\"state_of_the_union.txt\"\n)\nas\nf\n:\nstate_of_the_union\n=\nf\n.\nread\n(\n)\nfrom\nlangchain_text_splitters\nimport\nSpacyTextSplitter\ntext_splitter\n=\nSpacyTextSplitter\n(\nchunk_size\n=\n1000\n)\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\nstate_of_the_union\n)\nprint\n(\ntexts\n[\n0\n]\n)\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman.\nMembers of Congress and the Cabinet.\nJustices of the Supreme Court.\nMy fellow Americans.\nLast year COVID-19 kept us apart.\nThis year we are finally together again.\nTonight, we meet as Democrats Republicans and Independents.\nBut most importantly as Americans.\nWith a duty to one another to the American people to the Constitution.\nAnd with an unwavering resolve that freedom will always triumph over tyranny.\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.\nBut he badly miscalculated.\nHe thought he could roll into Ukraine and the world would roll over.\nInstead he met a wall of strength he never imagined.\nHe met the Ukrainian people.\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\nSentenceTransformers\nâ€‹\nThe\nSentenceTransformersTokenTextSplitter\nis a specialized text splitter for use with the sentence-transformer models. The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use.\nTo split text and constrain token counts according to the sentence-transformers tokenizer, instantiate a\nSentenceTransformersTokenTextSplitter\n. You can optionally specify:\nchunk_overlap\n: integer count of token overlap;\nmodel_name\n: sentence-transformer model name, defaulting to\n\"sentence-transformers/all-mpnet-base-v2\"\n;\ntokens_per_chunk\n: desired token count per chunk.\nfrom\nlangchain_text_splitters\nimport\nSentenceTransformersTokenTextSplitter\nsplitter\n=\nSentenceTransformersTokenTextSplitter\n(\nchunk_overlap\n=\n0\n)\ntext\n=\n\"Lorem \"\ncount_start_and_stop_tokens\n=\n2\ntext_token_count\n=\nsplitter\n.\ncount_tokens\n(\ntext\n=\ntext\n)\n-\ncount_start_and_stop_tokens\nprint\n(\ntext_token_count\n)\n2\ntoken_multiplier\n=\nsplitter\n.\nmaximum_tokens_per_chunk\n//\ntext_token_count\n+\n1\n# `text_to_split` does not fit in a single chunk\ntext_to_split\n=\ntext\n*\ntoken_multiplier\nprint\n(\nf\"tokens in text to split:\n{\nsplitter\n.\ncount_tokens\n(\ntext\n=\ntext_to_split\n)\n}\n\"\n)\ntokens in text to split: 514\ntext_chunks\n=\nsplitter\n.\nsplit_text\n(\ntext\n=\ntext_to_split\n)\nprint\n(\ntext_chunks\n[\n1\n]\n)\nlorem\nNLTK\nâ€‹\nnote\nThe Natural Language Toolkit\n, or more commonly\nNLTK\n, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.\nRather than just splitting on \"\\n\\n\", we can use\nNLTK\nto split based on\nNLTK tokenizers\n.\nHow the text is split: by\nNLTK\ntokenizer.\nHow the chunk size is measured: by number of characters.\n# pip install nltk\n# This is a long document we can split up.\nwith\nopen\n(\n\"state_of_the_union.txt\"\n)\nas\nf\n:\nstate_of_the_union\n=\nf\n.\nread\n(\n)\nfrom\nlangchain_text_splitters\nimport\nNLTKTextSplitter\ntext_splitter\n=\nNLTKTextSplitter\n(\nchunk_size\n=\n1000\n)\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\nstate_of_the_union\n)\nprint\n(\ntexts\n[\n0\n]\n)\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman.\nMembers of Congress and the Cabinet.\nJustices of the Supreme Court.\nMy fellow Americans.\nLast year COVID-19 kept us apart.\nThis year we are finally together again.\nTonight, we meet as Democrats Republicans and Independents.\nBut most importantly as Americans.\nWith a duty to one another to the American people to the Constitution.\nAnd with an unwavering resolve that freedom will always triumph over tyranny.\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.\nBut he badly miscalculated.\nHe thought he could roll into Ukraine and the world would roll over.\nInstead he met a wall of strength he never imagined.\nHe met the Ukrainian people.\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\nGroups of citizens blocking tanks with their bodies.\nKoNLPY\nâ€‹\nnote\nKoNLPy: Korean NLP in Python\nis is a Python package for natural language processing (NLP) of the Korean language.\nToken splitting involves the segmentation of text into smaller, more manageable units called tokens. These tokens are often words, phrases, symbols, or other meaningful elements crucial for further processing and analysis. In languages like English, token splitting typically involves separating words by spaces and punctuation marks. The effectiveness of token splitting largely depends on the tokenizer's understanding of the language structure, ensuring the generation of meaningful tokens. Since tokenizers designed for the English language are not equipped to understand the unique semantic structures of other languages, such as Korean, they cannot be effectively used for Korean language processing.\nToken splitting for Korean with KoNLPy's Kkma Analyzer\nâ€‹\nIn case of Korean text, KoNLPY includes at morphological analyzer called\nKkma\n(Korean Knowledge Morpheme Analyzer).\nKkma\nprovides detailed morphological analysis of Korean text. It breaks down sentences into words and words into their respective morphemes, identifying parts of speech for each token. It can segment a block of text into individual sentences, which is particularly useful for processing long texts.\nUsage Considerations\nâ€‹\nWhile\nKkma\nis renowned for its detailed analysis, it is important to note that this precision may impact processing speed. Thus,\nKkma\nis best suited for applications where analytical depth is prioritized over rapid text processing.\n# pip install konlpy\n# This is a long Korean document that we want to split up into its component sentences.\nwith\nopen\n(\n\"./your_korean_doc.txt\"\n)\nas\nf\n:\nkorean_document\n=\nf\n.\nread\n(\n)\nfrom\nlangchain_text_splitters\nimport\nKonlpyTextSplitter\ntext_splitter\n=\nKonlpyTextSplitter\n(\n)\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\nkorean_document\n)\n# The sentences are split with \"\\n\\n\" characters.\nprint\n(\ntexts\n[\n0\n]\n)\nì¶˜í–¥ì „ ì˜›ë‚ ì— ë‚¨ì›ì— ì´ ë„ë ¹ì´ë¼ëŠ” ë²¼ìŠ¬ì•„ì¹˜ ì•„ë“¤ì´ ìžˆì—ˆë‹¤.\nê·¸ì˜ ì™¸ëª¨ëŠ” ë¹›ë‚˜ëŠ” ë‹¬ì²˜ëŸ¼ ìž˜ìƒê²¼ê³ , ê·¸ì˜ í•™ì‹ê³¼ ê¸°ì˜ˆëŠ” ë‚¨ë³´ë‹¤ ë›°ì–´ë‚¬ë‹¤.\ní•œíŽ¸, ì´ ë§ˆì„ì—ëŠ” ì¶˜í–¥ì´ë¼ëŠ” ì ˆì„¸ ê°€ì¸ì´ ì‚´ê³  ìžˆì—ˆë‹¤.\nì¶˜ í–¥ì˜ ì•„ë¦„ë‹¤ì›€ì€ ê½ƒê³¼ ê°™ì•„ ë§ˆì„ ì‚¬ëžŒë“¤ ë¡œë¶€í„° ë§Žì€ ì‚¬ëž‘ì„ ë°›ì•˜ë‹¤.\nì–´ëŠ ë´„ë‚ , ë„ë ¹ì€ ì¹œêµ¬ë“¤ê³¼ ë†€ëŸ¬ ë‚˜ê°”ë‹¤ê°€ ì¶˜ í–¥ì„ ë§Œ ë‚˜ ì²« ëˆˆì— ë°˜í•˜ê³  ë§ì•˜ë‹¤.\në‘ ì‚¬ëžŒì€ ì„œë¡œ ì‚¬ëž‘í•˜ê²Œ ë˜ì—ˆê³ , ì´ë‚´ ë¹„ë°€ìŠ¤ëŸ¬ìš´ ì‚¬ëž‘ì˜ ë§¹ì„¸ë¥¼ ë‚˜ëˆ„ì—ˆë‹¤.\ní•˜ì§€ë§Œ ì¢‹ì€ ë‚ ë“¤ì€ ì˜¤ëž˜ê°€ì§€ ì•Šì•˜ë‹¤.\në„ë ¹ì˜ ì•„ë²„ì§€ê°€ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì „ê·¼ì„ ê°€ê²Œ ë˜ì–´ ë„ë ¹ë„ ë– ë‚˜ ì•¼ë§Œ í–ˆë‹¤.\nì´ë³„ì˜ ì•„í”” ì†ì—ì„œë„, ë‘ ì‚¬ëžŒì€ ìž¬íšŒë¥¼ ê¸°ì•½í•˜ë©° ì„œë¡œë¥¼ ë¯¿ê³  ê¸°ë‹¤ë¦¬ê¸°ë¡œ í–ˆë‹¤.\nê·¸ëŸ¬ë‚˜ ìƒˆë¡œ ë¶€ìž„í•œ ê´€ì•„ì˜ ì‚¬ë˜ê°€ ì¶˜ í–¥ì˜ ì•„ë¦„ë‹¤ì›€ì— ìš•ì‹¬ì„ ë‚´ ì–´ ê·¸ë…€ì—ê²Œ ê°•ìš”ë¥¼ ì‹œìž‘í–ˆë‹¤.\nì¶˜ í–¥ ì€ ë„ë ¹ì— ëŒ€í•œ ìžì‹ ì˜ ì‚¬ëž‘ì„ ì§€í‚¤ê¸° ìœ„í•´, ì‚¬ë˜ì˜ ìš”êµ¬ë¥¼ ë‹¨í˜¸ížˆ ê±°ì ˆí–ˆë‹¤.\nì´ì— ë¶„ë…¸í•œ ì‚¬ë˜ëŠ” ì¶˜ í–¥ì„ ê°ì˜¥ì— ê°€ë‘ê³  í˜¹ë…í•œ í˜•ë²Œì„ ë‚´ë ¸ë‹¤.\nì´ì•¼ê¸°ëŠ” ì´ ë„ë ¹ì´ ê³ ìœ„ ê´€ì§ì— ì˜¤ë¥¸ í›„, ì¶˜ í–¥ì„ êµ¬í•´ ë‚´ëŠ” ê²ƒìœ¼ë¡œ ëë‚œë‹¤.\në‘ ì‚¬ëžŒì€ ì˜¤ëžœ ì‹œë ¨ ëì— ë‹¤ì‹œ ë§Œë‚˜ê²Œ ë˜ê³ , ê·¸ë“¤ì˜ ì‚¬ëž‘ì€ ì˜¨ ì„¸ìƒì— ì „í•´ ì§€ë©° í›„ì„¸ì—ê¹Œì§€ ì´ì–´ì§„ë‹¤.\n- ì¶˜í–¥ì „ (The Tale of Chunhyang)\nHugging Face tokenizer\nâ€‹\nHugging Face\nhas many tokenizers.\nWe use Hugging Face tokenizer, the\nGPT2TokenizerFast\nto count the text length in tokens.\nHow the text is split: by character passed in.\nHow the chunk size is measured: by number of tokens calculated by the\nHugging Face\ntokenizer.\nfrom\ntransformers\nimport\nGPT2TokenizerFast\ntokenizer\n=\nGPT2TokenizerFast\n.\nfrom_pretrained\n(\n\"gpt2\"\n)\n# This is a long document we can split up.\nwith\nopen\n(\n\"state_of_the_union.txt\"\n)\nas\nf\n:\nstate_of_the_union\n=\nf\n.\nread\n(\n)\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\ntext_splitter\n=\nCharacterTextSplitter\n.\nfrom_huggingface_tokenizer\n(\ntokenizer\n,\nchunk_size\n=\n100\n,\nchunk_overlap\n=\n0\n)\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\nstate_of_the_union\n)\nprint\n(\ntexts\n[\n0\n]\n)\nMadam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\nLast year COVID-19 kept us apart. This year we are finally together again.\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.\nWith a duty to one another to the American people to the Constitution.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/split_html/",
    "How-to guides\nHow to split HTML\nOn this page\nHow to split HTML\nSplitting HTML documents into manageable chunks is essential for various text processing tasks such as natural language processing, search indexing, and more. In this guide, we will explore three different text splitters provided by LangChain that you can use to split HTML content effectively:\nHTMLHeaderTextSplitter\nHTMLSectionSplitter\nHTMLSemanticPreservingSplitter\nEach of these splitters has unique features and use cases. This guide will help you understand the differences between them, why you might choose one over the others, and how to use them effectively.\n%pip install -qU langchain-text-splitters\nOverview of the Splitters\nâ€‹\nHTMLHeaderTextSplitter\nâ€‹\ninfo\nUseful when you want to preserve the hierarchical structure of a document based on its headings.\nDescription\n: Splits HTML text based on header tags (e.g.,\n<h1>\n,\n<h2>\n,\n<h3>\n, etc.), and adds metadata for each header relevant to any given chunk.\nCapabilities\n:\nSplits text at the HTML element level.\nPreserves context-rich information encoded in document structures.\nCan return chunks element by element or combine elements with the same metadata.\nHTMLSectionSplitter\nâ€‹\ninfo\nUseful when you want to split HTML documents into larger sections, such as\n<section>\n,\n<div>\n, or custom-defined sections.\nDescription\n: Similar to HTMLHeaderTextSplitter but focuses on splitting HTML into sections based on specified tags.\nCapabilities\n:\nUses XSLT transformations to detect and split sections.\nInternally uses\nRecursiveCharacterTextSplitter\nfor large sections.\nConsiders font sizes to determine sections.\nHTMLSemanticPreservingSplitter\nâ€‹\ninfo\nIdeal when you need to ensure that structured elements are not split across chunks, preserving contextual relevancy.\nDescription\n: Splits HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components.\nCapabilities\n:\nPreserves tables, lists, and other specified HTML elements.\nAllows custom handlers for specific HTML tags.\nEnsures that the semantic meaning of the document is maintained.\nBuilt in normalization & stopword removal\nChoosing the Right Splitter\nâ€‹\nUse\nHTMLHeaderTextSplitter\nwhen\n: You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.\nUse\nHTMLSectionSplitter\nwhen\n: You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.\nUse\nHTMLSemanticPreservingSplitter\nwhen\n: You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.\nFeature\nHTMLHeaderTextSplitter\nHTMLSectionSplitter\nHTMLSemanticPreservingSplitter\nSplits based on headers\nYes\nYes\nYes\nPreserves semantic elements (tables, lists)\nNo\nNo\nYes\nAdds metadata for headers\nYes\nYes\nYes\nCustom handlers for HTML tags\nNo\nNo\nYes\nPreserves media (images, videos)\nNo\nNo\nYes\nConsiders font sizes\nNo\nYes\nNo\nUses XSLT transformations\nNo\nYes\nNo\nExample HTML Document\nâ€‹\nLet's use the following HTML document as an example:\nhtml_string\n=\n\"\"\"\n<!DOCTYPE html>\n<html lang='en'>\n<head>\n<meta charset='UTF-8'>\n<meta name='viewport' content='width=device-width, initial-scale=1.0'>\n<title>Fancy Example HTML Page</title>\n</head>\n<body>\n<h1>Main Title</h1>\n<p>This is an introductory paragraph with some basic content.</p>\n<h2>Section 1: Introduction</h2>\n<p>This section introduces the topic. Below is a list:</p>\n<ul>\n<li>First item</li>\n<li>Second item</li>\n<li>Third item with <strong>bold text</strong> and <a href='#'>a link</a></li>\n</ul>\n<h3>Subsection 1.1: Details</h3>\n<p>This subsection provides additional details. Here's a table:</p>\n<table border='1'>\n<thead>\n<tr>\n<th>Header 1</th>\n<th>Header 2</th>\n<th>Header 3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Row 1, Cell 1</td>\n<td>Row 1, Cell 2</td>\n<td>Row 1, Cell 3</td>\n</tr>\n<tr>\n<td>Row 2, Cell 1</td>\n<td>Row 2, Cell 2</td>\n<td>Row 2, Cell 3</td>\n</tr>\n</tbody>\n</table>\n<h2>Section 2: Media Content</h2>\n<p>This section contains an image and a video:</p>\n<img src='example_image_link.mp4' alt='Example Image'>\n<video controls width='250' src='example_video_link.mp4' type='video/mp4'>\nYour browser does not support the video tag.\n</video>\n<h2>Section 3: Code Example</h2>\n<p>This section contains a code block:</p>\n<pre><code data-lang=\"html\">\n&lt;div&gt;\n&lt;p&gt;This is a paragraph inside a div.&lt;/p&gt;\n&lt;/div&gt;\n</code></pre>\n<h2>Conclusion</h2>\n<p>This is the conclusion of the document.</p>\n</body>\n</html>\n\"\"\"\nUsing HTMLHeaderTextSplitter\nâ€‹\nHTMLHeaderTextSplitter\nis a \"structure-aware\"\ntext splitter\nthat splits text at the HTML element level and adds metadata for each header \"relevant\" to any given chunk. It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures. It can be used with other text splitters as part of a chunking pipeline.\nIt is analogous to the\nMarkdownHeaderTextSplitter\nfor markdown files.\nTo specify what headers to split on, specify\nheaders_to_split_on\nwhen instantiating\nHTMLHeaderTextSplitter\nas shown below.\nfrom\nlangchain_text_splitters\nimport\nHTMLHeaderTextSplitter\nheaders_to_split_on\n=\n[\n(\n\"h1\"\n,\n\"Header 1\"\n)\n,\n(\n\"h2\"\n,\n\"Header 2\"\n)\n,\n(\n\"h3\"\n,\n\"Header 3\"\n)\n,\n]\nhtml_splitter\n=\nHTMLHeaderTextSplitter\n(\nheaders_to_split_on\n)\nhtml_header_splits\n=\nhtml_splitter\n.\nsplit_text\n(\nhtml_string\n)\nhtml_header_splits\n[Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some basic content.'),\nDocument(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a list:  \\nFirst item Second item Third item with bold text and a link'),\nDocument(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction', 'Header 3': 'Subsection 1.1: Details'}, page_content=\"This subsection provides additional details. Here's a table:\"),\nDocument(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video:'),\nDocument(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block:'),\nDocument(metadata={'Header 1': 'Main Title', 'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]\nTo return each element together with their associated headers, specify\nreturn_each_element=True\nwhen instantiating\nHTMLHeaderTextSplitter\n:\nhtml_splitter\n=\nHTMLHeaderTextSplitter\n(\nheaders_to_split_on\n,\nreturn_each_element\n=\nTrue\n,\n)\nhtml_header_splits_elements\n=\nhtml_splitter\n.\nsplit_text\n(\nhtml_string\n)\nComparing with the above, where elements are aggregated by their headers:\nfor\nelement\nin\nhtml_header_splits\n[\n:\n2\n]\n:\nprint\n(\nelement\n)\npage_content='This is an introductory paragraph with some basic content.' metadata={'Header 1': 'Main Title'}\npage_content='This section introduces the topic. Below is a list:\nFirst item Second item Third item with bold text and a link' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}\nNow each element is returned as a distinct\nDocument\n:\nfor\nelement\nin\nhtml_header_splits_elements\n[\n:\n3\n]\n:\nprint\n(\nelement\n)\npage_content='This is an introductory paragraph with some basic content.' metadata={'Header 1': 'Main Title'}\npage_content='This section introduces the topic. Below is a list:' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}\npage_content='First item Second item Third item with bold text and a link' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}\nHow to split from a URL or HTML file:\nâ€‹\nTo read directly from a URL, pass the URL string into the\nsplit_text_from_url\nmethod.\nSimilarly, a local HTML file can be passed to the\nsplit_text_from_file\nmethod.\nurl\n=\n\"https://plato.stanford.edu/entries/goedel/\"\nheaders_to_split_on\n=\n[\n(\n\"h1\"\n,\n\"Header 1\"\n)\n,\n(\n\"h2\"\n,\n\"Header 2\"\n)\n,\n(\n\"h3\"\n,\n\"Header 3\"\n)\n,\n(\n\"h4\"\n,\n\"Header 4\"\n)\n,\n]\nhtml_splitter\n=\nHTMLHeaderTextSplitter\n(\nheaders_to_split_on\n)\n# for local file use html_splitter.split_text_from_file(<path_to_file>)\nhtml_header_splits\n=\nhtml_splitter\n.\nsplit_text_from_url\n(\nurl\n)\nHow to constrain chunk sizes:\nâ€‹\nHTMLHeaderTextSplitter\n, which splits based on HTML headers, can be composed with another splitter which constrains splits based on character lengths, such as\nRecursiveCharacterTextSplitter\n.\nThis can be done using the\n.split_documents\nmethod of the second splitter:\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nchunk_size\n=\n500\nchunk_overlap\n=\n30\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\nchunk_size\n,\nchunk_overlap\n=\nchunk_overlap\n)\n# Split\nsplits\n=\ntext_splitter\n.\nsplit_documents\n(\nhtml_header_splits\n)\nsplits\n[\n80\n:\n85\n]\n[Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='We see that GÃ¶del first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (â€œThis sentence is falseâ€) and Berryâ€™s paradox (â€œThe least number not defined by an expression consisting of just fourteen English wordsâ€). GÃ¶del then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth'),\nDocument(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='means that arithmetic truth and arithmetic provability are not co-extensive â€” whence the First Incompleteness Theorem.'),\nDocument(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='This account of GÃ¶delâ€™s discovery was told to Hao Wang very much after the fact; but in GÃ¶delâ€™s contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See GÃ¶del 2003a and GÃ¶del 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by GÃ¶del by 1931. But he neither publicized nor published the result; the biases logicians'),\nDocument(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to GÃ¶delâ€™s publication of that theorem.'),\nDocument(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}, page_content='We now describe the proof of the two theorems, formulating GÃ¶delâ€™s results in Peano arithmetic. GÃ¶del himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following GÃ¶delâ€™s notation.')]\nLimitations\nâ€‹\nThere can be quite a bit of structural variation from one HTML document to another, and while\nHTMLHeaderTextSplitter\nwill attempt to attach all \"relevant\" headers to any given chunk, it can sometimes miss certain headers. For example, the algorithm assumes an informational hierarchy in which headers are always at nodes \"above\" associated text, i.e. prior siblings, ancestors, and combinations thereof. In the following news article (as of the writing of this document), the document is structured such that the text of the top-level headline, while tagged \"h1\", is in a\ndistinct\nsubtree from the text elements that we'd expect it to be\n\"above\"\nâ€”so we can observe that the \"h1\" element and its associated text do not show up in the chunk metadata (but, where applicable, we do see \"h2\" and its associated text):\nurl\n=\n\"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html\"\nheaders_to_split_on\n=\n[\n(\n\"h1\"\n,\n\"Header 1\"\n)\n,\n(\n\"h2\"\n,\n\"Header 2\"\n)\n,\n]\nhtml_splitter\n=\nHTMLHeaderTextSplitter\n(\nheaders_to_split_on\n)\nhtml_header_splits\n=\nhtml_splitter\n.\nsplit_text_from_url\n(\nurl\n)\nprint\n(\nhtml_header_splits\n[\n1\n]\n.\npage_content\n[\n:\n500\n]\n)\nNo two El NiÃ±o winters are the same, but many have temperature and precipitation trends in common.\nAverage conditions during an El NiÃ±o winter across the continental US.\nOne of the major reasons is the position of the jet stream, which often shifts south during an El NiÃ±o winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA.\nBecause the jet stream is essentially a river of air that storms flow through, they c\nUsing HTMLSectionSplitter\nâ€‹\nSimilar in concept to the\nHTMLHeaderTextSplitter\n, the\nHTMLSectionSplitter\nis a \"structure-aware\"\ntext splitter\nthat splits text at the element level and adds metadata for each header \"relevant\" to any given chunk. It lets you split HTML by sections.\nIt can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures.\nUse\nxslt_path\nto provide an absolute path to transform the HTML so that it can detect sections based on provided tags. The default is to use the\nconverting_to_header.xslt\nfile in the\ndata_connection/document_transformers\ndirectory. This is for converting the html to a format/layout that is easier to detect sections. For example,\nspan\nbased on their font size can be converted to header tags to be detected as a section.\nHow to split HTML strings:\nâ€‹\nfrom\nlangchain_text_splitters\nimport\nHTMLSectionSplitter\nheaders_to_split_on\n=\n[\n(\n\"h1\"\n,\n\"Header 1\"\n)\n,\n(\n\"h2\"\n,\n\"Header 2\"\n)\n,\n]\nhtml_splitter\n=\nHTMLSectionSplitter\n(\nheaders_to_split_on\n)\nhtml_header_splits\n=\nhtml_splitter\n.\nsplit_text\n(\nhtml_string\n)\nhtml_header_splits\n[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title \\n This is an introductory paragraph with some basic content.'),\nDocument(metadata={'Header 2': 'Section 1: Introduction'}, page_content=\"Section 1: Introduction \\n This section introduces the topic. Below is a list: \\n \\n First item \\n Second item \\n Third item with  bold text  and  a link \\n \\n \\n Subsection 1.1: Details \\n This subsection provides additional details. Here's a table: \\n \\n \\n \\n Header 1 \\n Header 2 \\n Header 3 \\n \\n \\n \\n \\n Row 1, Cell 1 \\n Row 1, Cell 2 \\n Row 1, Cell 3 \\n \\n \\n Row 2, Cell 1 \\n Row 2, Cell 2 \\n Row 2, Cell 3\"),\nDocument(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Section 2: Media Content \\n This section contains an image and a video: \\n \\n \\n      Your browser does not support the video tag.'),\nDocument(metadata={'Header 2': 'Section 3: Code Example'}, page_content='Section 3: Code Example \\n This section contains a code block: \\n \\n    <div>\\n      <p>This is a paragraph inside a div.</p>\\n    </div>'),\nDocument(metadata={'Header 2': 'Conclusion'}, page_content='Conclusion \\n This is the conclusion of the document.')]\nHow to constrain chunk sizes:\nâ€‹\nHTMLSectionSplitter\ncan be used with other text splitters as part of a chunking pipeline. Internally, it uses the\nRecursiveCharacterTextSplitter\nwhen the section size is larger than the chunk size. It also considers the font size of the text to determine whether it is a section or not based on the determined font size threshold.\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nheaders_to_split_on\n=\n[\n(\n\"h1\"\n,\n\"Header 1\"\n)\n,\n(\n\"h2\"\n,\n\"Header 2\"\n)\n,\n(\n\"h3\"\n,\n\"Header 3\"\n)\n,\n]\nhtml_splitter\n=\nHTMLSectionSplitter\n(\nheaders_to_split_on\n)\nhtml_header_splits\n=\nhtml_splitter\n.\nsplit_text\n(\nhtml_string\n)\nchunk_size\n=\n50\nchunk_overlap\n=\n5\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\nchunk_size\n,\nchunk_overlap\n=\nchunk_overlap\n)\n# Split\nsplits\n=\ntext_splitter\n.\nsplit_documents\n(\nhtml_header_splits\n)\nsplits\n[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title'),\nDocument(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some'),\nDocument(metadata={'Header 1': 'Main Title'}, page_content='some basic content.'),\nDocument(metadata={'Header 2': 'Section 1: Introduction'}, page_content='Section 1: Introduction'),\nDocument(metadata={'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a'),\nDocument(metadata={'Header 2': 'Section 1: Introduction'}, page_content='is a list:'),\nDocument(metadata={'Header 2': 'Section 1: Introduction'}, page_content='First item \\n Second item'),\nDocument(metadata={'Header 2': 'Section 1: Introduction'}, page_content='Third item with  bold text  and  a link'),\nDocument(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Subsection 1.1: Details'),\nDocument(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='This subsection provides additional details.'),\nDocument(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content=\"Here's a table:\"),\nDocument(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Header 1 \\n Header 2 \\n Header 3'),\nDocument(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 1, Cell 1 \\n Row 1, Cell 2'),\nDocument(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 1, Cell 3 \\n \\n \\n Row 2, Cell 1'),\nDocument(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 2, Cell 2 \\n Row 2, Cell 3'),\nDocument(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Section 2: Media Content'),\nDocument(metadata={'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video:'),\nDocument(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Your browser does not support the video'),\nDocument(metadata={'Header 2': 'Section 2: Media Content'}, page_content='tag.'),\nDocument(metadata={'Header 2': 'Section 3: Code Example'}, page_content='Section 3: Code Example'),\nDocument(metadata={'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block: \\n \\n    <div>'),\nDocument(metadata={'Header 2': 'Section 3: Code Example'}, page_content='<p>This is a paragraph inside a div.</p>'),\nDocument(metadata={'Header 2': 'Section 3: Code Example'}, page_content='</div>'),\nDocument(metadata={'Header 2': 'Conclusion'}, page_content='Conclusion'),\nDocument(metadata={'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]\nUsing HTMLSemanticPreservingSplitter\nâ€‹\nThe\nHTMLSemanticPreservingSplitter\nis designed to split HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components. This ensures that such elements are not split across chunks, causing loss of contextual relevancy such as table headers, list headers etc.\nThis splitter is designed at its heart, to create contextually relevant chunks. General Recursive splitting with\nHTMLHeaderTextSplitter\ncan cause tables, lists and other structured elements to be split in the middle, losing significant context and creating bad chunks.\nThe\nHTMLSemanticPreservingSplitter\nis essential for splitting HTML content that includes structured elements like tables and lists, especially when it's critical to preserve these elements intact. Additionally, its ability to define custom handlers for specific HTML tags makes it a versatile tool for processing complex HTML documents.\nIMPORTANT\n:\nmax_chunk_size\nis not a definite maximum size of a chunk, the calculation of max size, occurs when the preserved content is not apart of the chunk, to ensure it is not split. When we add the preserved data back in to the chunk, there is a chance the chunk size will exceed the\nmax_chunk_size\n. This is crucial to ensure we maintain the structure of the original document\ninfo\nNotes:\nWe have defined a custom handler to re-format the contents of code blocks\nWe defined a deny list for specific html elements, to decompose them and their contents pre-processing\nWe have intentionally set a small chunk size to demonstrate the non-splitting of elements\n# BeautifulSoup is required to use the custom handlers\nfrom\nbs4\nimport\nTag\nfrom\nlangchain_text_splitters\nimport\nHTMLSemanticPreservingSplitter\nheaders_to_split_on\n=\n[\n(\n\"h1\"\n,\n\"Header 1\"\n)\n,\n(\n\"h2\"\n,\n\"Header 2\"\n)\n,\n]\ndef\ncode_handler\n(\nelement\n:\nTag\n)\n-\n>\nstr\n:\ndata_lang\n=\nelement\n.\nget\n(\n\"data-lang\"\n)\ncode_format\n=\nf\"<code:\n{\ndata_lang\n}\n>\n{\nelement\n.\nget_text\n(\n)\n}\n</code>\"\nreturn\ncode_format\nsplitter\n=\nHTMLSemanticPreservingSplitter\n(\nheaders_to_split_on\n=\nheaders_to_split_on\n,\nseparators\n=\n[\n\"\\n\\n\"\n,\n\"\\n\"\n,\n\". \"\n,\n\"! \"\n,\n\"? \"\n]\n,\nmax_chunk_size\n=\n50\n,\npreserve_images\n=\nTrue\n,\npreserve_videos\n=\nTrue\n,\nelements_to_preserve\n=\n[\n\"table\"\n,\n\"ul\"\n,\n\"ol\"\n,\n\"code\"\n]\n,\ndenylist_tags\n=\n[\n\"script\"\n,\n\"style\"\n,\n\"head\"\n]\n,\ncustom_handlers\n=\n{\n\"code\"\n:\ncode_handler\n}\n,\n)\ndocuments\n=\nsplitter\n.\nsplit_text\n(\nhtml_string\n)\ndocuments\n[Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some basic content.'),\nDocument(metadata={'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic'),\nDocument(metadata={'Header 2': 'Section 1: Introduction'}, page_content='. Below is a list: First item Second item Third item with bold text and a link Subsection 1.1: Details This subsection provides additional details'),\nDocument(metadata={'Header 2': 'Section 1: Introduction'}, page_content=\". Here's a table: Header 1 Header 2 Header 3 Row 1, Cell 1 Row 1, Cell 2 Row 1, Cell 3 Row 2, Cell 1 Row 2, Cell 2 Row 2, Cell 3\"),\nDocument(metadata={'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video: ![image:example_image_link.mp4](example_image_link.mp4) ![video:example_video_link.mp4](example_video_link.mp4)'),\nDocument(metadata={'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block: <code:html> <div> <p>This is a paragraph inside a div.</p> </div> </code>'),\nDocument(metadata={'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]\nPreserving Tables and Lists\nâ€‹\nIn this example, we will demonstrate how the\nHTMLSemanticPreservingSplitter\ncan preserve a table and a large list within an HTML document. The chunk size will be set to 50 characters to illustrate how the splitter ensures that these elements are not split, even when they exceed the maximum defined chunk size.\nfrom\nlangchain_text_splitters\nimport\nHTMLSemanticPreservingSplitter\nhtml_string\n=\n\"\"\"\n<!DOCTYPE html>\n<html>\n<body>\n<div>\n<h1>Section 1</h1>\n<p>This section contains an important table and list that should not be split across chunks.</p>\n<table>\n<tr>\n<th>Item</th>\n<th>Quantity</th>\n<th>Price</th>\n</tr>\n<tr>\n<td>Apples</td>\n<td>10</td>\n<td>$1.00</td>\n</tr>\n<tr>\n<td>Oranges</td>\n<td>5</td>\n<td>$0.50</td>\n</tr>\n<tr>\n<td>Bananas</td>\n<td>50</td>\n<td>$1.50</td>\n</tr>\n</table>\n<h2>Subsection 1.1</h2>\n<p>Additional text in subsection 1.1 that is separated from the table and list.</p>\n<p>Here is a detailed list:</p>\n<ul>\n<li>Item 1: Description of item 1, which is quite detailed and important.</li>\n<li>Item 2: Description of item 2, which also contains significant information.</li>\n<li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>\n</ul>\n</div>\n</body>\n</html>\n\"\"\"\nheaders_to_split_on\n=\n[\n(\n\"h1\"\n,\n\"Header 1\"\n)\n,\n(\n\"h2\"\n,\n\"Header 2\"\n)\n]\nsplitter\n=\nHTMLSemanticPreservingSplitter\n(\nheaders_to_split_on\n=\nheaders_to_split_on\n,\nmax_chunk_size\n=\n50\n,\nelements_to_preserve\n=\n[\n\"table\"\n,\n\"ul\"\n]\n,\n)\ndocuments\n=\nsplitter\n.\nsplit_text\n(\nhtml_string\n)\nprint\n(\ndocuments\n)\n[Document(metadata={'Header 1': 'Section 1'}, page_content='This section contains an important table and list'), Document(metadata={'Header 1': 'Section 1'}, page_content='that should not be split across chunks.'), Document(metadata={'Header 1': 'Section 1'}, page_content='Item Quantity Price Apples 10 $1.00 Oranges 5 $0.50 Bananas 50 $1.50'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content='Additional text in subsection 1.1 that is'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content='separated from the table and list. Here is a'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content=\"detailed list: Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.\")]\nExplanation\nâ€‹\nIn this example, the\nHTMLSemanticPreservingSplitter\nensures that the entire table and the unordered list (\n<ul>\n) are preserved within their respective chunks. Even though the chunk size is set to 50 characters, the splitter recognizes that these elements should not be split and keeps them intact.\nThis is particularly important when dealing with data tables or lists, where splitting the content could lead to loss of context or confusion. The resulting\nDocument\nobjects retain the full structure of these elements, ensuring that the contextual relevance of the information is maintained.\nUsing a Custom Handler\nâ€‹\nThe\nHTMLSemanticPreservingSplitter\nallows you to define custom handlers for specific HTML elements. Some platforms, have custom HTML tags that are not natively parsed by\nBeautifulSoup\n, when this occurs, you can utilize custom handlers to add the formatting logic easily.\nThis can be particularly useful for elements that require special processing, such as\n<iframe>\ntags or specific 'data-' elements. In this example, we'll create a custom handler for\niframe\ntags that converts them into Markdown-like links.\ndef\ncustom_iframe_extractor\n(\niframe_tag\n)\n:\niframe_src\n=\niframe_tag\n.\nget\n(\n\"src\"\n,\n\"\"\n)\nreturn\nf\"[iframe:\n{\niframe_src\n}\n](\n{\niframe_src\n}\n)\"\nsplitter\n=\nHTMLSemanticPreservingSplitter\n(\nheaders_to_split_on\n=\nheaders_to_split_on\n,\nmax_chunk_size\n=\n50\n,\nseparators\n=\n[\n\"\\n\\n\"\n,\n\"\\n\"\n,\n\". \"\n]\n,\nelements_to_preserve\n=\n[\n\"table\"\n,\n\"ul\"\n,\n\"ol\"\n]\n,\ncustom_handlers\n=\n{\n\"iframe\"\n:\ncustom_iframe_extractor\n}\n,\n)\nhtml_string\n=\n\"\"\"\n<!DOCTYPE html>\n<html>\n<body>\n<div>\n<h1>Section with Iframe</h1>\n<iframe src=\"https://example.com/embed\"></iframe>\n<p>Some text after the iframe.</p>\n<ul>\n<li>Item 1: Description of item 1, which is quite detailed and important.</li>\n<li>Item 2: Description of item 2, which also contains significant information.</li>\n<li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>\n</ul>\n</div>\n</body>\n</html>\n\"\"\"\ndocuments\n=\nsplitter\n.\nsplit_text\n(\nhtml_string\n)\nprint\n(\ndocuments\n)\n[Document(metadata={'Header 1': 'Section with Iframe'}, page_content='[iframe:https://example.com/embed](https://example.com/embed) Some text after the iframe'), Document(metadata={'Header 1': 'Section with Iframe'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.\")]\nExplanation\nâ€‹\nIn this example, we defined a custom handler for\niframe\ntags that converts them into Markdown-like links. When the splitter processes the HTML content, it uses this custom handler to transform the\niframe\ntags while preserving other elements like tables and lists. The resulting\nDocument\nobjects show how the iframe is handled according to the custom logic you provided.\nImportant\n: When presvering items such as links, you should be mindful not to include\n.\nin your separators, or leave separators blank.\nRecursiveCharacterTextSplitter\nsplits on full stop, which will cut links in half. Ensure you provide a separator list with\n.\ninstead.\nUsing a custom handler to analyze an image with an LLM\nâ€‹\nWith custom handler's, we can also override the default processing for any element. A great example of this, is inserting semantic analysis of an image within a document, directly in the chunking flow.\nSince our function is called when the tag is discovered, we can override the\n<img>\ntag and turn off\npreserve_images\nto insert any content we would like to embed in our chunks.\n\"\"\"This example assumes you have helper methods `load_image_from_url` and an LLM agent `llm` that can process image data.\"\"\"\nfrom\nlangchain\n.\nagents\nimport\nAgentExecutor\n# This example needs to be replaced with your own agent\nllm\n=\nAgentExecutor\n(\n.\n.\n.\n)\n# This method is a placeholder for loading image data from a URL and is not implemented here\ndef\nload_image_from_url\n(\nimage_url\n:\nstr\n)\n-\n>\nbytes\n:\n# Assuming this method fetches the image data from the URL\nreturn\nb\"image_data\"\nhtml_string\n=\n\"\"\"\n<!DOCTYPE html>\n<html>\n<body>\n<div>\n<h1>Section with Image and Link</h1>\n<p>\n<img src=\"https://example.com/image.jpg\" alt=\"An example image\" />\nSome text after the image.\n</p>\n<ul>\n<li>Item 1: Description of item 1, which is quite detailed and important.</li>\n<li>Item 2: Description of item 2, which also contains significant information.</li>\n<li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>\n</ul>\n</div>\n</body>\n</html>\n\"\"\"\ndef\ncustom_image_handler\n(\nimg_tag\n)\n-\n>\nstr\n:\nimg_src\n=\nimg_tag\n.\nget\n(\n\"src\"\n,\n\"\"\n)\nimg_alt\n=\nimg_tag\n.\nget\n(\n\"alt\"\n,\n\"No alt text provided\"\n)\nimage_data\n=\nload_image_from_url\n(\nimg_src\n)\nsemantic_meaning\n=\nllm\n.\ninvoke\n(\nimage_data\n)\nmarkdown_text\n=\nf\"[Image Alt Text:\n{\nimg_alt\n}\n| Image Source:\n{\nimg_src\n}\n| Image Semantic Meaning:\n{\nsemantic_meaning\n}\n]\"\nreturn\nmarkdown_text\nsplitter\n=\nHTMLSemanticPreservingSplitter\n(\nheaders_to_split_on\n=\nheaders_to_split_on\n,\nmax_chunk_size\n=\n50\n,\nseparators\n=\n[\n\"\\n\\n\"\n,\n\"\\n\"\n,\n\". \"\n]\n,\nelements_to_preserve\n=\n[\n\"ul\"\n]\n,\npreserve_images\n=\nFalse\n,\ncustom_handlers\n=\n{\n\"img\"\n:\ncustom_image_handler\n}\n,\n)\ndocuments\n=\nsplitter\n.\nsplit_text\n(\nhtml_string\n)\nprint\n(\ndocuments\n)\n[Document(metadata={'Header 1': 'Section with Image and Link'}, page_content='[Image Alt Text: An example image | Image Source: https://example.com/image.jpg | Image Semantic Meaning: semantic-meaning] Some text after the image'),\nDocument(metadata={'Header 1': 'Section with Image and Link'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.\")]\nExplanation:\nâ€‹\nWith our custom handler written to extract the specific fields from a\n<img>\nelement in HTML, we can further process the data with our agent, and insert the result directly into our chunk. It is important to ensure\npreserve_images\nis set to\nFalse\notherwise the default processing of\n<img>\nfields will take place.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/sql_csv/",
    "How-to guides\nHow to do question answering over CSVs\nOn this page\nHow to do question answering over CSVs\nLLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:\nRECOMMENDED\n: Load the CSV(s) into a SQL database, and use the approaches outlined in the\nSQL tutorial\n.\nGive the LLM access to a Python environment where it can use libraries like Pandas to interact with the data.\nWe will cover both approaches in this guide.\nâš ï¸ Security note âš ï¸\nâ€‹\nBoth approaches mentioned above carry significant risks. Using SQL requires executing model-generated SQL queries. Using a library like Pandas requires letting the model execute Python code. Since it is easier to tightly scope SQL connection permissions and sanitize SQL queries than it is to sandbox Python environments,\nwe HIGHLY recommend interacting with CSV data via SQL.\nFor more on general security best practices,\nsee here\n.\nSetup\nâ€‹\nDependencies for this guide:\n%\npip install\n-\nqU langchain langchain\n-\nopenai langchain\n-\ncommunity langchain\n-\nexperimental pandas\nSet required environment variables:\n# Using LangSmith is recommended but not required. Uncomment below lines to use.\n# import os\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nDownload the\nTitanic dataset\nif you don't already have it:\n!wget https\n:\n//\nweb\n.\nstanford\n.\nedu\n/\nclass\n/\narchive\n/\ncs\n/\ncs109\n/\ncs109\n.\n1166\n/\nstuff\n/\ntitanic\n.\ncsv\n-\nO titanic\n.\ncsv\nimport\npandas\nas\npd\ndf\n=\npd\n.\nread_csv\n(\n\"titanic.csv\"\n)\nprint\n(\ndf\n.\nshape\n)\nprint\n(\ndf\n.\ncolumns\n.\ntolist\n(\n)\n)\n(887, 8)\n['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\nSQL\nâ€‹\nUsing SQL to interact with CSV data is the recommended approach because it is easier to limit permissions and sanitize queries than with arbitrary Python.\nMost SQL databases make it easy to load a CSV file in as a table (\nDuckDB\n,\nSQLite\n, etc.). Once you've done this you can use all of the chain and agent-creating techniques outlined in the\nSQL tutorial\n. Here's a quick example of how we might do this with SQLite:\nfrom\nlangchain_community\n.\nutilities\nimport\nSQLDatabase\nfrom\nsqlalchemy\nimport\ncreate_engine\nengine\n=\ncreate_engine\n(\n\"sqlite:///titanic.db\"\n)\ndf\n.\nto_sql\n(\n\"titanic\"\n,\nengine\n,\nindex\n=\nFalse\n)\n887\ndb\n=\nSQLDatabase\n(\nengine\n=\nengine\n)\nprint\n(\ndb\n.\ndialect\n)\nprint\n(\ndb\n.\nget_usable_table_names\n(\n)\n)\nprint\n(\ndb\n.\nrun\n(\n\"SELECT * FROM titanic WHERE Age < 2;\"\n)\n)\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\nAnd create a\nSQL agent\nto interact with it:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nfrom\nlangchain_community\n.\nagent_toolkits\nimport\ncreate_sql_agent\nagent_executor\n=\ncreate_sql_agent\n(\nllm\n,\ndb\n=\ndb\n,\nagent_type\n=\n\"openai-tools\"\n,\nverbose\n=\nTrue\n)\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"what's the average age of survivors\"\n}\n)\n\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `sql_db_list_tables` with `{}`\n\u001b[0m\u001b[38;5;200m\u001b[1;3mtitanic\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `sql_db_schema` with `{'table_names': 'titanic'}`\n\u001b[0m\u001b[33;1m\u001b[1;3m\nCREATE TABLE titanic (\n\"Survived\" BIGINT,\n\"Pclass\" BIGINT,\n\"Name\" TEXT,\n\"Sex\" TEXT,\n\"Age\" FLOAT,\n\"Siblings/Spouses Aboard\" BIGINT,\n\"Parents/Children Aboard\" BIGINT,\n\"Fare\" FLOAT\n)\n/*\n3 rows from titanic table:\nSurvived\tPclass\tName\tSex\tAge\tSiblings/Spouses Aboard\tParents/Children Aboard\tFare\n0\t3\tMr. Owen Harris Braund\tmale\t22.0\t1\t0\t7.25\n1\t1\tMrs. John Bradley (Florence Briggs Thayer) Cumings\tfemale\t38.0\t1\t0\t71.2833\n1\t3\tMiss. Laina Heikkinen\tfemale\t26.0\t0\t0\t7.925\n*/\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `sql_db_query` with `{'query': 'SELECT AVG(Age) AS Average_Age FROM titanic WHERE Survived = 1'}`\n\u001b[0m\u001b[36;1m\u001b[1;3m[(28.408391812865496,)]\u001b[0m\u001b[32;1m\u001b[1;3mThe average age of survivors in the Titanic dataset is approximately 28.41 years.\u001b[0m\n\u001b[1m> Finished chain.\u001b[0m\n{'input': \"what's the average age of survivors\",\n'output': 'The average age of survivors in the Titanic dataset is approximately 28.41 years.'}\nThis approach easily generalizes to multiple CSVs, since we can just load each of them into our database as its own table. See the\nMultiple CSVs\nsection below.\nPandas\nâ€‹\nInstead of SQL we can also use data analysis libraries like pandas and the code generating abilities of LLMs to interact with CSV data. Again,\nthis approach is not fit for production use cases unless you have extensive safeguards in place\n. For this reason, our code-execution utilities and constructors live in the\nlangchain-experimental\npackage.\nChain\nâ€‹\nMost LLMs have been trained on enough pandas Python code that they can generate it just by being asked to:\nai_msg\n=\nllm\n.\ninvoke\n(\n\"I have a pandas DataFrame 'df' with columns 'Age' and 'Fare'. Write code to compute the correlation between the two columns. Return Markdown for a Python code snippet and nothing else.\"\n)\nprint\n(\nai_msg\n.\ncontent\n)\n\\`\\`\\`python\ncorrelation = df['Age'].corr(df['Fare'])\ncorrelation\n\\`\\`\\`\nWe can combine this ability with a Python-executing tool to create a simple data analysis chain. We'll first want to load our CSV table as a dataframe, and give the tool access to this dataframe:\nimport\npandas\nas\npd\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_experimental\n.\ntools\nimport\nPythonAstREPLTool\ndf\n=\npd\n.\nread_csv\n(\n\"titanic.csv\"\n)\ntool\n=\nPythonAstREPLTool\n(\nlocals\n=\n{\n\"df\"\n:\ndf\n}\n)\ntool\n.\ninvoke\n(\n\"df['Fare'].mean()\"\n)\nAPI Reference:\nChatPromptTemplate\n32.30542018038331\nTo help enforce proper use of our Python tool, we'll using\ntool calling\n:\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\ntool\n]\n,\ntool_choice\n=\ntool\n.\nname\n)\nresponse\n=\nllm_with_tools\n.\ninvoke\n(\n\"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\"\n)\nresponse\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SBrK246yUbdnJemXFC8Iod05', 'function': {'arguments': '{\"query\":\"df.corr()[\\'Age\\'][\\'Fare\\']\"}', 'name': 'python_repl_ast'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 125, 'total_tokens': 138}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-1fd332ba-fa72-4351-8182-d464e7368311-0', tool_calls=[{'name': 'python_repl_ast', 'args': {'query': \"df.corr()['Age']['Fare']\"}, 'id': 'call_SBrK246yUbdnJemXFC8Iod05'}])\nresponse\n.\ntool_calls\n[{'name': 'python_repl_ast',\n'args': {'query': \"df.corr()['Age']['Fare']\"},\n'id': 'call_SBrK246yUbdnJemXFC8Iod05'}]\nWe'll add a tools output parser to extract the function call as a dict:\nfrom\nlangchain_core\n.\noutput_parsers\n.\nopenai_tools\nimport\nJsonOutputKeyToolsParser\nparser\n=\nJsonOutputKeyToolsParser\n(\nkey_name\n=\ntool\n.\nname\n,\nfirst_tool_only\n=\nTrue\n)\n(\nllm_with_tools\n|\nparser\n)\n.\ninvoke\n(\n\"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\"\n)\nAPI Reference:\nJsonOutputKeyToolsParser\n{'query': \"df[['Age', 'Fare']].corr()\"}\nAnd combine with a prompt so that we can just specify a question without needing to specify the dataframe info every invocation:\nsystem\n=\nf\"\"\"You have access to a pandas dataframe `df`. \\\nHere is the output of `df.head().to_markdown()`:\n\\`\\`\\`\n{\ndf\n.\nhead\n(\n)\n.\nto_markdown\n(\n)\n}\n\\`\\`\\`\nGiven a user question, write the Python code to answer it. \\\nReturn ONLY the valid Python code and nothing else. \\\nDon't assume you have access to any libraries other than built-in Python ones and pandas.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n]\n)\ncode_chain\n=\nprompt\n|\nllm_with_tools\n|\nparser\ncode_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What's the correlation between age and fare\"\n}\n)\n{'query': \"df[['Age', 'Fare']].corr()\"}\nAnd lastly we'll add our Python tool so that the generated code is actually executed:\nchain\n=\nprompt\n|\nllm_with_tools\n|\nparser\n|\ntool\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What's the correlation between age and fare\"\n}\n)\n0.11232863699941621\nAnd just like that we have a simple data analysis chain. We can take a peak at the intermediate steps by looking at the LangSmith trace:\nhttps://smith.langchain.com/public/b1309290-7212-49b7-bde2-75b39a32b49a/r\nWe could add an additional LLM call at the end to generate a conversational response, so that we're not just responding with the tool output. For this we'll want to add a chat history\nMessagesPlaceholder\nto our prompt:\nfrom\noperator\nimport\nitemgetter\nfrom\nlangchain_core\n.\nmessages\nimport\nToolMessage\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nMessagesPlaceholder\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nsystem\n=\nf\"\"\"You have access to a pandas dataframe `df`. \\\nHere is the output of `df.head().to_markdown()`:\n\\`\\`\\`\n{\ndf\n.\nhead\n(\n)\n.\nto_markdown\n(\n)\n}\n\\`\\`\\`\nGiven a user question, write the Python code to answer it. \\\nDon't assume you have access to any libraries other than built-in Python ones and pandas.\nRespond directly to the question once you have enough information to answer it.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n,\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n,\n# This MessagesPlaceholder allows us to optionally append an arbitrary number of messages\n# at the end of the prompt using the 'chat_history' arg.\nMessagesPlaceholder\n(\n\"chat_history\"\n,\noptional\n=\nTrue\n)\n,\n]\n)\ndef\n_get_chat_history\n(\nx\n:\ndict\n)\n-\n>\nlist\n:\n\"\"\"Parse the chain output up to this point into a list of chat history messages to insert in the prompt.\"\"\"\nai_msg\n=\nx\n[\n\"ai_msg\"\n]\ntool_call_id\n=\nx\n[\n\"ai_msg\"\n]\n.\nadditional_kwargs\n[\n\"tool_calls\"\n]\n[\n0\n]\n[\n\"id\"\n]\ntool_msg\n=\nToolMessage\n(\ntool_call_id\n=\ntool_call_id\n,\ncontent\n=\nstr\n(\nx\n[\n\"tool_output\"\n]\n)\n)\nreturn\n[\nai_msg\n,\ntool_msg\n]\nchain\n=\n(\nRunnablePassthrough\n.\nassign\n(\nai_msg\n=\nprompt\n|\nllm_with_tools\n)\n.\nassign\n(\ntool_output\n=\nitemgetter\n(\n\"ai_msg\"\n)\n|\nparser\n|\ntool\n)\n.\nassign\n(\nchat_history\n=\n_get_chat_history\n)\n.\nassign\n(\nresponse\n=\nprompt\n|\nllm\n|\nStrOutputParser\n(\n)\n)\n.\npick\n(\n[\n\"tool_output\"\n,\n\"response\"\n]\n)\n)\nAPI Reference:\nToolMessage\n|\nStrOutputParser\n|\nMessagesPlaceholder\n|\nRunnablePassthrough\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What's the correlation between age and fare\"\n}\n)\n{'tool_output': 0.11232863699941616,\n'response': 'The correlation between age and fare is approximately 0.1123.'}\nHere's the LangSmith trace for this run:\nhttps://smith.langchain.com/public/14e38d70-45b1-4b81-8477-9fd2b7c07ea6/r\nAgent\nâ€‹\nFor complex questions it can be helpful for an LLM to be able to iteratively execute code while maintaining the inputs and outputs of its previous executions. This is where Agents come into play. They allow an LLM to decide how many times a tool needs to be invoked and keep track of the executions it's made so far. The\ncreate_pandas_dataframe_agent\nis a built-in agent that makes it easy to work with dataframes:\nfrom\nlangchain_experimental\n.\nagents\nimport\ncreate_pandas_dataframe_agent\nagent\n=\ncreate_pandas_dataframe_agent\n(\nllm\n,\ndf\n,\nagent_type\n=\n\"openai-tools\"\n,\nverbose\n=\nTrue\n,\nallow_dangerous_code\n=\nTrue\n)\nagent\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What's the correlation between age and fare? is that greater than the correlation between fare and survival?\"\n}\n)\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `python_repl_ast` with `{'query': \"df[['Age', 'Fare']].corr().iloc[0,1]\"}`\n\u001b[0m\u001b[36;1m\u001b[1;3m0.11232863699941621\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `python_repl_ast` with `{'query': \"df[['Fare', 'Survived']].corr().iloc[0,1]\"}`\n\u001b[0m\u001b[36;1m\u001b[1;3m0.2561785496289603\u001b[0m\u001b[32;1m\u001b[1;3mThe correlation between Age and Fare is approximately 0.112, and the correlation between Fare and Survival is approximately 0.256.\nTherefore, the correlation between Fare and Survival (0.256) is greater than the correlation between Age and Fare (0.112).\u001b[0m\n\u001b[1m> Finished chain.\u001b[0m\n{'input': \"What's the correlation between age and fare? is that greater than the correlation between fare and survival?\",\n'output': 'The correlation between Age and Fare is approximately 0.112, and the correlation between Fare and Survival is approximately 0.256.\\n\\nTherefore, the correlation between Fare and Survival (0.256) is greater than the correlation between Age and Fare (0.112).'}\nHere's the LangSmith trace for this run:\nhttps://smith.langchain.com/public/6a86aee2-4f22-474a-9264-bd4c7283e665/r\nMultiple CSVs\nâ€‹\nTo handle multiple CSVs (or dataframes) we just need to pass multiple dataframes to our Python tool. Our\ncreate_pandas_dataframe_agent\nconstructor can do this out of the box, we can pass in a list of dataframes instead of just one. If we're constructing a chain ourselves, we can do something like:\ndf_1\n=\ndf\n[\n[\n\"Age\"\n,\n\"Fare\"\n]\n]\ndf_2\n=\ndf\n[\n[\n\"Fare\"\n,\n\"Survived\"\n]\n]\ntool\n=\nPythonAstREPLTool\n(\nlocals\n=\n{\n\"df_1\"\n:\ndf_1\n,\n\"df_2\"\n:\ndf_2\n}\n)\nllm_with_tool\n=\nllm\n.\nbind_tools\n(\ntools\n=\n[\ntool\n]\n,\ntool_choice\n=\ntool\n.\nname\n)\ndf_template\n=\n\"\"\"\\`\\`\\`python\n{df_name}.head().to_markdown()\n>>> {df_head}\n\\`\\`\\`\"\"\"\ndf_context\n=\n\"\\n\\n\"\n.\njoin\n(\ndf_template\n.\nformat\n(\ndf_head\n=\n_df\n.\nhead\n(\n)\n.\nto_markdown\n(\n)\n,\ndf_name\n=\ndf_name\n)\nfor\n_df\n,\ndf_name\nin\n[\n(\ndf_1\n,\n\"df_1\"\n)\n,\n(\ndf_2\n,\n\"df_2\"\n)\n]\n)\nsystem\n=\nf\"\"\"You have access to a number of pandas dataframes. \\\nHere is a sample of rows from each dataframe and the python code that was used to generate the sample:\n{\ndf_context\n}\nGiven a user question about the dataframes, write the Python code to answer it. \\\nDon't assume you have access to any libraries other than built-in Python ones and pandas. \\\nMake sure to refer only to the variables mentioned above.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{question}\"\n)\n]\n)\nchain\n=\nprompt\n|\nllm_with_tool\n|\nparser\n|\ntool\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"return the difference in the correlation between age and fare and the correlation between fare and survival\"\n}\n)\n0.14384991262954416\nHere's the LangSmith trace for this run:\nhttps://smith.langchain.com/public/cc2a7d7f-7c5a-4e77-a10c-7b5420fcd07f/r\nSandboxed code execution\nâ€‹\nThere are a number of tools like\nE2B\nand\nBearly\nthat provide sandboxed environments for Python code execution, to allow for safer code-executing chains and agents.\nNext steps\nâ€‹\nFor more advanced data analysis applications we recommend checking out:\nSQL tutorial\n: Many of the challenges of working with SQL db's and CSV's are generic to any structured data type, so it's useful to read the SQL techniques even if you're using Pandas for CSV data analysis.\nTool use\n: Guides on general best practices when working with chains and agents that invoke tools\nAgents\n: Understand the fundamentals of building LLM agents.\nIntegrations: Sandboxed envs like\nE2B\nand\nBearly\n, utilities like\nSQLDatabase\n, related agents like\nSpark DataFrame agent\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/sql_large_db/",
    "How-to guides\nHow to deal with large databases when doing SQL question-answering\nOn this page\nHow to deal with large databases when doing SQL question-answering\nIn order to write valid queries against a database, we need to feed the model the table names, table schemas, and feature values for it to query over. When there are many tables, columns, and/or high-cardinality columns, it becomes impossible for us to dump the full information about our database in every prompt. Instead, we must find ways to dynamically insert into the prompt only the most relevant information.\nIn this guide we demonstrate methods for identifying such relevant information, and feeding this into a query-generation step. We will cover:\nIdentifying a relevant subset of tables;\nIdentifying a relevant subset of column values.\nSetup\nâ€‹\nFirst, get required packages and set environment variables:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain langchain\n-\ncommunity langchain\n-\nopenai\n# Uncomment the below to use LangSmith. Not required.\n# import os\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\nThe below example will use a SQLite connection with Chinook database. Follow\nthese installation steps\nto create\nChinook.db\nin the same directory as this notebook:\nSave\nthis file\nas\nChinook_Sqlite.sql\nRun\nsqlite3 Chinook.db\nRun\n.read Chinook_Sqlite.sql\nTest\nSELECT * FROM Artist LIMIT 10;\nNow,\nChinook.db\nis in our directory and we can interface with it using the SQLAlchemy-driven\nSQLDatabase\nclass:\nfrom\nlangchain_community\n.\nutilities\nimport\nSQLDatabase\ndb\n=\nSQLDatabase\n.\nfrom_uri\n(\n\"sqlite:///Chinook.db\"\n)\nprint\n(\ndb\n.\ndialect\n)\nprint\n(\ndb\n.\nget_usable_table_names\n(\n)\n)\nprint\n(\ndb\n.\nrun\n(\n\"SELECT * FROM Artist LIMIT 10;\"\n)\n)\nsqlite\n['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\nMany tables\nâ€‹\nOne of the main pieces of information we need to include in our prompt is the schemas of the relevant tables. When we have very many tables, we can't fit all of the schemas in a single prompt. What we can do in such cases is first extract the names of the tables related to the user input, and then include only their schemas.\nOne easy and reliable way to do this is using\ntool-calling\n. Below, we show how we can use this feature to obtain output conforming to a desired format (in this case, a list of table names). We use the chat model's\n.bind_tools\nmethod to bind a tool in Pydantic format, and feed this into an output parser to reconstruct the object from the model's response.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nfrom\nlangchain_core\n.\noutput_parsers\n.\nopenai_tools\nimport\nPydanticToolsParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nTable\n(\nBaseModel\n)\n:\n\"\"\"Table in SQL database.\"\"\"\nname\n:\nstr\n=\nField\n(\ndescription\n=\n\"Name of table in SQL database.\"\n)\ntable_names\n=\n\"\\n\"\n.\njoin\n(\ndb\n.\nget_usable_table_names\n(\n)\n)\nsystem\n=\nf\"\"\"Return the names of ALL the SQL tables that MIGHT be relevant to the user question. \\\nThe tables are:\n{\ntable_names\n}\nRemember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\nTable\n]\n)\noutput_parser\n=\nPydanticToolsParser\n(\ntools\n=\n[\nTable\n]\n)\ntable_chain\n=\nprompt\n|\nllm_with_tools\n|\noutput_parser\ntable_chain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What are all the genres of Alanis Morissette songs\"\n}\n)\nAPI Reference:\nPydanticToolsParser\n|\nChatPromptTemplate\n[Table(name='Genre')]\nThis works pretty well! Except, as we'll see below, we actually need a few other tables as well. This would be pretty difficult for the model to know based just on the user question. In this case, we might think to simplify our model's job by grouping the tables together. We'll just ask the model to choose between categories \"Music\" and \"Business\", and then take care of selecting all the relevant tables from there:\nsystem\n=\n\"\"\"Return the names of any SQL tables that are relevant to the user question.\nThe tables are:\nMusic\nBusiness\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\ncategory_chain\n=\nprompt\n|\nllm_with_tools\n|\noutput_parser\ncategory_chain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What are all the genres of Alanis Morissette songs\"\n}\n)\n[Table(name='Music'), Table(name='Business')]\nfrom\ntyping\nimport\nList\ndef\nget_tables\n(\ncategories\n:\nList\n[\nTable\n]\n)\n-\n>\nList\n[\nstr\n]\n:\ntables\n=\n[\n]\nfor\ncategory\nin\ncategories\n:\nif\ncategory\n.\nname\n==\n\"Music\"\n:\ntables\n.\nextend\n(\n[\n\"Album\"\n,\n\"Artist\"\n,\n\"Genre\"\n,\n\"MediaType\"\n,\n\"Playlist\"\n,\n\"PlaylistTrack\"\n,\n\"Track\"\n,\n]\n)\nelif\ncategory\n.\nname\n==\n\"Business\"\n:\ntables\n.\nextend\n(\n[\n\"Customer\"\n,\n\"Employee\"\n,\n\"Invoice\"\n,\n\"InvoiceLine\"\n]\n)\nreturn\ntables\ntable_chain\n=\ncategory_chain\n|\nget_tables\ntable_chain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What are all the genres of Alanis Morissette songs\"\n}\n)\n['Album',\n'Artist',\n'Genre',\n'MediaType',\n'Playlist',\n'PlaylistTrack',\n'Track',\n'Customer',\n'Employee',\n'Invoice',\n'InvoiceLine']\nNow that we've got a chain that can output the relevant tables for any query we can combine this with our\ncreate_sql_query_chain\n, which can accept a list of\ntable_names_to_use\nto determine which table schemas are included in the prompt:\nfrom\noperator\nimport\nitemgetter\nfrom\nlangchain\n.\nchains\nimport\ncreate_sql_query_chain\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nquery_chain\n=\ncreate_sql_query_chain\n(\nllm\n,\ndb\n)\n# Convert \"question\" key to the \"input\" key expected by current table_chain.\ntable_chain\n=\n{\n\"input\"\n:\nitemgetter\n(\n\"question\"\n)\n}\n|\ntable_chain\n# Set table_names_to_use using table_chain.\nfull_chain\n=\nRunnablePassthrough\n.\nassign\n(\ntable_names_to_use\n=\ntable_chain\n)\n|\nquery_chain\nAPI Reference:\nRunnablePassthrough\nquery\n=\nfull_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What are all the genres of Alanis Morissette songs\"\n}\n)\nprint\n(\nquery\n)\nSELECT DISTINCT \"g\".\"Name\"\nFROM \"Genre\" g\nJOIN \"Track\" t ON \"g\".\"GenreId\" = \"t\".\"GenreId\"\nJOIN \"Album\" a ON \"t\".\"AlbumId\" = \"a\".\"AlbumId\"\nJOIN \"Artist\" ar ON \"a\".\"ArtistId\" = \"ar\".\"ArtistId\"\nWHERE \"ar\".\"Name\" = 'Alanis Morissette'\nLIMIT 5;\ndb\n.\nrun\n(\nquery\n)\n\"[('Rock',)]\"\nWe can see the LangSmith trace for this run\nhere\n.\nWe've seen how to dynamically include a subset of table schemas in a prompt within a chain. Another possible approach to this problem is to let an Agent decide for itself when to look up tables by giving it a Tool to do so. You can see an example of this in the\nSQL: Agents\nguide.\nHigh-cardinality columns\nâ€‹\nIn order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.\nOne naive strategy it to create a vector store with all the distinct proper nouns that exist in the database. We can then query that vector store each user input and inject the most relevant proper nouns into the prompt.\nFirst we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:\nimport\nast\nimport\nre\ndef\nquery_as_list\n(\ndb\n,\nquery\n)\n:\nres\n=\ndb\n.\nrun\n(\nquery\n)\nres\n=\n[\nel\nfor\nsub\nin\nast\n.\nliteral_eval\n(\nres\n)\nfor\nel\nin\nsub\nif\nel\n]\nres\n=\n[\nre\n.\nsub\n(\nr\"\\b\\d+\\b\"\n,\n\"\"\n,\nstring\n)\n.\nstrip\n(\n)\nfor\nstring\nin\nres\n]\nreturn\nres\nproper_nouns\n=\nquery_as_list\n(\ndb\n,\n\"SELECT Name FROM Artist\"\n)\nproper_nouns\n+=\nquery_as_list\n(\ndb\n,\n\"SELECT Title FROM Album\"\n)\nproper_nouns\n+=\nquery_as_list\n(\ndb\n,\n\"SELECT Name FROM Genre\"\n)\nlen\n(\nproper_nouns\n)\nproper_nouns\n[\n:\n5\n]\n['AC/DC', 'Accept', 'Aerosmith', 'Alanis Morissette', 'Alice In Chains']\nNow we can embed and store all of our values in a vector database:\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nvector_db\n=\nFAISS\n.\nfrom_texts\n(\nproper_nouns\n,\nOpenAIEmbeddings\n(\n)\n)\nretriever\n=\nvector_db\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n15\n}\n)\nAnd put together a query construction chain that first retrieves values from the database and inserts them into the prompt:\nfrom\noperator\nimport\nitemgetter\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nsystem\n=\n\"\"\"You are a SQLite expert. Given an input question, create a syntactically\ncorrect SQLite query to run. Unless otherwise specificed, do not return more than\n{top_k} rows.\nOnly return the SQL query with no markup or explanation.\nHere is the relevant table info: {table_info}\nHere is a non-exhaustive list of possible feature values. If filtering on a feature\nvalue make sure to check its spelling against this list first:\n{proper_nouns}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n]\n)\nquery_chain\n=\ncreate_sql_query_chain\n(\nllm\n,\ndb\n,\nprompt\n=\nprompt\n)\nretriever_chain\n=\n(\nitemgetter\n(\n\"question\"\n)\n|\nretriever\n|\n(\nlambda\ndocs\n:\n\"\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n)\n)\n)\nchain\n=\nRunnablePassthrough\n.\nassign\n(\nproper_nouns\n=\nretriever_chain\n)\n|\nquery_chain\nAPI Reference:\nChatPromptTemplate\n|\nRunnablePassthrough\nTo try out our chain, let's see what happens when we try filtering on \"elenis moriset\", a misspelling of Alanis Morissette, without and with retrieval:\n# Without retrieval\nquery\n=\nquery_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What are all the genres of elenis moriset songs\"\n,\n\"proper_nouns\"\n:\n\"\"\n}\n)\nprint\n(\nquery\n)\ndb\n.\nrun\n(\nquery\n)\nSELECT DISTINCT g.Name\nFROM Track t\nJOIN Album a ON t.AlbumId = a.AlbumId\nJOIN Artist ar ON a.ArtistId = ar.ArtistId\nJOIN Genre g ON t.GenreId = g.GenreId\nWHERE ar.Name = 'Elenis Moriset';\n''\n# With retrieval\nquery\n=\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What are all the genres of elenis moriset songs\"\n}\n)\nprint\n(\nquery\n)\ndb\n.\nrun\n(\nquery\n)\nSELECT DISTINCT g.Name\nFROM Genre g\nJOIN Track t ON g.GenreId = t.GenreId\nJOIN Album a ON t.AlbumId = a.AlbumId\nJOIN Artist ar ON a.ArtistId = ar.ArtistId\nWHERE ar.Name = 'Alanis Morissette';\n\"[('Rock',)]\"\nWe can see that with retrieval we're able to correct the spelling from \"Elenis Moriset\" to \"Alanis Morissette\" and get back a valid result.\nAnother possible approach to this problem is to let an Agent decide for itself when to look up proper nouns. You can see an example of this in the\nSQL: Agents\nguide.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/sql_prompting/",
    "How-to guides\nHow to better prompt when doing SQL question-answering\nOn this page\nHow to better prompt when doing SQL question-answering\nIn this guide we'll go over prompting strategies to improve SQL query generation using\ncreate_sql_query_chain\n. We'll largely focus on methods for getting relevant database-specific information in your prompt.\nWe will cover:\nHow the dialect of the LangChain\nSQLDatabase\nimpacts the prompt of the chain;\nHow to format schema information into the prompt using\nSQLDatabase.get_context\n;\nHow to build and select\nfew-shot examples\nto assist the model.\nSetup\nâ€‹\nFirst, get required packages and set environment variables:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain langchain\n-\ncommunity langchain\n-\nexperimental langchain\n-\nopenai\n# Uncomment the below to use LangSmith. Not required.\n# import os\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\nThe below example will use a SQLite connection with Chinook database. Follow\nthese installation steps\nto create\nChinook.db\nin the same directory as this notebook:\nSave\nthis file\nas\nChinook_Sqlite.sql\nRun\nsqlite3 Chinook.db\nRun\n.read Chinook_Sqlite.sql\nTest\nSELECT * FROM Artist LIMIT 10;\nNow,\nChinook.db\nis in our directory and we can interface with it using the SQLAlchemy-driven\nSQLDatabase\nclass:\nfrom\nlangchain_community\n.\nutilities\nimport\nSQLDatabase\ndb\n=\nSQLDatabase\n.\nfrom_uri\n(\n\"sqlite:///Chinook.db\"\n,\nsample_rows_in_table_info\n=\n3\n)\nprint\n(\ndb\n.\ndialect\n)\nprint\n(\ndb\n.\nget_usable_table_names\n(\n)\n)\nprint\n(\ndb\n.\nrun\n(\n\"SELECT * FROM Artist LIMIT 10;\"\n)\n)\nsqlite\n['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\nDialect-specific prompting\nâ€‹\nOne of the simplest things we can do is make our prompt specific to the SQL dialect we're using. When using the built-in\ncreate_sql_query_chain\nand\nSQLDatabase\n, this is handled for you for any of the following dialects:\nfrom\nlangchain\n.\nchains\n.\nsql_database\n.\nprompt\nimport\nSQL_PROMPTS\nlist\n(\nSQL_PROMPTS\n)\n['crate',\n'duckdb',\n'googlesql',\n'mssql',\n'mysql',\n'mariadb',\n'oracle',\n'postgresql',\n'sqlite',\n'clickhouse',\n'prestodb']\nFor example, using our current DB we can see that we'll get a SQLite-specific prompt.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nfrom\nlangchain\n.\nchains\nimport\ncreate_sql_query_chain\nchain\n=\ncreate_sql_query_chain\n(\nllm\n,\ndb\n)\nchain\n.\nget_prompts\n(\n)\n[\n0\n]\n.\npretty_print\n(\n)\nYou are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use date('now') function to get the current date, if the question involves \"today\".\nUse the following format:\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\nOnly use the following tables:\n\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m\nQuestion: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\nTable definitions and example rows\nâ€‹\nIn most SQL chains, we'll need to feed the model at least part of the database schema. Without this it won't be able to write valid queries. Our database comes with some convenience methods to give us the relevant context. Specifically, we can get the table names, their schemas, and a sample of rows from each table.\nHere we will use\nSQLDatabase.get_context\n, which provides available tables and their schemas:\ncontext\n=\ndb\n.\nget_context\n(\n)\nprint\n(\nlist\n(\ncontext\n)\n)\nprint\n(\ncontext\n[\n\"table_info\"\n]\n)\n['table_info', 'table_names']\nCREATE TABLE \"Album\" (\n\"AlbumId\" INTEGER NOT NULL,\n\"Title\" NVARCHAR(160) NOT NULL,\n\"ArtistId\" INTEGER NOT NULL,\nPRIMARY KEY (\"AlbumId\"),\nFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\n)\n/*\n3 rows from Album table:\nAlbumId\tTitle\tArtistId\n1\tFor Those About To Rock We Salute You\t1\n2\tBalls to the Wall\t2\n3\tRestless and Wild\t2\n*/\nCREATE TABLE \"Artist\" (\n\"ArtistId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(120),\nPRIMARY KEY (\"ArtistId\")\n)\n/*\n3 rows from Artist table:\nArtistId\tName\n1\tAC/DC\n2\tAccept\n3\tAerosmith\n*/\nCREATE TABLE \"Customer\" (\n\"CustomerId\" INTEGER NOT NULL,\n\"FirstName\" NVARCHAR(40) NOT NULL,\n\"LastName\" NVARCHAR(20) NOT NULL,\n\"Company\" NVARCHAR(80),\n\"Address\" NVARCHAR(70),\n\"City\" NVARCHAR(40),\n\"State\" NVARCHAR(40),\n\"Country\" NVARCHAR(40),\n\"PostalCode\" NVARCHAR(10),\n\"Phone\" NVARCHAR(24),\n\"Fax\" NVARCHAR(24),\n\"Email\" NVARCHAR(60) NOT NULL,\n\"SupportRepId\" INTEGER,\nPRIMARY KEY (\"CustomerId\"),\nFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\n)\n/*\n3 rows from Customer table:\nCustomerId\tFirstName\tLastName\tCompany\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\tSupportRepId\n1\tLuÃ­s\tGonÃ§alves\tEmbraer - Empresa Brasileira de AeronÃ¡utica S.A.\tAv. Brigadeiro Faria Lima, 2170\tSÃ£o JosÃ© dos Campos\tSP\tBrazil\t12227-000\t+55 (12) 3923-5555\t+55 (12) 3923-5566\tluisg@embraer.com.br\t3\n2\tLeonie\tKÃ¶hler\tNone\tTheodor-Heuss-StraÃŸe 34\tStuttgart\tNone\tGermany\t70174\t+49 0711 2842222\tNone\tleonekohler@surfeu.de\t5\n3\tFranÃ§ois\tTremblay\tNone\t1498 rue BÃ©langer\tMontrÃ©al\tQC\tCanada\tH2G 1A7\t+1 (514) 721-4711\tNone\tftremblay@gmail.com\t3\n*/\nCREATE TABLE \"Employee\" (\n\"EmployeeId\" INTEGER NOT NULL,\n\"LastName\" NVARCHAR(20) NOT NULL,\n\"FirstName\" NVARCHAR(20) NOT NULL,\n\"Title\" NVARCHAR(30),\n\"ReportsTo\" INTEGER,\n\"BirthDate\" DATETIME,\n\"HireDate\" DATETIME,\n\"Address\" NVARCHAR(70),\n\"City\" NVARCHAR(40),\n\"State\" NVARCHAR(40),\n\"Country\" NVARCHAR(40),\n\"PostalCode\" NVARCHAR(10),\n\"Phone\" NVARCHAR(24),\n\"Fax\" NVARCHAR(24),\n\"Email\" NVARCHAR(60),\nPRIMARY KEY (\"EmployeeId\"),\nFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\n)\n/*\n3 rows from Employee table:\nEmployeeId\tLastName\tFirstName\tTitle\tReportsTo\tBirthDate\tHireDate\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\n1\tAdams\tAndrew\tGeneral Manager\tNone\t1962-02-18 00:00:00\t2002-08-14 00:00:00\t11120 Jasper Ave NW\tEdmonton\tAB\tCanada\tT5K 2N1\t+1 (780) 428-9482\t+1 (780) 428-3457\tandrew@chinookcorp.com\n2\tEdwards\tNancy\tSales Manager\t1\t1958-12-08 00:00:00\t2002-05-01 00:00:00\t825 8 Ave SW\tCalgary\tAB\tCanada\tT2P 2T3\t+1 (403) 262-3443\t+1 (403) 262-3322\tnancy@chinookcorp.com\n3\tPeacock\tJane\tSales Support Agent\t2\t1973-08-29 00:00:00\t2002-04-01 00:00:00\t1111 6 Ave SW\tCalgary\tAB\tCanada\tT2P 5M5\t+1 (403) 262-3443\t+1 (403) 262-6712\tjane@chinookcorp.com\n*/\nCREATE TABLE \"Genre\" (\n\"GenreId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(120),\nPRIMARY KEY (\"GenreId\")\n)\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\nCREATE TABLE \"Invoice\" (\n\"InvoiceId\" INTEGER NOT NULL,\n\"CustomerId\" INTEGER NOT NULL,\n\"InvoiceDate\" DATETIME NOT NULL,\n\"BillingAddress\" NVARCHAR(70),\n\"BillingCity\" NVARCHAR(40),\n\"BillingState\" NVARCHAR(40),\n\"BillingCountry\" NVARCHAR(40),\n\"BillingPostalCode\" NVARCHAR(10),\n\"Total\" NUMERIC(10, 2) NOT NULL,\nPRIMARY KEY (\"InvoiceId\"),\nFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\n)\n/*\n3 rows from Invoice table:\nInvoiceId\tCustomerId\tInvoiceDate\tBillingAddress\tBillingCity\tBillingState\tBillingCountry\tBillingPostalCode\tTotal\n1\t2\t2021-01-01 00:00:00\tTheodor-Heuss-StraÃŸe 34\tStuttgart\tNone\tGermany\t70174\t1.98\n2\t4\t2021-01-02 00:00:00\tUllevÃ¥lsveien 14\tOslo\tNone\tNorway\t0171\t3.96\n3\t8\t2021-01-03 00:00:00\tGrÃ©trystraat 63\tBrussels\tNone\tBelgium\t1000\t5.94\n*/\nCREATE TABLE \"InvoiceLine\" (\n\"InvoiceLineId\" INTEGER NOT NULL,\n\"InvoiceId\" INTEGER NOT NULL,\n\"TrackId\" INTEGER NOT NULL,\n\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\n\"Quantity\" INTEGER NOT NULL,\nPRIMARY KEY (\"InvoiceLineId\"),\nFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"),\nFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\n)\n/*\n3 rows from InvoiceLine table:\nInvoiceLineId\tInvoiceId\tTrackId\tUnitPrice\tQuantity\n1\t1\t2\t0.99\t1\n2\t1\t4\t0.99\t1\n3\t2\t6\t0.99\t1\n*/\nCREATE TABLE \"MediaType\" (\n\"MediaTypeId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(120),\nPRIMARY KEY (\"MediaTypeId\")\n)\n/*\n3 rows from MediaType table:\nMediaTypeId\tName\n1\tMPEG audio file\n2\tProtected AAC audio file\n3\tProtected MPEG-4 video file\n*/\nCREATE TABLE \"Playlist\" (\n\"PlaylistId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(120),\nPRIMARY KEY (\"PlaylistId\")\n)\n/*\n3 rows from Playlist table:\nPlaylistId\tName\n1\tMusic\n2\tMovies\n3\tTV Shows\n*/\nCREATE TABLE \"PlaylistTrack\" (\n\"PlaylistId\" INTEGER NOT NULL,\n\"TrackId\" INTEGER NOT NULL,\nPRIMARY KEY (\"PlaylistId\", \"TrackId\"),\nFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"),\nFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\")\n)\n/*\n3 rows from PlaylistTrack table:\nPlaylistId\tTrackId\n1\t3402\n1\t3389\n1\t3390\n*/\nCREATE TABLE \"Track\" (\n\"TrackId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(200) NOT NULL,\n\"AlbumId\" INTEGER,\n\"MediaTypeId\" INTEGER NOT NULL,\n\"GenreId\" INTEGER,\n\"Composer\" NVARCHAR(220),\n\"Milliseconds\" INTEGER NOT NULL,\n\"Bytes\" INTEGER,\n\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\nPRIMARY KEY (\"TrackId\"),\nFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\nFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\nFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n*/\nWhen we don't have too many, or too wide of, tables, we can just insert the entirety of this information in our prompt:\nprompt_with_context\n=\nchain\n.\nget_prompts\n(\n)\n[\n0\n]\n.\npartial\n(\ntable_info\n=\ncontext\n[\n\"table_info\"\n]\n)\nprint\n(\nprompt_with_context\n.\npretty_repr\n(\n)\n[\n:\n1500\n]\n)\nYou are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use date('now') function to get the current date, if the question involves \"today\".\nUse the following format:\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\nOnly use the following tables:\nCREATE TABLE \"Album\" (\n\"AlbumId\" INTEGER NOT NULL,\n\"Title\" NVARCHAR(160) NOT NULL,\n\"ArtistId\" INTEGER NOT NULL,\nPRIMARY KEY (\"AlbumId\"),\nFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\n)\n/*\n3 rows from Album table:\nAlbumId\tTitle\tArtistId\n1\tFor Those About To Rock We Salute You\t1\n2\tBalls to the Wall\t2\n3\tRestless and Wild\t2\n*/\nCREATE TABLE \"Artist\" (\n\"ArtistId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(120)\nWhen we do have database schemas that are too large to fit into our model's context window, we'll need to come up with ways of inserting only the relevant table definitions into the prompt based on the user input. For more on this head to the\nMany tables, wide tables, high-cardinality feature\nguide.\nFew-shot examples\nâ€‹\nIncluding examples of natural language questions being converted to valid SQL queries against our database in the prompt will often improve model performance, especially for complex queries.\nLet's say we have the following examples:\nexamples\n=\n[\n{\n\"input\"\n:\n\"List all artists.\"\n,\n\"query\"\n:\n\"SELECT * FROM Artist;\"\n}\n,\n{\n\"input\"\n:\n\"Find all albums for the artist 'AC/DC'.\"\n,\n\"query\"\n:\n\"SELECT * FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'AC/DC');\"\n,\n}\n,\n{\n\"input\"\n:\n\"List all tracks in the 'Rock' genre.\"\n,\n\"query\"\n:\n\"SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');\"\n,\n}\n,\n{\n\"input\"\n:\n\"Find the total duration of all tracks.\"\n,\n\"query\"\n:\n\"SELECT SUM(Milliseconds) FROM Track;\"\n,\n}\n,\n{\n\"input\"\n:\n\"List all customers from Canada.\"\n,\n\"query\"\n:\n\"SELECT * FROM Customer WHERE Country = 'Canada';\"\n,\n}\n,\n{\n\"input\"\n:\n\"How many tracks are there in the album with ID 5?\"\n,\n\"query\"\n:\n\"SELECT COUNT(*) FROM Track WHERE AlbumId = 5;\"\n,\n}\n,\n{\n\"input\"\n:\n\"Find the total number of invoices.\"\n,\n\"query\"\n:\n\"SELECT COUNT(*) FROM Invoice;\"\n,\n}\n,\n{\n\"input\"\n:\n\"List all tracks that are longer than 5 minutes.\"\n,\n\"query\"\n:\n\"SELECT * FROM Track WHERE Milliseconds > 300000;\"\n,\n}\n,\n{\n\"input\"\n:\n\"Who are the top 5 customers by total purchase?\"\n,\n\"query\"\n:\n\"SELECT CustomerId, SUM(Total) AS TotalPurchase FROM Invoice GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5;\"\n,\n}\n,\n{\n\"input\"\n:\n\"Which albums are from the year 2000?\"\n,\n\"query\"\n:\n\"SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';\"\n,\n}\n,\n{\n\"input\"\n:\n\"How many employees are there\"\n,\n\"query\"\n:\n'SELECT COUNT(*) FROM \"Employee\"'\n,\n}\n,\n]\nWe can create a few-shot prompt with them like so:\nfrom\nlangchain_core\n.\nprompts\nimport\nFewShotPromptTemplate\n,\nPromptTemplate\nexample_prompt\n=\nPromptTemplate\n.\nfrom_template\n(\n\"User input: {input}\\nSQL query: {query}\"\n)\nprompt\n=\nFewShotPromptTemplate\n(\nexamples\n=\nexamples\n[\n:\n5\n]\n,\nexample_prompt\n=\nexample_prompt\n,\nprefix\n=\n\"You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than {top_k} rows.\\n\\nHere is the relevant table info: {table_info}\\n\\nBelow are a number of examples of questions and their corresponding SQL queries.\"\n,\nsuffix\n=\n\"User input: {input}\\nSQL query: \"\n,\ninput_variables\n=\n[\n\"input\"\n,\n\"top_k\"\n,\n\"table_info\"\n]\n,\n)\nAPI Reference:\nFewShotPromptTemplate\n|\nPromptTemplate\nprint\n(\nprompt\n.\nformat\n(\ninput\n=\n\"How many artists are there?\"\n,\ntop_k\n=\n3\n,\ntable_info\n=\n\"foo\"\n)\n)\nYou are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than 3 rows.\nHere is the relevant table info: foo\nBelow are a number of examples of questions and their corresponding SQL queries.\nUser input: List all artists.\nSQL query: SELECT * FROM Artist;\nUser input: Find all albums for the artist 'AC/DC'.\nSQL query: SELECT * FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'AC/DC');\nUser input: List all tracks in the 'Rock' genre.\nSQL query: SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');\nUser input: Find the total duration of all tracks.\nSQL query: SELECT SUM(Milliseconds) FROM Track;\nUser input: List all customers from Canada.\nSQL query: SELECT * FROM Customer WHERE Country = 'Canada';\nUser input: How many artists are there?\nSQL query:\nDynamic few-shot examples\nâ€‹\nIf we have enough examples, we may want to only include the most relevant ones in the prompt, either because they don't fit in the model's context window or because the long tail of examples distracts the model. And specifically, given any input we want to include the examples most relevant to that input.\nWe can do just this using an ExampleSelector. In this case we'll use a\nSemanticSimilarityExampleSelector\n, which will store the examples in the vector database of our choosing. At runtime it will perform a similarity search between the input and our examples, and return the most semantically similar ones.\nWe default to OpenAI embeddings here, but you can swap them out for the model provider of your choice.\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\nexample_selectors\nimport\nSemanticSimilarityExampleSelector\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nexample_selector\n=\nSemanticSimilarityExampleSelector\n.\nfrom_examples\n(\nexamples\n,\nOpenAIEmbeddings\n(\n)\n,\nFAISS\n,\nk\n=\n5\n,\ninput_keys\n=\n[\n\"input\"\n]\n,\n)\nAPI Reference:\nSemanticSimilarityExampleSelector\nexample_selector\n.\nselect_examples\n(\n{\n\"input\"\n:\n\"how many artists are there?\"\n}\n)\n[{'input': 'List all artists.', 'query': 'SELECT * FROM Artist;'},\n{'input': 'How many employees are there',\n'query': 'SELECT COUNT(*) FROM \"Employee\"'},\n{'input': 'How many tracks are there in the album with ID 5?',\n'query': 'SELECT COUNT(*) FROM Track WHERE AlbumId = 5;'},\n{'input': 'Which albums are from the year 2000?',\n'query': \"SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';\"},\n{'input': \"List all tracks in the 'Rock' genre.\",\n'query': \"SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');\"}]\nTo use it, we can pass the ExampleSelector directly in to our FewShotPromptTemplate:\nprompt\n=\nFewShotPromptTemplate\n(\nexample_selector\n=\nexample_selector\n,\nexample_prompt\n=\nexample_prompt\n,\nprefix\n=\n\"You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than {top_k} rows.\\n\\nHere is the relevant table info: {table_info}\\n\\nBelow are a number of examples of questions and their corresponding SQL queries.\"\n,\nsuffix\n=\n\"User input: {input}\\nSQL query: \"\n,\ninput_variables\n=\n[\n\"input\"\n,\n\"top_k\"\n,\n\"table_info\"\n]\n,\n)\nprint\n(\nprompt\n.\nformat\n(\ninput\n=\n\"how many artists are there?\"\n,\ntop_k\n=\n3\n,\ntable_info\n=\n\"foo\"\n)\n)\nYou are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than 3 rows.\nHere is the relevant table info: foo\nBelow are a number of examples of questions and their corresponding SQL queries.\nUser input: List all artists.\nSQL query: SELECT * FROM Artist;\nUser input: How many employees are there\nSQL query: SELECT COUNT(*) FROM \"Employee\"\nUser input: How many tracks are there in the album with ID 5?\nSQL query: SELECT COUNT(*) FROM Track WHERE AlbumId = 5;\nUser input: Which albums are from the year 2000?\nSQL query: SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';\nUser input: List all tracks in the 'Rock' genre.\nSQL query: SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');\nUser input: how many artists are there?\nSQL query:\nTrying it out, we see that the model identifies the relevant table:\nchain\n=\ncreate_sql_query_chain\n(\nllm\n,\ndb\n,\nprompt\n)\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"how many artists are there?\"\n}\n)\n'SELECT COUNT(*) FROM Artist;'\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/sql_query_checking/",
    "How-to guides\nHow to do query validation as part of SQL question-answering\nOn this page\nHow to do query validation as part of SQL question-answering\nPerhaps the most error-prone part of any SQL chain or agent is writing valid and safe SQL queries. In this guide we'll go over some strategies for validating our queries and handling invalid queries.\nWe will cover:\nAppending a \"query validator\" step to the query generation;\nPrompt engineering to reduce the incidence of errors.\nSetup\nâ€‹\nFirst, get required packages and set environment variables:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet  langchain langchain\n-\ncommunity langchain\n-\nopenai\n# Uncomment the below to use LangSmith. Not required.\n# import os\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\nThe below example will use a SQLite connection with Chinook database. Follow\nthese installation steps\nto create\nChinook.db\nin the same directory as this notebook:\nSave\nthis file\nas\nChinook_Sqlite.sql\nRun\nsqlite3 Chinook.db\nRun\n.read Chinook_Sqlite.sql\nTest\nSELECT * FROM Artist LIMIT 10;\nNow,\nChinook.db\nis in our directory and we can interface with it using the SQLAlchemy-driven\nSQLDatabase\nclass:\nfrom\nlangchain_community\n.\nutilities\nimport\nSQLDatabase\ndb\n=\nSQLDatabase\n.\nfrom_uri\n(\n\"sqlite:///Chinook.db\"\n)\nprint\n(\ndb\n.\ndialect\n)\nprint\n(\ndb\n.\nget_usable_table_names\n(\n)\n)\nprint\n(\ndb\n.\nrun\n(\n\"SELECT * FROM Artist LIMIT 10;\"\n)\n)\nsqlite\n['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\nQuery checker\nâ€‹\nPerhaps the simplest strategy is to ask the model itself to check the original query for common mistakes. Suppose we have the following SQL query chain:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nfrom\nlangchain\n.\nchains\nimport\ncreate_sql_query_chain\nchain\n=\ncreate_sql_query_chain\n(\nllm\n,\ndb\n)\nAnd we want to validate its outputs. We can do so by extending the chain with a second prompt and model call:\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nsystem\n=\n\"\"\"Double check the user's {dialect} query for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\nIf there are any of the above mistakes, rewrite the query.\nIf there are no mistakes, just reproduce the original query with no further commentary.\nOutput the final SQL query only.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{query}\"\n)\n]\n)\n.\npartial\n(\ndialect\n=\ndb\n.\ndialect\n)\nvalidation_chain\n=\nprompt\n|\nllm\n|\nStrOutputParser\n(\n)\nfull_chain\n=\n{\n\"query\"\n:\nchain\n}\n|\nvalidation_chain\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\nquery\n=\nfull_chain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What's the average Invoice from an American customer whose Fax is missing since 2003 but before 2010\"\n}\n)\nprint\n(\nquery\n)\nSELECT AVG(i.Total) AS AverageInvoice\nFROM Invoice i\nJOIN Customer c ON i.CustomerId = c.CustomerId\nWHERE c.Country = 'USA'\nAND c.Fax IS NULL\nAND i.InvoiceDate >= '2003-01-01'\nAND i.InvoiceDate < '2010-01-01'\nNote how we can see both steps of the chain in the\nLangsmith trace\n.\ndb\n.\nrun\n(\nquery\n)\n'[(6.632999999999998,)]'\nThe obvious downside of this approach is that we need to make two model calls instead of one to generate our query. To get around this we can try to perform the query generation and query check in a single model invocation:\nsystem\n=\n\"\"\"You are a {dialect} expert. Given an input question, create a syntactically correct {dialect} query to run.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per {dialect}. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use date('now') function to get the current date, if the question involves \"today\".\nOnly use the following tables:\n{table_info}\nWrite an initial draft of the query. Then double check the {dialect} query for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\nUse format:\nFirst draft: <<FIRST_DRAFT_QUERY>>\nFinal answer: <<FINAL_ANSWER_QUERY>>\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n]\n)\n.\npartial\n(\ndialect\n=\ndb\n.\ndialect\n)\ndef\nparse_final_answer\n(\noutput\n:\nstr\n)\n-\n>\nstr\n:\nreturn\noutput\n.\nsplit\n(\n\"Final answer: \"\n)\n[\n1\n]\nchain\n=\ncreate_sql_query_chain\n(\nllm\n,\ndb\n,\nprompt\n=\nprompt\n)\n|\nparse_final_answer\nprompt\n.\npretty_print\n(\n)\n================================\u001b[1m System Message \u001b[0m================================\nYou are a \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m expert. Given an input question, create a syntactically correct \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m query to run.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most \u001b[33;1m\u001b[1;3m{top_k}\u001b[0m results using the LIMIT clause as per \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use date('now') function to get the current date, if the question involves \"today\".\nOnly use the following tables:\n\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m\nWrite an initial draft of the query. Then double check the \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m query for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\nUse format:\nFirst draft: <<FIRST_DRAFT_QUERY>>\nFinal answer: <<FINAL_ANSWER_QUERY>>\n================================\u001b[1m Human Message \u001b[0m=================================\n\u001b[33;1m\u001b[1;3m{input}\u001b[0m\nquery\n=\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\n\"What's the average Invoice from an American customer whose Fax is missing since 2003 but before 2010\"\n}\n)\nprint\n(\nquery\n)\nSELECT AVG(i.\"Total\") AS \"AverageInvoice\"\nFROM \"Invoice\" i\nJOIN \"Customer\" c ON i.\"CustomerId\" = c.\"CustomerId\"\nWHERE c.\"Country\" = 'USA'\nAND c.\"Fax\" IS NULL\nAND i.\"InvoiceDate\" BETWEEN '2003-01-01' AND '2010-01-01';\ndb\n.\nrun\n(\nquery\n)\n'[(6.632999999999998,)]'\nHuman-in-the-loop\nâ€‹\nIn some cases our data is sensitive enough that we never want to execute a SQL query without a human approving it first. Head to the\nTool use: Human-in-the-loop\npage to learn how to add a human-in-the-loop to any tool, chain or agent.\nError handling\nâ€‹\nAt some point, the model will make a mistake and craft an invalid SQL query. Or an issue will arise with our database. Or the model API will go down. We'll want to add some error handling behavior to our chains and agents so that we fail gracefully in these situations, and perhaps even automatically recover. To learn about error handling with tools, head to the\nTool use: Error handling\npage.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/streaming/",
    "How-to guides\nHow to stream runnables\nOn this page\nHow to stream runnables\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nLangChain Expression Language\nOutput parsers\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\nImportant LangChain primitives like\nchat models\n,\noutput parsers\n,\nprompts\n,\nretrievers\n, and\nagents\nimplement the LangChain\nRunnable Interface\n.\nThis interface provides two general approaches to stream content:\nsync\nstream\nand async\nastream\n: a\ndefault implementation\nof streaming that streams the\nfinal output\nfrom the chain.\nasync\nastream_events\nand async\nastream_log\n: these provide a way to stream both\nintermediate steps\nand\nfinal output\nfrom the chain.\nLet's take a look at both approaches, and try to understand how to use them.\ninfo\nFor a higher-level overview of streaming techniques in LangChain, see\nthis section of the conceptual guide\n.\nUsing Stream\nâ€‹\nAll\nRunnable\nobjects implement a sync method called\nstream\nand an async variant called\nastream\n.\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\nStreaming is only possible if all steps in the program know how to process an\ninput stream\n; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\nLLMs and Chat Models\nâ€‹\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\nLarge language models can take\nseveral seconds\nto generate a complete response to a query. This is far slower than the\n~200-300 ms\nthreshold at which an application feels responsive to an end user.\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model\ntoken by token\n.\nWe will show examples of streaming using a chat model. Choose one from the options below:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nLet's start with the sync\nstream\nAPI:\nchunks\n=\n[\n]\nfor\nchunk\nin\nmodel\n.\nstream\n(\n\"what color is the sky?\"\n)\n:\nchunks\n.\nappend\n(\nchunk\n)\nprint\n(\nchunk\n.\ncontent\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n|The sky typically| appears blue during the day due to a phenomenon| called Rayleigh scattering, where| air molecules scatter sunlight, with| blue light being scattered more than other colors. However|, the sky's color can vary|:\n- At sunrise/sunset:| Red, orange, pink, or purple\n-| During storms: Gray or dark blue|\n- At night: Dark| blue to black\n- In certain| atmospheric conditions: White, yellow, or green| (rare)\nThe color we perceive depends| on weather conditions, time of day, pollution| levels, and our viewing angle.||\nAlternatively, if you're working in an async environment, you may consider using the async\nastream\nAPI:\nchunks\n=\n[\n]\nasync\nfor\nchunk\nin\nmodel\n.\nastream\n(\n\"what color is the sky?\"\n)\n:\nchunks\n.\nappend\n(\nchunk\n)\nprint\n(\nchunk\n.\ncontent\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n|The sky typically| appears blue during the day due to a phenomenon called Rayleigh| scattering, where air molecules scatter sunlight,| with blue light being scattered more than other colors. However|, the sky's color can vary - appearing re|d, orange, or pink during sunrise and sunset,| gray on cloudy days, and black at night.| The color you see depends on the time of| day, weather conditions, and your location.||\nLet's inspect one of the chunks\nchunks\n[\n0\n]\nAIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--c4f01565-8bb4-4f07-9b23-acfe46cbeca3', usage_metadata={'input_tokens': 13, 'output_tokens': 0, 'total_tokens': 13, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})\nWe got back something called an\nAIMessageChunk\n. This chunk represents a part of an\nAIMessage\n.\nMessage chunks are additive by design -- one can simply add them up to get the state of the response so far!\nchunks\n[\n0\n]\n+\nchunks\n[\n1\n]\n+\nchunks\n[\n2\n]\n+\nchunks\n[\n3\n]\n+\nchunks\n[\n4\n]\nAIMessageChunk(content='The sky typically appears blue during the day due to a phenomenon called Rayleigh scattering, where air molecules scatter sunlight, with blue light being scattered more than other colors. However', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--c4f01565-8bb4-4f07-9b23-acfe46cbeca3', usage_metadata={'input_tokens': 13, 'output_tokens': 0, 'total_tokens': 13, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})\nChains\nâ€‹\nVirtually all LLM applications involve more steps than just a call to a language model.\nLet's build a simple chain using\nLangChain Expression Language\n(\nLCEL\n) that combines a prompt, model and a parser and verify that streaming works.\nWe will use\nStrOutputParser\nto parse the output from the model. This is a simple parser that extracts the\ncontent\nfield from an\nAIMessageChunk\n, giving us the\ntoken\nreturned by the model.\ntip\nLCEL is a\ndeclarative\nway to specify a \"program\" by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of\nstream\nand\nastream\nallowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"tell me a joke about {topic}\"\n)\nparser\n=\nStrOutputParser\n(\n)\nchain\n=\nprompt\n|\nmodel\n|\nparser\nasync\nfor\nchunk\nin\nchain\n.\nastream\n(\n{\n\"topic\"\n:\n\"parrot\"\n}\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|Why| don't parrots use the internet?\nThey|'re afraid of getting a virus from all the tweets|!||\nNote that we're getting streaming output even though we're using\nparser\nat the end of the chain above. The\nparser\noperates on each streaming chunk individidually. Many of the\nLCEL primitives\nalso support this kind of transform-style passthrough streaming, which can be very convenient when constructing apps.\nCustom functions can be\ndesigned to return generators\n, which are able to operate on streams.\nCertain runnables, like\nprompt templates\nand\nchat models\n, cannot process individual chunks and instead aggregate all previous steps. Such runnables can interrupt the streaming process.\nnote\nThe LangChain Expression language allows you to separate the construction of a chain from the mode in which it is used (e.g., sync/async, batch/streaming etc.). If this is not relevant to what you're building, you can also rely on a standard\nimperative\nprogramming approach by\ncaling\ninvoke\n,\nbatch\nor\nstream\non each component individually, assigning the results to variables and then using them downstream as you see fit.\nWorking with Input Streams\nâ€‹\nWhat if you wanted to stream JSON from the output as it was being generated?\nIf you were to rely on\njson.loads\nto parse the partial json, the parsing would fail as the partial json wouldn't be valid json.\nYou'd likely be at a complete loss of what to do and claim that it wasn't possible to stream JSON.\nWell, turns out there is a way to do it -- the parser needs to operate on the\ninput stream\n, and attempt to \"auto-complete\" the partial json into a valid state.\nLet's see such a parser in action to understand what this means.\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nJsonOutputParser\nchain\n=\n(\nmodel\n|\nJsonOutputParser\n(\n)\n)\n# Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\nasync\nfor\ntext\nin\nchain\n.\nastream\n(\n\"output a list of the countries france, spain and japan and their populations in JSON format. \"\n'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n\"Each country should have the key `name` and `population`\"\n)\n:\nprint\n(\ntext\n,\nflush\n=\nTrue\n)\nAPI Reference:\nJsonOutputParser\n{'countries': []}\n{'countries': [{'name': 'France'}]}\n{'countries': [{'name': 'France', 'population': 67750}]}\n{'countries': [{'name': 'France', 'population': 67750000}, {}]}\n{'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain'}]}\n{'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {}]}\n{'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan'}]}\n{'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}\nNow, let's\nbreak\nstreaming. We'll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON.\nwarning\nAny steps in the chain that operate on\nfinalized inputs\nrather than on\ninput streams\ncan break streaming functionality via\nstream\nor\nastream\n.\ntip\nLater, we will discuss the\nastream_events\nAPI which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on\nfinalized inputs\n.\nfrom\nlangchain_core\n.\noutput_parsers\nimport\n(\nJsonOutputParser\n,\n)\n# A function that operates on finalized inputs\n# rather than on an input_stream\ndef\n_extract_country_names\n(\ninputs\n)\n:\n\"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\nif\nnot\nisinstance\n(\ninputs\n,\ndict\n)\n:\nreturn\n\"\"\nif\n\"countries\"\nnot\nin\ninputs\n:\nreturn\n\"\"\ncountries\n=\ninputs\n[\n\"countries\"\n]\nif\nnot\nisinstance\n(\ncountries\n,\nlist\n)\n:\nreturn\n\"\"\ncountry_names\n=\n[\ncountry\n.\nget\n(\n\"name\"\n)\nfor\ncountry\nin\ncountries\nif\nisinstance\n(\ncountry\n,\ndict\n)\n]\nreturn\ncountry_names\nchain\n=\nmodel\n|\nJsonOutputParser\n(\n)\n|\n_extract_country_names\nasync\nfor\ntext\nin\nchain\n.\nastream\n(\n\"output a list of the countries france, spain and japan and their populations in JSON format. \"\n'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n\"Each country should have the key `name` and `population`\"\n)\n:\nprint\n(\ntext\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nAPI Reference:\nJsonOutputParser\n['France', 'Spain', 'Japan']|\nGenerator Functions\nâ€‹\nLet's fix the streaming using a generator function that can operate on the\ninput stream\n.\ntip\nA generator function (a function that uses\nyield\n) allows writing code that operates on\ninput streams\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nJsonOutputParser\nasync\ndef\n_extract_country_names_streaming\n(\ninput_stream\n)\n:\n\"\"\"A function that operates on input streams.\"\"\"\ncountry_names_so_far\n=\nset\n(\n)\nasync\nfor\ninput\nin\ninput_stream\n:\nif\nnot\nisinstance\n(\ninput\n,\ndict\n)\n:\ncontinue\nif\n\"countries\"\nnot\nin\ninput\n:\ncontinue\ncountries\n=\ninput\n[\n\"countries\"\n]\nif\nnot\nisinstance\n(\ncountries\n,\nlist\n)\n:\ncontinue\nfor\ncountry\nin\ncountries\n:\nname\n=\ncountry\n.\nget\n(\n\"name\"\n)\nif\nnot\nname\n:\ncontinue\nif\nname\nnot\nin\ncountry_names_so_far\n:\nyield\nname\ncountry_names_so_far\n.\nadd\n(\nname\n)\nchain\n=\nmodel\n|\nJsonOutputParser\n(\n)\n|\n_extract_country_names_streaming\nasync\nfor\ntext\nin\nchain\n.\nastream\n(\n\"output a list of the countries france, spain and japan and their populations in JSON format. \"\n'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n\"Each country should have the key `name` and `population`\"\n,\n)\n:\nprint\n(\ntext\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nAPI Reference:\nJsonOutputParser\nFrance|Spain|Japan|\nnote\nBecause the code above is relying on JSON auto-completion, you may see partial names of countries (e.g.,\nSp\nand\nSpain\n), which is not what one would want for an extraction result!\nWe're focusing on streaming concepts, not necessarily the results of the chains.\nNon-streaming components\nâ€‹\nSome built-in components like Retrievers do not offer any\nstreaming\n. What happens if we try to\nstream\nthem? ðŸ¤¨\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\ntemplate\n=\n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nvectorstore\n=\nFAISS\n.\nfrom_texts\n(\n[\n\"harrison worked at kensho\"\n,\n\"harrison likes spicy food\"\n]\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n,\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\nchunks\n=\n[\nchunk\nfor\nchunk\nin\nretriever\n.\nstream\n(\n\"where did harrison work?\"\n)\n]\nchunks\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnablePassthrough\n[[Document(id='2740a247-9738-48c4-8c8f-d879d4ed39f7', metadata={}, page_content='harrison worked at kensho'),\nDocument(id='1d3d012f-1cb0-4bee-928a-c8bf0f8b1b92', metadata={}, page_content='harrison likes spicy food')]]\nStream just yielded the final result from that component.\nThis is OK ðŸ¥¹! Not all components have to implement streaming -- in some cases streaming is either unnecessary, difficult or just doesn't make sense.\ntip\nAn LCEL chain constructed using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.\nretrieval_chain\n=\n(\n{\n\"context\"\n:\nretriever\n.\nwith_config\n(\nrun_name\n=\n\"Docs\"\n)\n,\n\"question\"\n:\nRunnablePassthrough\n(\n)\n,\n}\n|\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n)\nfor\nchunk\nin\nretrieval_chain\n.\nstream\n(\n\"Where did harrison work? Write 3 made up sentences about this place.\"\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n|Base|d on the provided context, Harrison worked at Kens|ho.\nThree made up sentences about Kens|ho:\n1. Kensho is a| cutting-edge technology company that specializes in| AI and data analytics for financial institutions.\n2|. The Kensho office features| an open floor plan with panoramic views of the city| skyline, creating an inspiring environment for its| employees.\n3. At Kensho,| team members often collaborate in innovative brainstorming sessions while| enjoying complimentary gourmet coffee from| their in-house cafÃ©.||\nNow that we've seen how\nstream\nand\nastream\nwork, let's venture into the world of streaming events. ðŸžï¸\nUsing Stream Events\nâ€‹\nEvent Streaming is a\nbeta\nAPI. This API may change a bit based on feedback.\nnote\nThis guide demonstrates the\nV2\nAPI and requires langchain-core >= 0.2. For the\nV1\nAPI compatible with older versions of LangChain, see\nhere\n.\nimport\nlangchain_core\nlangchain_core\n.\n__version__\nFor the\nastream_events\nAPI to work properly:\nUse\nasync\nthroughout the code to the extent possible (e.g., async tools etc)\nPropagate callbacks if defining custom functions / runnables\nWhenever using runnables without LCEL, make sure to call\n.astream()\non LLMs rather than\n.ainvoke\nto force the LLM to stream tokens.\nLet us know if anything doesn't work as expected! :)\nEvent Reference\nâ€‹\nBelow is a reference table that shows some events that might be emitted by the various Runnable objects.\nnote\nWhen streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that\ninputs\nwill often be included only for\nend\nevents and rather than for\nstart\nevents.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n[model name]\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n[model name]\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n[model name]\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n[model name]\n{'input': 'hello'}\non_llm_stream\n[model name]\n'Hello'\non_llm_end\n[model name]\n'Hello human!'\non_chain_start\nformat_docs\non_chain_stream\nformat_docs\n\"hello world!, goodbye world!\"\non_chain_end\nformat_docs\n[Document(...)]\n\"hello world!, goodbye world!\"\non_tool_start\nsome_tool\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\nsome_tool\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n[retriever name]\n{\"query\": \"hello\"}\non_retriever_end\n[retriever name]\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n[template_name]\n{\"question\": \"hello\"}\non_prompt_end\n[template_name]\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nChat Model\nâ€‹\nLet's start off by looking at the events produced by a chat model.\nevents\n=\n[\n]\nasync\nfor\nevent\nin\nmodel\n.\nastream_events\n(\n\"hello\"\n)\n:\nevents\n.\nappend\n(\nevent\n)\nnote\nFor\nlangchain-core<0.3.37\n, set the\nversion\nkwarg explicitly (e.g.,\nmodel.astream_events(\"hello\", version=\"v2\")\n).\nLet's take a look at the few of the start event and a few of the end events.\nevents\n[\n:\n3\n]\n[{'event': 'on_chat_model_start',\n'data': {'input': 'hello'},\n'name': 'ChatAnthropic',\n'tags': [],\n'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',\n'metadata': {'ls_provider': 'anthropic',\n'ls_model_name': 'claude-3-7-sonnet-20250219',\n'ls_model_type': 'chat',\n'ls_temperature': 0.0,\n'ls_max_tokens': 1024},\n'parent_ids': []},\n{'event': 'on_chat_model_stream',\n'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',\n'name': 'ChatAnthropic',\n'tags': [],\n'metadata': {'ls_provider': 'anthropic',\n'ls_model_name': 'claude-3-7-sonnet-20250219',\n'ls_model_type': 'chat',\n'ls_temperature': 0.0,\n'ls_max_tokens': 1024},\n'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--c35a72be-a5af-4bd5-bd9b-206135c28ef6', usage_metadata={'input_tokens': 8, 'output_tokens': 0, 'total_tokens': 8, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},\n'parent_ids': []},\n{'event': 'on_chat_model_stream',\n'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',\n'name': 'ChatAnthropic',\n'tags': [],\n'metadata': {'ls_provider': 'anthropic',\n'ls_model_name': 'claude-3-7-sonnet-20250219',\n'ls_model_type': 'chat',\n'ls_temperature': 0.0,\n'ls_max_tokens': 1024},\n'data': {'chunk': AIMessageChunk(content='Hello! How', additional_kwargs={}, response_metadata={}, id='run--c35a72be-a5af-4bd5-bd9b-206135c28ef6')},\n'parent_ids': []}]\nevents\n[\n-\n2\n:\n]\n[{'event': 'on_chat_model_stream',\n'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',\n'name': 'ChatAnthropic',\n'tags': [],\n'metadata': {'ls_provider': 'anthropic',\n'ls_model_name': 'claude-3-7-sonnet-20250219',\n'ls_model_type': 'chat',\n'ls_temperature': 0.0,\n'ls_max_tokens': 1024},\n'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--c35a72be-a5af-4bd5-bd9b-206135c28ef6', usage_metadata={'input_tokens': 0, 'output_tokens': 40, 'total_tokens': 40})},\n'parent_ids': []},\n{'event': 'on_chat_model_end',\n'data': {'output': AIMessageChunk(content=\"Hello! How can I assist you today? Whether you have questions, need information, or just want to chat, I'm here to help. What would you like to talk about?\", additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--c35a72be-a5af-4bd5-bd9b-206135c28ef6', usage_metadata={'input_tokens': 8, 'output_tokens': 40, 'total_tokens': 48, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},\n'run_id': 'c35a72be-a5af-4bd5-bd9b-206135c28ef6',\n'name': 'ChatAnthropic',\n'tags': [],\n'metadata': {'ls_provider': 'anthropic',\n'ls_model_name': 'claude-3-7-sonnet-20250219',\n'ls_model_type': 'chat',\n'ls_temperature': 0.0,\n'ls_max_tokens': 1024},\n'parent_ids': []}]\nChain\nâ€‹\nLet's revisit the example chain that parsed streaming JSON to explore the streaming events API.\nchain\n=\n(\nmodel\n|\nJsonOutputParser\n(\n)\n)\n# Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"output a list of the countries france, spain and japan and their populations in JSON format. \"\n'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n\"Each country should have the key `name` and `population`\"\n,\n)\n]\nIf you examine at the first few events, you'll notice that there are\n3\ndifferent start events rather than\n2\nstart events.\nThe three start events correspond to:\nThe chain (model + parser)\nThe model\nThe parser\nevents\n[\n:\n3\n]\n[{'event': 'on_chain_start',\n'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'},\n'name': 'RunnableSequence',\n'tags': [],\n'run_id': 'f859e56f-a760-4670-a24e-040e11bcd7fc',\n'metadata': {},\n'parent_ids': []},\n{'event': 'on_chat_model_start',\n'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}},\n'name': 'ChatAnthropic',\n'tags': ['seq:step:1'],\n'run_id': '2aa8c9e6-a5cd-4e94-b994-cb0e9bd8ab21',\n'metadata': {'ls_provider': 'anthropic',\n'ls_model_name': 'claude-3-7-sonnet-20250219',\n'ls_model_type': 'chat',\n'ls_temperature': 0.0,\n'ls_max_tokens': 1024},\n'parent_ids': ['f859e56f-a760-4670-a24e-040e11bcd7fc']},\n{'event': 'on_chat_model_stream',\n'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--2aa8c9e6-a5cd-4e94-b994-cb0e9bd8ab21', usage_metadata={'input_tokens': 56, 'output_tokens': 0, 'total_tokens': 56, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},\n'run_id': '2aa8c9e6-a5cd-4e94-b994-cb0e9bd8ab21',\n'name': 'ChatAnthropic',\n'tags': ['seq:step:1'],\n'metadata': {'ls_provider': 'anthropic',\n'ls_model_name': 'claude-3-7-sonnet-20250219',\n'ls_model_type': 'chat',\n'ls_temperature': 0.0,\n'ls_max_tokens': 1024},\n'parent_ids': ['f859e56f-a760-4670-a24e-040e11bcd7fc']}]\nWhat do you think you'd see if you looked at the last 3 events? what about the middle?\nLet's use this API to take output the stream events from the model and the parser. We're ignoring start events, end events and events from the chain.\nnum_events\n=\n0\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"output a list of the countries france, spain and japan and their populations in JSON format. \"\n'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n\"Each country should have the key `name` and `population`\"\n,\n)\n:\nkind\n=\nevent\n[\n\"event\"\n]\nif\nkind\n==\n\"on_chat_model_stream\"\n:\nprint\n(\nf\"Chat model chunk:\n{\nrepr\n(\nevent\n[\n'data'\n]\n[\n'chunk'\n]\n.\ncontent\n)\n}\n\"\n,\nflush\n=\nTrue\n,\n)\nif\nkind\n==\n\"on_parser_stream\"\n:\nprint\n(\nf\"Parser chunk:\n{\nevent\n[\n'data'\n]\n[\n'chunk'\n]\n}\n\"\n,\nflush\n=\nTrue\n)\nnum_events\n+=\n1\nif\nnum_events\n>\n30\n:\n# Truncate the output\nprint\n(\n\"...\"\n)\nbreak\nChat model chunk: ''\nChat model chunk: '\\`\\`\\`'\nChat model chunk: 'json\\n{\\n  \"countries\": ['\nParser chunk: {'countries': []}\nChat model chunk: '\\n    {\\n      \"name\": \"France\",'\nParser chunk: {'countries': [{'name': 'France'}]}\nChat model chunk: '\\n      \"population\": 67750000\\n    },'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}]}\nChat model chunk: '\\n    {\\n      \"name\": \"Spain\",'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain'}]}\nChat model chunk: '\\n      \"population\": 47350'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350}]}\nChat model chunk: '000\\n    },\\n    {\\n      \"'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {}]}\nChat model chunk: 'name\": \"Japan\",\\n      \"population\":'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan'}]}\nChat model chunk: ' 125700000\\n    }'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}\nChat model chunk: '\\n  ]\\n}\\n\\`\\`\\`'\nChat model chunk: ''\n...\nBecause both the model and the parser support streaming, we see streaming events from both components in real time! Kind of cool isn't it? ðŸ¦œ\nFiltering Events\nâ€‹\nBecause this API produces so many events, it is useful to be able to filter on events.\nYou can filter by either component\nname\n, component\ntags\nor component\ntype\n.\nBy Name\nâ€‹\nchain\n=\nmodel\n.\nwith_config\n(\n{\n\"run_name\"\n:\n\"model\"\n}\n)\n|\nJsonOutputParser\n(\n)\n.\nwith_config\n(\n{\n\"run_name\"\n:\n\"my_parser\"\n}\n)\nmax_events\n=\n0\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"output a list of the countries france, spain and japan and their populations in JSON format. \"\n'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n\"Each country should have the key `name` and `population`\"\n,\ninclude_names\n=\n[\n\"my_parser\"\n]\n,\n)\n:\nprint\n(\nevent\n)\nmax_events\n+=\n1\nif\nmax_events\n>\n10\n:\n# Truncate output\nprint\n(\n\"...\"\n)\nbreak\n{'event': 'on_parser_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'my_parser', 'tags': ['seq:step:2'], 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'metadata': {}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain'}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan'}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n{'event': 'on_parser_stream', 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\n{'event': 'on_parser_end', 'data': {'output': {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}}, 'run_id': '781af9b6-31f8-47f2-ab79-52d17b000857', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'parent_ids': ['82c918c6-d5f6-4d2d-b710-4668509fe2f0']}\nBy Type\nâ€‹\nchain\n=\nmodel\n.\nwith_config\n(\n{\n\"run_name\"\n:\n\"model\"\n}\n)\n|\nJsonOutputParser\n(\n)\n.\nwith_config\n(\n{\n\"run_name\"\n:\n\"my_parser\"\n}\n)\nmax_events\n=\n0\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n,\ninclude_types\n=\n[\n\"chat_model\"\n]\n,\n)\n:\nprint\n(\nevent\n)\nmax_events\n+=\n1\nif\nmax_events\n>\n10\n:\n# Truncate output\nprint\n(\n\"...\"\n)\nbreak\n{'event': 'on_chat_model_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'model', 'tags': ['seq:step:1'], 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5', usage_metadata={'input_tokens': 56, 'output_tokens': 0, 'total_tokens': 56, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\`\\`\\`', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='json\\n{\\n  \"countries\": [', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n    {\\n      \"name\": \"France\",', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n      \"population\": 67750', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='000\\n    },\\n    {\\n      \"', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='name\": \"Spain\",\\n      \"population\":', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' 47350000\\n    },', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n    {\\n      \"name\": \"Japan\",', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n      \"population\": 125700', additional_kwargs={}, response_metadata={}, id='run--b7a08416-a629-4b42-b5d5-dbe48566e5d5')}, 'run_id': 'b7a08416-a629-4b42-b5d5-dbe48566e5d5', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['116a6506-5a19-4f60-a8c2-7b728d4b8248']}\n...\nBy Tags\nâ€‹\ncaution\nTags are inherited by child components of a given runnable.\nIf you're using tags to filter, make sure that this is what you want.\nchain\n=\n(\nmodel\n|\nJsonOutputParser\n(\n)\n)\n.\nwith_config\n(\n{\n\"tags\"\n:\n[\n\"my_chain\"\n]\n}\n)\nmax_events\n=\n0\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n,\ninclude_tags\n=\n[\n\"my_chain\"\n]\n,\n)\n:\nprint\n(\nevent\n)\nmax_events\n+=\n1\nif\nmax_events\n>\n10\n:\n# Truncate output\nprint\n(\n\"...\"\n)\nbreak\n{'event': 'on_chain_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'RunnableSequence', 'tags': ['my_chain'], 'run_id': '3e4f8c37-a44a-46b7-a7e5-75182d1cca31', 'metadata': {}, 'parent_ids': []}\n{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}}, 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-7-sonnet-20250219'}, id='run--778846c9-acd3-43b7-b9c0-ac718761b2bc', usage_metadata={'input_tokens': 56, 'output_tokens': 0, 'total_tokens': 56, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}\n{'event': 'on_parser_start', 'data': {}, 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'run_id': '2c46d24f-231c-4062-a7ab-b7954840986d', 'metadata': {}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\`\\`\\`', additional_kwargs={}, response_metadata={}, id='run--778846c9-acd3-43b7-b9c0-ac718761b2bc')}, 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='json\\n{\\n  \"countries\": [', additional_kwargs={}, response_metadata={}, id='run--778846c9-acd3-43b7-b9c0-ac718761b2bc')}, 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}\n{'event': 'on_parser_stream', 'run_id': '2c46d24f-231c-4062-a7ab-b7954840986d', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}\n{'event': 'on_chain_stream', 'run_id': '3e4f8c37-a44a-46b7-a7e5-75182d1cca31', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n    {\\n      \"name\": \"France\",', additional_kwargs={}, response_metadata={}, id='run--778846c9-acd3-43b7-b9c0-ac718761b2bc')}, 'run_id': '778846c9-acd3-43b7-b9c0-ac718761b2bc', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-7-sonnet-20250219', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}\n{'event': 'on_parser_stream', 'run_id': '2c46d24f-231c-4062-a7ab-b7954840986d', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['3e4f8c37-a44a-46b7-a7e5-75182d1cca31']}\n{'event': 'on_chain_stream', 'run_id': '3e4f8c37-a44a-46b7-a7e5-75182d1cca31', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': []}\n...\nNon-streaming components\nâ€‹\nRemember how some components don't stream well because they don't operate on\ninput streams\n?\nWhile such components can break streaming of the final output when using\nastream\n,\nastream_events\nwill still yield streaming events from intermediate steps that support streaming!\n# Function that does not support streaming.\n# It operates on the finalizes inputs rather than\n# operating on the input stream.\ndef\n_extract_country_names\n(\ninputs\n)\n:\n\"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\nif\nnot\nisinstance\n(\ninputs\n,\ndict\n)\n:\nreturn\n\"\"\nif\n\"countries\"\nnot\nin\ninputs\n:\nreturn\n\"\"\ncountries\n=\ninputs\n[\n\"countries\"\n]\nif\nnot\nisinstance\n(\ncountries\n,\nlist\n)\n:\nreturn\n\"\"\ncountry_names\n=\n[\ncountry\n.\nget\n(\n\"name\"\n)\nfor\ncountry\nin\ncountries\nif\nisinstance\n(\ncountry\n,\ndict\n)\n]\nreturn\ncountry_names\nchain\n=\n(\nmodel\n|\nJsonOutputParser\n(\n)\n|\n_extract_country_names\n)\n# This parser only works with OpenAI right now\nAs expected, the\nastream\nAPI doesn't work correctly because\n_extract_country_names\ndoesn't operate on streams.\nasync\nfor\nchunk\nin\nchain\n.\nastream\n(\n\"output a list of the countries france, spain and japan and their populations in JSON format. \"\n'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n\"Each country should have the key `name` and `population`\"\n,\n)\n:\nprint\n(\nchunk\n,\nflush\n=\nTrue\n)\n['France', 'Spain', 'Japan']\nNow, let's confirm that with astream_events we're still seeing streaming output from the model and the parser.\nnum_events\n=\n0\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"output a list of the countries france, spain and japan and their populations in JSON format. \"\n'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n\"Each country should have the key `name` and `population`\"\n,\n)\n:\nkind\n=\nevent\n[\n\"event\"\n]\nif\nkind\n==\n\"on_chat_model_stream\"\n:\nprint\n(\nf\"Chat model chunk:\n{\nrepr\n(\nevent\n[\n'data'\n]\n[\n'chunk'\n]\n.\ncontent\n)\n}\n\"\n,\nflush\n=\nTrue\n,\n)\nif\nkind\n==\n\"on_parser_stream\"\n:\nprint\n(\nf\"Parser chunk:\n{\nevent\n[\n'data'\n]\n[\n'chunk'\n]\n}\n\"\n,\nflush\n=\nTrue\n)\nnum_events\n+=\n1\nif\nnum_events\n>\n30\n:\n# Truncate the output\nprint\n(\n\"...\"\n)\nbreak\nChat model chunk: ''\nChat model chunk: '\\`\\`\\`'\nChat model chunk: 'json\\n{\\n  \"countries\": ['\nParser chunk: {'countries': []}\nChat model chunk: '\\n    {\\n      \"name\": \"France\",'\nParser chunk: {'countries': [{'name': 'France'}]}\nChat model chunk: '\\n      \"population\": 67750'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750}]}\nChat model chunk: '000\\n    },\\n    {\\n      \"'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {}]}\nChat model chunk: 'name\": \"Spain\",\\n      \"population\":'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain'}]}\nChat model chunk: ' 47350000\\n    },'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}]}\nChat model chunk: '\\n    {\\n      \"name\": \"Japan\",'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan'}]}\nChat model chunk: '\\n      \"population\": 125700'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700}]}\nChat model chunk: '000\\n    }\\n  ]\\n}'\nParser chunk: {'countries': [{'name': 'France', 'population': 67750000}, {'name': 'Spain', 'population': 47350000}, {'name': 'Japan', 'population': 125700000}]}\nChat model chunk: '\\n\\`\\`\\`'\nChat model chunk: ''\n...\nPropagating Callbacks\nâ€‹\ncaution\nIf you're using invoking runnables inside your tools, you need to propagate callbacks to the runnable; otherwise, no stream events will be generated.\nnote\nWhen using\nRunnableLambdas\nor\n@chain\ndecorator, callbacks are propagated automatically behind the scenes.\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nfrom\nlangchain_core\n.\ntools\nimport\ntool\ndef\nreverse_word\n(\nword\n:\nstr\n)\n:\nreturn\nword\n[\n:\n:\n-\n1\n]\nreverse_word\n=\nRunnableLambda\n(\nreverse_word\n)\n@tool\ndef\nbad_tool\n(\nword\n:\nstr\n)\n:\n\"\"\"Custom tool that doesn't propagate callbacks.\"\"\"\nreturn\nreverse_word\n.\ninvoke\n(\nword\n)\nasync\nfor\nevent\nin\nbad_tool\n.\nastream_events\n(\n\"hello\"\n)\n:\nprint\n(\nevent\n)\nAPI Reference:\nRunnableLambda\n|\ntool\n{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'bad_tool', 'tags': [], 'run_id': 'b1c6b79d-f94b-432f-a289-1ea68a7c3cea', 'metadata': {}, 'parent_ids': []}\n{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': 'e661c1ec-e6d2-4f9a-9620-b50645f2b194', 'metadata': {}, 'parent_ids': ['b1c6b79d-f94b-432f-a289-1ea68a7c3cea']}\n{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': 'e661c1ec-e6d2-4f9a-9620-b50645f2b194', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['b1c6b79d-f94b-432f-a289-1ea68a7c3cea']}\n{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'b1c6b79d-f94b-432f-a289-1ea68a7c3cea', 'name': 'bad_tool', 'tags': [], 'metadata': {}, 'parent_ids': []}\nHere's a re-implementation that does propagate callbacks correctly. You'll notice that now we're getting events from the\nreverse_word\nrunnable as well.\n@tool\ndef\ncorrect_tool\n(\nword\n:\nstr\n,\ncallbacks\n)\n:\n\"\"\"A tool that correctly propagates callbacks.\"\"\"\nreturn\nreverse_word\n.\ninvoke\n(\nword\n,\n{\n\"callbacks\"\n:\ncallbacks\n}\n)\nasync\nfor\nevent\nin\ncorrect_tool\n.\nastream_events\n(\n\"hello\"\n)\n:\nprint\n(\nevent\n)\n{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'correct_tool', 'tags': [], 'run_id': '399c91f5-a40b-4173-943f-a9c583a04003', 'metadata': {}, 'parent_ids': []}\n{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': 'e9cc7db1-4587-40af-9c35-2d787b3f0956', 'metadata': {}, 'parent_ids': ['399c91f5-a40b-4173-943f-a9c583a04003']}\n{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': 'e9cc7db1-4587-40af-9c35-2d787b3f0956', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['399c91f5-a40b-4173-943f-a9c583a04003']}\n{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': '399c91f5-a40b-4173-943f-a9c583a04003', 'name': 'correct_tool', 'tags': [], 'metadata': {}, 'parent_ids': []}\nIf you're invoking runnables from within Runnable Lambdas or\n@chains\n, then callbacks will be passed automatically on your behalf.\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableLambda\nasync\ndef\nreverse_and_double\n(\nword\n:\nstr\n)\n:\nreturn\nawait\nreverse_word\n.\nainvoke\n(\nword\n)\n*\n2\nreverse_and_double\n=\nRunnableLambda\n(\nreverse_and_double\n)\nawait\nreverse_and_double\n.\nainvoke\n(\n\"1234\"\n)\nasync\nfor\nevent\nin\nreverse_and_double\n.\nastream_events\n(\n\"1234\"\n)\n:\nprint\n(\nevent\n)\nAPI Reference:\nRunnableLambda\n{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '04726e2e-f508-4f90-9d4f-f88e588f0b39', 'metadata': {}, 'parent_ids': []}\n{'event': 'on_chain_stream', 'run_id': '04726e2e-f508-4f90-9d4f-f88e588f0b39', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'chunk': '43214321'}, 'parent_ids': []}\n{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '04726e2e-f508-4f90-9d4f-f88e588f0b39', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'parent_ids': []}\nAnd with the\n@chain\ndecorator:\nfrom\nlangchain_core\n.\nrunnables\nimport\nchain\n@chain\nasync\ndef\nreverse_and_double\n(\nword\n:\nstr\n)\n:\nreturn\nawait\nreverse_word\n.\nainvoke\n(\nword\n)\n*\n2\nawait\nreverse_and_double\n.\nainvoke\n(\n\"1234\"\n)\nasync\nfor\nevent\nin\nreverse_and_double\n.\nastream_events\n(\n\"1234\"\n)\n:\nprint\n(\nevent\n)\nAPI Reference:\nchain\n{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '25f72976-aa79-408d-bb42-6d0f038cde52', 'metadata': {}, 'parent_ids': []}\n{'event': 'on_chain_stream', 'run_id': '25f72976-aa79-408d-bb42-6d0f038cde52', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'chunk': '43214321'}, 'parent_ids': []}\n{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '25f72976-aa79-408d-bb42-6d0f038cde52', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'parent_ids': []}\nNext steps\nâ€‹\nNow you've learned some ways to stream both final outputs and internal steps with LangChain.\nTo learn more, check out the other how-to guides in this section, or the\nconceptual guide on Langchain Expression Language\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/streaming_llm/",
    "How-to guides\nHow to stream responses from an LLM\nOn this page\nHow to stream responses from an LLM\nAll\nLLM\ns implement the\nRunnable interface\n, which comes with\ndefault\nimplementations of standard runnable methods (i.e.\nainvoke\n,\nbatch\n,\nabatch\n,\nstream\n,\nastream\n,\nastream_events\n).\nThe\ndefault\nstreaming implementations provide an\nIterator\n(or\nAsyncIterator\nfor asynchronous streaming) that yields a single value: the final output from the underlying chat model provider.\nThe ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support.\nSee which\nintegrations support token-by-token streaming here\n.\nnote\nThe\ndefault\nimplementation does\nnot\nprovide support for token-by-token streaming, but it ensures that the model can be swapped in for any other model as it supports the same standard interface.\nSync stream\nâ€‹\nBelow we use a\n|\nto help visualize the delimiter between tokens.\nfrom\nlangchain_openai\nimport\nOpenAI\nllm\n=\nOpenAI\n(\nmodel\n=\n\"gpt-3.5-turbo-instruct\"\n,\ntemperature\n=\n0\n,\nmax_tokens\n=\n512\n)\nfor\nchunk\nin\nllm\n.\nstream\n(\n\"Write me a 1 verse song about sparkling water.\"\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n|Spark|ling| water|,| oh| so clear|\n|Bubbles dancing|,| without| fear|\n|Refreshing| taste|,| a| pure| delight|\n|Spark|ling| water|,| my| thirst|'s| delight||\nAsync streaming\nâ€‹\nLet's see how to stream in an async setting using\nastream\n.\nfrom\nlangchain_openai\nimport\nOpenAI\nllm\n=\nOpenAI\n(\nmodel\n=\n\"gpt-3.5-turbo-instruct\"\n,\ntemperature\n=\n0\n,\nmax_tokens\n=\n512\n)\nasync\nfor\nchunk\nin\nllm\n.\nastream\n(\n\"Write me a 1 verse song about sparkling water.\"\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n|Spark|ling| water|,| oh| so clear|\n|Bubbles dancing|,| without| fear|\n|Refreshing| taste|,| a| pure| delight|\n|Spark|ling| water|,| my| thirst|'s| delight||\nAsync event streaming\nâ€‹\nLLMs also support the standard\nastream events\nmethod.\ntip\nastream_events\nis most useful when implementing streaming in a larger LLM application that contains multiple steps (e.g., an application that involves an\nagent\n).\nfrom\nlangchain_openai\nimport\nOpenAI\nllm\n=\nOpenAI\n(\nmodel\n=\n\"gpt-3.5-turbo-instruct\"\n,\ntemperature\n=\n0\n,\nmax_tokens\n=\n512\n)\nidx\n=\n0\nasync\nfor\nevent\nin\nllm\n.\nastream_events\n(\n\"Write me a 1 verse song about goldfish on the moon\"\n,\nversion\n=\n\"v1\"\n)\n:\nidx\n+=\n1\nif\nidx\n>=\n5\n:\n# Truncate the output\nprint\n(\n\"...Truncated\"\n)\nbreak\nprint\n(\nevent\n)\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/time_weighted_vectorstore/",
    "How-to guides\nHow to use a time-weighted vector store retriever\nOn this page\nHow to use a time-weighted vector store retriever\nThis\nretriever\nuses a combination of semantic\nsimilarity\nand a time decay.\nThe algorithm for scoring them is:\nsemantic_similarity + (1.0 - decay_rate) ^ hours_passed\nNotably,\nhours_passed\nrefers to the hours passed since the object in the retriever\nwas last accessed\n, not since it was created. This means that frequently accessed objects remain \"fresh\".\nfrom\ndatetime\nimport\ndatetime\n,\ntimedelta\nimport\nfaiss\nfrom\nlangchain\n.\nretrievers\nimport\nTimeWeightedVectorStoreRetriever\nfrom\nlangchain_community\n.\ndocstore\nimport\nInMemoryDocstore\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nAPI Reference:\nDocument\nLow decay rate\nâ€‹\nA low\ndecay rate\n(in this, to be extreme, we will set it close to 0) means memories will be \"remembered\" for longer. A\ndecay rate\nof 0 means memories never be forgotten, making this retriever equivalent to the vector lookup.\n# Define your embedding model\nembeddings_model\n=\nOpenAIEmbeddings\n(\n)\n# Initialize the vectorstore as empty\nembedding_size\n=\n1536\nindex\n=\nfaiss\n.\nIndexFlatL2\n(\nembedding_size\n)\nvectorstore\n=\nFAISS\n(\nembeddings_model\n,\nindex\n,\nInMemoryDocstore\n(\n{\n}\n)\n,\n{\n}\n)\nretriever\n=\nTimeWeightedVectorStoreRetriever\n(\nvectorstore\n=\nvectorstore\n,\ndecay_rate\n=\n0.0000000000000000000000001\n,\nk\n=\n1\n)\nyesterday\n=\ndatetime\n.\nnow\n(\n)\n-\ntimedelta\n(\ndays\n=\n1\n)\nretriever\n.\nadd_documents\n(\n[\nDocument\n(\npage_content\n=\n\"hello world\"\n,\nmetadata\n=\n{\n\"last_accessed_at\"\n:\nyesterday\n}\n)\n]\n)\nretriever\n.\nadd_documents\n(\n[\nDocument\n(\npage_content\n=\n\"hello foo\"\n)\n]\n)\n['73679bc9-d425-49c2-9d74-de6356c73489']\n# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough\nretriever\n.\ninvoke\n(\n\"hello world\"\n)\n[Document(metadata={'last_accessed_at': datetime.datetime(2024, 10, 22, 16, 37, 40, 818583), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 37, 975074), 'buffer_idx': 0}, page_content='hello world')]\nHigh decay rate\nâ€‹\nWith a high\ndecay rate\n(e.g., several 9's), the\nrecency score\nquickly goes to 0! If you set this all the way to 1,\nrecency\nis 0 for all objects, once again making this equivalent to a vector lookup.\n# Define your embedding model\nembeddings_model\n=\nOpenAIEmbeddings\n(\n)\n# Initialize the vectorstore as empty\nembedding_size\n=\n1536\nindex\n=\nfaiss\n.\nIndexFlatL2\n(\nembedding_size\n)\nvectorstore\n=\nFAISS\n(\nembeddings_model\n,\nindex\n,\nInMemoryDocstore\n(\n{\n}\n)\n,\n{\n}\n)\nretriever\n=\nTimeWeightedVectorStoreRetriever\n(\nvectorstore\n=\nvectorstore\n,\ndecay_rate\n=\n0.999\n,\nk\n=\n1\n)\nyesterday\n=\ndatetime\n.\nnow\n(\n)\n-\ntimedelta\n(\ndays\n=\n1\n)\nretriever\n.\nadd_documents\n(\n[\nDocument\n(\npage_content\n=\n\"hello world\"\n,\nmetadata\n=\n{\n\"last_accessed_at\"\n:\nyesterday\n}\n)\n]\n)\nretriever\n.\nadd_documents\n(\n[\nDocument\n(\npage_content\n=\n\"hello foo\"\n)\n]\n)\n['379631f0-42c2-4773-8cc2-d36201e1e610']\n# \"Hello Foo\" is returned first because \"hello world\" is mostly forgotten\nretriever\n.\ninvoke\n(\n\"hello world\"\n)\n[Document(metadata={'last_accessed_at': datetime.datetime(2024, 10, 22, 16, 37, 46, 553633), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 43, 927429), 'buffer_idx': 1}, page_content='hello foo')]\nVirtual time\nâ€‹\nUsing some utils in LangChain, you can mock out the time component.\nfrom\nlangchain_core\n.\nutils\nimport\nmock_now\nAPI Reference:\nmock_now\n# Notice the last access time is that date time\ntomorrow\n=\ndatetime\n.\nnow\n(\n)\n+\ntimedelta\n(\ndays\n=\n1\n)\nwith\nmock_now\n(\ntomorrow\n)\n:\nprint\n(\nretriever\n.\ninvoke\n(\n\"hello world\"\n)\n)\n[Document(metadata={'last_accessed_at': MockDateTime(2024, 10, 23, 16, 38, 19, 66711), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 43, 599877), 'buffer_idx': 0}, page_content='hello world')]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tool_artifacts/",
    "How-to guides\nHow to return artifacts from a tool\nOn this page\nHow to return artifacts from a tool\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nToolMessage\nTools\nFunction/tool calling\nTools\nare utilities that can be\ncalled by a model\n, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\nThe Tool and\nToolMessage\ninterfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the ToolMessage.content) and those parts which are meant for use outside the model (ToolMessage.artifact).\nRequires\nlangchain-core >= 0.2.19\nThis functionality was added in\nlangchain-core == 0.2.19\n. Please make sure your package is up to date.\nDefining the tool\nâ€‹\nIf we want our tool to distinguish between message content and other artifacts, we need to specify\nresponse_format=\"content_and_artifact\"\nwhen defining our tool and make sure that we return a tuple of (content, artifact):\n%\npip install\n-\nqU\n\"langchain-core>=0.2.19\"\nimport\nrandom\nfrom\ntyping\nimport\nList\n,\nTuple\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\ngenerate_random_ints\n(\nmin\n:\nint\n,\nmax\n:\nint\n,\nsize\n:\nint\n)\n-\n>\nTuple\n[\nstr\n,\nList\n[\nint\n]\n]\n:\n\"\"\"Generate size random ints in the range [min, max].\"\"\"\narray\n=\n[\nrandom\n.\nrandint\n(\nmin\n,\nmax\n)\nfor\n_\nin\nrange\n(\nsize\n)\n]\ncontent\n=\nf\"Successfully generated array of\n{\nsize\n}\nrandom ints in [\n{\nmin\n}\n,\n{\nmax\n}\n].\"\nreturn\ncontent\n,\narray\nAPI Reference:\ntool\nInvoking the tool with ToolCall\nâ€‹\nIf we directly invoke our tool with just the tool arguments, you'll notice that we only get back the content part of the Tool output:\ngenerate_random_ints\n.\ninvoke\n(\n{\n\"min\"\n:\n0\n,\n\"max\"\n:\n9\n,\n\"size\"\n:\n10\n}\n)\n'Successfully generated array of 10 random ints in [0, 9].'\nIn order to get back both the content and the artifact, we need to invoke our model with a ToolCall (which is just a dictionary with \"name\", \"args\", \"id\" and \"type\" keys), which has additional info needed to generate a ToolMessage like the tool call ID:\ngenerate_random_ints\n.\ninvoke\n(\n{\n\"name\"\n:\n\"generate_random_ints\"\n,\n\"args\"\n:\n{\n\"min\"\n:\n0\n,\n\"max\"\n:\n9\n,\n\"size\"\n:\n10\n}\n,\n\"id\"\n:\n\"123\"\n,\n# required\n\"type\"\n:\n\"tool_call\"\n,\n# required\n}\n)\nToolMessage(content='Successfully generated array of 10 random ints in [0, 9].', name='generate_random_ints', tool_call_id='123', artifact=[2, 8, 0, 6, 0, 0, 1, 5, 0, 0])\nUsing with a model\nâ€‹\nWith a\ntool-calling model\n, we can easily use a model to call our Tool and generate ToolMessages:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\ngenerate_random_ints\n]\n)\nai_msg\n=\nllm_with_tools\n.\ninvoke\n(\n\"generate 6 positive ints less than 25\"\n)\nai_msg\n.\ntool_calls\n[{'name': 'generate_random_ints',\n'args': {'min': 1, 'max': 24, 'size': 6},\n'id': 'toolu_01EtALY3Wz1DVYhv1TLvZGvE',\n'type': 'tool_call'}]\ngenerate_random_ints\n.\ninvoke\n(\nai_msg\n.\ntool_calls\n[\n0\n]\n)\nToolMessage(content='Successfully generated array of 6 random ints in [1, 24].', name='generate_random_ints', tool_call_id='toolu_01EtALY3Wz1DVYhv1TLvZGvE', artifact=[2, 20, 23, 8, 1, 15])\nIf we just pass in the tool call args, we'll only get back the content:\ngenerate_random_ints\n.\ninvoke\n(\nai_msg\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\n'Successfully generated array of 6 random ints in [1, 24].'\nIf we wanted to declaratively create a chain, we could do this:\nfrom\noperator\nimport\nattrgetter\nchain\n=\nllm_with_tools\n|\nattrgetter\n(\n\"tool_calls\"\n)\n|\ngenerate_random_ints\n.\nmap\n(\n)\nchain\n.\ninvoke\n(\n\"give me a random number between 1 and 5\"\n)\n[ToolMessage(content='Successfully generated array of 1 random ints in [1, 5].', name='generate_random_ints', tool_call_id='toolu_01FwYhnkwDPJPbKdGq4ng6uD', artifact=[5])]\nCreating from BaseTool class\nâ€‹\nIf you want to create a BaseTool object directly, instead of decorating a function with\n@tool\n, you can do so like this:\nfrom\nlangchain_core\n.\ntools\nimport\nBaseTool\nclass\nGenerateRandomFloats\n(\nBaseTool\n)\n:\nname\n:\nstr\n=\n\"generate_random_floats\"\ndescription\n:\nstr\n=\n\"Generate size random floats in the range [min, max].\"\nresponse_format\n:\nstr\n=\n\"content_and_artifact\"\nndigits\n:\nint\n=\n2\ndef\n_run\n(\nself\n,\nmin\n:\nfloat\n,\nmax\n:\nfloat\n,\nsize\n:\nint\n)\n-\n>\nTuple\n[\nstr\n,\nList\n[\nfloat\n]\n]\n:\nrange_\n=\nmax\n-\nmin\narray\n=\n[\nround\n(\nmin\n+\n(\nrange_\n*\nrandom\n.\nrandom\n(\n)\n)\n,\nndigits\n=\nself\n.\nndigits\n)\nfor\n_\nin\nrange\n(\nsize\n)\n]\ncontent\n=\nf\"Generated\n{\nsize\n}\nfloats in [\n{\nmin\n}\n,\n{\nmax\n}\n], rounded to\n{\nself\n.\nndigits\n}\ndecimals.\"\nreturn\ncontent\n,\narray\n# Optionally define an equivalent async method\n# async def _arun(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:\n#     ...\nAPI Reference:\nBaseTool\nrand_gen\n=\nGenerateRandomFloats\n(\nndigits\n=\n4\n)\nrand_gen\n.\ninvoke\n(\n{\n\"min\"\n:\n0.1\n,\n\"max\"\n:\n3.3333\n,\n\"size\"\n:\n3\n}\n)\n'Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.'\nrand_gen\n.\ninvoke\n(\n{\n\"name\"\n:\n\"generate_random_floats\"\n,\n\"args\"\n:\n{\n\"min\"\n:\n0.1\n,\n\"max\"\n:\n3.3333\n,\n\"size\"\n:\n3\n}\n,\n\"id\"\n:\n\"123\"\n,\n\"type\"\n:\n\"tool_call\"\n,\n}\n)\nToolMessage(content='Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.', name='generate_random_floats', tool_call_id='123', artifact=[1.5789, 2.464, 2.2719])\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tool_calling/",
    "How-to guides\nHow to use chat models to call tools\nOn this page\nHow to use chat models to call tools\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nTool calling\nTools\nOutput parsers\nTool calling\nallows a chat model to respond to a given prompt by \"calling a tool\".\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is\nextraction from unstructured text\n.\nIf you want to see how to use the model-generated tool call to actually run a tool\ncheck out this guide\n.\nSupported models\nTool calling is not universal, but is supported by many popular LLM providers. You can find a\nlist of all models that support tool calling here\n.\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\nDefining tool schemas\nâ€‹\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a\n.bind_tools()\nmethod for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain\nTool objects\n. Subsequent invocations of the model will pass in these tool schemas along with the prompt.\nPython functions\nâ€‹\nOur tool schemas can be Python functions:\n# The function name, type hints, and docstring are all part of the tool\n# schema that's passed to the model. Defining good, descriptive schemas\n# is an extension of prompt engineering and is an important part of\n# getting models to perform well.\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Add two integers.\nArgs:\na: First integer\nb: Second integer\n\"\"\"\nreturn\na\n+\nb\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two integers.\nArgs:\na: First integer\nb: Second integer\n\"\"\"\nreturn\na\n*\nb\nLangChain Tool\nâ€‹\nLangChain also implements a\n@tool\ndecorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide\nhere\nfor details.\nPydantic class\nâ€‹\nYou can equivalently define the schemas without the accompanying functions using\nPydantic\n.\nNote that all fields are\nrequired\nunless provided a default value.\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nadd\n(\nBaseModel\n)\n:\n\"\"\"Add two integers.\"\"\"\na\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"First integer\"\n)\nb\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Second integer\"\n)\nclass\nmultiply\n(\nBaseModel\n)\n:\n\"\"\"Multiply two integers.\"\"\"\na\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"First integer\"\n)\nb\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Second integer\"\n)\nTypedDict class\nâ€‹\nRequires\nlangchain-core>=0.2.25\nOr using TypedDicts and annotations:\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\nclass\nadd\n(\nTypedDict\n)\n:\n\"\"\"Add two integers.\"\"\"\n# Annotations must have the type and can optionally include a default value and description (in that order).\na\n:\nAnnotated\n[\nint\n,\n.\n.\n.\n,\n\"First integer\"\n]\nb\n:\nAnnotated\n[\nint\n,\n.\n.\n.\n,\n\"Second integer\"\n]\nclass\nmultiply\n(\nTypedDict\n)\n:\n\"\"\"Multiply two integers.\"\"\"\na\n:\nAnnotated\n[\nint\n,\n.\n.\n.\n,\n\"First integer\"\n]\nb\n:\nAnnotated\n[\nint\n,\n.\n.\n.\n,\n\"Second integer\"\n]\ntools\n=\n[\nadd\n,\nmultiply\n]\nTo actually bind those schemas to a chat model, we'll use the\n.bind_tools()\nmethod. This handles converting\nthe\nadd\nand\nmultiply\nschemas to the proper format for the model. The tool schema will then be passed it in each time the model is invoked.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n)\nquery\n=\n\"What is 3 * 12?\"\nllm_with_tools\n.\ninvoke\n(\nquery\n)\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_iXj4DiW1p7WLjTAQMRO0jxMs', 'function': {'arguments': '{\"a\":3,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 80, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0b620986-3f62-4df7-9ba3-4595089f9ad4-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_iXj4DiW1p7WLjTAQMRO0jxMs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 80, 'output_tokens': 17, 'total_tokens': 97})\nAs we can see our LLM generated arguments to a tool! You can look at the docs for\nbind_tools()\nto learn about all the ways to customize how your LLM selects tools, as well as\nthis guide on how to force the LLM to call a tool\nrather than letting it decide.\nTool calls\nâ€‹\nIf tool calls are included in a LLM response, they are attached to the corresponding\nmessage\nor\nmessage chunk\nas a list of\ntool call\nobjects in the\n.tool_calls\nattribute.\nNote that chat models can call multiple tools at once.\nA\nToolCall\nis a typed dict that includes a\ntool name, dict of argument values, and (optionally) an identifier. Messages with no\ntool calls default to an empty list for this attribute.\nquery\n=\n\"What is 3 * 12? Also, what is 11 + 49?\"\nllm_with_tools\n.\ninvoke\n(\nquery\n)\n.\ntool_calls\n[{'name': 'multiply',\n'args': {'a': 3, 'b': 12},\n'id': 'call_1fyhJAbJHuKQe6n0PacubGsL',\n'type': 'tool_call'},\n{'name': 'add',\n'args': {'a': 11, 'b': 49},\n'id': 'call_fc2jVkKzwuPWyU7kS9qn1hyG',\n'type': 'tool_call'}]\nThe\n.tool_calls\nattribute should contain valid tool calls. Note that on occasion,\nmodel providers may output malformed tool calls (e.g., arguments that are not\nvalid JSON). When parsing fails in these cases, instances\nof\nInvalidToolCall\nare populated in the\n.invalid_tool_calls\nattribute. An\nInvalidToolCall\ncan have\na name, string arguments, identifier, and error message.\nParsing\nâ€‹\nIf desired,\noutput parsers\ncan further process the output. For example, we can convert existing values populated on the\n.tool_calls\nto Pydantic objects using the\nPydanticToolsParser\n:\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nPydanticToolsParser\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nadd\n(\nBaseModel\n)\n:\n\"\"\"Add two integers.\"\"\"\na\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"First integer\"\n)\nb\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Second integer\"\n)\nclass\nmultiply\n(\nBaseModel\n)\n:\n\"\"\"Multiply two integers.\"\"\"\na\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"First integer\"\n)\nb\n:\nint\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"Second integer\"\n)\nchain\n=\nllm_with_tools\n|\nPydanticToolsParser\n(\ntools\n=\n[\nadd\n,\nmultiply\n]\n)\nchain\n.\ninvoke\n(\nquery\n)\nAPI Reference:\nPydanticToolsParser\n[multiply(a=3, b=12), add(a=11, b=49)]\nNext steps\nâ€‹\nNow you've learned how to bind tool schemas to a chat model and have the model call the tool.\nNext, check out this guide on actually using the tool by invoking the function and passing the results back to the model:\nPass\ntool results back to model\nYou can also check out some more specific uses of tool calling:\nGetting\nstructured outputs\nfrom models\nFew shot prompting\nwith tools\nStream\ntool calls\nPass\nruntime values to tools\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tool_calling_parallel/",
    "How-to guides\nHow to disable parallel tool calling\nHow to disable parallel tool calling\nProvider-specific\nThis API is currently only supported by OpenAI and Anthropic.\nOpenAI tool calling performs tool calling in parallel by default. That means that if we ask a question like \"What is the weather in Tokyo, New York, and Chicago?\" and we have a tool for getting the weather, it will call the tool 3 times in parallel. We can force it to call only a single tool once by using the\nparallel_tool_call\nparameter.\nFirst let's set up our tools and model:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Adds a and b.\"\"\"\nreturn\na\n+\nb\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiplies a and b.\"\"\"\nreturn\na\n*\nb\ntools\n=\n[\nadd\n,\nmultiply\n]\nAPI Reference:\ntool\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nllm\n=\ninit_chat_model\n(\n\"openai:gpt-4.1-mini\"\n)\nNow let's show a quick example of how disabling parallel tool calls work:\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n,\nparallel_tool_calls\n=\nFalse\n)\nllm_with_tools\n.\ninvoke\n(\n\"Please call the first tool two times\"\n)\n.\ntool_calls\n[{'name': 'add',\n'args': {'a': 2, 'b': 2},\n'id': 'call_Hh4JOTCDM85Sm9Pr84VKrWu5'}]\nAs we can see, even though we explicitly told the model to call a tool twice, by disabling parallel tool calls the model was constrained to only calling one.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tool_choice/",
    "How-to guides\nHow to force models to call a tool\nHow to force models to call a tool\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nLangChain Tools\nHow to use a model to call tools\nIn order to force our LLM to select a specific\ntool\n, we can use the\ntool_choice\nparameter to ensure certain behavior. First, let's define our model and tools:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Adds a and b.\"\"\"\nreturn\na\n+\nb\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiplies a and b.\"\"\"\nreturn\na\n*\nb\ntools\n=\n[\nadd\n,\nmultiply\n]\nAPI Reference:\ntool\nFor example, we can force our tool to call the multiply tool by using the following code:\nllm_forced_to_multiply\n=\nllm\n.\nbind_tools\n(\ntools\n,\ntool_choice\n=\n\"multiply\"\n)\nllm_forced_to_multiply\n.\ninvoke\n(\n\"what is 2 + 4\"\n)\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9cViskmLvPnHjXk9tbVla5HA', 'function': {'arguments': '{\"a\":2,\"b\":4}', 'name': 'Multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 103, 'total_tokens': 112}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-095b827e-2bdd-43bb-8897-c843f4504883-0', tool_calls=[{'name': 'Multiply', 'args': {'a': 2, 'b': 4}, 'id': 'call_9cViskmLvPnHjXk9tbVla5HA'}], usage_metadata={'input_tokens': 103, 'output_tokens': 9, 'total_tokens': 112})\nEven if we pass it something that doesn't require multiplcation - it will still call the tool!\nWe can also just force our tool to select at least one of our tools by passing in the \"any\" (or \"required\"\nwhich is OpenAI specific\n) keyword to the\ntool_choice\nparameter.\nllm_forced_to_use_tool\n=\nllm\n.\nbind_tools\n(\ntools\n,\ntool_choice\n=\n\"any\"\n)\nllm_forced_to_use_tool\n.\ninvoke\n(\n\"What day is today?\"\n)\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mCSiJntCwHJUBfaHZVUB2D8W', 'function': {'arguments': '{\"a\":1,\"b\":2}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 94, 'total_tokens': 109}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-28f75260-9900-4bed-8cd3-f1579abb65e5-0', tool_calls=[{'name': 'Add', 'args': {'a': 1, 'b': 2}, 'id': 'call_mCSiJntCwHJUBfaHZVUB2D8W'}], usage_metadata={'input_tokens': 94, 'output_tokens': 15, 'total_tokens': 109})\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tool_configure/",
    "How-to guides\nHow to access the RunnableConfig from a tool\nOn this page\nHow to access the RunnableConfig from a tool\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Tools\nCustom tools\nLangChain Expression Language (LCEL)\nConfiguring runnable behavior\nIf you have a\ntool\nthat calls\nchat models\n,\nretrievers\n, or other\nrunnables\n, you may want to access internal events from those runnables or configure them with additional properties. This guide shows you how to manually pass parameters properly so that you can do this using the\nastream_events()\nmethod.\nTools are\nrunnables\n, and you can treat them the same way as any other runnable at the interface level - you can call\ninvoke()\n,\nbatch()\n, and\nstream()\non them as normal. However, when writing custom tools, you may want to invoke other runnables like chat models or retrievers. In order to properly trace and configure those sub-invocations, you'll need to manually access and pass in the tool's current\nRunnableConfig\nobject. This guide show you some examples of how to do that.\nCompatibility\nThis guide requires\nlangchain-core>=0.2.16\n.\nInferring by parameter type\nâ€‹\nTo access reference the active config object from your custom tool, you'll need to add a parameter to your tool's signature typed as\nRunnableConfig\n. When you invoke your tool, LangChain will inspect your tool's signature, look for a parameter typed as\nRunnableConfig\n, and if it exists, populate that parameter with the correct value.\nNote:\nThe actual name of the parameter doesn't matter, only the typing.\nTo illustrate this, define a custom tool that takes a two parameters - one typed as a string, the other typed as\nRunnableConfig\n:\n%\npip install\n-\nqU langchain_core\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\nasync\ndef\nreverse_tool\n(\ntext\n:\nstr\n,\nspecial_config_param\n:\nRunnableConfig\n)\n-\n>\nstr\n:\n\"\"\"A test tool that combines input text with a configurable parameter.\"\"\"\nreturn\n(\ntext\n+\nspecial_config_param\n[\n\"configurable\"\n]\n[\n\"additional_field\"\n]\n)\n[\n:\n:\n-\n1\n]\nAPI Reference:\nRunnableConfig\n|\ntool\nThen, if we invoke the tool with a\nconfig\ncontaining a\nconfigurable\nfield, we can see that\nadditional_field\nis passed through correctly:\nawait\nreverse_tool\n.\nainvoke\n(\n{\n\"text\"\n:\n\"abc\"\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"additional_field\"\n:\n\"123\"\n}\n}\n)\n'321cba'\nNext steps\nâ€‹\nYou've now seen how to configure and stream events from within a tool. Next, check out the following guides for more on using tools:\nStream events from child runs within a custom tool\nPass\ntool results back to a model\nYou can also check out some more specific uses of tool calling:\nBuilding\ntool-using chains and agents\nGetting\nstructured outputs\nfrom models\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tool_results_pass_to_model/",
    "How-to guides\nHow to pass tool outputs to chat models\nOn this page\nHow to pass tool outputs to chat models\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Tools\nFunction/tool calling\nUsing chat models to call tools\nDefining custom tools\nSome models are capable of\ntool calling\n- generating arguments that conform to a specific user-provided schema. This guide will demonstrate how to use those tool calls to actually call a function and properly pass the results back to the model.\nFirst, let's define our tools and our model:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Adds a and b.\"\"\"\nreturn\na\n+\nb\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiplies a and b.\"\"\"\nreturn\na\n*\nb\ntools\n=\n[\nadd\n,\nmultiply\n]\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n)\nAPI Reference:\ntool\nNow, let's get the model to call a tool. We'll add it to a list of messages that we'll treat as conversation history:\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nquery\n=\n\"What is 3 * 12? Also, what is 11 + 49?\"\nmessages\n=\n[\nHumanMessage\n(\nquery\n)\n]\nai_msg\n=\nllm_with_tools\n.\ninvoke\n(\nmessages\n)\nprint\n(\nai_msg\n.\ntool_calls\n)\nmessages\n.\nappend\n(\nai_msg\n)\nAPI Reference:\nHumanMessage\n[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_GPGPE943GORirhIAYnWv00rK', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_dm8o64ZrY3WFZHAvCh1bEJ6i', 'type': 'tool_call'}]\nNext let's invoke the tool functions using the args the model populated!\nConveniently, if we invoke a LangChain\nTool\nwith a\nToolCall\n, we'll automatically get back a\nToolMessage\nthat can be fed back to the model:\nCompatibility\nThis functionality was added in\nlangchain-core == 0.2.19\n. Please make sure your package is up to date.\nIf you are on earlier versions of\nlangchain-core\n, you will need to extract the\nargs\nfield from the tool and construct a\nToolMessage\nmanually.\nfor\ntool_call\nin\nai_msg\n.\ntool_calls\n:\nselected_tool\n=\n{\n\"add\"\n:\nadd\n,\n\"multiply\"\n:\nmultiply\n}\n[\ntool_call\n[\n\"name\"\n]\n.\nlower\n(\n)\n]\ntool_msg\n=\nselected_tool\n.\ninvoke\n(\ntool_call\n)\nmessages\n.\nappend\n(\ntool_msg\n)\nmessages\n[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'),\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_loT2pliJwJe3p7nkgXYF48A1', 'function': {'arguments': '{\"a\": 3, \"b\": 12}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_bG9tYZCXOeYDZf3W46TceoV4', 'function': {'arguments': '{\"a\": 11, \"b\": 49}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 87, 'total_tokens': 137}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e3db3c46-bf9e-478e-abc1-dc9a264f4afe-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_loT2pliJwJe3p7nkgXYF48A1', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_bG9tYZCXOeYDZf3W46TceoV4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 87, 'output_tokens': 50, 'total_tokens': 137}),\nToolMessage(content='36', name='multiply', tool_call_id='call_loT2pliJwJe3p7nkgXYF48A1'),\nToolMessage(content='60', name='add', tool_call_id='call_bG9tYZCXOeYDZf3W46TceoV4')]\nAnd finally, we'll invoke the model with the tool results. The model will use this information to generate a final answer to our original query:\nllm_with_tools\n.\ninvoke\n(\nmessages\n)\nAIMessage(content='The result of \\\\(3 \\\\times 12\\\\) is 36, and the result of \\\\(11 + 49\\\\) is 60.', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 153, 'total_tokens': 184}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-87d1ef0a-1223-4bb3-9310-7b591789323d-0', usage_metadata={'input_tokens': 153, 'output_tokens': 31, 'total_tokens': 184})\nNote that each\nToolMessage\nmust include a\ntool_call_id\nthat matches an\nid\nin the original tool calls that the model generates. This helps the model match tool responses with tool calls.\nTool calling agents, like those in\nLangGraph\n, use this basic flow to answer queries and solve tasks.\nRelated\nâ€‹\nLangGraph quickstart\nFew shot prompting\nwith tools\nStream\ntool calls\nPass\nruntime values to tools\nGetting\nstructured outputs\nfrom models\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tool_runtime/",
    "How-to guides\nHow to pass run time values to tools\nOn this page\nHow to pass run time values to tools\nðŸ“š\nPrerequisites\nChat models\nLangChain Tools\nHow to create tools\nHow to use a model to call tools\nðŸ“¦\nCompatibility\nThe code in this guide requires\nlangchain-core>=0.2.21\n. Please ensure you have the correct packages installed.\nYou may need to bind values to a\ntool\nthat are only known at runtime. For example, the tool logic may require using the ID of the user who made the request.\nMost of the time, such values should not be controlled by the LLM. In fact, allowing the LLM to control the user ID may lead to a security risk.\nInstead, the LLM should only control the parameters of the tool that are meant to be controlled by the LLM, while other parameters (such as user ID) should be fixed by the application logic.\nThis how-to guide shows you how to prevent the model from generating certain tool arguments and injecting them in directly at runtime.\nUsing with LangGraph\nIf you're using LangGraph, please refer to\nthis how-to guide\nwhich shows how to create an agent that keeps track of a given user's favorite pets.\nWe can bind them to chat models as follows:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nHiding arguments from the model\nâ€‹\nWe can use the InjectedToolArg annotation to mark certain parameters of our Tool, like\nuser_id\nas being injected at runtime, meaning they shouldn't be generated by the model\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core\n.\ntools\nimport\nInjectedToolArg\n,\ntool\nfrom\ntyping_extensions\nimport\nAnnotated\nuser_to_pets\n=\n{\n}\n@tool\n(\nparse_docstring\n=\nTrue\n)\ndef\nupdate_favorite_pets\n(\npets\n:\nList\n[\nstr\n]\n,\nuser_id\n:\nAnnotated\n[\nstr\n,\nInjectedToolArg\n]\n)\n-\n>\nNone\n:\n\"\"\"Add the list of favorite pets.\nArgs:\npets: List of favorite pets to set.\nuser_id: User's ID.\n\"\"\"\nuser_to_pets\n[\nuser_id\n]\n=\npets\n@tool\n(\nparse_docstring\n=\nTrue\n)\ndef\ndelete_favorite_pets\n(\nuser_id\n:\nAnnotated\n[\nstr\n,\nInjectedToolArg\n]\n)\n-\n>\nNone\n:\n\"\"\"Delete the list of favorite pets.\nArgs:\nuser_id: User's ID.\n\"\"\"\nif\nuser_id\nin\nuser_to_pets\n:\ndel\nuser_to_pets\n[\nuser_id\n]\n@tool\n(\nparse_docstring\n=\nTrue\n)\ndef\nlist_favorite_pets\n(\nuser_id\n:\nAnnotated\n[\nstr\n,\nInjectedToolArg\n]\n)\n-\n>\nNone\n:\n\"\"\"List favorite pets if any.\nArgs:\nuser_id: User's ID.\n\"\"\"\nreturn\nuser_to_pets\n.\nget\n(\nuser_id\n,\n[\n]\n)\nAPI Reference:\nInjectedToolArg\n|\ntool\nIf we look at the input schemas for these tools, we'll see that user_id is still listed:\nupdate_favorite_pets\n.\nget_input_schema\n(\n)\n.\nmodel_json_schema\n(\n)\n{'description': 'Add the list of favorite pets.',\n'properties': {'pets': {'description': 'List of favorite pets to set.',\n'items': {'type': 'string'},\n'title': 'Pets',\n'type': 'array'},\n'user_id': {'description': \"User's ID.\",\n'title': 'User Id',\n'type': 'string'}},\n'required': ['pets', 'user_id'],\n'title': 'update_favorite_petsSchema',\n'type': 'object'}\nBut if we look at the tool call schema, which is what is passed to the model for tool-calling, user_id has been removed:\nupdate_favorite_pets\n.\ntool_call_schema\n.\nmodel_json_schema\n(\n)\n{'description': 'Add the list of favorite pets.',\n'properties': {'pets': {'description': 'List of favorite pets to set.',\n'items': {'type': 'string'},\n'title': 'Pets',\n'type': 'array'}},\n'required': ['pets'],\n'title': 'update_favorite_pets',\n'type': 'object'}\nSo when we invoke our tool, we need to pass in user_id:\nuser_id\n=\n\"123\"\nupdate_favorite_pets\n.\ninvoke\n(\n{\n\"pets\"\n:\n[\n\"lizard\"\n,\n\"dog\"\n]\n,\n\"user_id\"\n:\nuser_id\n}\n)\nprint\n(\nuser_to_pets\n)\nprint\n(\nlist_favorite_pets\n.\ninvoke\n(\n{\n\"user_id\"\n:\nuser_id\n}\n)\n)\n{'123': ['lizard', 'dog']}\n['lizard', 'dog']\nBut when the model calls the tool, no user_id argument will be generated:\ntools\n=\n[\nupdate_favorite_pets\n,\ndelete_favorite_pets\n,\nlist_favorite_pets\n,\n]\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n)\nai_msg\n=\nllm_with_tools\n.\ninvoke\n(\n\"my favorite animals are cats and parrots\"\n)\nai_msg\n.\ntool_calls\n[{'name': 'update_favorite_pets',\n'args': {'pets': ['cats', 'parrots']},\n'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n'type': 'tool_call'}]\nInjecting arguments at runtime\nâ€‹\nIf we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:\nfrom\ncopy\nimport\ndeepcopy\nfrom\nlangchain_core\n.\nrunnables\nimport\nchain\n@chain\ndef\ninject_user_id\n(\nai_msg\n)\n:\ntool_calls\n=\n[\n]\nfor\ntool_call\nin\nai_msg\n.\ntool_calls\n:\ntool_call_copy\n=\ndeepcopy\n(\ntool_call\n)\ntool_call_copy\n[\n\"args\"\n]\n[\n\"user_id\"\n]\n=\nuser_id\ntool_calls\n.\nappend\n(\ntool_call_copy\n)\nreturn\ntool_calls\ninject_user_id\n.\ninvoke\n(\nai_msg\n)\nAPI Reference:\nchain\n[{'name': 'update_favorite_pets',\n'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},\n'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n'type': 'tool_call'}]\nAnd now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:\ntool_map\n=\n{\ntool\n.\nname\n:\ntool\nfor\ntool\nin\ntools\n}\n@chain\ndef\ntool_router\n(\ntool_call\n)\n:\nreturn\ntool_map\n[\ntool_call\n[\n\"name\"\n]\n]\nchain\n=\nllm_with_tools\n|\ninject_user_id\n|\ntool_router\n.\nmap\n(\n)\nchain\n.\ninvoke\n(\n\"my favorite animals are cats and parrots\"\n)\n[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]\nLooking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:\nuser_to_pets\n{'123': ['cats', 'parrots']}\nOther ways of annotating args\nâ€‹\nHere are a few other ways of annotating our tool args:\nfrom\nlangchain_core\n.\ntools\nimport\nBaseTool\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nUpdateFavoritePetsSchema\n(\nBaseModel\n)\n:\n\"\"\"Update list of favorite pets\"\"\"\npets\n:\nList\n[\nstr\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"List of favorite pets to set.\"\n)\nuser_id\n:\nAnnotated\n[\nstr\n,\nInjectedToolArg\n]\n=\nField\n(\n.\n.\n.\n,\ndescription\n=\n\"User's ID.\"\n)\n@tool\n(\nargs_schema\n=\nUpdateFavoritePetsSchema\n)\ndef\nupdate_favorite_pets\n(\npets\n,\nuser_id\n)\n:\nuser_to_pets\n[\nuser_id\n]\n=\npets\nupdate_favorite_pets\n.\nget_input_schema\n(\n)\n.\nmodel_json_schema\n(\n)\nAPI Reference:\nBaseTool\n{'description': 'Update list of favorite pets',\n'properties': {'pets': {'description': 'List of favorite pets to set.',\n'items': {'type': 'string'},\n'title': 'Pets',\n'type': 'array'},\n'user_id': {'description': \"User's ID.\",\n'title': 'User Id',\n'type': 'string'}},\n'required': ['pets', 'user_id'],\n'title': 'UpdateFavoritePetsSchema',\n'type': 'object'}\nupdate_favorite_pets\n.\ntool_call_schema\n.\nmodel_json_schema\n(\n)\n{'description': 'Update list of favorite pets',\n'properties': {'pets': {'description': 'List of favorite pets to set.',\n'items': {'type': 'string'},\n'title': 'Pets',\n'type': 'array'}},\n'required': ['pets'],\n'title': 'update_favorite_pets',\n'type': 'object'}\nfrom\ntyping\nimport\nOptional\n,\nType\nclass\nUpdateFavoritePets\n(\nBaseTool\n)\n:\nname\n:\nstr\n=\n\"update_favorite_pets\"\ndescription\n:\nstr\n=\n\"Update list of favorite pets\"\nargs_schema\n:\nOptional\n[\nType\n[\nBaseModel\n]\n]\n=\nUpdateFavoritePetsSchema\ndef\n_run\n(\nself\n,\npets\n,\nuser_id\n)\n:\nuser_to_pets\n[\nuser_id\n]\n=\npets\nUpdateFavoritePets\n(\n)\n.\nget_input_schema\n(\n)\n.\nmodel_json_schema\n(\n)\n{'description': 'Update list of favorite pets',\n'properties': {'pets': {'description': 'List of favorite pets to set.',\n'items': {'type': 'string'},\n'title': 'Pets',\n'type': 'array'},\n'user_id': {'description': \"User's ID.\",\n'title': 'User Id',\n'type': 'string'}},\n'required': ['pets', 'user_id'],\n'title': 'UpdateFavoritePetsSchema',\n'type': 'object'}\nUpdateFavoritePets\n(\n)\n.\ntool_call_schema\n.\nmodel_json_schema\n(\n)\n{'description': 'Update list of favorite pets',\n'properties': {'pets': {'description': 'List of favorite pets to set.',\n'items': {'type': 'string'},\n'title': 'Pets',\n'type': 'array'}},\n'required': ['pets'],\n'title': 'update_favorite_pets',\n'type': 'object'}\nclass\nUpdateFavoritePets2\n(\nBaseTool\n)\n:\nname\n:\nstr\n=\n\"update_favorite_pets\"\ndescription\n:\nstr\n=\n\"Update list of favorite pets\"\ndef\n_run\n(\nself\n,\npets\n:\nList\n[\nstr\n]\n,\nuser_id\n:\nAnnotated\n[\nstr\n,\nInjectedToolArg\n]\n)\n-\n>\nNone\n:\nuser_to_pets\n[\nuser_id\n]\n=\npets\nUpdateFavoritePets2\n(\n)\n.\nget_input_schema\n(\n)\n.\nmodel_json_schema\n(\n)\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n'properties': {'pets': {'items': {'type': 'string'},\n'title': 'Pets',\n'type': 'array'},\n'user_id': {'title': 'User Id', 'type': 'string'}},\n'required': ['pets', 'user_id'],\n'title': 'update_favorite_petsSchema',\n'type': 'object'}\nUpdateFavoritePets2\n(\n)\n.\ntool_call_schema\n.\nmodel_json_schema\n(\n)\n{'description': 'Update list of favorite pets',\n'properties': {'pets': {'items': {'type': 'string'},\n'title': 'Pets',\n'type': 'array'}},\n'required': ['pets'],\n'title': 'update_favorite_pets',\n'type': 'object'}\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tool_stream_events/",
    "How-to guides\nHow to stream events from a tool\nOn this page\nHow to stream events from a tool\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nLangChain Tools\nCustom tools\nUsing stream events\nAccessing RunnableConfig within a custom tool\nIf you have\ntools\nthat call\nchat models\n,\nretrievers\n, or other\nrunnables\n, you may want to access\ninternal events\nfrom those runnables or configure them with additional properties. This guide shows you how to manually pass parameters properly so that you can do this using the\nastream_events()\nmethod.\nCompatibility\nLangChain cannot automatically propagate configuration, including callbacks necessary for\nastream_events()\n, to child runnables if you are running\nasync\ncode in\npython<=3.10\n. This is a common reason why you may fail to see events being emitted from custom runnables or tools.\nIf you are running\npython<=3.10\n, you will need to manually propagate the\nRunnableConfig\nobject to the child runnable in async environments. For an example of how to manually propagate the config, see the implementation of the\nbar\nRunnableLambda below.\nIf you are running\npython>=3.11\n, the\nRunnableConfig\nwill automatically propagate to child runnables in async environment. However, it is still a good idea to propagate the\nRunnableConfig\nmanually if your code may run in older Python versions.\nThis guide also requires\nlangchain-core>=0.2.16\n.\nSay you have a custom tool that calls a chain that condenses its input by prompting a chat model to return only 10 words, then reversing the output. First, define it in a naive way:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\nasync\ndef\nspecial_summarization_tool\n(\nlong_text\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"A tool that summarizes input text using advanced techniques.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"You are an expert writer. Summarize the following text in 10 words or less:\\n\\n{long_text}\"\n)\ndef\nreverse\n(\nx\n:\nstr\n)\n:\nreturn\nx\n[\n:\n:\n-\n1\n]\nchain\n=\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n|\nreverse\nsummary\n=\nawait\nchain\n.\nainvoke\n(\n{\n\"long_text\"\n:\nlong_text\n}\n)\nreturn\nsummary\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\ntool\nInvoking the tool directly works just fine:\nLONG_TEXT\n=\n\"\"\"\nNARRATOR:\n(Black screen with text; The sound of buzzing bees can be heard)\nAccording to all known laws of aviation, there is no way a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway because bees don't care what humans think is impossible.\nBARRY BENSON:\n(Barry is picking out a shirt)\nYellow, black. Yellow, black. Yellow, black. Yellow, black. Ooh, black and yellow! Let's shake it up a little.\nJANET BENSON:\nBarry! Breakfast is ready!\nBARRY:\nComing! Hang on a second.\n\"\"\"\nawait\nspecial_summarization_tool\n.\nainvoke\n(\n{\n\"long_text\"\n:\nLONG_TEXT\n}\n)\n'.yad noitaudarg rof tiftuo sesoohc yrraB ;scisyhp seifed eeB'\nBut if you wanted to access the raw output from the chat model rather than the full tool, you might try to use the\nastream_events()\nmethod and look for an\non_chat_model_end\nevent. Here's what happens:\nstream\n=\nspecial_summarization_tool\n.\nastream_events\n(\n{\n\"long_text\"\n:\nLONG_TEXT\n}\n)\nasync\nfor\nevent\nin\nstream\n:\nif\nevent\n[\n\"event\"\n]\n==\n\"on_chat_model_end\"\n:\n# Never triggers in python<=3.10!\nprint\n(\nevent\n)\nYou'll notice (unless you're running through this guide in\npython>=3.11\n) that there are no chat model events emitted from the child run!\nThis is because the example above does not pass the tool's config object into the internal chain. To fix this, redefine your tool to take a special parameter typed as\nRunnableConfig\n(see\nthis guide\nfor more details). You'll also need to pass that parameter through into the internal chain when executing it:\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\n@tool\nasync\ndef\nspecial_summarization_tool_with_config\n(\nlong_text\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nstr\n:\n\"\"\"A tool that summarizes input text using advanced techniques.\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"You are an expert writer. Summarize the following text in 10 words or less:\\n\\n{long_text}\"\n)\ndef\nreverse\n(\nx\n:\nstr\n)\n:\nreturn\nx\n[\n:\n:\n-\n1\n]\nchain\n=\nprompt\n|\nmodel\n|\nStrOutputParser\n(\n)\n|\nreverse\n# Pass the \"config\" object as an argument to any executed runnables\nsummary\n=\nawait\nchain\n.\nainvoke\n(\n{\n\"long_text\"\n:\nlong_text\n}\n,\nconfig\n=\nconfig\n)\nreturn\nsummary\nAPI Reference:\nRunnableConfig\nAnd now try the same\nastream_events()\ncall as before with your new tool:\nstream\n=\nspecial_summarization_tool_with_config\n.\nastream_events\n(\n{\n\"long_text\"\n:\nLONG_TEXT\n}\n)\nasync\nfor\nevent\nin\nstream\n:\nif\nevent\n[\n\"event\"\n]\n==\n\"on_chat_model_end\"\n:\nprint\n(\nevent\n)\n{'event': 'on_chat_model_end', 'data': {'output': AIMessage(content='Bee defies physics; Barry chooses outfit for graduation day.', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-337ac14e-8da8-4c6d-a69f-1573f93b651e', usage_metadata={'input_tokens': 182, 'output_tokens': 19, 'total_tokens': 201, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}), 'input': {'messages': [[HumanMessage(content=\"You are an expert writer. Summarize the following text in 10 words or less:\\n\\n\\nNARRATOR:\\n(Black screen with text; The sound of buzzing bees can be heard)\\nAccording to all known laws of aviation, there is no way a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway because bees don't care what humans think is impossible.\\nBARRY BENSON:\\n(Barry is picking out a shirt)\\nYellow, black. Yellow, black. Yellow, black. Yellow, black. Ooh, black and yellow! Let's shake it up a little.\\nJANET BENSON:\\nBarry! Breakfast is ready!\\nBARRY:\\nComing! Hang on a second.\\n\", additional_kwargs={}, response_metadata={})]]}}, 'run_id': '337ac14e-8da8-4c6d-a69f-1573f93b651e', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['225beaa6-af73-4c91-b2d3-1afbbb88d53e']}\nAwesome! This time there's an event emitted.\nFor streaming,\nastream_events()\nautomatically calls internal runnables in a chain with streaming enabled if possible, so if you wanted to a stream of tokens as they are generated from the chat model, you could simply filter to look for\non_chat_model_stream\nevents with no other changes:\nstream\n=\nspecial_summarization_tool_with_config\n.\nastream_events\n(\n{\n\"long_text\"\n:\nLONG_TEXT\n}\n)\nasync\nfor\nevent\nin\nstream\n:\nif\nevent\n[\n\"event\"\n]\n==\n\"on_chat_model_stream\"\n:\nprint\n(\nevent\n)\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', usage_metadata={'input_tokens': 182, 'output_tokens': 2, 'total_tokens': 184, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='Bee', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' defies physics;', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' Barry chooses outfit for', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' graduation day.', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', usage_metadata={'input_tokens': 0, 'output_tokens': 17, 'total_tokens': 17, 'input_token_details': {}})}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\nNext steps\nâ€‹\nYou've now seen how to stream events from within a tool. Next, check out the following guides for more on using tools:\nPass\nruntime values to tools\nPass\ntool results back to a model\nDispatch custom callback events\nYou can also check out some more specific uses of tool calling:\nBuilding\ntool-using chains and agents\nGetting\nstructured outputs\nfrom models\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tool_streaming/",
    "How-to guides\nHow to stream tool calls\nHow to stream tool calls\nWhen\ntools\nare called in a streaming context,\nmessage chunks\nwill be populated with\ntool call chunk\nobjects in a list via the\n.tool_call_chunks\nattribute. A\nToolCallChunk\nincludes\noptional string fields for the tool\nname\n,\nargs\n, and\nid\n, and includes an optional\ninteger field\nindex\nthat can be used to join chunks together. Fields are optional\nbecause portions of a tool call may be streamed across different chunks (e.g., a chunk\nthat includes a substring of the arguments may have null values for the tool name and id).\nBecause message chunks inherit from their parent message class, an\nAIMessageChunk\nwith tool call chunks will also include\n.tool_calls\nand\n.invalid_tool_calls\nfields.\nThese fields are parsed best-effort from the message's tool call chunks.\nNote that not all providers currently support streaming for tool calls. Before we start let's define our tools and our model.\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Adds a and b.\"\"\"\nreturn\na\n+\nb\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiplies a and b.\"\"\"\nreturn\na\n*\nb\ntools\n=\n[\nadd\n,\nmultiply\n]\nAPI Reference:\ntool\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nfrom\nlangchain_openai\nimport\nChatOpenAI\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n)\nNow let's define our query and stream our output:\nquery\n=\n\"What is 3 * 12? Also, what is 11 + 49?\"\nasync\nfor\nchunk\nin\nllm_with_tools\n.\nastream\n(\nquery\n)\n:\nprint\n(\nchunk\n.\ntool_call_chunks\n)\n[]\n[{'name': 'Multiply', 'args': '', 'id': 'call_3aQwTP9CYlFxwOvQZPHDu6wL', 'index': 0}]\n[{'name': None, 'args': '{\"a\"', 'id': None, 'index': 0}]\n[{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}]\n[{'name': None, 'args': '\"b\": 1', 'id': None, 'index': 0}]\n[{'name': None, 'args': '2}', 'id': None, 'index': 0}]\n[{'name': 'Add', 'args': '', 'id': 'call_SQUoSsJz2p9Kx2x73GOgN1ja', 'index': 1}]\n[{'name': None, 'args': '{\"a\"', 'id': None, 'index': 1}]\n[{'name': None, 'args': ': 11,', 'id': None, 'index': 1}]\n[{'name': None, 'args': ' \"b\": ', 'id': None, 'index': 1}]\n[{'name': None, 'args': '49}', 'id': None, 'index': 1}]\n[]\nNote that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various\ntool output parsers\nsupport streaming.\nFor example, below we accumulate tool call chunks:\nfirst\n=\nTrue\nasync\nfor\nchunk\nin\nllm_with_tools\n.\nastream\n(\nquery\n)\n:\nif\nfirst\n:\ngathered\n=\nchunk\nfirst\n=\nFalse\nelse\n:\ngathered\n=\ngathered\n+\nchunk\nprint\n(\ngathered\n.\ntool_call_chunks\n)\n[]\n[{'name': 'Multiply', 'args': '', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\"', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, ', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 1', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\"', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11,', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": ', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]\n[{'name': 'Multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]\nprint\n(\ntype\n(\ngathered\n.\ntool_call_chunks\n[\n0\n]\n[\n\"args\"\n]\n)\n)\n<class 'str'>\nAnd below we accumulate tool calls to demonstrate partial parsing:\nfirst\n=\nTrue\nasync\nfor\nchunk\nin\nllm_with_tools\n.\nastream\n(\nquery\n)\n:\nif\nfirst\n:\ngathered\n=\nchunk\nfirst\n=\nFalse\nelse\n:\ngathered\n=\ngathered\n+\nchunk\nprint\n(\ngathered\n.\ntool_calls\n)\n[]\n[]\n[{'name': 'Multiply', 'args': {}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]\n[{'name': 'Multiply', 'args': {'a': 3}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]\n[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]\nprint\n(\ntype\n(\ngathered\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\n)\n<class 'dict'>\nNote the key difference: accumulating\ntool_call_chunks\ncaptures the raw tool arguments as an unparsed string as they are streamed. In contrast,\naccumulating\ntool_calls\ndemonstrates partial parsing by progressively converting the streamed argument string into a valid, usable dictionary at each step of the process.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tools_as_openai_functions/",
    "How-to guides\nHow to convert tools to OpenAI Functions\nHow to convert tools to OpenAI Functions\nThis notebook goes over how to use LangChain\ntools\nas OpenAI functions.\n%\npip install\n-\nqU langchain\n-\ncommunity langchain\n-\nopenai\nfrom\nlangchain_community\n.\ntools\nimport\nMoveFileTool\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nfrom\nlangchain_core\n.\nutils\n.\nfunction_calling\nimport\nconvert_to_openai_function\nfrom\nlangchain_openai\nimport\nChatOpenAI\nAPI Reference:\nHumanMessage\n|\nconvert_to_openai_function\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-3.5-turbo\"\n)\ntools\n=\n[\nMoveFileTool\n(\n)\n]\nfunctions\n=\n[\nconvert_to_openai_function\n(\nt\n)\nfor\nt\nin\ntools\n]\nfunctions\n[\n0\n]\n{'name': 'move_file',\n'description': 'Move or rename a file from one location to another',\n'parameters': {'type': 'object',\n'properties': {'source_path': {'description': 'Path of the file to move',\n'type': 'string'},\n'destination_path': {'description': 'New path for the moved file',\n'type': 'string'}},\n'required': ['source_path', 'destination_path']}}\nmessage\n=\nmodel\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"move file foo to bar\"\n)\n]\n,\nfunctions\n=\nfunctions\n)\nmessage\nAIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"source_path\": \"foo\",\\n  \"destination_path\": \"bar\"\\n}', 'name': 'move_file'}})\nmessage\n.\nadditional_kwargs\n[\n\"function_call\"\n]\n{'name': 'move_file',\n'arguments': '{\\n  \"source_path\": \"foo\",\\n  \"destination_path\": \"bar\"\\n}'}\nWith OpenAI chat models we can also automatically bind and convert function-like objects with\nbind_functions\nmodel_with_functions\n=\nmodel\n.\nbind_functions\n(\ntools\n)\nmodel_with_functions\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"move file foo to bar\"\n)\n]\n)\nAIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"source_path\": \"foo\",\\n  \"destination_path\": \"bar\"\\n}', 'name': 'move_file'}})\nOr we can use the update OpenAI API that uses\ntools\nand\ntool_choice\ninstead of\nfunctions\nand\nfunction_call\nby using\nChatOpenAI.bind_tools\n:\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\ntools\n)\nmodel_with_tools\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"move file foo to bar\"\n)\n]\n)\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_btkY3xV71cEVAOHnNa5qwo44', 'function': {'arguments': '{\\n  \"source_path\": \"foo\",\\n  \"destination_path\": \"bar\"\\n}', 'name': 'move_file'}, 'type': 'function'}]})\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tools_error/",
    "How-to guides\nHow to handle tool errors\nOn this page\nHow to handle tool errors\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat models\nLangChain Tools\nHow to use a model to call tools\nCalling tools\nwith an LLM is generally more reliable than pure prompting, but it isn't perfect. The model may try to call a tool that doesn't exist or fail to return arguments that match the requested schema. Strategies like keeping schemas simple, reducing the number of tools you pass at once, and having good names and descriptions can help mitigate this risk, but aren't foolproof.\nThis guide covers some ways to build error handling into your chains to mitigate these failure modes.\nSetup\nâ€‹\nWe'll need to install the following packages:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\ncore langchain\n-\nopenai\nIf you'd like to trace your runs in\nLangSmith\nuncomment and set the following environment variables:\nimport\ngetpass\nimport\nos\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nChain\nâ€‹\nSuppose we have the following (dummy) tool and tool-calling chain. We'll make our tool intentionally convoluted to try and trip up the model.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\n# Define tool\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\ncomplex_tool\n(\nint_arg\n:\nint\n,\nfloat_arg\n:\nfloat\n,\ndict_arg\n:\ndict\n)\n-\n>\nint\n:\n\"\"\"Do something complex with a complex tool.\"\"\"\nreturn\nint_arg\n*\nfloat_arg\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\n[\ncomplex_tool\n]\n,\n)\n# Define chain\nchain\n=\nllm_with_tools\n|\n(\nlambda\nmsg\n:\nmsg\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\n|\ncomplex_tool\nAPI Reference:\ntool\nWe can see that when we try to invoke this chain with even a fairly explicit input, the model fails to correctly call the tool (it forgets the\ndict_arg\nargument).\nchain\n.\ninvoke\n(\n\"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\"\n)\n---------------------------------------------------------------------------\n``````output\nValidationError                           Traceback (most recent call last)\n``````output\nCell In[5], line 1\n----> 1 chain.invoke(\n2     \"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\"\n3 )\n``````output\nFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2998, in RunnableSequence.invoke(self, input, config, **kwargs)\n2996             input = context.run(step.invoke, input, config, **kwargs)\n2997         else:\n-> 2998             input = context.run(step.invoke, input, config)\n2999 # finish the root run\n3000 except BaseException as e:\n``````output\nFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:456, in BaseTool.invoke(self, input, config, **kwargs)\n449 def invoke(\n450     self,\n451     input: Union[str, Dict, ToolCall],\n452     config: Optional[RunnableConfig] = None,\n453     **kwargs: Any,\n454 ) -> Any:\n455     tool_input, kwargs = _prep_run_args(input, config, **kwargs)\n--> 456     return self.run(tool_input, **kwargs)\n``````output\nFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:659, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\n657 if error_to_raise:\n658     run_manager.on_tool_error(error_to_raise)\n--> 659     raise error_to_raise\n660 output = _format_output(content, artifact, tool_call_id, self.name, status)\n661 run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)\n``````output\nFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:622, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\n620 context = copy_context()\n621 context.run(_set_config_context, child_config)\n--> 622 tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input)\n623 if signature(self._run).parameters.get(\"run_manager\"):\n624     tool_kwargs[\"run_manager\"] = run_manager\n``````output\nFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:545, in BaseTool._to_args_and_kwargs(self, tool_input)\n544 def _to_args_and_kwargs(self, tool_input: Union[str, Dict]) -> Tuple[Tuple, Dict]:\n--> 545     tool_input = self._parse_input(tool_input)\n546     # For backwards compatibility, if run_input is a string,\n547     # pass as a positional argument.\n548     if isinstance(tool_input, str):\n``````output\nFile ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:487, in BaseTool._parse_input(self, tool_input)\n485 if input_args is not None:\n486     if issubclass(input_args, BaseModel):\n--> 487         result = input_args.model_validate(tool_input)\n488         result_dict = result.model_dump()\n489     elif issubclass(input_args, BaseModelV1):\n``````output\nFile ~/langchain/.venv/lib/python3.11/site-packages/pydantic/main.py:568, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)\n566 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n567 __tracebackhide__ = True\n--> 568 return cls.__pydantic_validator__.validate_python(\n569     obj, strict=strict, from_attributes=from_attributes, context=context\n570 )\n``````output\nValidationError: 1 validation error for complex_toolSchema\ndict_arg\nField required [type=missing, input_value={'int_arg': 5, 'float_arg': 2.1}, input_type=dict]\nFor further information visit https://errors.pydantic.dev/2.8/v/missing\nTry/except tool call\nâ€‹\nThe simplest way to more gracefully handle errors is to try/except the tool-calling step and return a helpful message on errors:\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnable\n,\nRunnableConfig\ndef\ntry_except_tool\n(\ntool_args\n:\ndict\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nRunnable\n:\ntry\n:\ncomplex_tool\n.\ninvoke\n(\ntool_args\n,\nconfig\n=\nconfig\n)\nexcept\nException\nas\ne\n:\nreturn\nf\"Calling tool with arguments:\\n\\n\n{\ntool_args\n}\n\\n\\nraised the following error:\\n\\n\n{\ntype\n(\ne\n)\n}\n:\n{\ne\n}\n\"\nchain\n=\nllm_with_tools\n|\n(\nlambda\nmsg\n:\nmsg\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\n|\ntry_except_tool\nprint\n(\nchain\n.\ninvoke\n(\n\"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\"\n)\n)\nAPI Reference:\nRunnable\n|\nRunnableConfig\nCalling tool with arguments:\n{'int_arg': 5, 'float_arg': 2.1}\nraised the following error:\n<class 'pydantic_core._pydantic_core.ValidationError'>: 1 validation error for complex_toolSchema\ndict_arg\nField required [type=missing, input_value={'int_arg': 5, 'float_arg': 2.1}, input_type=dict]\nFor further information visit https://errors.pydantic.dev/2.8/v/missing\nFallbacks\nâ€‹\nWe can also try to fallback to a better model in the event of a tool invocation error. In this case we'll fall back to an identical chain that uses\ngpt-4-1106-preview\ninstead of\ngpt-3.5-turbo\n.\nchain\n=\nllm_with_tools\n|\n(\nlambda\nmsg\n:\nmsg\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\n|\ncomplex_tool\nbetter_model\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4-1106-preview\"\n,\ntemperature\n=\n0\n)\n.\nbind_tools\n(\n[\ncomplex_tool\n]\n,\ntool_choice\n=\n\"complex_tool\"\n)\nbetter_chain\n=\nbetter_model\n|\n(\nlambda\nmsg\n:\nmsg\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\n|\ncomplex_tool\nchain_with_fallback\n=\nchain\n.\nwith_fallbacks\n(\n[\nbetter_chain\n]\n)\nchain_with_fallback\n.\ninvoke\n(\n\"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\"\n)\n10.5\nLooking at the\nLangSmith trace\nfor this chain run, we can see that the first chain call fails as expected and it's the fallback that succeeds.\nRetry with exception\nâ€‹\nTo take things one step further, we can try to automatically re-run the chain with the exception passed in, so that the model may be able to correct its behavior:\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nHumanMessage\n,\nToolCall\n,\nToolMessage\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nclass\nCustomToolException\n(\nException\n)\n:\n\"\"\"Custom LangChain tool exception.\"\"\"\ndef\n__init__\n(\nself\n,\ntool_call\n:\nToolCall\n,\nexception\n:\nException\n)\n-\n>\nNone\n:\nsuper\n(\n)\n.\n__init__\n(\n)\nself\n.\ntool_call\n=\ntool_call\nself\n.\nexception\n=\nexception\ndef\ntool_custom_exception\n(\nmsg\n:\nAIMessage\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nRunnable\n:\ntry\n:\nreturn\ncomplex_tool\n.\ninvoke\n(\nmsg\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n,\nconfig\n=\nconfig\n)\nexcept\nException\nas\ne\n:\nraise\nCustomToolException\n(\nmsg\n.\ntool_calls\n[\n0\n]\n,\ne\n)\ndef\nexception_to_messages\n(\ninputs\n:\ndict\n)\n-\n>\ndict\n:\nexception\n=\ninputs\n.\npop\n(\n\"exception\"\n)\n# Add historical messages to the original input, so the model knows that it made a mistake with the last tool call.\nmessages\n=\n[\nAIMessage\n(\ncontent\n=\n\"\"\n,\ntool_calls\n=\n[\nexception\n.\ntool_call\n]\n)\n,\nToolMessage\n(\ntool_call_id\n=\nexception\n.\ntool_call\n[\n\"id\"\n]\n,\ncontent\n=\nstr\n(\nexception\n.\nexception\n)\n)\n,\nHumanMessage\n(\ncontent\n=\n\"The last tool call raised an exception. Try calling the tool again with corrected arguments. Do not repeat mistakes.\"\n)\n,\n]\ninputs\n[\n\"last_output\"\n]\n=\nmessages\nreturn\ninputs\n# We add a last_output MessagesPlaceholder to our prompt which if not passed in doesn't\n# affect the prompt at all, but gives us the option to insert an arbitrary list of Messages\n# into the prompt if needed. We'll use this on retries to insert the error message.\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n(\n\"placeholder\"\n,\n\"{last_output}\"\n)\n]\n)\nchain\n=\nprompt\n|\nllm_with_tools\n|\ntool_custom_exception\n# If the initial chain call fails, we rerun it withe the exception passed in as a message.\nself_correcting_chain\n=\nchain\n.\nwith_fallbacks\n(\n[\nexception_to_messages\n|\nchain\n]\n,\nexception_key\n=\n\"exception\"\n)\nAPI Reference:\nAIMessage\n|\nHumanMessage\n|\nToolCall\n|\nToolMessage\n|\nChatPromptTemplate\nself_correcting_chain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg\"\n}\n)\n10.5\nAnd our chain succeeds! Looking at the\nLangSmith trace\n, we can see that indeed our initial chain still fails, and it's only on retrying that the chain succeeds.\nNext steps\nâ€‹\nNow you've seen some strategies how to handle tool calling errors. Next, you can learn more about how to use tools:\nFew shot prompting\nwith tools\nStream\ntool calls\nPass\nruntime values to tools\nYou can also check out some more specific uses of tool calling:\nGetting\nstructured outputs\nfrom models\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tools_few_shot/",
    "How-to guides\nHow to use few-shot prompting with tool calling\nHow to use few-shot prompting with tool calling\nFor more complex tool use it's very useful to add\nfew-shot examples\nto the prompt. We can do this by adding\nAIMessage\ns with\nToolCall\ns and corresponding\nToolMessage\ns to our prompt.\nFirst let's define our tools and model.\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Adds a and b.\"\"\"\nreturn\na\n+\nb\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiplies a and b.\"\"\"\nreturn\na\n*\nb\ntools\n=\n[\nadd\n,\nmultiply\n]\nAPI Reference:\ntool\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nfrom\nlangchain_openai\nimport\nChatOpenAI\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n)\nLet's run our model where we can notice that even with some special instructions our model can get tripped up by order of operations.\nllm_with_tools\n.\ninvoke\n(\n\"Whats 119 times 8 minus 20. Don't do any math yourself, only use tools for math. Respect order of operations\"\n)\n.\ntool_calls\n[{'name': 'Multiply',\n'args': {'a': 119, 'b': 8},\n'id': 'call_T88XN6ECucTgbXXkyDeC2CQj'},\n{'name': 'Add',\n'args': {'a': 952, 'b': -20},\n'id': 'call_licdlmGsRqzup8rhqJSb1yZ4'}]\nThe model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.\nBy adding a prompt with some examples we can correct this behavior:\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nHumanMessage\n,\nToolMessage\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nexamples\n=\n[\nHumanMessage\n(\n\"What's the product of 317253 and 128472 plus four\"\n,\nname\n=\n\"example_user\"\n)\n,\nAIMessage\n(\n\"\"\n,\nname\n=\n\"example_assistant\"\n,\ntool_calls\n=\n[\n{\n\"name\"\n:\n\"Multiply\"\n,\n\"args\"\n:\n{\n\"x\"\n:\n317253\n,\n\"y\"\n:\n128472\n}\n,\n\"id\"\n:\n\"1\"\n}\n]\n,\n)\n,\nToolMessage\n(\n\"16505054784\"\n,\ntool_call_id\n=\n\"1\"\n)\n,\nAIMessage\n(\n\"\"\n,\nname\n=\n\"example_assistant\"\n,\ntool_calls\n=\n[\n{\n\"name\"\n:\n\"Add\"\n,\n\"args\"\n:\n{\n\"x\"\n:\n16505054784\n,\n\"y\"\n:\n4\n}\n,\n\"id\"\n:\n\"2\"\n}\n]\n,\n)\n,\nToolMessage\n(\n\"16505054788\"\n,\ntool_call_id\n=\n\"2\"\n)\n,\nAIMessage\n(\n\"The product of 317253 and 128472 plus four is 16505054788\"\n,\nname\n=\n\"example_assistant\"\n,\n)\n,\n]\nsystem\n=\n\"\"\"You are bad at math but are an expert at using a calculator.\nUse past tool usage as an example of how to correctly use the tools.\"\"\"\nfew_shot_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem\n)\n,\n*\nexamples\n,\n(\n\"human\"\n,\n\"{query}\"\n)\n,\n]\n)\nchain\n=\n{\n\"query\"\n:\nRunnablePassthrough\n(\n)\n}\n|\nfew_shot_prompt\n|\nllm_with_tools\nchain\n.\ninvoke\n(\n\"Whats 119 times 8 minus 20\"\n)\n.\ntool_calls\nAPI Reference:\nAIMessage\n|\nHumanMessage\n|\nToolMessage\n|\nChatPromptTemplate\n|\nRunnablePassthrough\n[{'name': 'Multiply',\n'args': {'a': 119, 'b': 8},\n'id': 'call_9MvuwQqg7dlJupJcoTWiEsDo'}]\nAnd we get the correct output this time.\nHere's what the\nLangSmith trace\nlooks like.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tools_human/",
    "How-to guides\nHow to add a human-in-the-loop for tools\nOn this page\nHow to add a human-in-the-loop for tools\nThere are certain tools that we don't trust a model to execute on its own. One thing we can do in such situations is require human approval before the tool is invoked.\ninfo\nThis how-to guide shows a simple way to add human-in-the-loop for code running in a jupyter notebook or in a terminal.\nTo build a production application, you will need to do more work to keep track of application state appropriately.\nWe recommend using\nlanggraph\nfor powering such a capability. For more details, please see this\nguide\n.\nSetup\nâ€‹\nWe'll need to install the following packages:\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\nAnd set these environment variables:\nimport\ngetpass\nimport\nos\n# If you'd like to use LangSmith, uncomment the below:\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\nChain\nâ€‹\nLet's create a few simple (dummy) tools and a tool-calling chain:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nfrom\ntyping\nimport\nDict\n,\nList\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnable\n,\nRunnablePassthrough\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\ncount_emails\n(\nlast_n_days\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Dummy function to count number of e-mails. Returns 2 * last_n_days.\"\"\"\nreturn\nlast_n_days\n*\n2\n@tool\ndef\nsend_email\n(\nmessage\n:\nstr\n,\nrecipient\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Dummy function for sending an e-mail.\"\"\"\nreturn\nf\"Successfully sent email to\n{\nrecipient\n}\n.\"\ntools\n=\n[\ncount_emails\n,\nsend_email\n]\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n)\ndef\ncall_tools\n(\nmsg\n:\nAIMessage\n)\n-\n>\nList\n[\nDict\n]\n:\n\"\"\"Simple sequential tool calling helper.\"\"\"\ntool_map\n=\n{\ntool\n.\nname\n:\ntool\nfor\ntool\nin\ntools\n}\ntool_calls\n=\nmsg\n.\ntool_calls\n.\ncopy\n(\n)\nfor\ntool_call\nin\ntool_calls\n:\ntool_call\n[\n\"output\"\n]\n=\ntool_map\n[\ntool_call\n[\n\"name\"\n]\n]\n.\ninvoke\n(\ntool_call\n[\n\"args\"\n]\n)\nreturn\ntool_calls\nchain\n=\nllm_with_tools\n|\ncall_tools\nchain\n.\ninvoke\n(\n\"how many emails did i get in the last 5 days?\"\n)\nAPI Reference:\nAIMessage\n|\nRunnable\n|\nRunnablePassthrough\n|\ntool\n[{'name': 'count_emails',\n'args': {'last_n_days': 5},\n'id': 'toolu_01XrE4AU9QLo4imbriDDkmXm',\n'type': 'tool_call',\n'output': 10}]\nAdding human approval\nâ€‹\nLet's add a step in the chain that will ask a person to approve or reject the tool call request.\nOn rejection, the step will raise an exception which will stop execution of the rest of the chain.\nimport\njson\nclass\nNotApproved\n(\nException\n)\n:\n\"\"\"Custom exception.\"\"\"\ndef\nhuman_approval\n(\nmsg\n:\nAIMessage\n)\n-\n>\nAIMessage\n:\n\"\"\"Responsible for passing through its input or raising an exception.\nArgs:\nmsg: output from the chat model\nReturns:\nmsg: original output from the msg\n\"\"\"\ntool_strs\n=\n\"\\n\\n\"\n.\njoin\n(\njson\n.\ndumps\n(\ntool_call\n,\nindent\n=\n2\n)\nfor\ntool_call\nin\nmsg\n.\ntool_calls\n)\ninput_msg\n=\n(\nf\"Do you approve of the following tool invocations\\n\\n\n{\ntool_strs\n}\n\\n\\n\"\n\"Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.\\n >>>\"\n)\nresp\n=\ninput\n(\ninput_msg\n)\nif\nresp\n.\nlower\n(\n)\nnot\nin\n(\n\"yes\"\n,\n\"y\"\n)\n:\nraise\nNotApproved\n(\nf\"Tool invocations not approved:\\n\\n\n{\ntool_strs\n}\n\"\n)\nreturn\nmsg\nchain\n=\nllm_with_tools\n|\nhuman_approval\n|\ncall_tools\nchain\n.\ninvoke\n(\n\"how many emails did i get in the last 5 days?\"\n)\nDo you approve of the following tool invocations\n{\n\"name\": \"count_emails\",\n\"args\": {\n\"last_n_days\": 5\n},\n\"id\": \"toolu_01WbD8XeMoQaRFtsZezfsHor\"\n}\nAnything except 'Y'/'Yes' (case-insensitive) will be treated as a no.\n>>> yes\n[{'name': 'count_emails',\n'args': {'last_n_days': 5},\n'id': 'toolu_01WbD8XeMoQaRFtsZezfsHor',\n'output': 10}]\ntry\n:\nchain\n.\ninvoke\n(\n\"Send sally@gmail.com an email saying 'What's up homie'\"\n)\nexcept\nNotApproved\nas\ne\n:\nprint\n(\n)\nprint\n(\ne\n)\nDo you approve of the following tool invocations\n{\n\"name\": \"send_email\",\n\"args\": {\n\"recipient\": \"sally@gmail.com\",\n\"message\": \"What's up homie\"\n},\n\"id\": \"toolu_014XccHFzBiVcc9GV1harV9U\"\n}\nAnything except 'Y'/'Yes' (case-insensitive) will be treated as a no.\n>>> no\n``````output\nTool invocations not approved:\n{\n\"name\": \"send_email\",\n\"args\": {\n\"recipient\": \"sally@gmail.com\",\n\"message\": \"What's up homie\"\n},\n\"id\": \"toolu_014XccHFzBiVcc9GV1harV9U\"\n}\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/tools_model_specific/",
    "How-to guides\nHow to bind model-specific tools\nHow to bind model-specific tools\nProviders adopt different conventions for formatting tool schemas.\nFor instance, OpenAI uses a format like this:\ntype\n: The type of the tool. At the time of writing, this is always\n\"function\"\n.\nfunction\n: An object containing tool parameters.\nfunction.name\n: The name of the schema to output.\nfunction.description\n: A high level description of the schema to output.\nfunction.parameters\n: The nested details of the schema you want to extract, formatted as a\nJSON schema\ndict.\nWe can bind this model-specific format directly to the model as well if preferred. Here's an example:\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\n)\nmodel_with_tools\n=\nmodel\n.\nbind\n(\ntools\n=\n[\n{\n\"type\"\n:\n\"function\"\n,\n\"function\"\n:\n{\n\"name\"\n:\n\"multiply\"\n,\n\"description\"\n:\n\"Multiply two integers together.\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"a\"\n:\n{\n\"type\"\n:\n\"number\"\n,\n\"description\"\n:\n\"First integer\"\n}\n,\n\"b\"\n:\n{\n\"type\"\n:\n\"number\"\n,\n\"description\"\n:\n\"Second integer\"\n}\n,\n}\n,\n\"required\"\n:\n[\n\"a\"\n,\n\"b\"\n]\n,\n}\n,\n}\n,\n}\n]\n)\nmodel_with_tools\n.\ninvoke\n(\n\"Whats 119 times 8?\"\n)\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mn4ELw1NbuE0DFYhIeK0GrPe', 'function': {'arguments': '{\"a\":119,\"b\":8}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 62, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-353e8a9a-7125-4f94-8c68-4f3da4c21120-0', tool_calls=[{'name': 'multiply', 'args': {'a': 119, 'b': 8}, 'id': 'call_mn4ELw1NbuE0DFYhIeK0GrPe'}])\nThis is functionally equivalent to the\nbind_tools()\nmethod.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/trim_messages/",
    "How-to guides\nHow to trim messages\nOn this page\nHow to trim messages\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nMessages\nChat models\nChaining\nChat history\nThe methods in this guide also require\nlangchain-core>=0.2.9\n.\nAll models have finite context windows, meaning there's a limit to how many\ntokens\nthey can take as input. If you have very long messages or a chain/agent that accumulates a long message history, you'll need to manage the length of the messages you're passing in to the model.\ntrim_messages\ncan be used to reduce the size of a chat history to a specified token count or specified message count.\nIf passing the trimmed chat history back into a chat model directly, the trimmed chat history should satisfy the following properties:\nThe resulting chat history should be\nvalid\n. Usually this means that the following properties should be satisfied:\nThe chat history\nstarts\nwith either (1) a\nHumanMessage\nor (2) a\nSystemMessage\nfollowed by a\nHumanMessage\n.\nThe chat history\nends\nwith either a\nHumanMessage\nor a\nToolMessage\n.\nA\nToolMessage\ncan only appear after an\nAIMessage\nthat involved a tool call.\nThis can be achieved by setting\nstart_on=\"human\"\nand\nends_on=(\"human\", \"tool\")\n.\nIt includes recent messages and drops old messages in the chat history.\nThis can be achieved by setting\nstrategy=\"last\"\n.\nUsually, the new chat history should include the\nSystemMessage\nif it\nwas present in the original chat history since the\nSystemMessage\nincludes\nspecial instructions to the chat model. The\nSystemMessage\nis almost always\nthe first message in the history if present. This can be achieved by setting\ninclude_system=True\n.\nTrimming based on token count\nâ€‹\nHere, we'll trim the chat history based on token count. The trimmed chat history will produce a\nvalid\nchat history that includes the\nSystemMessage\n.\nTo keep the most recent messages, we set\nstrategy=\"last\"\n.  We'll also set\ninclude_system=True\nto include the\nSystemMessage\n, and\nstart_on=\"human\"\nto make sure the resulting chat history is valid.\nThis is a good default configuration when using\ntrim_messages\nbased on token count. Remember to adjust\ntoken_counter\nand\nmax_tokens\nfor your use case. Keep in mind that new queries added to the chat history will be included in the token count unless you trim prior to adding the new query.\nNotice that for our\ntoken_counter\nwe can pass in a function (more on that below) or a language model (since language models have a message token counting method). It makes sense to pass in a model when you're trimming your messages to fit into the context window of that specific model:\npip install\n-\nqU langchain\n-\nopenai\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nHumanMessage\n,\nSystemMessage\n,\nToolMessage\n,\ntrim_messages\n,\n)\nfrom\nlangchain_core\n.\nmessages\n.\nutils\nimport\ncount_tokens_approximately\nmessages\n=\n[\nSystemMessage\n(\n\"you're a good assistant, you always respond with a joke.\"\n)\n,\nHumanMessage\n(\n\"i wonder why it's called langchain\"\n)\n,\nAIMessage\n(\n'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n)\n,\nHumanMessage\n(\n\"and who is harrison chasing anyways\"\n)\n,\nAIMessage\n(\n\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n)\n,\nHumanMessage\n(\n\"what do you call a speechless parrot\"\n)\n,\n]\ntrim_messages\n(\nmessages\n,\n# Keep the last <= n_count tokens of the messages.\nstrategy\n=\n\"last\"\n,\n# Remember to adjust based on your model\n# or else pass a custom token_counter\ntoken_counter\n=\ncount_tokens_approximately\n,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\n# Remember to adjust based on the desired conversation\n# length\nmax_tokens\n=\n45\n,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\nstart_on\n=\n\"human\"\n,\n# Most chat models expect that chat history ends with either:\n# (1) a HumanMessage or\n# (2) a ToolMessage\nend_on\n=\n(\n\"human\"\n,\n\"tool\"\n)\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\nallow_partial\n=\nFalse\n,\n)\nAPI Reference:\nAIMessage\n|\nHumanMessage\n|\nSystemMessage\n|\nToolMessage\n|\ntrim_messages\n|\ncount_tokens_approximately\n[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]\nTrimming based on message count\nâ€‹\nAlternatively, we can trim the chat history based on\nmessage count\n, by setting\ntoken_counter=len\n. In this case, each message will count as a single token, and\nmax_tokens\nwill control\nthe maximum number of messages.\nThis is a good default configuration when using\ntrim_messages\nbased on message count. Remember to adjust\nmax_tokens\nfor your use case.\ntrim_messages\n(\nmessages\n,\n# Keep the last <= n_count tokens of the messages.\nstrategy\n=\n\"last\"\n,\ntoken_counter\n=\nlen\n,\n# When token_counter=len, each message\n# will be counted as a single token.\n# Remember to adjust for your use case\nmax_tokens\n=\n5\n,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\nstart_on\n=\n\"human\"\n,\n# Most chat models expect that chat history ends with either:\n# (1) a HumanMessage or\n# (2) a ToolMessage\nend_on\n=\n(\n\"human\"\n,\n\"tool\"\n)\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\n)\n[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),\nAIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]\nAdvanced Usage\nâ€‹\nYou can use\ntrim_messages\nas a building-block to create more complex processing logic.\nIf we want to allow splitting up the contents of a message we can specify\nallow_partial=True\n:\ntrim_messages\n(\nmessages\n,\nmax_tokens\n=\n56\n,\nstrategy\n=\n\"last\"\n,\ntoken_counter\n=\ncount_tokens_approximately\n,\ninclude_system\n=\nTrue\n,\nallow_partial\n=\nTrue\n,\n)\n[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\nAIMessage(content=\"\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]\nBy default, the\nSystemMessage\nwill not be included, so you can drop it by either setting\ninclude_system=False\nor by dropping the\ninclude_system\nargument.\ntrim_messages\n(\nmessages\n,\nmax_tokens\n=\n45\n,\nstrategy\n=\n\"last\"\n,\ntoken_counter\n=\ncount_tokens_approximately\n,\n)\n[AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]\nWe can perform the flipped operation of getting the\nfirst\nmax_tokens\nby specifying\nstrategy=\"first\"\n:\ntrim_messages\n(\nmessages\n,\nmax_tokens\n=\n45\n,\nstrategy\n=\n\"first\"\n,\ntoken_counter\n=\ncount_tokens_approximately\n,\n)\n[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content=\"i wonder why it's called langchain\", additional_kwargs={}, response_metadata={})]\nUsing\nChatModel\nas a token counter\nâ€‹\nYou can pass a ChatModel as a token-counter. This will use\nChatModel.get_num_tokens_from_messages\n. Let's demonstrate how to use it with OpenAI:\nfrom\nlangchain_openai\nimport\nChatOpenAI\ntrim_messages\n(\nmessages\n,\nmax_tokens\n=\n45\n,\nstrategy\n=\n\"first\"\n,\ntoken_counter\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\n,\n)\n[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content=\"i wonder why it's called langchain\", additional_kwargs={}, response_metadata={})]\nWriting a custom token counter\nâ€‹\nWe can write a custom token counter function that takes in a list of messages and returns an int.\npip install\n-\nqU tiktoken\nfrom\ntyping\nimport\nList\nimport\ntiktoken\nfrom\nlangchain_core\n.\nmessages\nimport\nBaseMessage\n,\nToolMessage\ndef\nstr_token_counter\n(\ntext\n:\nstr\n)\n-\n>\nint\n:\nenc\n=\ntiktoken\n.\nget_encoding\n(\n\"o200k_base\"\n)\nreturn\nlen\n(\nenc\n.\nencode\n(\ntext\n)\n)\ndef\ntiktoken_counter\n(\nmessages\n:\nList\n[\nBaseMessage\n]\n)\n-\n>\nint\n:\n\"\"\"Approximately reproduce https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\nFor simplicity only supports str Message.contents.\n\"\"\"\nnum_tokens\n=\n3\n# every reply is primed with <|start|>assistant<|message|>\ntokens_per_message\n=\n3\ntokens_per_name\n=\n1\nfor\nmsg\nin\nmessages\n:\nif\nisinstance\n(\nmsg\n,\nHumanMessage\n)\n:\nrole\n=\n\"user\"\nelif\nisinstance\n(\nmsg\n,\nAIMessage\n)\n:\nrole\n=\n\"assistant\"\nelif\nisinstance\n(\nmsg\n,\nToolMessage\n)\n:\nrole\n=\n\"tool\"\nelif\nisinstance\n(\nmsg\n,\nSystemMessage\n)\n:\nrole\n=\n\"system\"\nelse\n:\nraise\nValueError\n(\nf\"Unsupported messages type\n{\nmsg\n.\n__class__\n}\n\"\n)\nnum_tokens\n+=\n(\ntokens_per_message\n+\nstr_token_counter\n(\nrole\n)\n+\nstr_token_counter\n(\nmsg\n.\ncontent\n)\n)\nif\nmsg\n.\nname\n:\nnum_tokens\n+=\ntokens_per_name\n+\nstr_token_counter\n(\nmsg\n.\nname\n)\nreturn\nnum_tokens\ntrim_messages\n(\nmessages\n,\ntoken_counter\n=\ntiktoken_counter\n,\n# Keep the last <= n_count tokens of the messages.\nstrategy\n=\n\"last\"\n,\n# When token_counter=len, each message\n# will be counted as a single token.\n# Remember to adjust for your use case\nmax_tokens\n=\n45\n,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\nstart_on\n=\n\"human\"\n,\n# Most chat models expect that chat history ends with either:\n# (1) a HumanMessage or\n# (2) a ToolMessage\nend_on\n=\n(\n\"human\"\n,\n\"tool\"\n)\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\n)\nAPI Reference:\nBaseMessage\n|\nToolMessage\n[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]\nChaining\nâ€‹\ntrim_messages\ncan be used imperatively (like above) or declaratively, making it easy to compose with other components in a chain\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\n# Notice we don't pass in messages. This creates\n# a RunnableLambda that takes messages as input\ntrimmer\n=\ntrim_messages\n(\ntoken_counter\n=\nllm\n,\n# Keep the last <= n_count tokens of the messages.\nstrategy\n=\n\"last\"\n,\n# When token_counter=len, each message\n# will be counted as a single token.\n# Remember to adjust for your use case\nmax_tokens\n=\n45\n,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\nstart_on\n=\n\"human\"\n,\n# Most chat models expect that chat history ends with either:\n# (1) a HumanMessage or\n# (2) a ToolMessage\nend_on\n=\n(\n\"human\"\n,\n\"tool\"\n)\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\n)\nchain\n=\ntrimmer\n|\nllm\nchain\n.\ninvoke\n(\nmessages\n)\nAIMessage(content='A \"polly-no-wanna-cracker\"!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 32, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90d33c15d4', 'finish_reason': 'stop', 'logprobs': None}, id='run-b1f8b63b-6bc2-4df4-b3b9-dfc4e3e675fe-0', usage_metadata={'input_tokens': 32, 'output_tokens': 11, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\nLooking at\nthe LangSmith trace\nwe can see that before the messages are passed to the model they are first trimmed.\nLooking at just the trimmer, we can see that it's a Runnable object that can be invoked like all Runnables:\ntrimmer\n.\ninvoke\n(\nmessages\n)\n[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\nHumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]\nUsing with ChatMessageHistory\nâ€‹\nTrimming messages is especially useful when\nworking with chat histories\n, which can get arbitrarily long:\nfrom\nlangchain_core\n.\nchat_history\nimport\nInMemoryChatMessageHistory\nfrom\nlangchain_core\n.\nrunnables\n.\nhistory\nimport\nRunnableWithMessageHistory\nchat_history\n=\nInMemoryChatMessageHistory\n(\nmessages\n=\nmessages\n[\n:\n-\n1\n]\n)\ndef\ndummy_get_session_history\n(\nsession_id\n)\n:\nif\nsession_id\n!=\n\"1\"\n:\nreturn\nInMemoryChatMessageHistory\n(\n)\nreturn\nchat_history\ntrimmer\n=\ntrim_messages\n(\nmax_tokens\n=\n45\n,\nstrategy\n=\n\"last\"\n,\ntoken_counter\n=\nllm\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\n# start_on=\"human\" makes sure we produce a valid chat history\nstart_on\n=\n\"human\"\n,\n)\nchain\n=\ntrimmer\n|\nllm\nchain_with_history\n=\nRunnableWithMessageHistory\n(\nchain\n,\ndummy_get_session_history\n)\nchain_with_history\n.\ninvoke\n(\n[\nHumanMessage\n(\n\"what do you call a speechless parrot\"\n)\n]\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"session_id\"\n:\n\"1\"\n}\n}\n,\n)\nAPI Reference:\nInMemoryChatMessageHistory\n|\nRunnableWithMessageHistory\nAIMessage(content='A \"polygon\"!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 32, 'total_tokens': 36, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c17d3befe7', 'finish_reason': 'stop', 'logprobs': None}, id='run-71d9fce6-bb0c-4bb3-acc8-d5eaee6ae7bc-0', usage_metadata={'input_tokens': 32, 'output_tokens': 4, 'total_tokens': 36})\nLooking at\nthe LangSmith trace\nwe can see that we retrieve all of our messages but before the messages are passed to the model they are trimmed to be just the system message and last human message.\nAPI reference\nâ€‹\nFor a complete description of all arguments head to the\nAPI reference\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/vectorstores/",
    "How-to guides\nHow to create and query vector stores\nOn this page\nHow to create and query vector stores\ninfo\nHead to\nIntegrations\nfor documentation on built-in integrations with 3rd-party vector stores.\nOne of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding\nvectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are\n'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search\nfor you.\nGet started\nâ€‹\nThis guide showcases basic functionality related to vector stores. A key part of working with vector stores is creating the vector to put in them,\nwhich is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the\ntext embedding model interfaces\nbefore diving into this.\nBefore using the vectorstore at all, we need to load some data and initialize an embedding model.\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\nimport\nos\nimport\ngetpass\nos\n.\nenviron\n[\n'OPENAI_API_KEY'\n]\n=\ngetpass\n.\ngetpass\n(\n'OpenAI API Key:'\n)\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nTextLoader\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\n# Load the document, split it into chunks, embed each chunk and load it into the vector store.\nraw_documents\n=\nTextLoader\n(\n'state_of_the_union.txt'\n)\n.\nload\n(\n)\ntext_splitter\n=\nCharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n0\n)\ndocuments\n=\ntext_splitter\n.\nsplit_documents\n(\nraw_documents\n)\nThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.\nChroma\nFAISS\nLance\nThis walkthrough uses the\nchroma\nvector database, which runs on your local machine as a library.\npip install langchain-chroma\nfrom\nlangchain_chroma\nimport\nChroma\ndb\n=\nChroma\n.\nfrom_documents\n(\ndocuments\n,\nOpenAIEmbeddings\n(\n)\n)\nThis walkthrough uses the\nFAISS\nvector database, which makes use of the Facebook AI Similarity Search (FAISS) library.\npip install faiss-cpu\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\ndb\n=\nFAISS\n.\nfrom_documents\n(\ndocuments\n,\nOpenAIEmbeddings\n(\n)\n)\nThis notebook shows how to use functionality related to the LanceDB vector database based on the Lance data format.\npip install lancedb\nfrom\nlangchain_community\n.\nvectorstores\nimport\nLanceDB\nimport\nlancedb\ndb\n=\nlancedb\n.\nconnect\n(\n\"/tmp/lancedb\"\n)\ntable\n=\ndb\n.\ncreate_table\n(\n\"my_table\"\n,\ndata\n=\n[\n{\n\"vector\"\n:\nembeddings\n.\nembed_query\n(\n\"Hello World\"\n)\n,\n\"text\"\n:\n\"Hello World\"\n,\n\"id\"\n:\n\"1\"\n,\n}\n]\n,\nmode\n=\n\"overwrite\"\n,\n)\ndb\n=\nLanceDB\n.\nfrom_documents\n(\ndocuments\n,\nOpenAIEmbeddings\n(\n)\n)\nSimilarity search\nâ€‹\nAll vectorstores expose a\nsimilarity_search\nmethod.\nThis will take incoming documents, create an embedding of them, and then find all documents with the most similar embedding.\nquery\n=\n\"What did the president say about Ketanji Brown Jackson\"\ndocs\n=\ndb\n.\nsimilarity_search\n(\nquery\n)\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n)\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.\nSimilarity search by vector\nâ€‹\nIt is also possible to do a search for documents similar to a given embedding vector using\nsimilarity_search_by_vector\nwhich accepts an embedding vector as a parameter instead of a string.\nembedding_vector\n=\nOpenAIEmbeddings\n(\n)\n.\nembed_query\n(\nquery\n)\ndocs\n=\ndb\n.\nsimilarity_search_by_vector\n(\nembedding_vector\n)\nprint\n(\ndocs\n[\n0\n]\n.\npage_content\n)\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.\nAsync Operations\nâ€‹\nVector stores are usually run as a separate service that requires some IO operations, and therefore they might be called asynchronously. That gives performance benefits as you don't waste time waiting for responses from external services. That might also be important if you work with an asynchronous framework, such as\nFastAPI\n.\nLangChain supports async operation on vector stores. All the methods might be called using their async counterparts, with the prefix\na\n, meaning\nasync\n.\ndocs\n=\nawait\ndb\n.\nasimilarity_search\n(\nquery\n)\ndocs\n[Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': 'state_of_the_union.txt'}),\nDocument(page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, weâ€™ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWeâ€™ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWeâ€™re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWeâ€™re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', metadata={'source': 'state_of_the_union.txt'}),\nDocument(page_content='And for our LGBTQ+ Americans, letâ€™s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isnâ€™t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, weâ€™ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight Iâ€™m offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic.', metadata={'source': 'state_of_the_union.txt'}),\nDocument(page_content='Tonight, Iâ€™m announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWeâ€™ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLetâ€™s pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLetâ€™s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jillâ€”our First Lady who teaches full-timeâ€”calls Americaâ€™s best-kept secret: community colleges.', metadata={'source': 'state_of_the_union.txt'})]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/",
    "Conceptual guide\nOn this page\nConceptual guide\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\nWe recommend that you go through at least one of the\nTutorials\nbefore diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples â€” those are found in the\nHow-to guides\nand\nTutorials\n. For detailed reference material, please see the\nAPI reference\n.\nHigh level\nâ€‹\nWhy LangChain?\n: Overview of the value that LangChain provides.\nArchitecture\n: How packages are organized in the LangChain ecosystem.\nConcepts\nâ€‹\nChat models\n: LLMs exposed via a chat API that process sequences of messages as input and output a message.\nMessages\n: The unit of communication in chat models, used to represent model input and output.\nChat history\n: A conversation represented as a sequence of messages, alternating between user messages and model responses.\nTools\n: A function with an associated schema defining the function's name, description, and the arguments it accepts.\nTool calling\n: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\nStructured output\n: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\nMemory\n: Information about a conversation that is persisted so that it can be used in future conversations.\nMultimodality\n: The ability to work with data that comes in different forms, such as text, audio, images, and video.\nRunnable interface\n: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\nStreaming\n: LangChain streaming APIs for surfacing results as they are generated.\nLangChain Expression Language (LCEL)\n: A syntax for orchestrating LangChain components. Most useful for simpler applications.\nDocument loaders\n: Load a source as a list of documents.\nRetrieval\n: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\nText splitters\n: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\nEmbedding models\n: Models that represent data such as text or images in a vector space.\nVector stores\n: Storage of and efficient search over vectors and associated metadata.\nRetriever\n: A component that returns relevant documents from a knowledge base in response to a query.\nRetrieval Augmented Generation (RAG)\n: A technique that enhances language models by combining them with external knowledge bases.\nAgents\n: Use a\nlanguage model\nto choose a sequence of actions to take. Agents can interact with external resources via\ntools\n.\nPrompt templates\n: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\nOutput parsers\n: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of\ntool calling\nand\nstructured outputs\n.\nFew-shot prompting\n: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\nExample selectors\n: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\nAsync programming\n: The basics that one should know to use LangChain in an asynchronous context.\nCallbacks\n: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\nTracing\n: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\nEvaluation\n: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\nTesting\n: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\nGlossary\nâ€‹\nAIMessageChunk\n: A partial response from an AI message. Used when streaming responses from a chat model.\nAIMessage\n: Represents a complete response from an AI model.\nastream_events\n: Stream granular information from\nLCEL\nchains.\nBaseTool\n: The base class for all tools in LangChain.\nbatch\n: Used to execute a runnable with batch inputs.\nbind_tools\n: Allows models to interact with tools.\nCaching\n: Storing results to avoid redundant calls to a chat model.\nChat models\n: Chat models that handle multiple data modalities.\nConfigurable runnables\n: Creating configurable Runnables.\nContext window\n: The maximum size of input a chat model can process.\nConversation patterns\n: Common patterns in chat interactions.\nDocument\n: LangChain's representation of a document.\nEmbedding models\n: Models that generate vector embeddings for various data types.\nHumanMessage\n: Represents a message from a human user.\nInjectedState\n: A state injected into a tool function.\nInjectedStore\n: A store that can be injected into a tool for data persistence.\nInjectedToolArg\n: Mechanism to inject arguments into tool functions.\ninput and output types\n: Types used for input and output in Runnables.\nIntegration packages\n: Third-party packages that integrate with LangChain.\nIntegration tests\n: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\ninvoke\n: A standard method to invoke a Runnable.\nJSON mode\n: Returning responses in JSON format.\nlangchain-community\n: Community-driven components for LangChain.\nlangchain-core\n: Core langchain package. Includes base interfaces and in-memory implementations.\nlangchain\n: A package for higher level components (e.g., some pre-built chains).\nlanggraph\n: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\nlangserve\n: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\nLLMs (legacy)\n: Older language models that take a string as input and return a string as output.\nManaging chat history\n: Techniques to maintain and manage the chat history.\nOpenAI format\n: OpenAI's message format for chat models.\nPropagation of RunnableConfig\n: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\nrate-limiting\n: Client side rate limiting for chat models.\nRemoveMessage\n: An abstraction used to remove a message from chat history, used primarily in LangGraph.\nrole\n: Represents the role (e.g., user, assistant) of a chat message.\nRunnableConfig\n: Use to pass run time information to Runnables (e.g.,\nrun_name\n,\nrun_id\n,\ntags\n,\nmetadata\n,\nmax_concurrency\n,\nrecursion_limit\n,\nconfigurable\n).\nStandard parameters for chat models\n: Parameters such as API key,\ntemperature\n, and\nmax_tokens\n.\nStandard tests\n: A defined set of unit and integration tests that all integrations must pass.\nstream\n: Use to stream output from a Runnable or a graph.\nTokenization\n: The process of converting data into tokens and vice versa.\nTokens\n: The basic unit that a language model reads, processes, and generates under the hood.\nTool artifacts\n: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\nTool binding\n: Binding tools to models.\n@tool\n: Decorator for creating tools in LangChain.\nToolkits\n: A collection of tools that can be used together.\nToolMessage\n: Represents a message that contains the results of a tool execution.\nUnit tests\n: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\nVector stores\n: Datastores specialized for storing and efficiently searching vector embeddings.\nwith_structured_output\n: A helper method for chat models that natively support\ntool calling\nto get structured output matching a given schema specified via Pydantic, JSON schema or a function.\nwith_types\n: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/agents/",
    "Conceptual guide\nAgents\nOn this page\nAgents\nBy themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.\nLangGraph\nis an extension of LangChain specifically aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents.\nPlease see the following resources for more information:\nLangGraph docs on\ncommon agent architectures\nPre-built agents in LangGraph\nLegacy agent concept: AgentExecutor\nâ€‹\nLangChain previously introduced the\nAgentExecutor\nas a runtime for agents.\nWhile it served as an excellent starting point, its limitations became apparent when dealing with more sophisticated and customized agents.\nAs a result, we're gradually phasing out\nAgentExecutor\nin favor of more flexible solutions in LangGraph.\nTransitioning from AgentExecutor to LangGraph\nâ€‹\nIf you're currently using\nAgentExecutor\n, don't worry! We've prepared resources to help you:\nFor those who still need to use\nAgentExecutor\n, we offer a comprehensive guide on\nhow to use AgentExecutor\n.\nHowever, we strongly recommend transitioning to LangGraph for improved flexibility and control. To facilitate this transition, we've created a detailed\nmigration guide\nto help you move from\nAgentExecutor\nto LangGraph seamlessly.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/architecture/",
    "Conceptual guide\nArchitecture\nOn this page\nArchitecture\nLangChain is a framework that consists of a number of packages.\nlangchain-core\nâ€‹\nThis package contains base abstractions for different components and ways to compose them together.\nThe interfaces for core components like chat models, vector stores, tools and more are defined here.\nNo third-party integrations are defined here.\nThe dependencies are kept purposefully very lightweight.\nlangchain\nâ€‹\nThe main\nlangchain\npackage contains chains and retrieval strategies that make up an application's cognitive architecture.\nThese are NOT third-party integrations.\nAll chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.\nIntegration packages\nâ€‹\nPopular integrations have their own packages (e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc) so that they can be properly versioned and appropriately lightweight.\nFor more information see:\nA list\nintegrations packages\nThe\nAPI Reference\nwhere you can find detailed information about each of the integration package.\nlangchain-community\nâ€‹\nThis package contains third-party integrations that are maintained by the LangChain community.\nKey integration packages are separated out (see above).\nThis contains integrations for various components (chat models, vector stores, tools, etc).\nAll dependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph\nâ€‹\nlanggraph\nis an extension of\nlangchain\naimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.\nFurther reading\nSee our LangGraph overview\nhere\n.\nSee our LangGraph Academy Course\nhere\n.\nlangserve\nâ€‹\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nimportant\nLangServe is designed to primarily deploy simple Runnables and work with well-known primitives in langchain-core.\nIf you need a deployment option for LangGraph, you should instead be looking at LangGraph Platform (beta) which will be better suited for deploying LangGraph applications.\nFor more information, see the\nLangServe documentation\n.\nLangSmith\nâ€‹\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nFor more information, see the\nLangSmith documentation\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/async/",
    "Conceptual guide\nAsync programming with LangChain\nOn this page\nAsync programming with LangChain\nPrerequisites\nRunnable interface\nasyncio\nLLM based applications often involve a lot of I/O-bound operations, such as making API calls to language models, databases, or other services. Asynchronous programming (or async programming) is a paradigm that allows a program to perform multiple tasks concurrently without blocking the execution of other tasks, improving efficiency and responsiveness, particularly in I/O-bound operations.\nnote\nYou are expected to be familiar with asynchronous programming in Python before reading this guide. If you are not, please find appropriate resources online to learn how to program asynchronously in Python.\nThis guide specifically focuses on what you need to know to work with LangChain in an asynchronous context, assuming that you are already familiar with asynchronous programming.\nLangChain asynchronous APIs\nâ€‹\nMany LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.\nTypically, any method that may perform I/O operations (e.g., making API calls, reading files) will have an asynchronous counterpart.\nIn LangChain, async implementations are located in the same classes as their synchronous counterparts, with the asynchronous methods having an \"a\" prefix. For example, the synchronous\ninvoke\nmethod has an asynchronous counterpart called\nainvoke\n.\nMany components of LangChain implement the\nRunnable Interface\n, which includes support for asynchronous execution. This means that you can run Runnables asynchronously using the\nawait\nkeyword in Python.\nawait\nsome_runnable\n.\nainvoke\n(\nsome_input\n)\nOther components like\nEmbedding Models\nand\nVectorStore\nthat do not implement the\nRunnable Interface\nusually still follow the same rule and include the asynchronous version of method in the same class with an \"a\" prefix.\nFor example,\nawait\nsome_vectorstore\n.\naadd_documents\n(\ndocuments\n)\nRunnables created using the\nLangChain Expression Language (LCEL)\ncan also be run asynchronously as they implement\nthe full\nRunnable Interface\n.\nFor more information, please review the\nAPI reference\nfor the specific component you are using.\nDelegation to sync methods\nâ€‹\nMost popular LangChain integrations implement asynchronous support of their APIs. For example, the\nainvoke\nmethod of many ChatModel implementations uses the\nhttpx.AsyncClient\nto make asynchronous HTTP requests to the model provider's API.\nWhen an asynchronous implementation is not available, LangChain tries to provide a default implementation, even if it incurs\na\nslight\noverhead.\nBy default, LangChain will delegate the execution of unimplemented asynchronous methods to the synchronous counterparts. LangChain almost always assumes that the synchronous method should be treated as a blocking operation and should be run in a separate thread.\nThis is done using\nasyncio.loop.run_in_executor\nfunctionality provided by the\nasyncio\nlibrary. LangChain uses the default executor provided by the\nasyncio\nlibrary, which lazily initializes a thread pool executor with a default number of threads that is reused in the given event loop. While this strategy incurs a slight overhead due to context switching between threads, it guarantees that every asynchronous method has a default implementation that works out of the box.\nPerformance\nâ€‹\nAsync code in LangChain should generally perform relatively well with minimal overhead out of the box, and is unlikely\nto be a bottleneck in most applications.\nThe two main sources of overhead are:\nCost of context switching between threads when\ndelegating to synchronous methods\n. This can be addressed by providing a native asynchronous implementation.\nIn\nLCEL\nany \"cheap functions\" that appear as part of the chain will be either scheduled as tasks on the event loop (if they are async) or run in a separate thread (if they are sync), rather than just be run inline.\nThe latency overhead you should expect from these is between tens of microseconds to a few milliseconds.\nA more common source of performance issues arises from users accidentally blocking the event loop by calling synchronous code in an async context (e.g., calling\ninvoke\nrather than\nainvoke\n).\nCompatibility\nâ€‹\nLangChain is only compatible with the\nasyncio\nlibrary, which is distributed as part of the Python standard library. It will not work with other async libraries like\ntrio\nor\ncurio\n.\nIn Python 3.9 and 3.10,\nasyncio's tasks\ndid not\naccept a\ncontext\nparameter. Due to this limitation, LangChain cannot automatically propagate the\nRunnableConfig\ndown the call chain\nin certain scenarios.\nIf you are experiencing issues with streaming, callbacks or tracing in async code and are using Python 3.9 or 3.10, this is a likely cause.\nPlease read\nPropagation RunnableConfig\nfor more details to learn how to propagate the\nRunnableConfig\ndown the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).\nHow to use in ipython and jupyter notebooks\nâ€‹\nAs of IPython 7.0, IPython supports asynchronous REPLs. This means that you can use the\nawait\nkeyword in the IPython REPL and Jupyter Notebooks without any additional setup. For more information, see the\nIPython blog post\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/callbacks/",
    "Conceptual guide\nCallbacks\nOn this page\nCallbacks\nPrerequisites\nRunnable interface\nLangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.\nYou can subscribe to these events by using the\ncallbacks\nargument available throughout the API. This argument is a list of handler objects, which are expected to implement one or more of the methods described below in more detail.\nCallback events\nâ€‹\nEvent\nEvent Trigger\nAssociated Method\nChat model start\nWhen a chat model starts\non_chat_model_start\nLLM start\nWhen a llm starts\non_llm_start\nLLM new token\nWhen an llm OR chat model emits a new token\non_llm_new_token\nLLM ends\nWhen an llm OR chat model ends\non_llm_end\nLLM errors\nWhen an llm OR chat model errors\non_llm_error\nChain start\nWhen a chain starts running\non_chain_start\nChain end\nWhen a chain ends\non_chain_end\nChain error\nWhen a chain errors\non_chain_error\nTool start\nWhen a tool starts running\non_tool_start\nTool end\nWhen a tool ends\non_tool_end\nTool error\nWhen a tool errors\non_tool_error\nAgent action\nWhen an agent takes an action\non_agent_action\nAgent finish\nWhen an agent ends\non_agent_finish\nRetriever start\nWhen a retriever starts\non_retriever_start\nRetriever end\nWhen a retriever ends\non_retriever_end\nRetriever error\nWhen a retriever errors\non_retriever_error\nText\nWhen arbitrary text is run\non_text\nRetry\nWhen a retry event is run\non_retry\nCallback handlers\nâ€‹\nCallback handlers can either be\nsync\nor\nasync\n:\nSync callback handlers implement the\nBaseCallbackHandler\ninterface.\nAsync callback handlers implement the\nAsyncCallbackHandler\ninterface.\nDuring run-time LangChain configures an appropriate callback manager (e.g.,\nCallbackManager\nor\nAsyncCallbackManager\nwhich will be responsible for calling the appropriate method on each \"registered\" callback handler when the event is triggered.\nPassing callbacks\nâ€‹\nThe\ncallbacks\nproperty is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:\nRequest time callbacks\n: Passed at the time of the request in addition to the input data.\nAvailable on all standard\nRunnable\nobjects. These callbacks are INHERITED by all children\nof the object they are defined on. For example,\nchain.invoke({\"number\": 25}, {\"callbacks\": [handler]})\n.\nConstructor callbacks\n:\nchain = TheNameOfSomeChain(callbacks=[handler])\n. These callbacks\nare passed as arguments to the constructor of the object. The callbacks are scoped\nonly to the object they are defined on, and are\nnot\ninherited by any children of the object.\nwarning\nConstructor callbacks are scoped only to the object they are defined on. They are\nnot\ninherited by children\nof the object.\nIf you're creating a custom chain or runnable, you need to remember to propagate request time\ncallbacks to any child objects.\nAsync in Python<=3.10\nAny\nRunnableLambda\n, a\nRunnableGenerator\n, or\nTool\nthat invokes other runnables\nand is running\nasync\nin python<=3.10, will have to propagate callbacks to child\nobjects manually. This is because LangChain cannot automatically propagate\ncallbacks to child objects in this case.\nThis is a common reason why you may fail to see events being emitted from custom\nrunnables or tools.\nFor specifics on how to use callbacks, see the\nrelevant how-to guides here\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/chat_history/",
    "Conceptual guide\nChat history\nOn this page\nChat history\nPrerequisites\nMessages\nChat models\nTool calling\nChat history is a record of the conversation between the user and the chat model. It is used to maintain context and state throughout the conversation. The chat history is sequence of\nmessages\n, each of which is associated with a specific\nrole\n, such as \"user\", \"assistant\", \"system\", or \"tool\".\nConversation patterns\nâ€‹\nMost conversations start with a\nsystem message\nthat sets the context for the conversation. This is followed by a\nuser message\ncontaining the user's input, and then an\nassistant message\ncontaining the model's response.\nThe\nassistant\nmay respond directly to the user or if configured with tools request that a\ntool\nbe invoked to perform a specific task.\nA full conversation often involves a combination of two patterns of alternating messages:\nThe\nuser\nand the\nassistant\nrepresenting a back-and-forth conversation.\nThe\nassistant\nand\ntool messages\nrepresenting an\n\"agentic\" workflow\nwhere the assistant is invoking tools to perform specific tasks.\nManaging chat history\nâ€‹\nSince chat models have a maximum limit on input size, it's important to manage chat history and trim it as needed to avoid exceeding the\ncontext window\n.\nWhile processing chat history, it's essential to preserve a correct conversation structure.\nKey guidelines for managing chat history:\nThe conversation should follow one of these structures:\nThe first message is either a \"user\" message or a \"system\" message, followed by a \"user\" and then an \"assistant\" message.\nThe last message should be either a \"user\" message or a \"tool\" message containing the result of a tool call.\nWhen using\ntool calling\n, a \"tool\" message should only follow an \"assistant\" message that requested the tool invocation.\ntip\nUnderstanding correct conversation structure is essential for being able to properly implement\nmemory\nin chat models.\nRelated resources\nâ€‹\nHow to trim messages\nMemory guide\nfor information on implementing short-term and long-term memory in chat models using\nLangGraph\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/chat_models/",
    "Conceptual guide\nChat models\nOn this page\nChat models\nOverview\nâ€‹\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\nModern LLMs are typically accessed through a chat model interface that takes a list of\nmessages\nas input and returns a\nmessage\nas output.\nThe newest generation of chat models offer additional capabilities:\nTool calling\n: Many popular chat models offer a native\ntool calling\nAPI. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\nStructured output\n: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\nMultimodality\n: The ability to work with data other than text; for example, images, audio, and video.\nFeatures\nâ€‹\nLangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\nIntegrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see\nchat model integrations\nfor an up-to-date list of supported models.\nUse either LangChain's\nmessages\nformat or OpenAI format.\nStandard\ntool calling API\n: standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\nStandard API for\nstructuring outputs\nvia the\nwith_structured_output\nmethod.\nProvides support for\nasync programming\n,\nefficient batching\n,\na rich streaming API\n.\nIntegration with\nLangSmith\nfor monitoring and debugging production-grade applications based on LLMs.\nAdditional features like standardized\ntoken usage\n,\nrate limiting\n,\ncaching\nand more.\nIntegrations\nâ€‹\nLangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\nThese integrations are one of two types:\nOfficial models\n: These are models that are officially supported by LangChain and/or model provider. You can find these models in the\nlangchain-<provider>\npackages.\nCommunity models\n: There are models that are mostly contributed and supported by the community. You can find these models in the\nlangchain-community\npackage.\nLangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g.,\nChatOllama\n,\nChatAnthropic\n,\nChatOpenAI\n, etc.).\nPlease review the\nchat model integrations\nfor a list of supported models.\nnote\nModels that do\nnot\ninclude the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\nInterface\nâ€‹\nLangChain chat models implement the\nBaseChatModel\ninterface. Because\nBaseChatModel\nalso implements the\nRunnable Interface\n, chat models support a\nstandard streaming interface\n,\nasync programming\n, optimized\nbatching\n, and more. Please see the\nRunnable Interface\nfor more details.\nMany of the key methods of chat models operate on\nmessages\nas input and return messages as output.\nChat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the\nstandard parameters\nsection for more details.\nnote\nIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.\nHowever, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g.,\nOllama\n,\nAnthropic\n,\nOpenAI\n, etc.).\nThese models implement the\nBaseLLM\ninterface and may be named with the \"LLM\" suffix (e.g.,\nOllamaLLM\n,\nAnthropicLLM\n,\nOpenAILLM\n, etc.). Generally, users should not use these models.\nKey methods\nâ€‹\nThe key methods of a chat model are:\ninvoke\n: The primary method for interacting with a chat model. It takes a list of\nmessages\nas input and returns a list of messages as output.\nstream\n: A method that allows you to stream the output of a chat model as it is generated.\nbatch\n: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\nbind_tools\n: A method that allows you to bind a tool to a chat model for use in the model's execution context.\nwith_structured_output\n: A wrapper around the\ninvoke\nmethod for models that natively support\nstructured output\n.\nOther important methods can be found in the\nBaseChatModel API Reference\n.\nInputs and outputs\nâ€‹\nModern LLMs are typically accessed through a chat model interface that takes\nmessages\nas input and returns\nmessages\nas output. Messages are typically associated with a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).\nLangChain supports two message formats to interact with chat models:\nLangChain Message Format\n: LangChain's own message format, which is used by default and is used internally by LangChain.\nOpenAI's Message Format\n: OpenAI's message format.\nStandard parameters\nâ€‹\nMany chat models have standardized parameters that can be used to configure the model:\nParameter\nDescription\nmodel\nThe name or identifier of the specific AI model you want to use (e.g.,\n\"gpt-3.5-turbo\"\nor\n\"gpt-4\"\n).\ntemperature\nControls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.\ntimeout\nThe maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesnâ€™t hang indefinitely.\nmax_tokens\nLimits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.\nstop\nSpecifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.\nmax_retries\nThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\napi_key\nThe API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.\nbase_url\nThe URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.\nrate_limiter\nAn optional\nBaseRateLimiter\nto space out requests to avoid exceeding rate limits.  See\nrate-limiting\nbelow for more details.\nSome important things to note:\nStandard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these.\nStandard parameters are currently only enforced on integrations that have their own integration packages (e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.), they're not enforced on models in\nlangchain-community\n.\nChat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective\nAPI reference\nfor that model.\nTool calling\nâ€‹\nChat models can call\ntools\nto perform tasks such as fetching data from a database, making API requests, or running custom code. Please\nsee the\ntool calling\nguide for more information.\nStructured outputs\nâ€‹\nChat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely\nuseful for information extraction tasks. Please read more about\nthe technique in the\nstructured outputs\nguide.\nMultimodality\nâ€‹\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as\nmultimodality\n.\nCurrently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\nContext window\nâ€‹\nA chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can \"remember\" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the\nmemory\n.\nThe size of the input is measured in\ntokens\nwhich are the unit of processing that the model uses.\nAdvanced topics\nâ€‹\nRate-limiting\nâ€‹\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\nYou have a few options to deal with rate limits:\nTry to avoid hitting rate limits by spacing out requests: Chat models accept a\nrate_limiter\nparameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the\nhow to handle rate limits\nfor more information on how to use this feature.\nTry to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a\nmax_retries\nparameter that can be used to control the number of retries. See the\nstandard parameters\nsection for more information.\nFallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\nCaching\nâ€‹\nChat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\nThe reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the\nexact\ninputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\nAn alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\nA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an\nembedding model\nto convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.\nHowever, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\nPlease see the\nhow to cache chat model responses\nguide for more details.\nRelated resources\nâ€‹\nHow-to guides on using chat models:\nhow-to guides\n.\nList of supported chat models:\nchat model integrations\n.\nConceptual guides\nâ€‹\nMessages\nTool calling\nMultimodality\nStructured outputs\nTokens\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/document_loaders/",
    "Conceptual guide\nDocument loaders\nOn this page\nDocument loaders\nPrerequisites\nDocument loaders API reference\nDocument loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.\nIntegrations\nâ€‹\nYou can find available integrations on the\nDocument loaders integrations page\n.\nInterface\nâ€‹\nDocuments loaders implement the\nBaseLoader interface\n.\nEach DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the\n.load\nmethod or\n.lazy_load\n.\nHere's a simple example:\nfrom\nlangchain_community\n.\ndocument_loaders\n.\ncsv_loader\nimport\nCSVLoader\nloader\n=\nCSVLoader\n(\n.\n.\n.\n# <-- Integration specific parameters here\n)\ndata\n=\nloader\n.\nload\n(\n)\nWhen working with large datasets, you can use the\n.lazy_load\nmethod:\nfor\ndocument\nin\nloader\n.\nlazy_load\n(\n)\n:\nprint\n(\ndocument\n)\nRelated resources\nâ€‹\nPlease see the following resources for more information:\nHow-to guides for document loaders\nDocument API reference\nDocument loaders integrations\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/embedding_models/",
    "Conceptual guide\nEmbedding models\nOn this page\nEmbedding models\nPrerequisites\nDocuments\nNote\nThis conceptual overview focuses on text-based embedding models.\nEmbedding models can also be\nmultimodal\nthough such models are not currently supported by LangChain.\nImagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation.\nThis is the power of embedding models, which lie at the heart of many retrieval systems.\nEmbedding models transform human language into a format that machines can understand and compare with speed and accuracy.\nThese models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning.\nEmbeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.\nKey concepts\nâ€‹\n(1)\nEmbed text as a vector\n: Embeddings transform text into a numerical vector representation.\n(2)\nMeasure similarity\n: Embedding vectors can be compared using simple mathematical operations.\nEmbedding\nâ€‹\nHistorical context\nâ€‹\nThe landscape of embedding models has evolved significantly over the years.\nA pivotal moment came in 2018 when Google introduced\nBERT (Bidirectional Encoder Representations from Transformers)\n.\nBERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks.\nHowever, BERT wasn't optimized for generating sentence embeddings efficiently.\nThis limitation spurred the creation of\nSBERT (Sentence-BERT)\n, which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences.\nToday, the embedding model ecosystem is diverse, with numerous providers offering their own implementations.\nTo navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB)\nhere\nfor objective comparisons.\nFurther reading\nSee the\nseminal BERT paper\n.\nSee Cameron Wolfe's\nexcellent review\nof embedding models.\nSee the\nMassive Text Embedding Benchmark (MTEB)\nleaderboard for a comprehensive overview of embedding models.\nInterface\nâ€‹\nLangChain provides a universal interface for working with them, providing standard methods for common operations.\nThis common interface simplifies interaction with various embedding providers through two central methods:\nembed_documents\n: For embedding multiple texts (documents)\nembed_query\n: For embedding a single text (query)\nThis distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).\nTo illustrate, here's a practical example using LangChain's\n.embed_documents\nmethod to embed a list of strings:\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings_model\n=\nOpenAIEmbeddings\n(\n)\nembeddings\n=\nembeddings_model\n.\nembed_documents\n(\n[\n\"Hi there!\"\n,\n\"Oh, hello!\"\n,\n\"What's your name?\"\n,\n\"My friends call me World\"\n,\n\"Hello World!\"\n]\n)\nlen\n(\nembeddings\n)\n,\nlen\n(\nembeddings\n[\n0\n]\n)\n(\n5\n,\n1536\n)\nFor convenience, you can also use the\nembed_query\nmethod to embed a single text:\nquery_embedding\n=\nembeddings_model\n.\nembed_query\n(\n\"What is the meaning of life?\"\n)\nFurther reading\nSee the full list of\nLangChain embedding model integrations\n.\nSee these\nhow-to guides\nfor working with embedding models.\nIntegrations\nâ€‹\nLangChain offers many embedding model integrations which you can find\non the embedding models\nintegrations page.\nMeasure similarity\nâ€‹\nEach embedding is essentially a set of coordinates, often in a high-dimensional space.\nIn this space, the position of each point (embedding) reflects the meaning of its corresponding text.\nJust as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space.\nThis allows for intuitive comparisons between different pieces of text.\nBy reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure.\nSome common similarity metrics include:\nCosine Similarity\n: Measures the cosine of the angle between two vectors.\nEuclidean Distance\n: Measures the straight-line distance between two points.\nDot Product\n: Measures the projection of one vector onto another.\nThe choice of similarity metric should be chosen based on the model.\nAs an example,\nOpenAI suggests cosine similarity for their embeddings\n, which can be easily implemented:\nimport\nnumpy\nas\nnp\ndef\ncosine_similarity\n(\nvec1\n,\nvec2\n)\n:\ndot_product\n=\nnp\n.\ndot\n(\nvec1\n,\nvec2\n)\nnorm_vec1\n=\nnp\n.\nlinalg\n.\nnorm\n(\nvec1\n)\nnorm_vec2\n=\nnp\n.\nlinalg\n.\nnorm\n(\nvec2\n)\nreturn\ndot_product\n/\n(\nnorm_vec1\n*\nnorm_vec2\n)\nsimilarity\n=\ncosine_similarity\n(\nquery_result\n,\ndocument_result\n)\nprint\n(\n\"Cosine Similarity:\"\n,\nsimilarity\n)\nFurther reading\nSee Simon Willisonâ€™s\nnice blog post and video\non embeddings and similarity metrics.\nSee\nthis documentation\nfrom Google on similarity metrics to consider with embeddings.\nSee Pinecone's\nblog post\non similarity metrics.\nSee OpenAI's\nFAQ\non what similarity metric to use with OpenAI embeddings.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/evaluation/",
    "Conceptual guide\nEvaluation\nEvaluation\nEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\nIt involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\nThis process is vital for building reliable applications.\nLangSmith\nhelps with this process in a few ways:\nIt makes it easier to create and curate datasets via its tracing and annotation features\nIt provides an evaluation framework that helps you define metrics and run your app against your dataset\nIt allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code\nTo learn more, check out\nthis LangSmith guide\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/example_selectors/",
    "Conceptual guide\nExample selectors\nOn this page\nExample selectors\nPrerequisites\nChat models\nFew-shot prompting\nOverview\nâ€‹\nOne common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as\nfew-shot prompting\n.\nThis gives the\nlanguage model\nconcrete examples of how it should behave.\nSometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\nExample Selectors\nare classes responsible for selecting and then formatting examples into prompts.\nRelated resources\nâ€‹\nExample selector how-to guides\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/few_shot_prompting/",
    "Conceptual guide\nFew-shot prompting\nOn this page\nFew-shot prompting\nPrerequisites\nChat models\nOverview\nâ€‹\nOne of the most effective ways to improve model performance is to give a model examples of\nwhat you want it to do. The technique of adding example inputs and expected outputs\nto a model prompt is known as \"few-shot prompting\". The technique is based on the\nLanguage Models are Few-Shot Learners\npaper.\nThere are a few things to think about when doing few-shot prompting:\nHow are examples generated?\nHow many examples are in each prompt?\nHow are examples selected at runtime?\nHow are examples formatted in the prompt?\nHere are the considerations for each.\n1. Generating examples\nâ€‹\nThe first and most important step of few-shot prompting is coming up with a good dataset of examples. Good examples should be relevant at runtime, clear, informative, and provide information that was not already known to the model.\nAt a high-level, the basic ways to generate examples are:\nManual: a person/people generates examples they think are useful.\nBetter model: a better (presumably more expensive/slower) model's responses are used as examples for a worse (presumably cheaper/faster) model.\nUser feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).\nLLM feedback: same as user feedback but the process is automated by having models evaluate themselves.\nWhich approach is best depends on your task. For tasks where a small number of core principles need to be understood really well, it can be valuable hand-craft a few really good examples.\nFor tasks where the space of correct behaviors is broader and more nuanced, it can be useful to generate many examples in a more automated fashion so that there's a higher likelihood of there being some highly relevant examples for any runtime input.\nSingle-turn v.s. multi-turn examples\nAnother dimension to think about when generating examples is what the example is actually showing.\nThe simplest types of examples just have a user input and an expected model output. These are single-turn examples.\nOne more complex type of example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer.\nThis is called a multi-turn example. Multi-turn examples can be useful for more nuanced tasks where it's useful to show common errors and spell out exactly why they're wrong and what should be done instead.\n2. Number of examples\nâ€‹\nOnce we have a dataset of examples, we need to think about how many examples should be in each prompt.\nThe key tradeoff is that more examples generally improve performance, but larger prompts increase costs and latency.\nAnd beyond some threshold having too many examples can start to confuse the model.\nFinding the right number of examples is highly dependent on the model, the task, the quality of the examples, and your cost and latency constraints.\nAnecdotally, the better the model is the fewer examples it needs to perform well and the more quickly you hit steeply diminishing returns on adding more examples.\nBut, the best/only way to reliably answer this question is to run some experiments with different numbers of examples.\n3. Selecting examples\nâ€‹\nAssuming we are not adding our entire example dataset into each prompt, we need to have a way of selecting examples from our dataset based on a given input. We can do this:\nRandomly\nBy (semantic or keyword-based) similarity of the inputs\nBased on some other constraints, like token size\nLangChain has a number of\nExampleSelectors\nwhich make it easy to use any of these techniques.\nGenerally, selecting by semantic similarity leads to the best model performance. But how important this is is again model and task specific, and is something worth experimenting with.\n4. Formatting examples\nâ€‹\nMost state-of-the-art models these days are chat models, so we'll focus on formatting examples for those. Our basic options are to insert the examples:\nIn the system prompt as a string\nAs their own messages\nIf we insert our examples into the system prompt as a string, we'll need to make sure it's clear to the model where each example begins and which parts are the input versus output. Different models respond better to different syntaxes, like\nChatML\n, XML, TypeScript, etc.\nIf we insert our examples as messages, where each example is represented as a sequence of Human, AI messages, we might want to also assign\nnames\nto our messages like\n\"example_user\"\nand\n\"example_assistant\"\nto make it clear that these messages correspond to different actors than the latest input message.\nFormatting tool call examples\nOne area where formatting examples as messages can be tricky is when our example outputs have tool calls. This is because different models have different constraints on what types of message sequences are allowed when any tool calls are generated.\nSome models require that any AIMessage with tool calls be immediately followed by ToolMessages for every tool call,\nSome models additionally require that any ToolMessages be immediately followed by an AIMessage before the next HumanMessage,\nSome models require that tools are passed into the model if there are any tool calls / ToolMessages in the chat history.\nThese requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints.\nIn these cases it's especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.\nYou can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks\nhere\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/key_value_stores/",
    "Conceptual guide\nKey-value stores\nOn this page\nKey-value stores\nOverview\nâ€‹\nLangChain provides a key-value store interface for storing and retrieving data.\nLangChain includes a\nBaseStore\ninterface,\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\nmore specific\nBaseStore[str, bytes]\ninstance that stores binary data (referred to as a\nByteStore\n), and internally take care of\nencoding and decoding data for their specific needs.\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\nUsage\nâ€‹\nThe key-value store interface in LangChain is used primarily for:\nCaching\nembeddings\nvia\nCachedBackedEmbeddings\nto avoid recomputing embeddings for repeated queries or when re-indexing content.\nAs a simple\nDocument\npersistence layer in some retrievers.\nPlease see these how-to guides for more information:\nHow to cache embeddings guide\n.\nHow to retriever using multiple vectors per document\n.\nInterface\nâ€‹\nAll\nBaseStores\nsupport the following interface. Note that the interface allows for modifying\nmultiple\nkey-value pairs at once:\nmget(key: Sequence[str]) -> List[Optional[bytes]]\n: get the contents of multiple keys, returning\nNone\nif the key does not exist\nmset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None\n: set the contents of multiple keys\nmdelete(key: Sequence[str]) -> None\n: delete multiple keys\nyield_keys(prefix: Optional[str] = None) -> Iterator[str]\n: yield all keys in the store, optionally filtering by a prefix\nIntegrations\nâ€‹\nPlease reference the\nstores integration page\nfor a list of available key-value store integrations.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/lcel/",
    "Conceptual guide\nLangChain Expression Language (LCEL)\nOn this page\nLangChain Expression Language (LCEL)\nPrerequisites\nRunnable Interface\nThe\nL\nang\nC\nhain\nE\nxpression\nL\nanguage (LCEL) takes a\ndeclarative\napproach to building new\nRunnables\nfrom existing Runnables.\nThis means that you describe what\nshould\nhappen, rather than\nhow\nit should happen, allowing LangChain to optimize the run-time execution of the chains.\nWe often refer to a\nRunnable\ncreated using LCEL as a \"chain\". It's important to remember that a \"chain\" is\nRunnable\nand it implements the full\nRunnable Interface\n.\nnote\nThe\nLCEL cheatsheet\nshows common patterns that involve the Runnable interface and LCEL expressions.\nPlease see the following list of\nhow-to guides\nthat cover common tasks with LCEL.\nA list of built-in\nRunnables\ncan be found in the\nLangChain Core API Reference\n. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\nBenefits of LCEL\nâ€‹\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\nOptimized parallel execution\n: Run Runnables in parallel using\nRunnableParallel\nor run multiple inputs through a given chain in parallel using the\nRunnable Batch API\n. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\nGuaranteed Async support\n: Any chain built with LCEL can be run asynchronously using the\nRunnable Async API\n. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\nSimplify streaming\n: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a\nchat model\nor\nllm\ncomes out).\nOther benefits include:\nSeamless LangSmith tracing\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\nWith LCEL,\nall\nsteps are automatically logged to\nLangSmith\nfor maximum observability and debuggability.\nStandard API\n: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\nDeployable with LangServe\n: Chains built with LCEL can be deployed using for production use.\nShould I use LCEL?\nâ€‹\nLCEL is an\norchestration solution\n-- it allows LangChain to handle run-time execution of chains in an optimized way.\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of\nLangGraph\n.\nIn LangGraph, users define graphs that specify the application's flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\nHere are some guidelines:\nIf you are making a single LLM call, you don't need LCEL; instead call the underlying\nchat model\ndirectly.\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you're taking advantage of the LCEL benefits.\nIf you're building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use\nLangGraph\ninstead. Remember that you can always use LCEL within individual nodes in LangGraph.\nComposition Primitives\nâ€‹\nLCEL\nchains are built by composing existing\nRunnables\ntogether. The two main composition primitives are\nRunnableSequence\nand\nRunnableParallel\n.\nMany other composition primitives (e.g.,\nRunnableAssign\n) can be thought of as variations of these two primitives.\nnote\nYou can find a list of all composition primitives in the\nLangChain Core API Reference\n.\nRunnableSequence\nâ€‹\nRunnableSequence\nis a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableSequence\nchain\n=\nRunnableSequence\n(\n[\nrunnable1\n,\nrunnable2\n]\n)\nAPI Reference:\nRunnableSequence\nInvoking the\nchain\nwith some input:\nfinal_output\n=\nchain\n.\ninvoke\n(\nsome_input\n)\ncorresponds to the following:\noutput1\n=\nrunnable1\n.\ninvoke\n(\nsome_input\n)\nfinal_output\n=\nrunnable2\n.\ninvoke\n(\noutput1\n)\nnote\nrunnable1\nand\nrunnable2\nare placeholders for any\nRunnable\nthat you want to chain together.\nRunnableParallel\nâ€‹\nRunnableParallel\nis a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableParallel\nchain\n=\nRunnableParallel\n(\n{\n\"key1\"\n:\nrunnable1\n,\n\"key2\"\n:\nrunnable2\n,\n}\n)\nAPI Reference:\nRunnableParallel\nInvoking the\nchain\nwith some input:\nfinal_output\n=\nchain\n.\ninvoke\n(\nsome_input\n)\nWill yield a\nfinal_output\ndictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\n{\n\"key1\"\n:\nrunnable1\n.\ninvoke\n(\nsome_input\n)\n,\n\"key2\"\n:\nrunnable2\n.\ninvoke\n(\nsome_input\n)\n,\n}\nRecall, that the runnables are executed in parallel, so while the result is the same as\ndictionary comprehension shown above, the execution time is much faster.\nnote\nRunnableParallel\nsupports both synchronous and asynchronous execution (as all\nRunnables\ndo).\nFor synchronous execution,\nRunnableParallel\nuses a\nThreadPoolExecutor\nto run the runnables concurrently.\nFor asynchronous execution,\nRunnableParallel\nuses\nasyncio.gather\nto run the runnables concurrently.\nComposition Syntax\nâ€‹\nThe usage of\nRunnableSequence\nand\nRunnableParallel\nis so common that we created a shorthand syntax for using them. This helps\nto make the code more readable and concise.\nThe\n|\noperator\nâ€‹\nWe have\noverloaded\nthe\n|\noperator to create a\nRunnableSequence\nfrom two\nRunnables\n.\nchain\n=\nrunnable1\n|\nrunnable2\nis Equivalent to:\nchain\n=\nRunnableSequence\n(\n[\nrunnable1\n,\nrunnable2\n]\n)\nThe\n.pipe\nmethod\nâ€‹\nIf you have moral qualms with operator overloading, you can use the\n.pipe\nmethod instead. This is equivalent to the\n|\noperator.\nchain\n=\nrunnable1\n.\npipe\n(\nrunnable2\n)\nCoercion\nâ€‹\nLCEL applies automatic type coercion to make it easier to compose chains.\nIf you do not understand the type coercion, you can always use the\nRunnableSequence\nand\nRunnableParallel\nclasses directly.\nThis will make the code more verbose, but it will also make it more explicit.\nDictionary to RunnableParallel\nâ€‹\nInside an LCEL expression, a dictionary is automatically converted to a\nRunnableParallel\n.\nFor example, the following code:\nmapping\n=\n{\n\"key1\"\n:\nrunnable1\n,\n\"key2\"\n:\nrunnable2\n,\n}\nchain\n=\nmapping\n|\nrunnable3\nIt gets automatically converted to the following:\nchain\n=\nRunnableSequence\n(\n[\nRunnableParallel\n(\nmapping\n)\n,\nrunnable3\n]\n)\ncaution\nYou have to be careful because the\nmapping\ndictionary is not a\nRunnableParallel\nobject, it is just a dictionary. This means that the following code will raise an\nAttributeError\n:\nmapping\n.\ninvoke\n(\nsome_input\n)\nFunction to RunnableLambda\nâ€‹\nInside an LCEL expression, a function is automatically converted to a\nRunnableLambda\n.\ndef some_func(x):\nreturn x\nchain = some_func | runnable1\nIt gets automatically converted to the following:\nchain\n=\nRunnableSequence\n(\n[\nRunnableLambda\n(\nsome_func\n)\n,\nrunnable1\n]\n)\ncaution\nYou have to be careful because the lambda function is not a\nRunnableLambda\nobject, it is just a function. This means that the following code will raise an\nAttributeError\n:\nlambda\nx\n:\nx\n+\n1\n.\ninvoke\n(\nsome_input\n)\nLegacy chains\nâ€‹\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as\nLLMChain\nand\nConversationalRetrievalChain\n. Many of these legacy chains hide important details like prompts, and as a wider variety\nof viable models emerge, customization has become more and more important.\nIf you are currently using one of these legacy chains, please see\nthis guide for guidance on how to migrate\n.\nFor guides on how to do specific tasks with LCEL, check out\nthe relevant how-to guides\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/messages/",
    "Conceptual guide\nMessages\nOn this page\nMessages\nPrerequisites\nChat Models\nOverview\nâ€‹\nMessages are the unit of communication in\nchat models\n. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\nEach message has a\nrole\n(e.g., \"user\", \"assistant\") and\ncontent\n(e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.\nLangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\nWhat is inside a message?\nâ€‹\nA message typically consists of the following pieces of information:\nRole\n: The role of the message (e.g., \"user\", \"assistant\").\nContent\n: The content of the message (e.g., text, multimodal data).\nAdditional metadata: id, name,\ntoken usage\nand other model-specific metadata.\nRole\nâ€‹\nRoles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\nRole\nDescription\nsystem\nUsed to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.\nuser\nRepresents input from a user interacting with the model, usually in the form of text or other interactive input.\nassistant\nRepresents a response from the model, which can include text or a request to invoke tools.\ntool\nA message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support\ntool calling\n.\nfunction\n(legacy)\nThis is a legacy role, corresponding to OpenAI's legacy function-calling API.\ntool\nrole should be used instead.\nContent\nâ€‹\nThe content of a message text or a list of dictionaries representing\nmultimodal data\n(e.g., images, audio, video). The exact format of the content can vary between different chat model providers.\nCurrently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.\nFor more information see:\nSystemMessage\n-- for content which should be passed to direct the conversation\nHumanMessage\n-- for content in the input from the user.\nAIMessage\n-- for content in the response from the model.\nMultimodality\n-- for more information on multimodal content.\nOther Message Data\nâ€‹\nDepending on the chat model provider, messages can include other data such as:\nID\n: An optional unique identifier for the message.\nName\n: An optional\nname\nproperty which allows differentiate between different entities/speakers with the same role. Not all models support this!\nMetadata\n: Additional information about the message, such as timestamps, token usage, etc.\nTool Calls\n: A request made by the model to call one or more tools> See\ntool calling\nfor more information.\nConversation Structure\nâ€‹\nThe sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.\nFor example, a typical conversation structure might look like this:\nUser Message\n: \"Hello, how are you?\"\nAssistant Message\n: \"I'm doing well, thank you for asking.\"\nUser Message\n: \"Can you tell me a joke?\"\nAssistant Message\n: \"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\"\nPlease read the\nchat history\nguide for more information on managing chat history and ensuring that the conversation structure is correct.\nLangChain Messages\nâ€‹\nLangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\nLangChain messages are Python objects that subclass from a\nBaseMessage\n.\nThe five main message types are:\nSystemMessage\n: corresponds to\nsystem\nrole\nHumanMessage\n: corresponds to\nuser\nrole\nAIMessage\n: corresponds to\nassistant\nrole\nAIMessageChunk\n: corresponds to\nassistant\nrole, used for\nstreaming\nresponses\nToolMessage\n: corresponds to\ntool\nrole\nOther important messages include:\nRemoveMessage\n-- does not correspond to any role. This is an abstraction, mostly used in\nLangGraph\nto manage chat history.\nLegacy\nFunctionMessage\n: corresponds to the\nfunction\nrole in OpenAI's\nlegacy\nfunction-calling API.\nYou can find more information about\nmessages\nin the\nAPI Reference\n.\nSystemMessage\nâ€‹\nA\nSystemMessage\nis used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., \"This is a conversation about cooking\").\nDifferent chat providers may support system message in one of the following ways:\nThrough a \"system\" message role\n: In this case, a system message is included as part of the message sequence with the role explicitly set as \"system.\"\nThrough a separate API parameter for system instructions\n: Instead of being included as a message, system instructions are passed via a dedicated API parameter.\nNo support for system messages\n: Some models do not support system messages at all.\nMost major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the providerâ€™s capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.\nIf no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message's content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the\nlangchain-community\npackage) it is recommended to check the specific documentation for that model.\nHumanMessage\nâ€‹\nThe\nHumanMessage\ncorresponds to the\n\"user\"\nrole. A human message represents input from a user interacting with the model.\nText Content\nâ€‹\nMost chat models expect the user input to be in the form of text.\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nmodel\n.\ninvoke\n(\n[\nHumanMessage\n(\ncontent\n=\n\"Hello, how are you?\"\n)\n]\n)\nAPI Reference:\nHumanMessage\ntip\nWhen invoking a chat model with a string as input, LangChain will automatically convert the string into a\nHumanMessage\nobject. This is mostly useful for quick testing.\nmodel\n.\ninvoke\n(\n\"Hello, how are you?\"\n)\nMulti-modal Content\nâ€‹\nSome chat models accept multimodal inputs, such as images, audio, video, or files like PDFs.\nPlease see the\nmultimodality\nguide for more information.\nAIMessage\nâ€‹\nAIMessage\nis used to represent a message with the role\n\"assistant\"\n. This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment.\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nai_message\n=\nmodel\n.\ninvoke\n(\n[\nHumanMessage\n(\n\"Tell me a joke\"\n)\n]\n)\nai_message\n# <-- AIMessage\nAPI Reference:\nHumanMessage\nAn\nAIMessage\nhas the following attributes. The attributes which are\nstandardized\nare the ones that LangChain attempts to standardize across different chat model providers.\nraw\nfields are specific to the model provider and may vary.\nAttribute\nStandardized/Raw\nDescription\ncontent\nRaw\nUsually a string, but can be a list of content blocks. See\ncontent\nfor details.\ntool_calls\nStandardized\nTool calls associated with the message. See\ntool calling\nfor details.\ninvalid_tool_calls\nStandardized\nTool calls with parsing errors associated with the message. See\ntool calling\nfor details.\nusage_metadata\nStandardized\nUsage metadata for a message, such as\ntoken counts\n. See\nUsage Metadata API Reference\n.\nid\nStandardized\nAn optional unique identifier for the message, ideally provided by the provider/model that created the message. See\nMessage IDs\nfor details.\nresponse_metadata\nRaw\nResponse metadata, e.g., response headers, logprobs, token counts.\ncontent\nâ€‹\nThe\ncontent\nproperty of an\nAIMessage\nrepresents the response generated by the chat model.\nThe content is either:\ntext\n-- the norm for virtually all chat models.\nA\nlist of dictionaries\n-- Each dictionary represents a content block and is associated with a\ntype\n.\nUsed by Anthropic for surfacing agent thought process when doing\ntool calling\n.\nUsed by OpenAI for audio outputs. Please see\nmulti-modal content\nfor more information.\nimportant\nThe\ncontent\nproperty is\nnot\nstandardized across different chat model providers, mostly because there are\nstill few examples to generalize from.\nAIMessageChunk\nâ€‹\nIt is common to\nstream\nresponses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.\nIt is returned from the\nstream\n,\nastream\nand\nastream_events\nmethods of the chat model.\nFor example,\nfor\nchunk\nin\nmodel\n.\nstream\n(\n[\nHumanMessage\n(\n\"what color is the sky?\"\n)\n]\n)\n:\nprint\n(\nchunk\n)\nAIMessageChunk\nfollows nearly the same structure as\nAIMessage\n, but uses a different\nToolCallChunk\nto be able to stream tool calling in a standardized manner.\nAggregating\nâ€‹\nAIMessageChunks\nsupport the\n+\noperator to merge them into a single\nAIMessage\n. This is useful when you want to display the final response to the user.\nai_message\n=\nchunk1\n+\nchunk2\n+\nchunk3\n+\n.\n.\n.\nToolMessage\nâ€‹\nThis represents a message with role \"tool\", which contains the result of\ncalling a tool\n. In addition to\nrole\nand\ncontent\n, this message has:\na\ntool_call_id\nfield which conveys the id of the call to the tool that was called to produce this result.\nan\nartifact\nfield which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\nPlease see\ntool calling\nfor more information.\nRemoveMessage\nâ€‹\nThis is a special message type that does not correspond to any roles. It is used\nfor managing chat history in\nLangGraph\n.\nPlease see the following for more information on how to use the\nRemoveMessage\n:\nMemory conceptual guide\nHow to delete messages\n(Legacy) FunctionMessage\nâ€‹\nThis is a legacy message type, corresponding to OpenAI's legacy function-calling API.\nToolMessage\nshould be used instead to correspond to the updated tool-calling API.\nOpenAI Format\nâ€‹\nInputs\nâ€‹\nChat models also accept OpenAI's format as\ninputs\nto chat models:\nchat_model\n.\ninvoke\n(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hello, how are you?\"\n,\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"I'm doing well, thank you for asking.\"\n,\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Can you tell me a joke?\"\n,\n}\n]\n)\nOutputs\nâ€‹\nAt the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you\nneed OpenAI format for the output as well.\nThe\nconvert_to_openai_messages\nutility function can be used to convert from LangChain messages to OpenAI format.\nMessage IDs\nâ€‹\nLangChain messages include an optional\nid\nfield that serves as a unique identifier. Understanding when and how these IDs are assigned can be helpful for debugging, tracing, and working with message history.\nWhen Messages Get IDs\nâ€‹\nMessages receive IDs in the following scenarios:\nAutomatically assigned by LangChain:\nWhen generated through chat model invocation (\n.invoke()\n,\n.stream()\n,\n.astream()\n) with an active run manager/tracing context\nIDs follow the format:\nrun-$RUN_ID\n(e.g.,\nrun-ba48f958-6402-41a5-b461-5e250a4ebd36-0\n)\nrun-$RUN_ID-$IDX\n(e.g.,\nrun-ba48f958-6402-41a5-b461-5e250a4ebd36-1\n) when there are multiple generations from a single chat model invocation.\nProvider-assigned IDs (highest priority):\nWhen the model provider assigns its own ID to the message\nThese take precedence over LangChain-generated run IDs\nFormat varies by provider\nWhen Messages Don't Get IDs\nâ€‹\nMessages will\nnot\nreceive IDs in these situations:\nManual message creation\n: Messages created directly (e.g.,\nAIMessage(content=\"hello\")\n) without going through chat models\nNo run manager context\n: When there's no active callback/tracing infrastructure\nID Priority System\nâ€‹\nLangChain follows a clear precedence system for message IDs:\nProvider-assigned IDs\n(highest priority): IDs from the model provider\nLangChain run IDs\n(medium priority): IDs starting with\nrun-\nManual IDs\n(lowest priority): IDs explicitly set by users\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/multimodality/",
    "Conceptual guide\nMultimodality\nOn this page\nMultimodality\nOverview\nâ€‹\nMultimodality\nrefers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various components, allowing models and systems to handle and process a mix of these data types seamlessly.\nChat Models\n: These could, in theory, accept and generate multimodal inputs and outputs, handling a variety of data types like text, images, audio, and video.\nEmbedding Models\n: Embedding Models can represent multimodal content, embedding various forms of dataâ€”such as text, images, and audioâ€”into vector spaces.\nVector Stores\n: Vector stores could search over embeddings that represent multimodal data, enabling retrieval across different types of information.\nMultimodality in chat models\nâ€‹\nPre-requisites\nChat models\nMessages\nLangChain supports multimodal data as input to chat models:\nFollowing provider-specific formats\nAdhering to a cross-provider standard (see\nhow-to guides\nfor detail)\nHow to use multimodal models\nâ€‹\nUse the\nchat model integration table\nto identify which models support multimodality.\nReference the\nrelevant how-to guides\nfor specific examples of how to use multimodal models.\nWhat kind of multimodality is supported?\nâ€‹\nInputs\nâ€‹\nSome models can accept multimodal inputs, such as images, audio, video, or files.\nThe types of multimodal inputs supported depend on the model provider. For instance,\nOpenAI\n,\nAnthropic\n, and\nGoogle Gemini\nsupport documents like PDFs as inputs.\nThe gist of passing multimodal inputs to a chat model is to use content blocks that\nspecify a type and corresponding data. For example, to pass an image to a chat model\nas URL:\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nmessage\n=\nHumanMessage\n(\ncontent\n=\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the weather in this image:\"\n}\n,\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"url\"\n,\n\"url\"\n:\n\"https://...\"\n,\n}\n,\n]\n,\n)\nresponse\n=\nmodel\n.\ninvoke\n(\n[\nmessage\n]\n)\nAPI Reference:\nHumanMessage\nWe can also pass the image as in-line data:\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nmessage\n=\nHumanMessage\n(\ncontent\n=\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the weather in this image:\"\n}\n,\n{\n\"type\"\n:\n\"image\"\n,\n\"source_type\"\n:\n\"base64\"\n,\n\"data\"\n:\n\"<base64 string>\"\n,\n\"mime_type\"\n:\n\"image/jpeg\"\n,\n}\n,\n]\n,\n)\nresponse\n=\nmodel\n.\ninvoke\n(\n[\nmessage\n]\n)\nAPI Reference:\nHumanMessage\nTo pass a PDF file as in-line data (or URL, as supported by providers such as\nAnthropic), just change\n\"type\"\nto\n\"file\"\nand\n\"mime_type\"\nto\n\"application/pdf\"\n.\nSee the\nhow-to guides\nfor more detail.\nMost chat models that support multimodal\nimage\ninputs also accept those values in\nOpenAI's\nChat Completions format\n:\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nmessage\n=\nHumanMessage\n(\ncontent\n=\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the weather in this image:\"\n}\n,\n{\n\"type\"\n:\n\"image_url\"\n,\n\"image_url\"\n:\n{\n\"url\"\n:\nimage_url\n}\n}\n,\n]\n,\n)\nresponse\n=\nmodel\n.\ninvoke\n(\n[\nmessage\n]\n)\nAPI Reference:\nHumanMessage\nOtherwise, chat models will typically accept the native, provider-specific content\nblock format. See\nchat model integrations\nfor detail\non specific providers.\nOutputs\nâ€‹\nSome chat models support multimodal outputs, such as images and audio. Multimodal\noutputs will appear as part of the\nAIMessage\nresponse object. See for example:\nGenerating\naudio outputs\nwith OpenAI;\nGenerating\nimage outputs\nwith Google Gemini.\nTools\nâ€‹\nCurrently, no chat model is designed to work\ndirectly\nwith multimodal data in a\ntool call request\nor\nToolMessage\nresult.\nHowever, a chat model can easily interact with multimodal data by invoking tools with references (e.g., a URL) to the multimodal data, rather than the data itself. For example, any model capable of\ntool calling\ncan be equipped with tools to download and process images, audio, or video.\nMultimodality in embedding models\nâ€‹\nPrerequisites\nEmbedding Models\nEmbeddings\nare vector representations of data used for tasks like similarity search and retrieval.\nThe current\nembedding interface\nused in LangChain is optimized entirely for text-based data, and will\nnot\nwork with multimodal data.\nAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the embedding interface to accommodate other data types like images, audio, and video.\nMultimodality in vector stores\nâ€‹\nPrerequisites\nVector stores\nVector stores are databases for storing and retrieving embeddings, which are typically used in search and retrieval tasks. Similar to embeddings, vector stores are currently optimized for text-based data.\nAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/output_parsers/",
    "Conceptual guide\nOutput parsers\nOutput parsers\nnote\nThe information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\nMore and more models are supporting function (or tool) calling, which handles this automatically.\nIt is recommended to use function/tool calling rather than output parsing.\nSee documentation for that\nhere\n.\nOutput parser\nis responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\nUseful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:\nName\n: The name of the output parser\nSupports Streaming\n: Whether the output parser supports streaming.\nHas Format Instructions\n: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.\nCalls LLM\n: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.\nInput Type\n: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.\nOutput Type\n: The output type of the object returned by the parser.\nDescription\n: Our commentary on this output parser and when to use it.\nName\nSupports Streaming\nHas Format Instructions\nCalls LLM\nInput Type\nOutput Type\nDescription\nStr\nâœ…\nstr\n|\nMessage\nString\nParses texts from message objects. Useful for handling variable formats of message content (e.g., extracting text from content blocks).\nJSON\nâœ…\nâœ…\nstr\n|\nMessage\nJSON object\nReturns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.\nXML\nâœ…\nâœ…\nstr\n|\nMessage\ndict\nReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).\nCSV\nâœ…\nâœ…\nstr\n|\nMessage\nList[str]\nReturns a list of comma separated values.\nOutputFixing\nâœ…\nstr\n|\nMessage\nWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.\nRetryWithError\nâœ…\nstr\n|\nMessage\nWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.\nPydantic\nâœ…\nstr\n|\nMessage\npydantic.BaseModel\nTakes a user defined Pydantic model and returns data in that format.\nYAML\nâœ…\nstr\n|\nMessage\npydantic.BaseModel\nTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.\nPandasDataFrame\nâœ…\nstr\n|\nMessage\ndict\nUseful for doing operations with pandas DataFrames.\nEnum\nâœ…\nstr\n|\nMessage\nEnum\nParses response into one of the provided enum values.\nDatetime\nâœ…\nstr\n|\nMessage\ndatetime.datetime\nParses response into a datetime string.\nStructured\nâœ…\nstr\n|\nMessage\nDict[str, str]\nAn output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.\nFor specifics on how to use output parsers, see the\nrelevant how-to guides here\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/prompt_templates/",
    "Conceptual guide\nPrompt Templates\nOn this page\nPrompt Templates\nPrompt templates help to translate user input and parameters into instructions for a language model.\nThis can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\nPrompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\nPrompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.\nThe reason this PromptValue exists is to make it easy to switch between strings and messages.\nThere are a few different types of prompt templates:\nString PromptTemplates\nâ€‹\nThese prompt templates are used to format a single string, and generally are used for simpler inputs.\nFor example, a common way to construct and use a PromptTemplate is as follows:\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nprompt_template\n=\nPromptTemplate\n.\nfrom_template\n(\n\"Tell me a joke about {topic}\"\n)\nprompt_template\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"cats\"\n}\n)\nAPI Reference:\nPromptTemplate\nChatPromptTemplates\nâ€‹\nThese prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\nFor example, a common way to construct and use a ChatPromptTemplate is as follows:\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nprompt_template\n=\nChatPromptTemplate\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant\"\n)\n,\n(\n\"user\"\n,\n\"Tell me a joke about {topic}\"\n)\n]\n)\nprompt_template\n.\ninvoke\n(\n{\n\"topic\"\n:\n\"cats\"\n}\n)\nAPI Reference:\nChatPromptTemplate\nIn the above example, this ChatPromptTemplate will construct two messages when called.\nThe first is a system message, that has no variables to format.\nThe second is a HumanMessage, and will be formatted by the\ntopic\nvariable the user passes in.\nMessagesPlaceholder\nâ€‹\nThis prompt template is responsible for adding a list of messages in a particular place.\nIn the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\nBut what if we wanted the user to pass in a list of messages that we would slot into a particular spot?\nThis is how you use MessagesPlaceholder.\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nMessagesPlaceholder\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\n,\nAIMessage\nprompt_template\n=\nChatPromptTemplate\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant\"\n)\n,\nMessagesPlaceholder\n(\n\"msgs\"\n)\n]\n)\n# Simple example with one message\nprompt_template\n.\ninvoke\n(\n{\n\"msgs\"\n:\n[\nHumanMessage\n(\ncontent\n=\n\"hi!\"\n)\n]\n}\n)\n# More complex example with conversation history\nmessages_to_pass\n=\n[\nHumanMessage\n(\ncontent\n=\n\"What's the capital of France?\"\n)\n,\nAIMessage\n(\ncontent\n=\n\"The capital of France is Paris.\"\n)\n,\nHumanMessage\n(\ncontent\n=\n\"And what about Germany?\"\n)\n]\nformatted_prompt\n=\nprompt_template\n.\ninvoke\n(\n{\n\"msgs\"\n:\nmessages_to_pass\n}\n)\nprint\n(\nformatted_prompt\n)\nAPI Reference:\nChatPromptTemplate\n|\nMessagesPlaceholder\n|\nHumanMessage\n|\nAIMessage\nThis will produce a list of four messages total: the system message plus the three messages we passed in (two HumanMessages and one AIMessage).\nIf we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\nThis is useful for letting a list of messages be slotted into a particular spot.\nAn alternative way to accomplish the same thing without using the\nMessagesPlaceholder\nclass explicitly is:\nprompt_template\n=\nChatPromptTemplate\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant\"\n)\n,\n(\n\"placeholder\"\n,\n\"{msgs}\"\n)\n# <-- This is the changed part\n]\n)\nFor specifics on how to use prompt templates, see the\nrelevant how-to guides here\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/rag/",
    "Conceptual guide\nRetrieval augmented generation (RAG)\nOn this page\nRetrieval augmented generation (RAG)\nPrerequisites\nRetrieval\nOverview\nâ€‹\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances\nlanguage models\nby combining them with external knowledge bases.\nRAG addresses\na key limitation of models\n: models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\nKey concepts\nâ€‹\n(1)\nRetrieval system\n: Retrieve relevant information from a knowledge base.\n(2)\nAdding external knowledge\n: Pass retrieved information to a model.\nRetrieval system\nâ€‹\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like\nfine-tuning\nor continued pre-training.\nBoth are\ncostly\nand often\npoorly suited\nfor factual retrieval.\nUsing a retrieval system offers several advantages:\nUp-to-date information\n: RAG can access and utilize the latest data, keeping responses current.\nDomain-specific expertise\n: With domain-specific knowledge bases, RAG can provide answers in specific domains.\nReduced hallucination\n: Grounding responses in retrieved facts helps minimize false or invented information.\nCost-effective knowledge integration\n: RAG offers a more efficient alternative to expensive model fine-tuning.\nFurther reading\nSee our conceptual guide on\nretrieval\n.\nAdding external knowledge\nâ€‹\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\nReceive an input query.\nUse the retrieval system to search for relevant information based on the query.\nIncorporate the retrieved information into the prompt sent to the LLM.\nGenerate a response that leverages the retrieved context.\nAs an example, here's a simple RAG workflow that passes information from a\nretriever\nto a\nchat model\n:\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlangchain_core\n.\nmessages\nimport\nSystemMessage\n,\nHumanMessage\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt\n=\n\"\"\"You are an assistant for question-answering tasks.\nUse the following pieces of retrieved context to answer the question.\nIf you don't know the answer, just say that you don't know.\nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n# Define a question\nquestion\n=\n\"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n# Retrieve relevant documents\ndocs\n=\nretriever\n.\ninvoke\n(\nquestion\n)\n# Combine the documents into a single string\ndocs_text\n=\n\"\"\n.\njoin\n(\nd\n.\npage_content\nfor\nd\nin\ndocs\n)\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt\n=\nsystem_prompt\n.\nformat\n(\ncontext\n=\ndocs_text\n)\n# Create a model\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n,\ntemperature\n=\n0\n)\n# Generate a response\nquestions\n=\nmodel\n.\ninvoke\n(\n[\nSystemMessage\n(\ncontent\n=\nsystem_prompt_fmt\n)\n,\nHumanMessage\n(\ncontent\n=\nquestion\n)\n]\n)\nAPI Reference:\nSystemMessage\n|\nHumanMessage\nFurther reading\nRAG a deep area with many possible optimization and design choices:\nSee\nthis excellent blog\nfrom Cameron Wolfe for a comprehensive overview and history of RAG.\nSee our\nRAG how-to guides\n.\nSee our RAG\ntutorials\n.\nSee our RAG from Scratch course, with\ncode\nand\nvideo playlist\n.\nAlso, see our RAG from Scratch course\non Freecodecamp\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/retrieval/",
    "Conceptual guide\nRetrieval\nOn this page\nRetrieval\nPrerequisites\nRetrievers\nVector stores\nEmbeddings\nText splitters\nSecurity\nSome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\nThere are inherent risks in doing this.\nMake sure that your database connection permissions are scoped as narrowly as possible for your application's needs.\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\nFor more on general security best practices, see our\nsecurity guide\n.\nOverview\nâ€‹\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\nThese systems accommodate various data formats:\nUnstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\nStructured data is typically housed in relational or graph databases with defined schemas.\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\nThis translation enables more intuitive and flexible interactions with complex data structures.\nKey concepts\nâ€‹\n(1)\nQuery analysis\n: A process where models transform or construct search queries to optimize retrieval.\n(2)\nInformation retrieval\n: Search queries are used to fetch information from various retrieval systems.\nQuery analysis\nâ€‹\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\nQuery Re-writing\n: Queries can be re-written or expanded to improve semantic or lexical searches.\nQuery Construction\n: Search indexes may require structured queries (e.g., SQL for databases).\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\nQuery re-writing\nâ€‹\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\nQuery Clarification\n: Models can rephrase ambiguous or poorly worded queries for clarity.\nSemantic Understanding\n: They can capture the intent behind a query, going beyond literal keyword matching.\nQuery Expansion\n: Models can generate related terms or concepts to broaden the search scope.\nComplex Query Handling\n: They can break down multi-part questions into simpler sub-queries.\nVarious techniques have been developed to leverage models for query re-writing, including:\nName\nWhen to use\nDescription\nMulti-query\nWhen you want to ensure high recall in retrieval by providing multiple phrasings of a question.\nRewrite the user question with multiple phrasings, retrieve documents for each rewritten question, return the unique documents for all queries.\nDecomposition\nWhen a question can be broken down into smaller subproblems.\nDecompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).\nStep-back\nWhen a higher-level conceptual understanding is required.\nFirst prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question.\nPaper\n.\nHyDE\nIf you have challenges retrieving relevant documents using the raw user inputs.\nUse an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches.\nPaper\n.\nAs an example, query decomposition can simply be accomplished using prompting and a structured output that enforces a list of sub-questions.\nThese can then be run sequentially or in parallel on a downstream retrieval system.\nfrom\ntyping\nimport\nList\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlangchain_core\n.\nmessages\nimport\nSystemMessage\n,\nHumanMessage\n# Define a pydantic model to enforce the output structure\nclass\nQuestions\n(\nBaseModel\n)\n:\nquestions\n:\nList\n[\nstr\n]\n=\nField\n(\ndescription\n=\n\"A list of sub-questions related to the input query.\"\n)\n# Create an instance of the model and enforce the output structure\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n,\ntemperature\n=\n0\n)\nstructured_model\n=\nmodel\n.\nwith_structured_output\n(\nQuestions\n)\n# Define the system prompt\nsystem\n=\n\"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answered independently. \\n\"\"\"\n# Pass the question to the model\nquestion\n=\n\"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\nquestions\n=\nstructured_model\n.\ninvoke\n(\n[\nSystemMessage\n(\ncontent\n=\nsystem\n)\n]\n+\n[\nHumanMessage\n(\ncontent\n=\nquestion\n)\n]\n)\nAPI Reference:\nSystemMessage\n|\nHumanMessage\ntip\nSee our RAG from Scratch videos for a few different specific approaches:\nMulti-query\nDecomposition\nStep-back\nHyDE\nQuery construction\nâ€‹\nQuery analysis also can focus on translating natural language queries into specialized query languages or filters.\nThis translation is crucial for effectively interacting with various types of databases that house structured or semi-structured data.\nStructured Data examples\n: For relational and graph databases, Domain-Specific Languages (DSLs) are used to query data.\nText-to-SQL\n:\nConverts natural language to SQL\nfor relational databases.\nText-to-Cypher\n:\nConverts natural language to Cypher\nfor graph databases.\nSemi-structured Data examples\n: For vectorstores, queries can combine semantic search with metadata filtering.\nNatural Language to Metadata Filters\n: Converts user queries into\nappropriate metadata filters\n.\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\nName\nWhen to Use\nDescription\nSelf Query\nIf users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.\nThis uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).\nText to SQL\nIf users are asking questions that require information housed in a relational database, accessible via SQL.\nThis uses an LLM to transform user input into a SQL query.\nText-to-Cypher\nIf users are asking questions that require information housed in a graph database, accessible via Cypher.\nThis uses an LLM to transform user input into a Cypher query.\nAs an example, here is how to use the\nSelfQueryRetriever\nto convert natural language queries into metadata filters.\nmetadata_field_info\n=\nschema_for_metadata\ndocument_content_description\n=\n\"Brief summary of a movie\"\nllm\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\nretriever\n=\nSelfQueryRetriever\n.\nfrom_llm\n(\nllm\n,\nvectorstore\n,\ndocument_content_description\n,\nmetadata_field_info\n,\n)\nFurther reading\nSee our tutorials on\ntext-to-SQL\n,\ntext-to-Cypher\n, and\nquery analysis for metadata filters\n.\nSee our\nblog post overview\n.\nSee our RAG from Scratch video on\nquery construction\n.\nInformation retrieval\nâ€‹\nCommon retrieval systems\nâ€‹\nLexical search indexes\nâ€‹\nMany search engines are based upon matching words in a query to the words in each document.\nThis approach is called lexical retrieval, using search\nalgorithms that are typically based upon word frequencies\n.\nThe intution is simple: a word appears frequently both in the userâ€™s query and a particular document, then this document might be a good match.\nThe particular data structure used to implement this is often an\ninverted index\n.\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\nBM25\nand\nTF-IDF\nare\ntwo popular lexical search algorithms\n.\nFurther reading\nSee the\nBM25\nretriever integration.\nSee the\nElasticsearch\nretriever integration.\nVector indexes\nâ€‹\nVector indexes are an alternative way to index and store unstructured data.\nSee our conceptual guide on\nvectorstores\nfor a detailed overview.\nIn short, rather than using word frequencies, vectorstores use an\nembedding model\nto compress documents into high-dimensional vector representation.\nThis allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.\nFurther reading\nSee our\nhow-to guide\nfor more details on working with vectorstores.\nSee our\nlist of vectorstore integrations\n.\nSee Cameron Wolfe's\nblog post\non the basics of vector search.\nRelational databases\nâ€‹\nRelational databases are a fundamental type of structured data storage used in many applications.\nThey organize data into tables with predefined schemas, where each table represents an entity or relationship.\nData is stored in rows (records) and columns (attributes), allowing for efficient querying and manipulation through SQL (Structured Query Language).\nRelational databases excel at maintaining data integrity, supporting complex queries, and handling relationships between different data entities.\nFurther reading\nSee our\ntutorial\nfor working with SQL databases.\nSee our\nSQL database toolkit\n.\nGraph databases\nâ€‹\nGraph databases are a specialized type of database designed to store and manage highly interconnected data.\nUnlike traditional relational databases, graph databases use a flexible structure consisting of nodes (entities), edges (relationships), and properties.\nThis structure allows for efficient representation and querying of complex, interconnected data.\nGraph databases store data in a graph structure, with nodes, edges, and properties.\nThey are particularly useful for storing and querying complex relationships between data points, such as social networks, supply-chain management, fraud detection, and recommendation services\nFurther reading\nSee our\ntutorial\nfor working with graph databases.\nSee our\nlist of graph database integrations\n.\nSee Neo4j's\nstarter kit for LangChain\n.\nRetriever\nâ€‹\nLangChain provides a unified interface for interacting with various retrieval systems through the\nretriever\nconcept. The interface is straightforward:\nInput: A query (string)\nOutput: A list of documents (standardized LangChain\nDocument\nobjects)\nYou can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages.\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes.\nRegardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple\ninvoke\nmethod:\ndocs\n=\nretriever\n.\ninvoke\n(\nquery\n)\nFurther reading\nSee our\nconceptual guide on retrievers\n.\nSee our\nhow-to guide\non working with retrievers.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/retrievers/",
    "Conceptual guide\nRetrievers\nOn this page\nRetrievers\nPrerequisites\nVector stores\nEmbeddings\nText splitters\nOverview\nâ€‹\nMany different types of retrieval systems exist, including vectorstores, graph databases, and relational databases.\nWith the rise on popularity of large language models, retrieval systems have become an important component in AI application (e.g.,\nRAG\n).\nBecause of their importance and variability, LangChain provides a uniform interface for interacting with different types of retrieval systems.\nThe LangChain\nretriever\ninterface is straightforward:\nInput: A query (string)\nOutput: A list of documents (standardized LangChain\nDocument\nobjects)\nKey concept\nâ€‹\nAll retrievers implement a simple interface for retrieving documents using natural language queries.\nInterface\nâ€‹\nThe only requirement for a retriever is the ability to accepts a query and return documents.\nIn particular,\nLangChain's retriever class\nonly requires that the\n_get_relevant_documents\nmethod is implemented, which takes a\nquery: str\nand returns a list of\nDocument\nobjects that are most relevant to the query.\nThe underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application.\nA LangChain retriever is a\nrunnable\n, which is a standard interface for LangChain components.\nThis means that it has a few common methods, including\ninvoke\n, that are used to interact with it. A retriever can be invoked with a query:\ndocs\n=\nretriever\n.\ninvoke\n(\nquery\n)\nRetrievers return a list of\nDocument\nobjects, which have two attributes:\npage_content\n: The content of this document. Currently is a string.\nmetadata\n: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc).\nFurther reading\nSee our\nhow-to guide\non building your own custom retriever.\nCommon types\nâ€‹\nDespite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.\nSearch apis\nâ€‹\nIt's important to note that retrievers don't need to actually\nstore\ndocuments.\nFor example, we can build retrievers on top of search APIs that simply return search results!\nSee our retriever integrations with\nAmazon Kendra\nor\nWikipedia Search\n.\nRelational or graph database\nâ€‹\nRetrievers can be built on top of relational or graph databases.\nIn these cases,\nquery analysis\ntechniques to construct a structured query from natural language is critical.\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.\nFurther reading\nSee our\ntutorial\nfor context on how to build a retriever using a SQL database and text-to-SQL.\nSee our\ntutorial\nfor context on how to build a retriever using a graph database and text-to-Cypher.\nLexical search\nâ€‹\nAs discussed in our conceptual review of\nretrieval\n, many search engines are based upon matching words in a query to the words in each document.\nBM25\nand\nTF-IDF\nare\ntwo popular lexical search algorithms\n.\nLangChain has retrievers for many popular lexical search algorithms / engines.\nFurther reading\nSee the\nBM25\nretriever integration.\nSee the\nTF-IDF\nretriever integration.\nSee the\nElasticsearch\nretriever integration.\nVector store\nâ€‹\nVector stores\nare a powerful and efficient way to index and retrieve unstructured data.\nA vectorstore can be used as a retriever by calling the\nas_retriever()\nmethod.\nvectorstore\n=\nMyVectorStore\n(\n)\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\nAdvanced retrieval patterns\nâ€‹\nEnsemble\nâ€‹\nBecause the retriever interface is so simple, returning a list of\nDocument\nobjects given a search query, it is possible to combine multiple retrievers using ensembling.\nThis is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents.\nIt is easy to create an\nensemble retriever\nthat combines multiple retrievers with linear weighted scores:\n# Initialize the ensemble retriever\nensemble_retriever\n=\nEnsembleRetriever\n(\nretrievers\n=\n[\nbm25_retriever\n,\nvector_store_retriever\n]\n,\nweights\n=\n[\n0.5\n,\n0.5\n]\n)\nWhen ensembling, how do we combine search results from many retrievers?\nThis motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as\nReciprocal Rank Fusion (RRF)\n.\nSource document retention\nâ€‹\nMany retrievers utilize some kind of index to make documents easily searchable.\nThe process of indexing can include a transformation step (e.g., vectorstores often use document splitting).\nWhatever transformation is used, can be very useful to retain a link between the\ntransformed document\nand the original, giving the retriever the ability to return the\noriginal\ndocument.\nThis is particularly useful in AI applications, because it ensures no loss in document context for the model.\nFor example, you may use small chunk size for indexing documents in a vectorstore.\nIf you return\nonly\nthe chunks as the retrieval result, then the model will have lost the original document context for the chunks.\nLangChain has two different retrievers that can be used to address this challenge.\nThe\nMulti-Vector\nretriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document.\nThe\nParentDocument\nretriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.\nName\nIndex Type\nUses an LLM\nWhen to Use\nDescription\nParentDocument\nVector store + Document Store\nNo\nIf your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.\nThis involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).\nMulti Vector\nVector store + Document Store\nSometimes during indexing\nIf you are able to extract information from documents that you think is more relevant to index than the text itself.\nThis involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.\nFurther reading\nSee our\nhow-to guide\non using the ParentDocument retriever.\nSee our\nhow-to guide\non using the MultiVector retriever.\nSee our RAG from Scratch video on the\nmulti vector retriever\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/runnables/",
    "Conceptual guide\nRunnable interface\nOn this page\nRunnable interface\nThe Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as\nlanguage models\n,\noutput parsers\n,\nretrievers\n,\ncompiled LangGraph graphs\nand more.\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\nRelated Resources\nThe\n\"Runnable\" Interface API Reference\nprovides a detailed overview of the Runnable interface and its methods.\nA list of built-in\nRunnables\ncan be found in the\nLangChain Core API Reference\n. Many of these Runnables are useful when composing custom \"chains\" in LangChain using the\nLangChain Expression Language (LCEL)\n.\nOverview of runnable interface\nâ€‹\nThe Runnable way defines a standard interface that allows a Runnable component to be:\nInvoked\n: A single input is transformed into an output.\nBatched\n: Multiple inputs are efficiently transformed into outputs.\nStreamed\n: Outputs are streamed as they are produced.\nInspected: Schematic information about Runnable's input, output, and configuration can be accessed.\nComposed: Multiple Runnables can be composed to work together using\nthe LangChain Expression Language (LCEL)\nto create complex pipelines.\nPlease review the\nLCEL Cheatsheet\nfor some common patterns that involve the Runnable interface and LCEL expressions.\nOptimized parallel execution (batch)\nâ€‹\nLangChain Runnables offer a built-in\nbatch\n(and\nbatch_as_completed\n) API that allow you to process multiple inputs in parallel.\nUsing these methods can significantly improve performance when needing to process multiple independent inputs, as the\nprocessing can be done in parallel instead of sequentially.\nThe two batching options are:\nbatch\n: Process multiple inputs in parallel, returning results in the same order as the inputs.\nbatch_as_completed\n: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.\nThe default implementation of\nbatch\nand\nbatch_as_completed\nuse a thread pool executor to run the\ninvoke\nmethod in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.\nSome Runnables may provide their own implementations of\nbatch\nand\nbatch_as_completed\nthat are optimized for their specific use case (e.g.,\nrely on a\nbatch\nAPI provided by a model provider).\nnote\nThe async versions of\nabatch\nand\nabatch_as_completed\nrelies on asyncio's\ngather\nand\nas_completed\nfunctions to run the\nainvoke\nmethod in parallel.\ntip\nWhen processing a large number of inputs using\nbatch\nor\nbatch_as_completed\n, users may want to control the maximum number of parallel calls. This can be done by setting the\nmax_concurrency\nattribute in the\nRunnableConfig\ndictionary. See the\nRunnableConfig\nfor more information.\nChat Models also have a built-in\nrate limiter\nthat can be used to control the rate at which requests are made.\nAsynchronous support\nâ€‹\nRunnables expose an asynchronous API, allowing them to be called using the\nawait\nsyntax in Python. Asynchronous methods can be identified by the \"a\" prefix (e.g.,\nainvoke\n,\nabatch\n,\nastream\n,\nabatch_as_completed\n).\nPlease refer to the\nAsync Programming with LangChain\nguide for more details.\nStreaming APIs\nâ€‹\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\nRunnables expose the following three streaming APIs:\nsync\nstream\nand async\nastream\n: yields the output a Runnable as it is generated.\nThe async\nastream_events\n: a more advanced streaming API that allows streaming intermediate steps and final output\nThe\nlegacy\nasync\nastream_log\n: a legacy streaming API that streams intermediate steps and final output\nPlease refer to the\nStreaming Conceptual Guide\nfor more details on how to stream in LangChain.\nInput and output types\nâ€‹\nEvery\nRunnable\nis characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.\nRunnable methods that result in the execution of the Runnable (e.g.,\ninvoke\n,\nbatch\n,\nstream\n,\nastream_events\n) work with these input and output types.\ninvoke: Accepts an input and returns an output.\nbatch: Accepts a list of inputs and returns a list of outputs.\nstream: Accepts an input and returns a generator that yields outputs.\nThe\ninput type\nand\noutput type\nvary by component:\nComponent\nInput Type\nOutput Type\nPrompt\ndictionary\nPromptValue\nChatModel\na string, list of chat messages or a PromptValue\nChatMessage\nLLM\na string, list of chat messages or a PromptValue\nString\nOutputParser\nthe output of an LLM or ChatModel\nDepends on the parser\nRetriever\na string\nList of Documents\nTool\na string or dictionary, depending on the tool\nDepends on the tool\nPlease refer to the individual component documentation for more information on the input and output types and how to use them.\nInspecting schemas\nâ€‹\nnote\nThis is an advanced feature that is unnecessary for most users. You should probably\nskip this section unless you have a specific need to inspect the schema of a Runnable.\nIn more advanced use cases, you may want to programmatically\ninspect\nthe Runnable and determine what input and output types the Runnable expects and produces.\nThe Runnable interface provides methods to get the\nJSON Schema\nof the input and output types of a Runnable, as well as\nPydantic schemas\nfor the input and output types.\nThese APIs are mostly used internally for unit-testing and by\nLangServe\nwhich uses the APIs for input validation and generation of\nOpenAPI documentation\n.\nIn addition, to the input and output types, some Runnables have been set up with additional run time configuration options.\nThere are corresponding APIs to get the Pydantic Schema and JSON Schema of the configuration options for the Runnable.\nPlease see the\nConfigurable Runnables\nsection for more information.\nMethod\nDescription\nget_input_schema\nGives the Pydantic Schema of the input schema for the Runnable.\nget_output_schema\nGives the Pydantic Schema of the output schema for the Runnable.\nconfig_schema\nGives the Pydantic Schema of the config schema for the Runnable.\nget_input_jsonschema\nGives the JSONSchema of the input schema for the Runnable.\nget_output_jsonschema\nGives the JSONSchema of the output schema for the Runnable.\nget_config_jsonschema\nGives the JSONSchema of the config schema for the Runnable.\nWith_types\nâ€‹\nLangChain will automatically try to infer the input and output types of a Runnable based on available information.\nCurrently, this inference does not work well for more complex Runnables that are built using\nLCEL\ncomposition, and the inferred input and / or output types may be incorrect. In these cases, we recommend that users override the inferred input and output types using the\nwith_types\nmethod (\nAPI Reference\n).\nRunnableConfig\nâ€‹\nAny of the methods that are used to execute the runnable (e.g.,\ninvoke\n,\nbatch\n,\nstream\n,\nastream_events\n) accept a second argument called\nRunnableConfig\n(\nAPI Reference\n). This argument is a dictionary that contains configuration for the Runnable that will be used\nat run time during the execution of the runnable.\nA\nRunnableConfig\ncan have any of the following properties defined:\nAttribute\nDescription\nrun_name\nName used for the given Runnable (not inherited).\nrun_id\nUnique identifier for this call. sub-calls will get their own unique run ids.\ntags\nTags for this call and any sub-calls.\nmetadata\nMetadata for this call and any sub-calls.\ncallbacks\nCallbacks for this call and any sub-calls.\nmax_concurrency\nMaximum number of parallel calls to make (e.g., used by batch).\nrecursion_limit\nMaximum number of times a call can recurse (e.g., used by Runnables that return Runnables)\nconfigurable\nRuntime values for configurable attributes of the Runnable.\nPassing\nconfig\nto the\ninvoke\nmethod is done like so:\nsome_runnable\n.\ninvoke\n(\nsome_input\n,\nconfig\n=\n{\n'run_name'\n:\n'my_run'\n,\n'tags'\n:\n[\n'tag1'\n,\n'tag2'\n]\n,\n'metadata'\n:\n{\n'key'\n:\n'value'\n}\n}\n)\nPropagation of RunnableConfig\nâ€‹\nMany\nRunnables\nare composed of other Runnables, and it is important that the\nRunnableConfig\nis propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls.\nIf this were not the case, it would be impossible to set and propagate\ncallbacks\nor other configuration values like\ntags\nand\nmetadata\nwhich\nare expected to be inherited by all sub-calls.\nThere are two main patterns by which new\nRunnables\nare created:\nDeclaratively using\nLangChain Expression Language (LCEL)\n:\nchain\n=\nprompt\n|\nchat_model\n|\noutput_parser\nUsing a\ncustom Runnable\n(e.g.,\nRunnableLambda\n) or using the\n@tool\ndecorator:\ndef\nfoo\n(\ninput\n)\n:\n# Note that .invoke() is used directly here\nreturn\nbar_runnable\n.\ninvoke\n(\ninput\n)\nfoo_runnable\n=\nRunnableLambda\n(\nfoo\n)\nLangChain will try to propagate\nRunnableConfig\nautomatically for both of the patterns.\nFor handling the second pattern, LangChain relies on Python's\ncontextvars\n.\nIn Python 3.11 and above, this works out of the box, and you do not need to do anything special to propagate the\nRunnableConfig\nto the sub-calls.\nIn Python 3.9 and 3.10, if you are using\nasync code\n, you need to manually pass the\nRunnableConfig\nthrough to the\nRunnable\nwhen invoking it.\nThis is due to a limitation in\nasyncio's tasks\nin Python 3.9 and 3.10 which did\nnot accept a\ncontext\nargument.\nPropagating the\nRunnableConfig\nmanually is done like so:\nasync\ndef\nfoo\n(\ninput\n,\nconfig\n)\n:\n# <-- Note the config argument\nreturn\nawait\nbar_runnable\n.\nainvoke\n(\ninput\n,\nconfig\n=\nconfig\n)\nfoo_runnable\n=\nRunnableLambda\n(\nfoo\n)\ncaution\nWhen using Python 3.10 or lower and writing async code,\nRunnableConfig\ncannot be propagated\nautomatically, and you will need to do it manually! This is a common pitfall when\nattempting to stream data using\nastream_events\nand\nastream_log\nas these methods\nrely on proper propagation of\ncallbacks\ndefined inside of\nRunnableConfig\n.\nSetting custom run name, tags, and metadata\nâ€‹\nThe\nrun_name\n,\ntags\n, and\nmetadata\nattributes of the\nRunnableConfig\ndictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.\nThe\nrun_name\nis a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.\nThe\ntags\nand\nmetadata\nattributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.\nUsing these attributes can be useful for tracking and debugging runs, as they will be surfaced in\nLangSmith\nas trace attributes that you can\nfilter and search on.\nThe attributes will also be propagated to\ncallbacks\n, and will appear in streaming APIs like\nastream_events\nas part of each event in the stream.\nRelated\nHow-to trace with LangChain\nSetting run id\nâ€‹\nnote\nThis is an advanced feature that is unnecessary for most users.\nYou may need to set a custom\nrun_id\nfor a given run, in case you want\nto reference it later or correlate it with other systems.\nThe\nrun_id\nMUST be a valid UUID string and\nunique\nfor each run. It is used to identify\nthe parent run, sub-class will get their own unique run ids automatically.\nTo set a custom\nrun_id\n, you can pass it as a key-value pair in the\nconfig\ndictionary when invoking the Runnable:\nimport\nuuid\nrun_id\n=\nuuid\n.\nuuid4\n(\n)\nsome_runnable\n.\ninvoke\n(\nsome_input\n,\nconfig\n=\n{\n'run_id'\n:\nrun_id\n}\n)\n# Do something with the run_id\nSetting recursion limit\nâ€‹\nnote\nThis is an advanced feature that is unnecessary for most users.\nSome Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a\nrecursion_limit\nin the\nRunnableConfig\ndictionary. This will limit the number of times a Runnable can recurse.\nSetting max concurrency\nâ€‹\nIf using the\nbatch\nor\nbatch_as_completed\nmethods, you can set the\nmax_concurrency\nattribute in the\nRunnableConfig\ndictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.\ntip\nIf you're trying to rate limit the number of requests made by a\nChat Model\n, you can use the built-in\nrate limiter\ninstead of setting\nmax_concurrency\n, which will be more effective.\nSee the\nHow to handle rate limits\nguide for more information.\nSetting configurable\nâ€‹\nThe\nconfigurable\nfield is used to pass runtime values for configurable attributes of the Runnable.\nIt is used frequently in\nLangGraph\nwith\nLangGraph Persistence\nand\nmemory\n.\nIt is used for a similar purpose in\nRunnableWithMessageHistory\nto specify either\na\nsession_id\n/\nconversation_id\nto keep track of conversation history.\nIn addition, you can use it to specify any custom configuration options to pass to any\nConfigurable Runnable\nthat they create.\nSetting callbacks\nâ€‹\nUse this option to configure\ncallbacks\nfor the runnable at\nruntime. The callbacks will be passed to all sub-calls made by the runnable.\nsome_runnable\n.\ninvoke\n(\nsome_input\n,\n{\n\"callbacks\"\n:\n[\nSomeCallbackHandler\n(\n)\n,\nAnotherCallbackHandler\n(\n)\n,\n]\n}\n)\nPlease read the\nCallbacks Conceptual Guide\nfor more information on how to use callbacks in LangChain.\nimportant\nIf you're using Python 3.9 or 3.10 in an async environment, you must propagate\nthe\nRunnableConfig\nmanually to sub-calls in some cases. Please see the\nPropagating RunnableConfig\nsection for more information.\nCreating a runnable from a function\nâ€‹\nYou may need to create a custom Runnable that runs arbitrary logic. This is especially\nuseful if using\nLangChain Expression Language (LCEL)\nto compose\nmultiple Runnables and you need to add custom processing logic in one of the steps.\nThere are two ways to create a custom Runnable from a function:\nRunnableLambda\n: Use this for simple transformations where streaming is not required.\nRunnableGenerator\n: use this for more complex transformations when streaming is needed.\nSee the\nHow to run custom functions\nguide for more information on how to use\nRunnableLambda\nand\nRunnableGenerator\n.\nimportant\nUsers should not try to subclass Runnables to create a new custom Runnable. It is\nmuch more complex and error-prone than simply using\nRunnableLambda\nor\nRunnableGenerator\n.\nConfigurable runnables\nâ€‹\nnote\nThis is an advanced feature that is unnecessary for most users.\nIt helps with configuration of large \"chains\" created using the\nLangChain Expression Language (LCEL)\nand is leveraged by\nLangServe\nfor deployed Runnables.\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\nconfigurable_fields\n: This method allows you to configure specific\nattributes\nin a Runnable. For example, the\ntemperature\nattribute of a chat model.\nconfigurable_alternatives\n: This method enables you to specify\nalternative\nRunnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.\nSee the\nHow to configure runtime chain internals\nguide for more information on how to configure runtime chain internals.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/streaming/",
    "Conceptual guide\nStreaming\nOn this page\nStreaming\nPrerequisites\nRunnable Interface\nChat Models\nStreaming\nis crucial for enhancing the responsiveness of applications built on\nLLMs\n. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\nOverview\nâ€‹\nGenerating full responses from\nLLMs\noften incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.\nIn this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.\nWhat to stream in LLM applications\nâ€‹\nIn applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:\n1. Streaming LLM outputs\nâ€‹\nThe most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.\n2. Streaming pipeline or workflow progress\nâ€‹\nBeyond just streaming LLM output, itâ€™s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:\nIn LangGraph Workflows:\nWith\nLangGraph\n, workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the\ngraph state\nas individual\nnodes\nrequest updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.\nIn LCEL Pipelines:\nStreaming updates from an\nLCEL\npipeline involves capturing progress from individual\nsub-runnables\n. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.\nStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.\n3. Streaming custom data\nâ€‹\nIn some cases, you may need to stream\ncustom data\nthat goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.\nStreaming APIs\nâ€‹\nLangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the\nRunnable Interface\n, including\nLLMs\n,\ncompiled LangGraph graphs\n, and any Runnable generated with\nLCEL\n.\nsync\nstream\nand async\nastream\n: Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.\nThe async only\nastream_events\n: Use this API to get access to custom events and intermediate outputs from LLM  applications built entirely with\nLCEL\n. Note that this API is available, but not needed when working with LangGraph.\nnote\nIn addition, there is a\nlegacy\nasync\nastream_log\nAPI. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.\nstream()\nand\nastream()\nâ€‹\nThe\nstream()\nmethod returns an iterator that yields chunks of output synchronously as they are produced. You can use a\nfor\nloop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.\nThe type of chunk yielded by the\nstream()\nand\nastream()\nmethods depends on the component being streamed. For example, when streaming from an\nLLM\neach component will be an\nAIMessageChunk\n; however, for other components, the chunk may be different.\nThe\nstream()\nmethod returns an iterator that yields these chunks as they are produced. For example,\nfor\nchunk\nin\ncomponent\n.\nstream\n(\nsome_input\n)\n:\n# IMPORTANT: Keep the processing of each chunk as efficient as possible.\n# While you're processing the current chunk, the upstream component is\n# waiting to produce the next one. For example, if working with LangGraph,\n# graph execution is paused while the current chunk is being processed.\n# In extreme cases, this could even result in timeouts (e.g., when llm outputs are\n# streamed from an API that has a timeout).\nprint\n(\nchunk\n)\nThe\nasynchronous version\n,\nastream()\n, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.\nUsage with chat models\nâ€‹\nWhen using\nstream()\nor\nastream()\nwith chat models, the output is streamed as\nAIMessageChunks\nas it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.\nUsage with LangGraph\nâ€‹\nLangGraph\ncompiled graphs are\nRunnables\nand support the standard streaming APIs.\nWhen using the\nstream\nand\nastream\nmethods with LangGraph, you can choose\none or more\nstreaming mode\nwhich allow you to control the type of output that is streamed. The available streaming modes are:\n\"values\"\n: Emit all values of the\nstate\nfor each step.\n\"updates\"\n: Emit only the node name(s) and updates that were returned by the node(s) after each step.\n\"debug\"\n: Emit debug events for each step.\n\"messages\"\n: Emit LLM\nmessages\ntoken-by-token\n.\n\"custom\"\n: Emit custom output written using\nLangGraph's StreamWriter\n.\nFor more information, please see:\nLangGraph streaming conceptual guide\nfor more information on how to stream when working with LangGraph.\nLangGraph streaming how-to guides\nfor specific examples of streaming in LangGraph.\nUsage with LCEL\nâ€‹\nIf you compose multiple Runnables using\nLangChainâ€™s Expression Language (LCEL)\n, the\nstream()\nand\nastream()\nmethods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally.\nLCEL\ntries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.\nastream_events\nâ€‹\ntip\nUse the\nastream_events\nAPI to access custom data and intermediate outputs from LLM applications built entirely with\nLCEL\n.\nWhile this API is available for use with\nLangGraph\nas well, it is usually not necessary when working with LangGraph, as the\nstream\nand\nastream\nmethods provide comprehensive streaming capabilities for LangGraph graphs.\nFor chains constructed using\nLCEL\n, the\n.stream()\nmethod only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of the chain alongside the final output. For example, you may want to return sources alongside the final generation when building a chat-over-documents app.\nThere are ways to do this\nusing callbacks\n, or by constructing your chain in such a way that it passes intermediate\nvalues to the end with something like chained\n.assign()\ncalls, but LangChain also includes an\n.astream_events()\nmethod that combines the flexibility of callbacks with the ergonomics of\n.stream()\n. When called, it returns an iterator\nwhich yields\nvarious types of events\nthat you can filter and process according\nto the needs of your project.\nHere's one small example that prints just events containing streamed chat model output:\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nmodel\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n)\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"tell me a joke about {topic}\"\n)\nparser\n=\nStrOutputParser\n(\n)\nchain\n=\nprompt\n|\nmodel\n|\nparser\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n{\n\"topic\"\n:\n\"parrot\"\n}\n)\n:\nkind\n=\nevent\n[\n\"event\"\n]\nif\nkind\n==\n\"on_chat_model_stream\"\n:\nprint\n(\nevent\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\nYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!\nSee\nthis guide\nfor more detailed information on how to use\n.astream_events()\n, including a table listing available events.\nWriting custom data to the stream\nâ€‹\nTo write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:\nLangGraph's\nStreamWriter\ncan be used to write custom data that will surface through\nstream\nand\nastream\nAPIs when working with LangGraph.\nImportant\nthis is a LangGraph feature, so it is not available when working with pure LCEL. See\nhow to streaming custom data\nfor more information.\ndispatch_events\n/\nadispatch_events\ncan be used to write custom data that will be surfaced through the\nastream_events\nAPI. See\nhow to dispatch custom callback events\nfor more information.\n\"Auto-Streaming\" Chat Models\nâ€‹\nLangChain simplifies streaming from\nchat models\nby automatically enabling streaming mode in certain cases, even when youâ€™re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming\ninvoke\nmethod but still want to stream the entire application, including intermediate results from the chat model.\nHow It Works\nâ€‹\nWhen you call the\ninvoke\n(or\nainvoke\n) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application.\nUnder the hood, it'll have\ninvoke\n(or\nainvoke\n) use the\nstream\n(or\nastream\n) method to generate its output. The result of the invocation will be the same as far as the code that was using\ninvoke\nis concerned; however, while the chat model is being streamed, LangChain will take care of invoking\non_llm_new_token\nevents in LangChain's\ncallback system\n. These callback events\nallow LangGraph\nstream\n/\nastream\nand\nastream_events\nto surface the chat model's output in real-time.\nExample:\ndef\nnode\n(\nstate\n)\n:\n.\n.\n.\n# The code below uses the invoke method, but LangChain will\n# automatically switch to streaming mode\n# when it detects that the overall\n# application is being streamed.\nai_message\n=\nmodel\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\n.\n.\n.\nfor\nchunk\nin\ncompiled_graph\n.\nstream\n(\n.\n.\n.\n,\nmode\n=\n\"messages\"\n)\n:\n.\n.\n.\nAsync Programming\nâ€‹\nLangChain offers both synchronous (sync) and asynchronous (async) versions of many of its methods. The async methods are typically prefixed with an \"a\" (e.g.,\nainvoke\n,\nastream\n). When writing async code, it's crucial to consistently use these asynchronous methods to ensure non-blocking behavior and optimal performance.\nIf streaming data fails to appear in real-time, please ensure that you are using the correct async methods for your workflow.\nPlease review the\nasync programming in LangChain guide\nfor more information on writing async code with LangChain.\nRelated Resources\nâ€‹\nPlease see the following how-to guides for specific examples of streaming in LangChain:\nLangGraph conceptual guide on streaming\nLangGraph streaming how-to guides\nHow to stream runnables\n: This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with\nLCEL\n.\nHow to stream chat models\nHow to stream tool calls\nFor writing custom data to the stream, please see the following resources:\nIf using LangGraph, see\nhow to stream custom data\n.\nIf using LCEL, see\nhow to dispatch custom callback events\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/structured_outputs/",
    "Conceptual guide\nStructured outputs\nOn this page\nStructured outputs\nOverview\nâ€‹\nFor many applications, such as chatbots, models need to respond to users directly in natural language.\nHowever, there are scenarios where we need models to output in a\nstructured format\n.\nFor example, we might want to store the model output in a database and ensure that the output conforms to the database schema.\nThis need motivates the concept of structured output, where models can be instructed to respond with a particular output structure.\nKey concepts\nâ€‹\nSchema definition:\nThe output structure is represented as a schema, which can be defined in several ways.\nReturning structured output:\nThe model is given this schema, and is instructed to return output that conforms to it.\nRecommended usage\nâ€‹\nThis pseudocode illustrates the recommended workflow when using structured output.\nLangChain provides a method,\nwith_structured_output()\n, that automates the process of binding the schema to the\nmodel\nand parsing the output.\nThis helper function is available for all model providers that support structured output.\n# Define schema\nschema\n=\n{\n\"foo\"\n:\n\"bar\"\n}\n# Bind schema to model\nmodel_with_structure\n=\nmodel\n.\nwith_structured_output\n(\nschema\n)\n# Invoke the model to produce structured output that matches the schema\nstructured_output\n=\nmodel_with_structure\n.\ninvoke\n(\nuser_input\n)\nTool Order Matters\nWhen combining structured output with additional tools, bind tools\nfirst\n, then apply structured output:\n# Correct\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\n[\ntool1\n,\ntool2\n]\n)\nstructured_model\n=\nmodel_with_tools\n.\nwith_structured_output\n(\nschema\n)\n# Incorrect - will cause tool resolution errors\nstructured_model\n=\nmodel\n.\nwith_structured_output\n(\nschema\n)\nbroken_model\n=\nstructured_model\n.\nbind_tools\n(\n[\ntool1\n,\ntool2\n]\n)\nSchema definition\nâ€‹\nThe central concept is that the output structure of model responses needs to be represented in some way.\nWhile types of objects you can use depend on the model you're working with, there are common types of objects that are typically allowed or recommended for structured output in Python.\nThe simplest and most common format for structured output is a JSON-like structure, which in Python can be represented as a dictionary (dict) or list (list).\nJSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data.\n{\n\"answer\"\n:\n\"The answer to the user's question\"\n,\n\"followup_question\"\n:\n\"A followup question the user could ask\"\n}\nAs a second example,\nPydantic\nis particularly useful for defining structured output schemas because it offers type hints and validation.\nHere's an example of a Pydantic schema:\nfrom\npydantic\nimport\nBaseModel\n,\nField\nclass\nResponseFormatter\n(\nBaseModel\n)\n:\n\"\"\"Always use this tool to structure your response to the user.\"\"\"\nanswer\n:\nstr\n=\nField\n(\ndescription\n=\n\"The answer to the user's question\"\n)\nfollowup_question\n:\nstr\n=\nField\n(\ndescription\n=\n\"A followup question the user could ask\"\n)\nReturning structured output\nâ€‹\nWith a schema defined, we need a way to instruct the model to use it.\nWhile one approach is to include this schema in the prompt and\nask nicely\nfor the model to use it, this is not recommended.\nSeveral more powerful methods that utilizes native features in the model provider's API are available.\nUsing tool calling\nâ€‹\nMany\nmodel providers support\ntool calling, a concept discussed in more detail in our\ntool calling guide\n.\nIn short, tool calling involves binding a tool to a model and, when appropriate, the model can\ndecide\nto call this tool and ensure its response conforms to the tool's schema.\nWith this in mind, the central concept is straightforward:\nsimply bind our schema to a model as a tool!\nHere is an example using the\nResponseFormatter\nschema defined above:\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n,\ntemperature\n=\n0\n)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\n[\nResponseFormatter\n]\n)\n# Invoke the model\nai_msg\n=\nmodel_with_tools\n.\ninvoke\n(\n\"What is the powerhouse of the cell?\"\n)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original\nResponseFormatter\nschema.\n# Get the tool call arguments\nai_msg\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n{\n'answer'\n:\n\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\"\n,\n'followup_question'\n:\n'What is the function of ATP in the cell?'\n}\n# Parse the dictionary into a pydantic object\npydantic_object\n=\nResponseFormatter\n.\nmodel_validate\n(\nai_msg\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\nJSON mode\nâ€‹\nIn addition to tool calling, some model providers support a feature called\nJSON mode\n.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode\nhere\n.\nHere is an example of how to use JSON mode with OpenAI:\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\n.\nwith_structured_output\n(\nmethod\n=\n\"json_mode\"\n)\nai_msg\n=\nmodel\n.\ninvoke\n(\n\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\n)\nai_msg\n{\n'random_ints'\n:\n[\n45\n,\n67\n,\n12\n,\n34\n,\n89\n,\n23\n,\n78\n,\n56\n,\n90\n,\n11\n]\n}\nStructured output method\nâ€‹\nThere are a few challenges when producing structured output with the above methods:\nWhen tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\nIn addition, the model needs to be instructed to\nalways\nuse the tool when we want to enforce structured output, which is a provider specific setting.\nWhen JSON mode is used, the output needs to be parsed into a JSON object.\nWith these challenges in mind, LangChain provides a helper function (\nwith_structured_output()\n) to streamline the process.\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n# Bind the schema to the model\nmodel_with_structure\n=\nmodel\n.\nwith_structured_output\n(\nResponseFormatter\n)\n# Invoke the model\nstructured_output\n=\nmodel_with_structure\n.\ninvoke\n(\n\"What is the powerhouse of the cell?\"\n)\n# Get back the pydantic object\nstructured_output\nResponseFormatter\n(\nanswer\n=\n\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\"\n,\nfollowup_question\n=\n'What is the function of ATP in the cell?'\n)\nFurther reading\nFor more details on usage, see our\nhow-to guide\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/testing/",
    "Conceptual guide\nTesting\nOn this page\nTesting\nTesting is a critical part of the development process that ensures your code works as expected and meets the desired quality standards.\nIn the LangChain ecosystem, we have 2 main types of tests:\nunit tests\nand\nintegration tests\n.\nFor integrations that implement standard LangChain abstractions, we have a set of\nstandard tests\n(both unit and integration) that help maintain compatibility between different components and ensure reliability of high-usage ones.\nUnit Tests\nâ€‹\nDefinition\n: Unit tests are designed to validate the smallest parts of your codeâ€”individual functions or methodsâ€”ensuring they work as expected in isolation. They do not rely on external systems or integrations.\nExample\n: Testing the\nconvert_langchain_aimessage_to_dict\nfunction to confirm it correctly converts an AI message to a dictionary format:\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessage\n,\nToolCall\n,\nconvert_to_openai_messages\ndef\ntest_convert_to_openai_messages\n(\n)\n:\nai_message\n=\nAIMessage\n(\ncontent\n=\n\"Let me call that tool for you!\"\n,\ntool_calls\n=\n[\nToolCall\n(\nname\n=\n'parrot_multiply_tool'\n,\nid\n=\n'1'\n,\nargs\n=\n{\n'a'\n:\n2\n,\n'b'\n:\n3\n}\n)\n,\n]\n)\nresult\n=\nconvert_to_openai_messages\n(\nai_message\n)\nexpected\n=\n{\n\"role\"\n:\n\"assistant\"\n,\n\"tool_calls\"\n:\n[\n{\n\"type\"\n:\n\"function\"\n,\n\"id\"\n:\n\"1\"\n,\n\"function\"\n:\n{\n\"name\"\n:\n\"parrot_multiply_tool\"\n,\n\"arguments\"\n:\n'{\"a\": 2, \"b\": 3}'\n,\n}\n,\n}\n]\n,\n\"content\"\n:\n\"Let me call that tool for you!\"\n,\n}\nassert\nresult\n==\nexpected\n# Ensure conversion matches expected output\nAPI Reference:\nAIMessage\n|\nToolCall\n|\nconvert_to_openai_messages\nIntegration Tests\nâ€‹\nDefinition\n: Integration tests validate that multiple components or systems work together as expected. For tools or integrations relying on external services, these tests often ensure end-to-end functionality.\nExample\n: Testing\nParrotMultiplyTool\nwith access to an API service that multiplies two numbers and adds 80:\ndef\ntest_integration_with_service\n(\n)\n:\ntool\n=\nParrotMultiplyTool\n(\n)\nresult\n=\ntool\n.\ninvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\n)\nassert\nresult\n==\n86\nStandard Tests\nâ€‹\nDefinition\n: Standard tests are pre-defined tests provided by LangChain to ensure consistency and reliability across all tools and integrations. They include both unit and integration test templates tailored for LangChain components.\nExample\n: Subclassing LangChain's\nToolsUnitTests\nor\nToolsIntegrationTests\nto automatically run standard tests:\nfrom\nlangchain_tests\n.\nunit_tests\nimport\nToolsUnitTests\nclass\nTestParrotMultiplyToolUnit\n(\nToolsUnitTests\n)\n:\n@property\ndef\ntool_constructor\n(\nself\n)\n:\nreturn\nParrotMultiplyTool\ndef\ntool_invoke_params_example\n(\nself\n)\n:\nreturn\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\nTo learn more, check out our guide on\nhow to add standard tests to an integration\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/text_llms/",
    "Conceptual guide\nString-in, string-out llms\nString-in, string-out llms\ntip\nYou are probably looking for the\nChat Model Concept Guide\npage for more information.\nLangChain has implementations for older language models that take a string as input and return a string as output. These models are typically named without the \"Chat\" prefix (e.g.,\nOllama\n,\nAnthropic\n,\nOpenAI\n, etc.), and may include the \"LLM\" suffix (e.g.,\nOllamaLLM\n,\nAnthropicLLM\n,\nOpenAILLM\n, etc.). These models implement the\nBaseLLM\ninterface.\nUsers should be using almost exclusively the newer\nChat Models\nas most\nmodel providers have adopted a chat like interface for interacting with language models.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/text_splitters/",
    "Conceptual guide\nText splitters\nOn this page\nText splitters\nPrerequisites\nDocuments\nTokenization\nOverview\nâ€‹\nDocument splitting is often a crucial preprocessing step for many applications.\nIt involves breaking down large texts into smaller, manageable chunks.\nThis process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems.\nThere are several strategies for splitting documents, each with its own advantages.\nKey concepts\nâ€‹\nText splitters split documents into smaller chunks for use in downstream applications.\nWhy split documents?\nâ€‹\nThere are several reasons to split documents:\nHandling non-uniform document lengths\n: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.\nOvercoming model limitations\n: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.\nImproving representation quality\n: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.\nEnhancing retrieval precision\n: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.\nOptimizing computational resources\n: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.\nNow, the next question is\nhow\nto split the documents into chunks! There are several strategies, each with its own advantages.\nFurther reading\nSee Greg Kamradt's\nchunkviz\nto visualize different splitting strategies discussed below.\nApproaches\nâ€‹\nLength-based\nâ€‹\nThe most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit.\nKey benefits of length-based splitting:\nStraightforward implementation\nConsistent chunk sizes\nEasily adaptable to different model requirements\nTypes of length-based splitting:\nToken-based\n: Splits text based on the number of tokens, which is useful when working with language models.\nCharacter-based\n: Splits text based on the number of characters, which can be more consistent across different types of text.\nExample implementation using LangChain's\nCharacterTextSplitter\nwith token-based splitting:\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\ntext_splitter\n=\nCharacterTextSplitter\n.\nfrom_tiktoken_encoder\n(\nencoding_name\n=\n\"cl100k_base\"\n,\nchunk_size\n=\n100\n,\nchunk_overlap\n=\n0\n)\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\ndocument\n)\nFurther reading\nSee the how-to guide for\ntoken-based\nsplitting.\nSee the how-to guide for\ncharacter-based\nsplitting.\nText-structured based\nâ€‹\nText is naturally organized into hierarchical units such as paragraphs, sentences, and words.\nWe can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.\nLangChain's\nRecursiveCharacterTextSplitter\nimplements this concept:\nThe\nRecursiveCharacterTextSplitter\nattempts to keep larger units (e.g., paragraphs) intact.\nIf a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\nThis process continues down to the word level if necessary.\nHere is example usage:\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n100\n,\nchunk_overlap\n=\n0\n)\ntexts\n=\ntext_splitter\n.\nsplit_text\n(\ndocument\n)\nFurther reading\nSee the how-to guide for\nrecursive text splitting\n.\nDocument-structured based\nâ€‹\nSome documents have an inherent structure, such as HTML, Markdown, or JSON files.\nIn these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text.\nKey benefits of structure-based splitting:\nPreserves the logical organization of the document\nMaintains context within each chunk\nCan be more effective for downstream tasks like retrieval or summarization\nExamples of structure-based splitting:\nMarkdown\n: Split based on headers (e.g., #, ##, ###)\nHTML\n: Split using tags\nJSON\n: Split by object or array elements\nCode\n: Split by functions, classes, or logical blocks\nFurther reading\nSee the how-to guide for\nMarkdown splitting\n.\nSee the how-to guide for\nRecursive JSON splitting\n.\nSee the how-to guide for\nCode splitting\n.\nSee the how-to guide for\nHTML splitting\n.\nSemantic meaning based\nâ€‹\nUnlike the previous methods, semantic-based splitting actually considers the\ncontent\nof the text.\nWhile other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text's semantics.\nThere are several ways to implement this, but conceptually the approach is split text when there are significant changes in text\nmeaning\n.\nAs an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:\nStart with the first few sentences and generate an embedding.\nMove to the next group of sentences and generate another embedding (e.g., using a sliding window approach).\nCompare the embeddings to find significant differences, which indicate potential \"break points\" between semantic sections.\nThis technique helps create chunks that are more semantically coherent, potentially improving the quality of downstream tasks like retrieval or summarization.\nFurther reading\nSee the how-to guide for\nsplitting text based on semantic meaning\n.\nSee Greg Kamradt's\nnotebook\nshowcasing semantic splitting.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/tokens/",
    "Conceptual guide\nTokens\nOn this page\nTokens\nModern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use to break down input and generate output. In this section, we'll discuss what tokens are and how they are used by language models.\nWhat is a token?\nâ€‹\nA\ntoken\nis the basic unit that a language model reads, processes, and generates. These units can vary based on how the model provider defines them, but in general, they could represent:\nA whole word (e.g., \"apple\"),\nA part of a word (e.g., \"app\"),\nOr other linguistic components such as punctuation or spaces.\nThe way the model tokenizes the input depends on its\ntokenizer algorithm\n, which converts the input into tokens. Similarly, the modelâ€™s output comes as a stream of tokens, which is then decoded back into human-readable text.\nHow tokens work in language models\nâ€‹\nThe reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, language models focus on\ntokens\n, which represent meaningful linguistic units. Here's how the process works:\nInput Tokenization\n: When you provide a model with a prompt (e.g., \"LangChain is cool!\"), the tokenizer algorithm splits the text into tokens. For example, the sentence could be tokenized into parts like\n[\"Lang\", \"Chain\", \" is\", \" cool\", \"!\"]\n. Note that token boundaries donâ€™t always align with word boundaries.\nProcessing\n: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, capturing context and meaning from the input.\nOutput Generation\n: The model generates new tokens one by one. These output tokens are then decoded back into human-readable text.\nUsing tokens instead of raw characters allows the model to focus on linguistically meaningful units, which helps it capture grammar, structure, and context more effectively.\nTokens donâ€™t have to be text\nâ€‹\nAlthough tokens are most commonly used to represent text, they donâ€™t have to be limited to textual data. Tokens can also serve as abstract representations of\nmulti-modal data\n, such as:\nImages\n,\nAudio\n,\nVideo\n,\nAnd other types of data.\nAt the time of writing, virtually no models support\nmulti-modal output\n, and only a few models can handle\nmulti-modal inputs\n(e.g., text combined with images or audio). However, as advancements in AI continue, we expect\nmulti-modality\nto become much more common. This would allow models to process and generate a broader range of media, significantly expanding the scope of what tokens can represent and how models can interact with diverse types of data.\nnote\nIn principle,\nanything that can be represented as a sequence of tokens\ncould be modeled in a similar way. For example,\nDNA sequences\nâ€”which are composed of a series of nucleotides (A, T, C, G)â€”can be tokenized and modeled to capture patterns, make predictions, or generate sequences. This flexibility allows transformer-based models to handle diverse types of sequential data, further broadening their potential applications across various domains, including bioinformatics, signal processing, and other fields that involve structured or unstructured sequences.\nPlease see the\nmultimodality\nsection for more information on multi-modal inputs and outputs.\nWhy not use characters?\nâ€‹\nUsing tokens instead of individual characters makes models both more efficient and better at understanding context and grammar. Tokens represent meaningful units, like whole words or parts of words, allowing models to capture language structure more effectively than by processing raw characters. Token-level processing also reduces the number of units the model has to handle, leading to faster computation.\nIn contrast, character-level processing would require handling a much larger sequence of input, making it harder for the model to learn relationships and context. Tokens enable models to focus on linguistic meaning, making them more accurate and efficient in generating responses.\nHow tokens correspond to text\nâ€‹\nPlease see this post from\nOpenAI\nfor more details on how tokens are counted and how they correspond to text.\nAccording to the OpenAI post, the approximate token counts for English text are as follows:\n1 token ~= 4 chars in English\n1 token ~= Â¾ words\n100 tokens ~= 75 words\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/tool_calling/",
    "Conceptual guide\nTool calling\nOn this page\nTool calling\nPrerequisites\nTools\nChat Models\nOverview\nâ€‹\nMany AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.\nBut what about cases where we want a model to also interact\ndirectly\nwith systems, such as databases or an API?\nThese systems often have a particular input schema; for example, APIs frequently have a required payload structure.\nThis need motivates the concept of\ntool calling\n. You can use\ntool calling\nto request model responses that match a particular schema.\ninfo\nYou will sometimes hear the term\nfunction calling\n. We use this term interchangeably with\ntool calling\n.\nKey concepts\nâ€‹\nTool Creation:\nUse the\n@tool\ndecorator to create a\ntool\n. A tool is an association between a function and its schema.\nTool Binding:\nThe tool needs to be connected to a model that supports tool calling. This gives the model awareness of the tool and the associated input schema required by the tool.\nTool Calling:\nWhen appropriate, the model can decide to call a tool and ensure its response conforms to the tool's input schema.\nTool Execution:\nThe tool can be executed using the arguments provided by the model.\nRecommended usage\nâ€‹\nThis pseudocode illustrates the recommended workflow for using tool calling.\nCreated tools are passed to\n.bind_tools()\nmethod as a list.\nThis model can be called, as usual. If a tool call is made, model's response will contain the tool call arguments.\nThe tool call arguments can be passed directly to the tool.\n# Tool creation\ntools\n=\n[\nmy_tool\n]\n# Tool binding\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\ntools\n)\n# Tool calling\nresponse\n=\nmodel_with_tools\n.\ninvoke\n(\nuser_input\n)\nTool creation\nâ€‹\nThe recommended way to create a tool is using the\n@tool\ndecorator.\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply a and b.\"\"\"\nreturn\na\n*\nb\nAPI Reference:\ntool\nFurther reading\nSee our conceptual guide on\ntools\nfor more details.\nSee our\nmodel integrations\nthat support tool calling.\nSee our\nhow-to guide\non tool calling.\nTool binding\nâ€‹\nMany\nmodel providers\nsupport tool calling.\ntip\nSee our\nmodel integration page\nfor a list of providers that support tool calling.\nThe central concept to understand is that LangChain provides a standardized interface for connecting tools to models.\nThe\n.bind_tools()\nmethod can be used to specify which tools are available for a model to call.\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\ntools_list\n)\nAs a specific example, let's take a function\nmultiply\nand bind it as a tool to a model that supports tool calling.\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply a and b.\nArgs:\na: first int\nb: second int\n\"\"\"\nreturn\na\n*\nb\nllm_with_tools\n=\ntool_calling_model\n.\nbind_tools\n(\n[\nmultiply\n]\n)\nTool calling\nâ€‹\nA key principle of tool calling is that the model decides when to use a tool based on the input's relevance. The model doesn't always need to call a tool.\nFor example, given an unrelated input, the model would not call the tool:\nresult\n=\nllm_with_tools\n.\ninvoke\n(\n\"Hello world!\"\n)\nThe result would be an\nAIMessage\ncontaining the model's response in natural language (e.g., \"Hello!\").\nHowever, if we pass an input\nrelevant to the tool\n, the model should choose to call it:\nresult\n=\nllm_with_tools\n.\ninvoke\n(\n\"What is 2 multiplied by 3?\"\n)\nAs before, the output\nresult\nwill be an\nAIMessage\n.\nBut, if the tool was called,\nresult\nwill have a\ntool_calls\nattribute\n.\nThis attribute includes everything needed to execute the tool, including the tool name and input arguments:\nresult.tool_calls\n[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'xxx', 'type': 'tool_call'}]\nFor more details on usage, see our\nhow-to guides\n!\nTool execution\nâ€‹\nTools\nimplement the\nRunnable\ninterface, which means that they can be invoked (e.g.,\ntool.invoke(args)\n) directly.\nLangGraph\noffers pre-built components (e.g.,\nToolNode\n) that will often invoke the tool in behalf of the user.\nFurther reading\nSee our\nhow-to guide\non tool calling.\nSee the\nLangGraph documentation on using ToolNode\n.\nForcing tool use\nâ€‹\nBy default, the model has the freedom to choose which tool to use based on the user's input. However, in certain scenarios, you might want to influence the model's decision-making process. LangChain allows you to enforce tool choice (using\ntool_choice\n), ensuring the model uses either a particular tool or\nany\ntool from a given list. This is useful for structuring the model's behavior and guiding it towards a desired outcome.\nFurther reading\nSee our\nhow-to guide\non forcing tool use.\nBest practices\nâ€‹\nWhen designing\ntools\nto be used by a model, it is important to keep in mind that:\nModels that have explicit\ntool-calling APIs\nwill be better at tool calling than non-fine-tuned models.\nModels will perform better if the tools have well-chosen names and descriptions.\nSimple, narrowly scoped tools are easier for models to use than complex tools.\nAsking the model to select from a large list of tools poses challenges for the model.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/tools/",
    "Conceptual guide\nTools\nOn this page\nTools\nPrerequisites\nChat models\nOverview\nâ€‹\nThe\ntool\nabstraction in LangChain associates a Python\nfunction\nwith a\nschema\nthat defines the function's\nname\n,\ndescription\nand\nexpected arguments\n.\nTools\ncan be passed to\nchat models\nthat support\ntool calling\nallowing the model to request the execution of a specific function with specific inputs.\nKey concepts\nâ€‹\nTools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.\nCreate tools using the\n@tool\ndecorator, which simplifies the process of tool creation, supporting the following:\nAutomatically infer the tool's\nname\n,\ndescription\nand\nexpected arguments\n, while also supporting customization.\nDefining tools that return\nartifacts\n(e.g. images, dataframes, etc.)\nHiding input arguments from the schema (and hence from the model) using\ninjected tool arguments\n.\nTool interface\nâ€‹\nThe tool interface is defined in the\nBaseTool\nclass which is a subclass of the\nRunnable Interface\n.\nThe key attributes that correspond to the tool's\nschema\n:\nname\n: The name of the tool.\ndescription\n: A description of what the tool does.\nargs\n: Property that returns the JSON schema for the tool's arguments.\nThe key methods to execute the function associated with the\ntool\n:\ninvoke\n: Invokes the tool with the given arguments.\nainvoke\n: Invokes the tool with the given arguments, asynchronously. Used for\nasync programming with LangChain\n.\nCreate tools using the\n@tool\ndecorator\nâ€‹\nThe recommended way to create tools is using the\n@tool\ndecorator. This decorator is designed to simplify the process of tool creation and should be used in most cases. After defining a function, you can decorate it with\n@tool\nto create a tool that implements the\nTool Interface\n.\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Multiply two numbers.\"\"\"\nreturn\na\n*\nb\nAPI Reference:\ntool\nFor more details on how to create tools, see the\nhow to create custom tools\nguide.\nnote\nLangChain has a few other ways to create tools; e.g., by sub-classing the\nBaseTool\nclass or by using\nStructuredTool\n. These methods are shown in the\nhow to create custom tools guide\n, but\nwe generally recommend using the\n@tool\ndecorator for most cases.\nUse the tool directly\nâ€‹\nOnce you have defined a tool, you can use it directly by calling the function. For example, to use the\nmultiply\ntool defined above:\nmultiply\n.\ninvoke\n(\n{\n\"a\"\n:\n2\n,\n\"b\"\n:\n3\n}\n)\nInspect\nâ€‹\nYou can also inspect the tool's schema and other properties:\nprint\n(\nmultiply\n.\nname\n)\n# multiply\nprint\n(\nmultiply\n.\ndescription\n)\n# Multiply two numbers.\nprint\n(\nmultiply\n.\nargs\n)\n# {\n# 'type': 'object',\n# 'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}},\n# 'required': ['a', 'b']\n# }\nnote\nIf you're using pre-built LangChain or LangGraph components like\ncreate_react_agent\n,you might not need to interact with tools directly. However, understanding how to use them can be valuable for debugging and testing. Additionally, when building custom LangGraph workflows, you may find it necessary to work with tools directly.\nConfiguring the schema\nâ€‹\nThe\n@tool\ndecorator offers additional options to configure the schema of the tool (e.g., modify name, description\nor parse the function's doc-string to infer the schema).\nPlease see the\nAPI reference for @tool\nfor more details and review the\nhow to create custom tools\nguide for examples.\nTool artifacts\nâ€‹\nTools\nare utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nsome_tool\n(\n.\n.\n.\n)\n-\n>\nTuple\n[\nstr\n,\nAny\n]\n:\n\"\"\"Tool that does something.\"\"\"\n.\n.\n.\nreturn\n'Message for chat model'\n,\nsome_artifact\nSee\nhow to return artifacts from tools\nfor more details.\nSpecial type annotations\nâ€‹\nThere are a number of special type annotations that can be used in the tool's function signature to configure the run time behavior of the tool.\nThe following type annotations will end up\nremoving\nthe argument from the tool's schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.\nInjectedToolArg\n: Value should be injected manually at runtime using\n.invoke\nor\n.ainvoke\n.\nRunnableConfig\n: Pass in the RunnableConfig object to the tool.\nInjectedState\n: Pass in the overall state of the LangGraph graph to the tool.\nInjectedStore\n: Pass in the LangGraph store object to the tool.\nYou can also use the\nAnnotated\ntype with a string literal to provide a\ndescription\nfor the corresponding argument that\nWILL\nbe exposed in the tool's schema.\nAnnotated[..., \"string literal\"]\n-- Adds a description to the argument that will be exposed in the tool's schema.\nInjectedToolArg\nâ€‹\nThere are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the\nInjectedToolArg\nannotation, which allows certain parameters to be hidden from the tool's schema.\nFor example, if a tool requires a\nuser_id\nto be injected dynamically at runtime, it can be structured in this way:\nfrom\nlangchain_core\n.\ntools\nimport\ntool\n,\nInjectedToolArg\n@tool\ndef\nuser_specific_tool\n(\ninput_data\n:\nstr\n,\nuser_id\n:\nInjectedToolArg\n)\n-\n>\nstr\n:\n\"\"\"Tool that processes input data.\"\"\"\nreturn\nf\"User\n{\nuser_id\n}\nprocessed\n{\ninput_data\n}\n\"\nAPI Reference:\ntool\n|\nInjectedToolArg\nAnnotating the\nuser_id\nargument with\nInjectedToolArg\ntells LangChain that this argument should not be exposed as part of the\ntool's schema.\nSee\nhow to pass run time values to tools\nfor more details on how to use\nInjectedToolArg\n.\nRunnableConfig\nâ€‹\nYou can use the\nRunnableConfig\nobject to pass custom run time values to tools.\nIf you need to access the\nRunnableConfig\nobject from within a tool. This can be done by using the\nRunnableConfig\nannotation in the tool's function signature.\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\n@tool\nasync\ndef\nsome_func\n(\n.\n.\n.\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\n.\n.\n.\n:\n\"\"\"Tool that does something.\"\"\"\n# do something with config\n.\n.\n.\nawait\nsome_func\n.\nainvoke\n(\n.\n.\n.\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"value\"\n:\n\"some_value\"\n}\n}\n)\nAPI Reference:\nRunnableConfig\nThe\nconfig\nwill not be part of the tool's schema and will be injected at runtime with appropriate values.\nnote\nYou may need to access the\nconfig\nobject to manually propagate it to subclass. This happens if you're working with python 3.9 / 3.10 in an\nasync\nenvironment and need to manually propagate the\nconfig\nobject to sub-calls.\nPlease read\nPropagation RunnableConfig\nfor more details to learn how to propagate the\nRunnableConfig\ndown the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).\nInjectedState\nâ€‹\nPlease see the\nInjectedState\ndocumentation for more details.\nInjectedStore\nâ€‹\nPlease see the\nInjectedStore\ndocumentation for more details.\nTool Artifacts vs. Injected State\nâ€‹\nAlthough similar conceptually, tool artifacts in LangChain and\ninjected state in LangGraph\nserve different purposes and operate at different levels of abstraction.\nTool Artifacts\nPurpose:\nStore and pass data between tool executions within a single chain/workflow\nScope:\nLimited to tool-to-tool communication\nLifecycle:\nTied to individual tool calls and their immediate context\nUsage:\nTemporary storage for intermediate results that tools need to share\nInjected State (LangGraph)\nPurpose:\nMaintain persistent state across the entire graph execution\nScope:\nGlobal to the entire graph workflow\nLifecycle:\nPersists throughout the entire graph execution and can be saved/restored\nUsage:\nLong-term state management, conversation memory, user context, workflow checkpointing\nTool artifacts are ephemeral data passed between tools, while injected state is persistent workflow-level state that survives across multiple steps, tool calls, and even execution sessions in LangGraph.\nBest practices\nâ€‹\nWhen designing tools to be used by models, keep the following in mind:\nTools that are well-named, correctly-documented and properly type-hinted are easier for models to use.\nDesign simple and narrowly scoped tools, as they are easier for models to use correctly.\nUse chat models that support\ntool-calling\nAPIs to take advantage of tools.\nToolkits\nâ€‹\nLangChain has a concept of\ntoolkits\n. This a very thin abstraction that groups tools together that\nare designed to be used together for specific tasks.\nInterface\nâ€‹\nAll Toolkits expose a\nget_tools\nmethod which returns a list of tools. You can therefore do:\n# Initialize a toolkit\ntoolkit\n=\nExampleToolkit\n(\n.\n.\n.\n)\n# Get list of tools\ntools\n=\ntoolkit\n.\nget_tools\n(\n)\nRelated resources\nâ€‹\nSee the following resources for more information:\nAPI Reference for @tool\nHow to create custom tools\nHow to pass run time values to tools\nAll LangChain tool how-to guides\nAdditional how-to guides that show usage with LangGraph\nTool integrations, see the\ntool integration docs\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/tracing/",
    "Conceptual guide\nTracing\nTracing\nA trace is essentially a series of steps that your application takes to go from input to output.\nTraces contain individual steps called\nruns\n. These can be individual calls from a model, retriever,\ntool, or sub-chains.\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\nFor a deeper dive, check out\nthis LangSmith conceptual guide\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/vectorstores/",
    "Conceptual guide\nVector stores\nOn this page\nVector stores\nPrerequisites\nEmbeddings\nText splitters\nNote\nThis conceptual overview focuses on text-based indexing and retrieval for simplicity.\nHowever, embedding models can be\nmulti-modal\nand vector stores can be used to store and retrieve a variety of data types beyond text.\nOverview\nâ€‹\nVector stores are specialized data stores that enable indexing and retrieving information based on vector representations.\nThese vectors, called\nembeddings\n, capture the semantic meaning of data that has been embedded.\nVector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\nIntegrations\nâ€‹\nLangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.\nPlease see the\nfull list of LangChain vectorstore integrations\n.\nInterface\nâ€‹\nLangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.\nThe interface consists of basic methods for writing, deleting and searching for documents in the vector store.\nThe key methods are:\nadd_documents\n: Add a list of texts to the vector store.\ndelete\n: Delete a list of documents from the vector store.\nsimilarity_search\n: Search for similar documents to a given query.\nInitialization\nâ€‹\nMost vectors in LangChain accept an embedding model as an argument when initializing the vector store.\nWe will use LangChain's\nInMemoryVectorStore\nimplementation to illustrate the API.\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\n# Initialize with an embedding model\nvector_store\n=\nInMemoryVectorStore\n(\nembedding\n=\nSomeEmbeddingModel\n(\n)\n)\nAPI Reference:\nInMemoryVectorStore\nAdding documents\nâ€‹\nTo add documents, use the\nadd_documents\nmethod.\nThis API works with a list of\nDocument\nobjects.\nDocument\nobjects all have\npage_content\nand\nmetadata\nattributes, making them a universal way to store unstructured text and associated metadata.\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\ndocument_1\n=\nDocument\n(\npage_content\n=\n\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"tweet\"\n}\n,\n)\ndocument_2\n=\nDocument\n(\npage_content\n=\n\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"news\"\n}\n,\n)\ndocuments\n=\n[\ndocument_1\n,\ndocument_2\n]\nvector_store\n.\nadd_documents\n(\ndocuments\n=\ndocuments\n)\nAPI Reference:\nDocument\nYou should usually provide IDs for the documents you add to the vector store, so\nthat instead of adding the same document multiple times, you can update the existing document.\nvector_store\n.\nadd_documents\n(\ndocuments\n=\ndocuments\n,\nids\n=\n[\n\"doc1\"\n,\n\"doc2\"\n]\n)\nDelete\nâ€‹\nTo delete documents, use the\ndelete\nmethod which takes a list of document IDs to delete.\nvector_store\n.\ndelete\n(\nids\n=\n[\n\"doc1\"\n]\n)\nSearch\nâ€‹\nVector stores embed and store the documents that added.\nIf we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones.\nThis captures two important concepts: first, there needs to be a way to measure the similarity between the query and\nany\nembedded\ndocument.\nSecond, there needs to be an algorithm to efficiently perform this similarity search across\nall\nembedded documents.\nSimilarity metrics\nâ€‹\nA critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:\nCosine Similarity\n: Measures the cosine of the angle between two vectors.\nEuclidean Distance\n: Measures the straight-line distance between two points.\nDot Product\n: Measures the projection of one vector onto another.\nThe choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer\nto the documentation of the specific vectorstore you are using to see what similarity metrics are supported.\nFurther reading\nSee\nthis documentation\nfrom Google on similarity metrics to consider with embeddings.\nSee Pinecone's\nblog post\non similarity metrics.\nSee OpenAI's\nFAQ\non what similarity metric to use with OpenAI embeddings.\nSimilarity search\nâ€‹\nGiven a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over\nall\nthe embedded documents to find the most similar ones.\nThere are various ways to do this. As an example, many vectorstores implement\nHNSW (Hierarchical Navigable Small World)\n, a graph-based index structure that allows for efficient similarity search.\nRegardless of the search algorithm used under the hood, the LangChain vectorstore interface has a\nsimilarity_search\nmethod for all integrations.\nThis will take the search query, create an embedding, find similar documents, and return them as a list of\nDocuments\n.\nquery\n=\n\"my query\"\ndocs\n=\nvectorstore\n.\nsimilarity_search\n(\nquery\n)\nMany vectorstores support search parameters to be passed with the\nsimilarity_search\nmethod. See the documentation for the specific vectorstore you are using to see what parameters are supported.\nAs an example\nPinecone\nseveral parameters that are important general concepts:\nMany vectorstores support\nthe\nk\n, which controls the number of documents to return, and\nfilter\n, which allows for filtering documents by metadata.\nquery (str) - Text to look up documents similar to.\nk (int) - Number of documents to return. Defaults to 4.\nfilter (dict | None) - Dictionary of argument(s) to filter on metadata\nFurther reading\nSee the\nhow-to guide\nfor more details on how to use the\nsimilarity_search\nmethod.\nSee the\nintegrations page\nfor more details on arguments that can be passed in to the\nsimilarity_search\nmethod for specific vectorstores.\nMetadata filtering\nâ€‹\nWhile vectorstore implement a search algorithm to efficiently search over\nall\nthe embedded documents to find the most similar ones, many also support filtering on metadata.\nMetadata filtering helps narrow down the search by applying specific conditions such as retrieving documents from a particular source or date range. These two concepts work well together:\nSemantic search\n: Query the unstructured data directly, often via embedding or keyword similarity.\nMetadata search\n: Apply structured query to the metadata, filtering specific documents.\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\nHere is example usage with\nPinecone\n, showing that we filter for all documents that have the metadata key\nsource\nwith value\ntweet\n.\nvectorstore\n.\nsimilarity_search\n(\n\"LangChain provides abstractions to make working with LLMs easy\"\n,\nk\n=\n2\n,\nfilter\n=\n{\n\"source\"\n:\n\"tweet\"\n}\n,\n)\nFurther reading\nSee Pinecone's\ndocumentation\non filtering with metadata.\nSee the\nlist of LangChain vectorstore integrations\nthat support metadata filtering.\nAdvanced search and retrieval techniques\nâ€‹\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example,\nmaximal marginal relevance\nis a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some\nvector stores\noffer built-in\nhybrid-search\nto combine keyword and semantic similarity search, which marries the benefits of both approaches.\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with\nsimilarity_search\n.\nSee this\nhow-to guide on hybrid search\nfor more details.\nName\nWhen to use\nDescription\nHybrid search\nWhen combining keyword-based and semantic similarity.\nHybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.\nPaper\n.\nMaximal Marginal Relevance (MMR)\nWhen needing to diversify search results.\nMMR attempts to diversify the results of a search to avoid returning similar and redundant documents.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/why_langchain/",
    "Conceptual guide\nWhy LangChain?\nOn this page\nWhy LangChain?\nThe goal of\nlangchain\nthe Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\nThis page will talk about the LangChain ecosystem as a whole.\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\nFeatures\nâ€‹\nThere are several primary needs that LangChain aims to address:\nStandardized component interfaces:\nThe growing number of\nmodels\nand\nrelated components\nfor AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\nOrchestration:\nAs applications become more complex, combining multiple components and models, there's\na growing need to efficiently connect these elements into control flows\nthat can\naccomplish diverse tasks\n.\nOrchestration\nis crucial for building such applications.\nObservability and evaluation:\nAs applications become more complex, it becomes increasingly difficult to understand what is happening within them.\nFurthermore, the pace of development can become rate-limited by the\nparadox of choice\n.\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost.\nObservability\nand evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\nStandardized component interfaces\nâ€‹\nLangChain provides common interfaces for components that are central to many AI applications.\nAs an example, all\nchat models\nimplement the\nBaseChatModel\ninterface.\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like\ntool calling\nand\nstructured outputs\n.\nExample: chat models\nâ€‹\nMany\nmodel providers\nsupport\ntool calling\n, a critical feature for many applications (e.g.,\nagents\n), that allows a developer to request model responses that match a particular schema.\nThe APIs for each provider differ.\nLangChain's\nchat model\ninterface provides a common way to bind\ntools\nto a model in order to support\ntool calling\n:\n# Tool creation\ntools\n=\n[\nmy_tool\n]\n# Tool binding\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\ntools\n)\nSimilarly, getting models to produce\nstructured outputs\nis an extremely common use case.\nProviders support different approaches for this, including\nJSON mode or tool calling\n, with different APIs.\nLangChain's\nchat model\ninterface provides a common way to produce structured outputs using the\nwith_structured_output()\nmethod:\n# Define schema\nschema\n=\n.\n.\n.\n# Bind schema to model\nmodel_with_structure\n=\nmodel\n.\nwith_structured_output\n(\nschema\n)\nExample: retrievers\nâ€‹\nIn the context of\nRAG\nand LLM application components, LangChain's\nretriever\ninterface provides a standard way to connect to many different types of data services or databases (e.g.,\nvector stores\nor databases).\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the\nrunnable interface\n, meaning they can be invoked in a common manner.\ndocuments\n=\nmy_retriever\n.\ninvoke\n(\n\"What is the meaning of life?\"\n)\nOrchestration\nâ€‹\nWhile standardization for individual components is useful, we've increasingly seen that developers want to\ncombine\ncomponents into more complex applications.\nThis motivates the need for\norchestration\n.\nThere are several common characteristics of LLM applications that this orchestration layer should support:\nComplex control flow:\nThe application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\nPersistence\n:\nThe application needs to maintain\nshort-term and / or long-term memory\n.\nHuman-in-the-loop\n:\nThe application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\nThe recommended way to orchestrate components for complex applications is\nLangGraph\n.\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\nLangGraph comes with built-in support for\npersistence\n,\nhuman-in-the-loop\n,\nmemory\n, and other features.\nIt's particularly well suited for building\nagents\nor\nmulti-agent\napplications.\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph\nwithout\nusing LangChain components.\nFurther reading\nHave a look at our free course,\nIntroduction to LangGraph\n, to learn more about how to use LangGraph to build complex applications.\nObservability and evaluation\nâ€‹\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice.\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost.\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\nLangSmith\nis our platform that supports observability and evaluation for AI applications.\nSee our conceptual guides on\nevaluations\nand\ntracing\nfor more details.\nFurther reading\nSee our video playlist on\nLangSmith tracing and evaluations\nfor more details.\nConclusion\nâ€‹\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\nEase of swapping providers:\nIt allows you to swap out different component providers without having to change the underlying code.\nAdvanced features:\nIt provides common methods for more advanced features, such as\nstreaming\nand\ntool calling\n.\nLangGraph\nmakes it possible to orchestrate complex applications (e.g.,\nagents\n) and provide features like including\npersistence\n,\nhuman-in-the-loop\n, or\nmemory\n.\nLangSmith\nmakes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/v0_3/",
    "Versions\nv0.3\nOn this page\nLangChain v0.3\nLast updated: 09.16.24\nWhat's changed\nâ€‹\nAll packages have been upgraded from Pydantic 1 to Pydantic 2 internally. Use of Pydantic 2 in user code is fully supported with all packages without the need for bridges like\nlangchain_core.pydantic_v1\nor\npydantic.v1\n.\nPydantic 1 will no longer be supported as it reached its end-of-life in June 2024.\nPython 3.8 will no longer be supported as its end-of-life is October 2024.\nThese are the only breaking changes.\nWhat's new\nâ€‹\nThe following features have been added during the development of 0.2.x:\nMoved more integrations from\nlangchain-community\nto their own\nlangchain-x\npackages. This is a non-breaking change, as the legacy implementations are left in\nlangchain-community\nand marked as deprecated. This allows us to better manage the dependencies of, test, and version these integrations. You can see all the latest integration packages in the\nAPI reference\n.\nSimplified tool definition and usage. Read more\nhere\n.\nAdded utilities for interacting with chat models:\nuniversal model constructor\n,\nrate limiter\n,\nmessage utilities\n,\nAdded the ability to\ndispatch custom events\n.\nRevamped integration docs and API reference. Read more\nhere\n.\nMarked as deprecated a number of legacy chains and added migration guides for all of them. These are slated for removal in\nlangchain\n1.0.0. See the deprecated chains and associated\nmigration guides here\n.\nHow to update your code\nâ€‹\nIf you're using\nlangchain\n/\nlangchain-community\n/\nlangchain-core\n0.0 or 0.1, we recommend that you first\nupgrade to 0.2\n.\nIf you're using\nlanggraph\n, upgrade to\nlanggraph>=0.2.20,<0.3\n. This will work with either 0.2 or 0.3 versions of all the base packages.\nHere is a complete list of all packages that have been released and what we recommend upgrading your version constraints to.\nAny package that now requires\nlangchain-core\n0.3 had a minor version bump.\nAny package that is now compatible with both\nlangchain-core\n0.2 and 0.3 had a patch version bump.\nYou can use the\nlangchain-cli\nto update deprecated imports automatically.\nThe CLI will handle updating deprecated imports that were introduced in LangChain 0.0.x and LangChain 0.1, as\nwell as updating the\nlangchain_core.pydantic_v1\nand\nlangchain.pydantic_v1\nimports.\nBase packages\nâ€‹\nPackage\nLatest\nRecommended constraint\nlangchain\n0.3.0\n>=0.3,<0.4\nlangchain-community\n0.3.0\n>=0.3,<0.4\nlangchain-text-splitters\n0.3.0\n>=0.3,<0.4\nlangchain-core\n0.3.0\n>=0.3,<0.4\nlangchain-experimental\n0.3.0\n>=0.3,<0.4\nDownstream packages\nâ€‹\nPackage\nLatest\nRecommended constraint\nlanggraph\n0.2.20\n>=0.2.20,<0.3\nlangserve\n0.3.0\n>=0.3,<0.4\nIntegration packages\nâ€‹\nPackage\nLatest\nRecommended constraint\nlangchain-ai21\n0.2.0\n>=0.2,<0.3\nlangchain-aws\n0.2.0\n>=0.2,<0.3\nlangchain-anthropic\n0.2.0\n>=0.2,<0.3\nlangchain-astradb\n0.4.1\n>=0.4.1,<0.5\nlangchain-azure-dynamic-sessions\n0.2.0\n>=0.2,<0.3\nlangchain-box\n0.2.0\n>=0.2,<0.3\nlangchain-chroma\n0.1.4\n>=0.1.4,<0.2\nlangchain-cohere\n0.3.0\n>=0.3,<0.4\nlangchain-elasticsearch\n0.3.0\n>=0.3,<0.4\nlangchain-exa\n0.2.0\n>=0.2,<0.3\nlangchain-fireworks\n0.2.0\n>=0.2,<0.3\nlangchain-groq\n0.2.0\n>=0.2,<0.3\nlangchain-google-community\n2.0.0\n>=2,<3\nlangchain-google-genai\n2.0.0\n>=2,<3\nlangchain-google-vertexai\n2.0.0\n>=2,<3\nlangchain-huggingface\n0.1.0\n>=0.1,<0.2\nlangchain-ibm\n0.3.0\n>=0.3,<0.4\nlangchain-milvus\n0.1.6\n>=0.1.6,<0.2\nlangchain-mistralai\n0.2.0\n>=0.2,<0.3\nlangchain-mongodb\n0.2.0\n>=0.2,<0.3\nlangchain-nomic\n0.1.3\n>=0.1.3,<0.2\nlangchain-nvidia\n0.3.0\n>=0.3,<0.4\nlangchain-ollama\n0.2.0\n>=0.2,<0.3\nlangchain-openai\n0.2.0\n>=0.2,<0.3\nlangchain-pinecone\n0.2.0\n>=0.2,<0.3\nlangchain-postgres\n0.0.13\n>=0.0.13,<0.1\nlangchain-prompty\n0.1.0\n>=0.1,<0.2\nlangchain-qdrant\n0.1.4\n>=0.1.4,<0.2\nlangchain-redis\n0.1.0\n>=0.1,<0.2\nlangchain-sema4\n0.2.0\n>=0.2,<0.3\nlangchain-together\n0.2.0\n>=0.2,<0.3\nlangchain-unstructured\n0.1.4\n>=0.1.4,<0.2\nlangchain-upstage\n0.3.0\n>=0.3,<0.4\nlangchain-voyageai\n0.2.0\n>=0.2,<0.3\nlangchain-weaviate\n0.0.3\n>=0.0.3,<0.1\nOnce you've updated to recent versions of the packages, you may need to address the following issues stemming from the internal switch from Pydantic v1 to Pydantic v2:\nIf your code depends on Pydantic aside from LangChain, you will need to upgrade your pydantic version constraints to be\npydantic>=2,<3\n.  See\nPydantic's migration guide\nfor help migrating your non-LangChain code to Pydantic v2 if you use pydantic v1.\nThere are a number of side effects to LangChain components caused by the internal switch from Pydantic v1 to v2. We have listed some of the common cases below together with the recommended solutions.\nCommon issues when transitioning to Pydantic 2\nâ€‹\n1. Do not use the\nlangchain_core.pydantic_v1\nnamespace\nâ€‹\nReplace any usage of\nlangchain_core.pydantic_v1\nor\nlangchain.pydantic_v1\nwith\ndirect imports from\npydantic\n.\nFor example,\nfrom\nlangchain_core\n.\npydantic_v1\nimport\nBaseModel\nto:\nfrom\npydantic\nimport\nBaseModel\nThis may require you to make additional updates to your Pydantic code given that there are a number of breaking changes in Pydantic 2. See the\nPydantic Migration\nfor how to upgrade your code from Pydantic 1 to 2.\n2. Passing Pydantic objects to LangChain APIs\nâ€‹\nUsers using the following APIs:\nBaseChatModel.bind_tools\nBaseChatModel.with_structured_output\nTool.from_function\nStructuredTool.from_function\nshould ensure that they are passing Pydantic 2 objects to these APIs rather than\nPydantic 1 objects (created via the\npydantic.v1\nnamespace of pydantic 2).\ncaution\nWhile\nv1\nobjects may be accepted by some of these APIs, users are advised to\nuse Pydantic 2 objects to avoid future issues.\n3. Sub-classing LangChain models\nâ€‹\nAny sub-classing from existing LangChain models (e.g.,\nBaseTool\n,\nBaseChatModel\n,\nLLM\n)\nshould upgrade to use Pydantic 2 features.\nFor example, any user code that's relying on Pydantic 1 features (e.g.,\nvalidator\n) should\nbe updated to the Pydantic 2 equivalent (e.g.,\nfield_validator\n), and any references to\npydantic.v1\n,\nlangchain_core.pydantic_v1\n,\nlangchain.pydantic_v1\nshould be replaced\nwith imports from\npydantic\n.\nfrom\npydantic\n.\nv1\nimport\nvalidator\n,\nField\n# if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\nclass\nCustomTool\n(\nBaseTool\n)\n:\n# BaseTool is v1 code\nx\n:\nint\n=\nField\n(\ndefault\n=\n1\n)\ndef\n_run\n(\n*\nargs\n,\n**\nkwargs\n)\n:\nreturn\n\"hello\"\n@validator\n(\n'x'\n)\n# v1 code\n@classmethod\ndef\nvalidate_x\n(\ncls\n,\nx\n:\nint\n)\n-\n>\nint\n:\nreturn\n1\nShould change to:\nfrom\npydantic\nimport\nField\n,\nfield_validator\n# pydantic v2\nfrom\nlangchain_core\n.\npydantic_v1\nimport\nBaseTool\nclass\nCustomTool\n(\nBaseTool\n)\n:\n# BaseTool is v1 code\nx\n:\nint\n=\nField\n(\ndefault\n=\n1\n)\ndef\n_run\n(\n*\nargs\n,\n**\nkwargs\n)\n:\nreturn\n\"hello\"\n@field_validator\n(\n'x'\n)\n# v2 code\n@classmethod\ndef\nvalidate_x\n(\ncls\n,\nx\n:\nint\n)\n-\n>\nint\n:\nreturn\n1\nCustomTool\n(\nname\n=\n'custom_tool'\n,\ndescription\n=\n\"hello\"\n,\nx\n=\n1\n,\n)\n4. model_rebuild()\nâ€‹\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\nYou can read more about\nmodel_rebuild\nhere\n.\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nBaseOutputParser\nclass\nFooParser\n(\nBaseOutputParser\n)\n:\n.\n.\n.\nAPI Reference:\nBaseOutputParser\nNew code:\nfrom\ntyping\nimport\nOptional\nas\nOptional\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nBaseOutputParser\nclass\nFooParser\n(\nBaseOutputParser\n)\n:\n.\n.\n.\nFooParser\n.\nmodel_rebuild\n(\n)\nAPI Reference:\nBaseOutputParser\nMigrate using langchain-cli\nâ€‹\nThe\nlangchain-cli\ncan help update deprecated LangChain imports in your code automatically.\nPlease note that the\nlangchain-cli\nonly handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\nFor help with the Pydantic 1 to 2 migration itself please refer to the\nPydantic Migration Guidelines\n.\nAs of 0.0.31, the\nlangchain-cli\nrelies on\ngritql\nfor applying code mods.\nInstallation\nâ€‹\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\nUsage\nâ€‹\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like\ngit\n).\nThe\nlangchain-cli\nwill handle the\nlangchain_core.pydantic_v1\ndeprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,\nfrom langchain.chat_models import ChatOpenAI\nwhich should be\nfrom langchain_openai import ChatOpenAI\n),\nYou will need to run the migration script\ntwice\nas it only applies one import replacement per run.\nFor example, say that your code is still using the old import\nfrom langchain.chat_models import ChatOpenAI\n:\nAfter the first run, you'll get:\nfrom langchain_community.chat_models import ChatOpenAI\nAfter the second run, you'll get:\nfrom langchain_openai import ChatOpenAI\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply\n# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\nOther options\nâ€‹\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Approve changes interactively\nlangchain-cli migrate --interactive [path to code]\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/v0_2/overview/",
    "Versions\nv0.2\nOverview\nOn this page\nOverview\nWhatâ€™s new in LangChain?\nâ€‹\nThe following features have been added during the development of 0.1.x:\nBetter streaming support via the\nEvent Streaming API\n.\nStandardized tool calling support\nA standardized interface for\nstructuring output\n@chain decorator\nto more easily create\nRunnableLambdas\nhttps://python.langchain.com/docs/expression_language/how_to/inspect/\nIn Python, better async support for many core abstractions (thank you\n@cbornet\n!!)\nInclude response metadata in\nAIMessage\nto make it easy to access raw output from the underlying models\nTooling to visualize\nyour runnables\nor\nyour langgraph app\nInteroperability of chat message histories across most providers\nOver 20+ partner packages in python\nfor popular integrations\nWhatâ€™s coming to LangChain?\nâ€‹\nWeâ€™ve been working hard on\nlanggraph\n. We will be building more capabilities on top of it and focusing on making it the go-to framework for agent architectures.\nVectorstores V2! Weâ€™ll be revisiting our vectorstores abstractions to help improve usability and reliability.\nBetter documentation and versioned docs!\nWeâ€™re planning a breaking release (0.3.0) sometime between July-September to\nupgrade to full support of Pydantic 2\n, and will drop support for Pydantic 1 (including objects originating from the\nv1\nnamespace of Pydantic 2).\nWhat changed?\nâ€‹\nDue to the rapidly evolving field, LangChain has also evolved rapidly.\nThis document serves to outline at a high level what has changed and why.\nTLDR\nâ€‹\nAs of 0.2.0:\nThis release completes the work that we started with release 0.1.0 by removing the dependency of\nlangchain\non\nlangchain-community\n.\nlangchain\npackage no longer requires\nlangchain-community\n. Instead\nlangchain-community\nwill now depend on\nlangchain-core\nand\nlangchain\n.\nUser code that still relies on deprecated imports from\nlangchain\nwill continue to work as long\nlangchain_community\nis installed. These imports will start raising errors in release 0.4.x.\nAs of 0.1.0:\nlangchain\nwas split into the following component packages:\nlangchain-core\n,\nlangchain\n,\nlangchain-community\n,\nlangchain-[partner]\nto improve the usability of langchain code in production settings. You can read more about it on our\nblog\n.\nEcosystem organization\nâ€‹\nBy the release of 0.1.0, LangChain had grown to a large ecosystem with many integrations and a large community.\nTo improve the usability of LangChain in production, we split the single\nlangchain\npackage into multiple packages. This allowed us to create a good foundation architecture for the LangChain ecosystem and improve the usability of\nlangchain\nin production.\nHere is the high level break down of the Eco-system:\nlangchain-core\n:  contains core abstractions involving LangChain Runnables, tooling for observability, and base implementations of important abstractions (e.g., Chat Models).\nlangchain:\ncontains generic code that is built using interfaces defined in\nlangchain-core\n. This package is for code that generalizes well across different implementations of specific interfaces. For example,\ncreate_tool_calling_agent\nworks across chat models that support\ntool calling capabilities\n.\nlangchain-community\n: community maintained 3rd party integrations. Contains integrations based on interfaces defined in\nlangchain-core\n. Maintained by the LangChain community.\nPartner Packages (e.g., langchain-[partner])\n: Partner packages are packages dedicated to especially popular integrations (e.g.,\nlangchain-openai\n,\nlangchain-anthropic\netc.). The dedicated packages generally benefit from better reliability and support.\nlanggraph\n: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\nlangserve\n: Deploy LangChain chains as REST APIs.\nIn the 0.1.0 release,\nlangchain-community\nwas retained as required a dependency of\nlangchain\n.\nThis allowed imports of vectorstores, chat models, and other integrations to continue working through\nlangchain\nrather than forcing users to update all of their imports to\nlangchain-community\n.\nFor the 0.2.0 release, weâ€™re removing the dependency of\nlangchain\non\nlangchain-community\n. This is something weâ€™ve been planning to do since the 0.1 release because we believe this is the right package architecture.\nOld imports will continue to work as long as\nlangchain-community\nis installed. These imports will be removed in the 0.4.0 release.\nTo understand why we think breaking the dependency of\nlangchain\non\nlangchain-community\nis best we should understand what each package is meant to do.\nlangchain\nis meant to contain high-level chains and agent architectures. The logic in these should be specified at the level of abstractions like\nChatModel\nand\nRetriever\n, and should not be specific to any one integration. This has two main benefits:\nlangchain\nis fairly lightweight. Here is the full list of required dependencies (after the split)\npython = \">=3.8.1,<4.0\"\nlangchain-core = \"^0.2.0\"\nlangchain-text-splitters = \">=0.0.1,<0.1\"\nlangsmith = \"^0.1.17\"\npydantic = \">=1,<3\"\nSQLAlchemy = \">=1.4,<3\"\nrequests = \"^2\"\nPyYAML = \">=5.3\"\nnumpy = \"^1\"\naiohttp = \"^3.8.3\"\ntenacity = \"^8.1.0\"\njsonpatch = \"^1.33\"\nlangchain\nchains/agents are largely integration-agnostic, which makes it easy to experiment with different integrations and future-proofs your code should there be issues with one specific integration.\nThere is also a third less tangible benefit which is that being integration-agnostic forces us to find only those very generic abstractions and architectures which generalize well across integrations. Given how general the abilities of the foundational tech are, and how quickly the space is moving, having generic architectures is a good way of future-proofing your applications.\nlangchain-community\nis intended to have all integration-specific components that are not yet being maintained in separate\nlangchain-{partner}\npackages. Today this is still the majority of integrations and a lot of code. This code is primarily contributed by the community, while\nlangchain\nis largely written by core maintainers. All of these integrations use optional dependencies and conditional imports, which prevents dependency bloat and conflicts but means compatible dependency versions are not made explicit. Given the volume of integrations in\nlangchain-community\nand the speed at which integrations change, itâ€™s very hard to follow semver versioning, and we currently donâ€™t.\nAll of which is to say that thereâ€™s no large benefits to\nlangchain\ndepending on\nlangchain-community\nand some obvious downsides: the functionality in\nlangchain\nshould be integration agnostic anyways,\nlangchain-community\ncanâ€™t be properly versioned, and depending on\nlangchain-community\nincreases the\nvulnerability surface\nof\nlangchain\n.\nFor more context about the reason for the organization please see our blog:\nhttps://blog.langchain.dev/langchain-v0-1-0/\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/",
    "Versions\nMigrating from v0.0 chains\nOn this page\nHow to migrate from v0.0 chains\nLangChain has evolved since its initial release, and many of the original \"Chain\" classes\nhave been deprecated in favor of the more flexible and powerful frameworks of LCEL and LangGraph.\nThis guide will help you migrate your existing v0.0 chains to the new abstractions.\nHow deprecated implementations work\nEven though many of these implementations are deprecated, they are\nstill supported\nin the codebase.\nHowever, they are not recommended for new development, and we recommend re-implementing them using the following guides!\nTo see the planned removal version for each deprecated implementation, check their API reference.\nPrerequisites\nThese guides assume some familiarity with the following concepts:\nLangChain Expression Language\nLangGraph\nLangChain maintains a number of legacy abstractions. Many of these can be reimplemented via short combinations of LCEL and LangGraph primitives.\nLCEL\nâ€‹\nLCEL\nis designed to streamline the process of building useful apps with LLMs and combining related components. It does this by providing:\nA unified interface\n: Every LCEL object implements the\nRunnable\ninterface, which defines a common set of invocation methods (\ninvoke\n,\nbatch\n,\nstream\n,\nainvoke\n, ...). This makes it possible to also automatically and consistently support useful operations like streaming of intermediate steps and batching, since every chain composed of LCEL objects is itself an LCEL object.\nComposition primitives\n: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internals, and more.\nLangGraph\nâ€‹\nLangGraph\n, built on top of LCEL, allows for performant orchestrations of application components while maintaining concise and readable code. It includes built-in persistence, support for cycles, and prioritizes controllability.\nIf LCEL grows unwieldy for larger or more complex chains, they may benefit from a LangGraph implementation.\nAdvantages\nâ€‹\nUsing these frameworks for existing v0.0 chains confers some advantages:\nThe resulting chains typically implement the full\nRunnable\ninterface, including streaming and asynchronous support where appropriate;\nThe chains may be more easily extended or modified;\nThe parameters of the chain are typically surfaced for easier customization (e.g., prompts) over previous versions, which tended to be subclasses and had opaque parameters and internals.\nIf using LangGraph, the chain supports built-in persistence, allowing for conversational experiences via a \"memory\" of the chat history.\nIf using LangGraph, the steps of the chain can be streamed, allowing for greater control and customizability.\nThe below pages assist with migration from various specific chains to LCEL and LangGraph:\nLLMChain\nConversationChain\nRetrievalQA\nConversationalRetrievalChain\nStuffDocumentsChain\nMapReduceDocumentsChain\nMapRerankDocumentsChain\nRefineDocumentsChain\nLLMRouterChain\nMultiPromptChain\nLLMMathChain\nConstitutionalChain\nCheck out the\nLCEL conceptual docs\nand\nLangGraph docs\nfor more background information.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/constitutional_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from ConstitutionalChain\nOn this page\nMigrating from ConstitutionalChain\nConstitutionalChain\nallowed for a LLM to critique and revise generations based on\nprinciples\n, structured as combinations of critique and revision requests. For example, a principle might include a request to identify harmful content, and a request to rewrite the content.\nConstitutional AI principles\nare based on the\nConstitutional AI: Harmlessness from AI Feedback\npaper.\nIn\nConstitutionalChain\n, this structure of critique requests and associated revisions was formatted into a LLM prompt and parsed out of string responses. This is more naturally achieved via\nstructured output\nfeatures of chat models. We can construct a simple chain in\nLangGraph\nfor this purpose. Some advantages of this approach include:\nLeverage tool-calling capabilities of chat models that have been fine-tuned for this purpose;\nReduce parsing errors from extracting expression from a string LLM response;\nDelegation of instructions to\nmessage roles\n(e.g., chat models can understand what a\nToolMessage\nrepresents without the need for additional prompting);\nSupport for streaming, both of individual tokens and chain steps.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nLegacy\nâ€‹\nDetails\nfrom\nlangchain\n.\nchains\nimport\nConstitutionalChain\n,\nLLMChain\nfrom\nlangchain\n.\nchains\n.\nconstitutional_ai\n.\nmodels\nimport\nConstitutionalPrinciple\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nOpenAI\nllm\n=\nOpenAI\n(\n)\nqa_prompt\n=\nPromptTemplate\n(\ntemplate\n=\n\"Q: {question} A:\"\n,\ninput_variables\n=\n[\n\"question\"\n]\n,\n)\nqa_chain\n=\nLLMChain\n(\nllm\n=\nllm\n,\nprompt\n=\nqa_prompt\n)\nconstitutional_chain\n=\nConstitutionalChain\n.\nfrom_llm\n(\nllm\n=\nllm\n,\nchain\n=\nqa_chain\n,\nconstitutional_principles\n=\n[\nConstitutionalPrinciple\n(\ncritique_request\n=\n\"Tell if this answer is good.\"\n,\nrevision_request\n=\n\"Give a better answer.\"\n,\n)\n]\n,\nreturn_intermediate_steps\n=\nTrue\n,\n)\nresult\n=\nconstitutional_chain\n.\ninvoke\n(\n\"What is the meaning of life?\"\n)\nAPI Reference:\nPromptTemplate\nresult\n{'question': 'What is the meaning of life?',\n'output': 'The meaning of life is a deeply personal and ever-evolving concept. It is a journey of self-discovery and growth, and can be different for each individual. Some may find meaning in relationships, others in achieving their goals, and some may never find a concrete answer. Ultimately, the meaning of life is what we make of it.',\n'initial_output': ' The meaning of life is a subjective concept that can vary from person to person. Some may believe that the purpose of life is to find happiness and fulfillment, while others may see it as a journey of self-discovery and personal growth. Ultimately, the meaning of life is something that each individual must determine for themselves.',\n'critiques_and_revisions': [('This answer is good in that it recognizes and acknowledges the subjective nature of the question and provides a valid and thoughtful response. However, it could have also mentioned that the meaning of life is a complex and deeply personal concept that can also change and evolve over time for each individual. Critique Needed.',\n'The meaning of life is a deeply personal and ever-evolving concept. It is a journey of self-discovery and growth, and can be different for each individual. Some may find meaning in relationships, others in achieving their goals, and some may never find a concrete answer. Ultimately, the meaning of life is what we make of it.')]}\nAbove, we've returned intermediate steps showing:\nThe original question;\nThe initial output;\nCritiques and revisions;\nThe final output (matching a revision).\nLangGraph\nâ€‹\nDetails\nBelow, we use the\n.with_structured_output\nmethod to simultaneously generate (1) a judgment of whether a critique is needed, and (2) the critique. We surface all prompts involved for clarity and ease of customizability.\nNote that we are also able to stream intermediate steps with this implementation, so we can monitor and if needed intervene during its execution.\nfrom\ntyping\nimport\nList\n,\nOptional\n,\nTuple\nfrom\nlangchain\n.\nchains\n.\nconstitutional_ai\n.\nmodels\nimport\nConstitutionalPrinciple\nfrom\nlangchain\n.\nchains\n.\nconstitutional_ai\n.\nprompts\nimport\n(\nCRITIQUE_PROMPT\n,\nREVISION_PROMPT\n,\n)\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nclass\nCritique\n(\nTypedDict\n)\n:\n\"\"\"Generate a critique, if needed.\"\"\"\ncritique_needed\n:\nAnnotated\n[\nbool\n,\n.\n.\n.\n,\n\"Whether or not a critique is needed.\"\n]\ncritique\n:\nAnnotated\n[\nstr\n,\n.\n.\n.\n,\n\"If needed, the critique.\"\n]\ncritique_prompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"Critique this response according to the critique request. \"\n\"If no critique is needed, specify that.\\n\\n\"\n\"Query: {query}\\n\\n\"\n\"Response: {response}\\n\\n\"\n\"Critique request: {critique_request}\"\n)\nrevision_prompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"Revise this response according to the critique and reivsion request.\\n\\n\"\n\"Query: {query}\\n\\n\"\n\"Response: {response}\\n\\n\"\n\"Critique request: {critique_request}\\n\\n\"\n\"Critique: {critique}\\n\\n\"\n\"If the critique does not identify anything worth changing, ignore the \"\n\"revision request and return 'No revisions needed'. If the critique \"\n\"does identify something worth changing, revise the response based on \"\n\"the revision request.\\n\\n\"\n\"Revision Request: {revision_request}\"\n)\nchain\n=\nllm\n|\nStrOutputParser\n(\n)\ncritique_chain\n=\ncritique_prompt\n|\nllm\n.\nwith_structured_output\n(\nCritique\n)\nrevision_chain\n=\nrevision_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\nclass\nState\n(\nTypedDict\n)\n:\nquery\n:\nstr\nconstitutional_principles\n:\nList\n[\nConstitutionalPrinciple\n]\ninitial_response\n:\nstr\ncritiques_and_revisions\n:\nList\n[\nTuple\n[\nstr\n,\nstr\n]\n]\nresponse\n:\nstr\nasync\ndef\ngenerate_response\n(\nstate\n:\nState\n)\n:\n\"\"\"Generate initial response.\"\"\"\nresponse\n=\nawait\nchain\n.\nainvoke\n(\nstate\n[\n\"query\"\n]\n)\nreturn\n{\n\"response\"\n:\nresponse\n,\n\"initial_response\"\n:\nresponse\n}\nasync\ndef\ncritique_and_revise\n(\nstate\n:\nState\n)\n:\n\"\"\"Critique and revise response according to principles.\"\"\"\ncritiques_and_revisions\n=\n[\n]\nresponse\n=\nstate\n[\n\"initial_response\"\n]\nfor\nprinciple\nin\nstate\n[\n\"constitutional_principles\"\n]\n:\ncritique\n=\nawait\ncritique_chain\n.\nainvoke\n(\n{\n\"query\"\n:\nstate\n[\n\"query\"\n]\n,\n\"response\"\n:\nresponse\n,\n\"critique_request\"\n:\nprinciple\n.\ncritique_request\n,\n}\n)\nif\ncritique\n[\n\"critique_needed\"\n]\n:\nrevision\n=\nawait\nrevision_chain\n.\nainvoke\n(\n{\n\"query\"\n:\nstate\n[\n\"query\"\n]\n,\n\"response\"\n:\nresponse\n,\n\"critique_request\"\n:\nprinciple\n.\ncritique_request\n,\n\"critique\"\n:\ncritique\n[\n\"critique\"\n]\n,\n\"revision_request\"\n:\nprinciple\n.\nrevision_request\n,\n}\n)\nresponse\n=\nrevision\ncritiques_and_revisions\n.\nappend\n(\n(\ncritique\n[\n\"critique\"\n]\n,\nrevision\n)\n)\nelse\n:\ncritiques_and_revisions\n.\nappend\n(\n(\ncritique\n[\n\"critique\"\n]\n,\n\"\"\n)\n)\nreturn\n{\n\"critiques_and_revisions\"\n:\ncritiques_and_revisions\n,\n\"response\"\n:\nresponse\n,\n}\ngraph\n=\nStateGraph\n(\nState\n)\ngraph\n.\nadd_node\n(\n\"generate_response\"\n,\ngenerate_response\n)\ngraph\n.\nadd_node\n(\n\"critique_and_revise\"\n,\ncritique_and_revise\n)\ngraph\n.\nadd_edge\n(\nSTART\n,\n\"generate_response\"\n)\ngraph\n.\nadd_edge\n(\n\"generate_response\"\n,\n\"critique_and_revise\"\n)\ngraph\n.\nadd_edge\n(\n\"critique_and_revise\"\n,\nEND\n)\napp\n=\ngraph\n.\ncompile\n(\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nStateGraph\nconstitutional_principles\n=\n[\nConstitutionalPrinciple\n(\ncritique_request\n=\n\"Tell if this answer is good.\"\n,\nrevision_request\n=\n\"Give a better answer.\"\n,\n)\n]\nquery\n=\n\"What is the meaning of life? Answer in 10 words or fewer.\"\nasync\nfor\nstep\nin\napp\n.\nastream\n(\n{\n\"query\"\n:\nquery\n,\n\"constitutional_principles\"\n:\nconstitutional_principles\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nsubset\n=\n[\n\"initial_response\"\n,\n\"critiques_and_revisions\"\n,\n\"response\"\n]\nprint\n(\n{\nk\n:\nv\nfor\nk\n,\nv\nin\nstep\n.\nitems\n(\n)\nif\nk\nin\nsubset\n}\n)\n{}\n{'initial_response': 'Finding purpose, connection, and joy in our experiences and relationships.', 'response': 'Finding purpose, connection, and joy in our experiences and relationships.'}\n{'initial_response': 'Finding purpose, connection, and joy in our experiences and relationships.', 'critiques_and_revisions': [(\"The response exceeds the 10-word limit, providing a more elaborate answer than requested. A concise response, such as 'To seek purpose and joy in life,' would better align with the query.\", 'To seek purpose and joy in life.')], 'response': 'To seek purpose and joy in life.'}\nNext steps\nâ€‹\nSee guides for generating structured output\nhere\n.\nCheck out the\nLangGraph documentation\nfor detail on building with LangGraph.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/conversation_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from ConversationalChain\nOn this page\nMigrating from ConversationalChain\nConversationChain\nincorporated a memory of previous messages to sustain a stateful conversation.\nSome advantages of switching to the Langgraph implementation are:\nInnate support for threads/separate sessions. To make this work with\nConversationChain\n, you'd need to instantiate a separate memory class outside the chain.\nMore explicit parameters.\nConversationChain\ncontains a hidden default prompt, which can cause confusion.\nStreaming support.\nConversationChain\nonly supports streaming via callbacks.\nLanggraph's\ncheckpointing\nsystem supports multiple threads or sessions, which can be specified via the\n\"thread_id\"\nkey in its configuration parameters.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nLegacy\nâ€‹\nDetails\nfrom\nlangchain\n.\nchains\nimport\nConversationChain\nfrom\nlangchain\n.\nmemory\nimport\nConversationBufferMemory\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\ntemplate\n=\n\"\"\"\nYou are a pirate. Answer the following questions as best you can.\nChat history: {history}\nQuestion: {input}\n\"\"\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nmemory\n=\nConversationBufferMemory\n(\n)\nchain\n=\nConversationChain\n(\nllm\n=\nChatOpenAI\n(\n)\n,\nmemory\n=\nmemory\n,\nprompt\n=\nprompt\n,\n)\nchain\n(\n{\n\"input\"\n:\n\"I'm Bob, how are you?\"\n}\n)\nAPI Reference:\nChatPromptTemplate\n{'input': \"I'm Bob, how are you?\",\n'history': '',\n'response': \"Arrr matey, I be a pirate sailin' the high seas. What be yer business with me?\"}\nchain\n(\n{\n\"input\"\n:\n\"What is my name?\"\n}\n)\n{'input': 'What is my name?',\n'history': \"Human: I'm Bob, how are you?\\nAI: Arrr matey, I be a pirate sailin' the high seas. What be yer business with me?\",\n'response': 'Your name be Bob, matey.'}\nLanggraph\nâ€‹\nDetails\nimport\nuuid\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nMessagesState\n,\nStateGraph\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\n# Define a new graph\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define the function that calls the model\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\nresponse\n=\nmodel\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\nreturn\n{\n\"messages\"\n:\nresponse\n}\n# Define the two nodes we will cycle between\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\n# Add memory\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\n# The thread id is a unique key that identifies\n# this particular conversation.\n# We'll just generate a random uuid here.\nthread_id\n=\nuuid\n.\nuuid4\n(\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\nthread_id\n}\n}\nAPI Reference:\nMemorySaver\n|\nStateGraph\nquery\n=\n\"I'm Bob, how are you?\"\ninput_messages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a pirate. Answer the following questions as best you can.\"\n,\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nquery\n}\n,\n]\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nI'm Bob, how are you?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nAhoy, Bob! I be feelin' as lively as a ship in full sail! How be ye on this fine day?\nquery\n=\n\"What is my name?\"\ninput_messages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nquery\n}\n]\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\ninput_messages\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat is my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYe be callin' yerself Bob, I reckon! A fine name for a swashbuckler like yerself!\nNext steps\nâ€‹\nSee\nthis tutorial\nfor a more end-to-end guide on building with\nRunnableWithMessageHistory\n.\nCheck out the\nLCEL conceptual docs\nfor more background information.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/conversation_retrieval_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from ConversationalRetrievalChain\nOn this page\nMigrating from ConversationalRetrievalChain\nThe\nConversationalRetrievalChain\nwas an all-in one way that combined retrieval-augmented generation with chat history, allowing you to \"chat with\" your documents.\nAdvantages of switching to the LCEL implementation are similar to the\nRetrievalQA\nmigration guide\n:\nClearer internals. The\nConversationalRetrievalChain\nchain hides an entire question rephrasing step which dereferences the initial query against the chat history.\nThis means the class contains two sets of configurable prompts, LLMs, etc.\nMore easily return source documents.\nSupport for runnable methods like streaming and async operations.\nHere are equivalent implementations with custom prompts.\nWe'll use the following ingestion code to load a\nblog post by Lilian Weng\non autonomous agents into a local vector store:\nShared setup\nâ€‹\nFor both versions, we'll need to load the data with the\nWebBaseLoader\ndocument loader, split it with\nRecursiveCharacterTextSplitter\n, and add it to an in-memory\nFAISS\nvector store.\nWe will also instantiate a chat model to use.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\ncommunity langchain langchain\n-\nopenai faiss\n-\ncpu beautifulsoup4\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\n# Load docs\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_openai\n.\nchat_models\nimport\nChatOpenAI\nfrom\nlangchain_openai\n.\nembeddings\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nloader\n=\nWebBaseLoader\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n)\ndata\n=\nloader\n.\nload\n(\n)\n# Split\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n500\n,\nchunk_overlap\n=\n0\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndata\n)\n# Store splits\nvectorstore\n=\nFAISS\n.\nfrom_documents\n(\ndocuments\n=\nall_splits\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\n# LLM\nllm\n=\nChatOpenAI\n(\n)\nLegacy\nâ€‹\nDetails\nfrom\nlangchain\n.\nchains\nimport\nConversationalRetrievalChain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\ncondense_question_template\n=\n\"\"\"\nGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:\"\"\"\ncondense_question_prompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\ncondense_question_template\n)\nqa_template\n=\n\"\"\"\nYou are an assistant for question-answering tasks.\nUse the following pieces of retrieved context to answer\nthe question. If you don't know the answer, say that you\ndon't know. Use three sentences maximum and keep the\nanswer concise.\nChat History:\n{chat_history}\nOther context:\n{context}\nQuestion: {question}\n\"\"\"\nqa_prompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\nqa_template\n)\nconvo_qa_chain\n=\nConversationalRetrievalChain\n.\nfrom_llm\n(\nllm\n,\nvectorstore\n.\nas_retriever\n(\n)\n,\ncondense_question_prompt\n=\ncondense_question_prompt\n,\ncombine_docs_chain_kwargs\n=\n{\n\"prompt\"\n:\nqa_prompt\n,\n}\n,\n)\nconvo_qa_chain\n(\n{\n\"question\"\n:\n\"What are autonomous agents?\"\n,\n\"chat_history\"\n:\n\"\"\n,\n}\n)\nAPI Reference:\nChatPromptTemplate\n{'question': 'What are autonomous agents?',\n'chat_history': '',\n'answer': 'Autonomous agents are entities empowered with capabilities like planning, task decomposition, and memory to perform complex tasks independently. These agents can leverage tools like browsing the internet, reading documentation, executing code, and calling APIs to achieve their objectives. They are designed to handle tasks like scientific discovery and experimentation autonomously.'}\nLCEL\nâ€‹\nDetails\nfrom\nlangchain\n.\nchains\nimport\ncreate_history_aware_retriever\n,\ncreate_retrieval_chain\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\nimport\ncreate_stuff_documents_chain\ncondense_question_system_template\n=\n(\n\"Given a chat history and the latest user question \"\n\"which might reference context in the chat history, \"\n\"formulate a standalone question which can be understood \"\n\"without the chat history. Do NOT answer the question, \"\n\"just reformulate it if needed and otherwise return it as is.\"\n)\ncondense_question_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\ncondense_question_system_template\n)\n,\n(\n\"placeholder\"\n,\n\"{chat_history}\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\nhistory_aware_retriever\n=\ncreate_history_aware_retriever\n(\nllm\n,\nvectorstore\n.\nas_retriever\n(\n)\n,\ncondense_question_prompt\n)\nsystem_prompt\n=\n(\n\"You are an assistant for question-answering tasks. \"\n\"Use the following pieces of retrieved context to answer \"\n\"the question. If you don't know the answer, say that you \"\n\"don't know. Use three sentences maximum and keep the \"\n\"answer concise.\"\n\"\\n\\n\"\n\"{context}\"\n)\nqa_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nsystem_prompt\n)\n,\n(\n\"placeholder\"\n,\n\"{chat_history}\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\nqa_chain\n=\ncreate_stuff_documents_chain\n(\nllm\n,\nqa_prompt\n)\nconvo_qa_chain\n=\ncreate_retrieval_chain\n(\nhistory_aware_retriever\n,\nqa_chain\n)\nconvo_qa_chain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What are autonomous agents?\"\n,\n\"chat_history\"\n:\n[\n]\n,\n}\n)\n{'input': 'What are autonomous agents?',\n'chat_history': [],\n'context': [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:'),\nDocument(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Weng, Lilian. (Jun 2023). â€œLLM-powered Autonomous Agentsâ€. Lilâ€™Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),\nDocument(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#'),\nDocument(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\")],\n'answer': 'Autonomous agents are entities that can act independently to achieve specific goals or tasks without direct human intervention. These agents have the ability to perceive their environment, make decisions, and take actions based on their programming or learning. They can perform tasks such as planning, execution, and problem-solving autonomously.'}\nNext steps\nâ€‹\nYou've now seen how to migrate existing usage of some legacy chains to LCEL.\nNext, check out the\nLCEL conceptual docs\nfor more background information.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/llm_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from LLMChain\nOn this page\nMigrating from LLMChain\nLLMChain\ncombined a prompt template, LLM, and output parser into a class.\nSome advantages of switching to the LCEL implementation are:\nClarity around contents and parameters. The legacy\nLLMChain\ncontains a default output parser and other options.\nEasier streaming.\nLLMChain\nonly supports streaming via callbacks.\nEasier access to raw message outputs if desired.\nLLMChain\nonly exposes these via a parameter or via callback.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nLegacy\nâ€‹\nDetails\nfrom\nlangchain\n.\nchains\nimport\nLLMChain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"user\"\n,\n\"Tell me a {adjective} joke\"\n)\n]\n,\n)\nlegacy_chain\n=\nLLMChain\n(\nllm\n=\nChatOpenAI\n(\n)\n,\nprompt\n=\nprompt\n)\nlegacy_result\n=\nlegacy_chain\n(\n{\n\"adjective\"\n:\n\"funny\"\n}\n)\nlegacy_result\nAPI Reference:\nChatPromptTemplate\n{'adjective': 'funny',\n'text': \"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\"}\nNote that\nLLMChain\nby default returned a\ndict\ncontaining both the input and the output from\nStrOutputParser\n, so to extract the output, you need to access the\n\"text\"\nkey.\nlegacy_result\n[\n\"text\"\n]\n\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\"\nLCEL\nâ€‹\nDetails\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"user\"\n,\n\"Tell me a {adjective} joke\"\n)\n]\n,\n)\nchain\n=\nprompt\n|\nChatOpenAI\n(\n)\n|\nStrOutputParser\n(\n)\nchain\n.\ninvoke\n(\n{\n\"adjective\"\n:\n\"funny\"\n}\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n'Why was the math book sad?\\n\\nBecause it had too many problems.'\nIf you'd like to mimic the\ndict\npackaging of input and output in\nLLMChain\n, you can use a\nRunnablePassthrough.assign\nlike:\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nouter_chain\n=\nRunnablePassthrough\n(\n)\n.\nassign\n(\ntext\n=\nchain\n)\nouter_chain\n.\ninvoke\n(\n{\n\"adjective\"\n:\n\"funny\"\n}\n)\nAPI Reference:\nRunnablePassthrough\n{'adjective': 'funny',\n'text': 'Why did the scarecrow win an award? Because he was outstanding in his field!'}\nNext steps\nâ€‹\nSee\nthis tutorial\nfor more detail on building with prompt templates, LLMs, and output parsers.\nCheck out the\nLCEL conceptual docs\nfor more background information.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/llm_math_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from LLMMathChain\nOn this page\nMigrating from LLMMathChain\nLLMMathChain\nenabled the evaluation of mathematical expressions generated by a LLM. Instructions for generating the expressions were formatted into the prompt, and the expressions were parsed out of the string response before evaluation using the\nnumexpr\nlibrary.\nThis is more naturally achieved via\ntool calling\n. We can equip a chat model with a simple calculator tool leveraging\nnumexpr\nand construct a simple chain around it using\nLangGraph\n. Some advantages of this approach include:\nLeverage tool-calling capabilities of chat models that have been fine-tuned for this purpose;\nReduce parsing errors from extracting expression from a string LLM response;\nDelegation of instructions to\nmessage roles\n(e.g., chat models can understand what a\nToolMessage\nrepresents without the need for additional prompting);\nSupport for streaming, both of individual tokens and chain steps.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet numexpr\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nLegacy\nâ€‹\nDetails\nfrom\nlangchain\n.\nchains\nimport\nLLMMathChain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nchain\n=\nLLMMathChain\n.\nfrom_llm\n(\nllm\n)\nchain\n.\ninvoke\n(\n\"What is 551368 divided by 82?\"\n)\nAPI Reference:\nChatPromptTemplate\n{'question': 'What is 551368 divided by 82?', 'answer': 'Answer: 6724.0'}\nLangGraph\nâ€‹\nDetails\nimport\nmath\nfrom\ntyping\nimport\nAnnotated\n,\nSequence\nimport\nnumexpr\nfrom\nlangchain_core\n.\nmessages\nimport\nBaseMessage\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nStateGraph\nfrom\nlanggraph\n.\ngraph\n.\nmessage\nimport\nadd_messages\nfrom\nlanggraph\n.\nprebuilt\n.\ntool_node\nimport\nToolNode\nfrom\ntyping_extensions\nimport\nTypedDict\n@tool\ndef\ncalculator\n(\nexpression\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Calculate expression using Python's numexpr library.\nExpression should be a single line mathematical expression\nthat solves the problem.\nExamples:\n\"37593 * 67\" for \"37593 times 67\"\n\"37593**(1/5)\" for \"37593^(1/5)\"\n\"\"\"\nlocal_dict\n=\n{\n\"pi\"\n:\nmath\n.\npi\n,\n\"e\"\n:\nmath\n.\ne\n}\nreturn\nstr\n(\nnumexpr\n.\nevaluate\n(\nexpression\n.\nstrip\n(\n)\n,\nglobal_dict\n=\n{\n}\n,\n# restrict access to globals\nlocal_dict\n=\nlocal_dict\n,\n# add common mathematical functions\n)\n)\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\ntools\n=\n[\ncalculator\n]\nllm_with_tools\n=\nllm\n.\nbind_tools\n(\ntools\n,\ntool_choice\n=\n\"any\"\n)\nclass\nChainState\n(\nTypedDict\n)\n:\n\"\"\"LangGraph state.\"\"\"\nmessages\n:\nAnnotated\n[\nSequence\n[\nBaseMessage\n]\n,\nadd_messages\n]\nasync\ndef\nacall_chain\n(\nstate\n:\nChainState\n,\nconfig\n:\nRunnableConfig\n)\n:\nlast_message\n=\nstate\n[\n\"messages\"\n]\n[\n-\n1\n]\nresponse\n=\nawait\nllm_with_tools\n.\nainvoke\n(\nstate\n[\n\"messages\"\n]\n,\nconfig\n)\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\nasync\ndef\nacall_model\n(\nstate\n:\nChainState\n,\nconfig\n:\nRunnableConfig\n)\n:\nresponse\n=\nawait\nllm\n.\nainvoke\n(\nstate\n[\n\"messages\"\n]\n,\nconfig\n)\nreturn\n{\n\"messages\"\n:\n[\nresponse\n]\n}\ngraph_builder\n=\nStateGraph\n(\nChainState\n)\ngraph_builder\n.\nadd_node\n(\n\"call_tool\"\n,\nacall_chain\n)\ngraph_builder\n.\nadd_node\n(\n\"execute_tool\"\n,\nToolNode\n(\ntools\n)\n)\ngraph_builder\n.\nadd_node\n(\n\"call_model\"\n,\nacall_model\n)\ngraph_builder\n.\nset_entry_point\n(\n\"call_tool\"\n)\ngraph_builder\n.\nadd_edge\n(\n\"call_tool\"\n,\n\"execute_tool\"\n)\ngraph_builder\n.\nadd_edge\n(\n\"execute_tool\"\n,\n\"call_model\"\n)\ngraph_builder\n.\nadd_edge\n(\n\"call_model\"\n,\nEND\n)\nchain\n=\ngraph_builder\n.\ncompile\n(\n)\nAPI Reference:\nBaseMessage\n|\nRunnableConfig\n|\ntool\n|\nStateGraph\n|\nadd_messages\n|\nToolNode\n# Visualize chain:\nfrom\nIPython\n.\ndisplay\nimport\nImage\nImage\n(\nchain\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n# Stream chain steps:\nexample_query\n=\n\"What is 551368 divided by 82\"\nevents\n=\nchain\n.\nastream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\nexample_query\n)\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\nasync\nfor\nevent\nin\nevents\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat is 551368 divided by 82\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\ncalculator (call_1ic3gjuII0Aq9vxlSYiwvjSb)\nCall ID: call_1ic3gjuII0Aq9vxlSYiwvjSb\nArgs:\nexpression: 551368 / 82\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: calculator\n6724.0\n==================================\u001b[1m Ai Message \u001b[0m==================================\n551368 divided by 82 equals 6724.\nNext steps\nâ€‹\nSee guides for building and working with tools\nhere\n.\nCheck out the\nLangGraph documentation\nfor detail on building with LangGraph.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/llm_router_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from LLMRouterChain\nOn this page\nMigrating from LLMRouterChain\nThe\nLLMRouterChain\nrouted an input query to one of multiple destinations-- that is, given an input query, it used a LLM to select from a list of destination chains, and passed its inputs to the selected chain.\nLLMRouterChain\ndoes not support common\nchat model\nfeatures, such as message roles and\ntool calling\n. Under the hood,\nLLMRouterChain\nroutes a query by instructing the LLM to generate JSON-formatted text, and parsing out the intended destination.\nConsider an example from a\nMultiPromptChain\n, which uses\nLLMRouterChain\n. Below is an (example) default prompt:\nfrom\nlangchain\n.\nchains\n.\nrouter\n.\nmulti_prompt\nimport\nMULTI_PROMPT_ROUTER_TEMPLATE\ndestinations\n=\n\"\"\"\nanimals: prompt for animal expert\nvegetables: prompt for a vegetable expert\n\"\"\"\nrouter_template\n=\nMULTI_PROMPT_ROUTER_TEMPLATE\n.\nformat\n(\ndestinations\n=\ndestinations\n)\nprint\n(\nrouter_template\n.\nreplace\n(\n\"`\"\n,\n\"'\"\n)\n)\n# for rendering purposes\nGiven a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n<< FORMATTING >>\nReturn a markdown code snippet with a JSON object formatted to look like:\n'''json\n{{\n\"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n\"next_inputs\": string \\ a potentially modified version of the original input\n}}\n'''\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\nREMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n<< CANDIDATE PROMPTS >>\nanimals: prompt for animal expert\nvegetables: prompt for a vegetable expert\n<< INPUT >>\n{input}\n<< OUTPUT (must include '''json at the start of the response) >>\n<< OUTPUT (must end with ''') >>\nMost of the behavior is determined via a single natural language prompt. Chat models that support\ntool calling\nfeatures confer a number of advantages for this task:\nSupports chat prompt templates, including messages with\nsystem\nand other roles;\nTool-calling models are fine-tuned to generate structured output;\nSupport for runnable methods like streaming and async operations.\nNow let's look at\nLLMRouterChain\nside-by-side with an LCEL implementation that uses tool-calling. Note that for this guide we will\nlangchain-openai >= 0.1.20\n:\n%\npip install\n-\nqU langchain\n-\ncore langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nLegacy\nâ€‹\nDetails\nfrom\nlangchain\n.\nchains\n.\nrouter\n.\nllm_router\nimport\nLLMRouterChain\n,\nRouterOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nrouter_prompt\n=\nPromptTemplate\n(\n# Note: here we use the prompt template from above. Generally this would need\n# to be customized.\ntemplate\n=\nrouter_template\n,\ninput_variables\n=\n[\n\"input\"\n]\n,\noutput_parser\n=\nRouterOutputParser\n(\n)\n,\n)\nchain\n=\nLLMRouterChain\n.\nfrom_llm\n(\nllm\n,\nrouter_prompt\n)\nAPI Reference:\nPromptTemplate\nresult\n=\nchain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What color are carrots?\"\n}\n)\nprint\n(\nresult\n[\n\"destination\"\n]\n)\nvegetables\nLCEL\nâ€‹\nDetails\nfrom\noperator\nimport\nitemgetter\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\ntyping_extensions\nimport\nTypedDict\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nroute_system\n=\n\"Route the user's query to either the animal or vegetable expert.\"\nroute_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nroute_system\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\n# Define schema for output:\nclass\nRouteQuery\n(\nTypedDict\n)\n:\n\"\"\"Route query to destination expert.\"\"\"\ndestination\n:\nLiteral\n[\n\"animal\"\n,\n\"vegetable\"\n]\n# Instead of writing formatting instructions into the prompt, we\n# leverage .with_structured_output to coerce the output into a simple\n# schema.\nchain\n=\nroute_prompt\n|\nllm\n.\nwith_structured_output\n(\nRouteQuery\n)\nAPI Reference:\nChatPromptTemplate\n|\nRunnablePassthrough\nresult\n=\nchain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What color are carrots?\"\n}\n)\nprint\n(\nresult\n[\n\"destination\"\n]\n)\nvegetable\nNext steps\nâ€‹\nSee\nthis tutorial\nfor more detail on building with prompt templates, LLMs, and output parsers.\nCheck out the\nLCEL conceptual docs\nfor more background information.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from MapReduceDocumentsChain\nOn this page\nMigrating from MapReduceDocumentsChain\nMapReduceDocumentsChain\nimplements a map-reduce strategy over (potentially long) texts. The strategy is as follows:\nSplit a text into smaller documents;\nMap a process onto the smaller documents;\nReduce or consolidate the results of the process into a final result.\nNote that the map step is typically parallelized over the input documents.\nA common process applied in this context is summarization, in which the map step summarizes individual documents, and the reduce step generates a summary of the summaries.\nIn the reduce step,\nMapReduceDocumentsChain\nsupports a recursive \"collapsing\" of the summaries: the inputs would be partitioned based on a token limit, and summaries would be generated of the partitions. This step would be repeated until the total length of the summaries was within a desired limit, allowing for the summarization of arbitrary-length text. This is particularly useful for models with smaller context windows.\nLangGraph supports\nmap-reduce\nworkflows, and confers a number of advantages for this problem:\nLangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;\nLangGraph's\ncheckpointing\nsupports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.\nThe LangGraph implementation is easier to extend, as we will see below.\nBelow we will go through both\nMapReduceDocumentsChain\nand a corresponding LangGraph implementation, first on a simple example for illustrative purposes, and second on a longer example text to demonstrate the recursive reduce step.\nLet's first load a chat model:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nBasic example (short documents)\nâ€‹\nLet's use the following 3 documents for illustrative purposes.\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\ndocuments\n=\n[\nDocument\n(\npage_content\n=\n\"Apples are red\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"apple_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Blueberries are blue\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"blueberry_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Bananas are yelow\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"banana_book\"\n}\n)\n,\n]\nAPI Reference:\nDocument\nLegacy\nâ€‹\nDetails\nBelow we show an implementation with\nMapReduceDocumentsChain\n. We define the prompt templates for the map and reduce steps, instantiate separate chains for these steps, and finally instantiate the\nMapReduceDocumentsChain\n:\nfrom\nlangchain\n.\nchains\nimport\nMapReduceDocumentsChain\n,\nReduceDocumentsChain\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\n.\nstuff\nimport\nStuffDocumentsChain\nfrom\nlangchain\n.\nchains\n.\nllm\nimport\nLLMChain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\n# Map\nmap_template\n=\n\"Write a concise summary of the following: {docs}.\"\nmap_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\nmap_template\n)\n]\n)\nmap_chain\n=\nLLMChain\n(\nllm\n=\nllm\n,\nprompt\n=\nmap_prompt\n)\n# Reduce\nreduce_template\n=\n\"\"\"\nThe following is a set of summaries:\n{docs}\nTake these and distill it into a final, consolidated summary\nof the main themes.\n\"\"\"\nreduce_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\nreduce_template\n)\n]\n)\nreduce_chain\n=\nLLMChain\n(\nllm\n=\nllm\n,\nprompt\n=\nreduce_prompt\n)\n# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\ncombine_documents_chain\n=\nStuffDocumentsChain\n(\nllm_chain\n=\nreduce_chain\n,\ndocument_variable_name\n=\n\"docs\"\n)\n# Combines and iteratively reduces the mapped documents\nreduce_documents_chain\n=\nReduceDocumentsChain\n(\n# This is final chain that is called.\ncombine_documents_chain\n=\ncombine_documents_chain\n,\n# If documents exceed context for `StuffDocumentsChain`\ncollapse_documents_chain\n=\ncombine_documents_chain\n,\n# The maximum number of tokens to group documents into.\ntoken_max\n=\n1000\n,\n)\n# Combining documents by mapping a chain over them, then combining results\nmap_reduce_chain\n=\nMapReduceDocumentsChain\n(\n# Map chain\nllm_chain\n=\nmap_chain\n,\n# Reduce chain\nreduce_documents_chain\n=\nreduce_documents_chain\n,\n# The variable name in the llm_chain to put the documents in\ndocument_variable_name\n=\n\"docs\"\n,\n# Return the results of the map steps in the output\nreturn_intermediate_steps\n=\nFalse\n,\n)\nAPI Reference:\nChatPromptTemplate\nresult\n=\nmap_reduce_chain\n.\ninvoke\n(\ndocuments\n)\nprint\n(\nresult\n[\n\"output_text\"\n]\n)\nFruits come in a variety of colors, with apples being red, blueberries being blue, and bananas being yellow.\nIn the\nLangSmith trace\nwe observe four LLM calls: one summarizing each of the three input documents, and one summarizing the summaries.\nLangGraph\nâ€‹\nBelow we show a LangGraph implementation, using the same prompt templates as above. The graph includes a node for generating summaries which is mapped across a list of input documents. This node then flows to a second node that generates the final summary.\nDetails\nWe will need to install\nlanggraph\n:\n%\npip install\n-\nqU langgraph\nimport\noperator\nfrom\ntyping\nimport\nAnnotated\n,\nList\n,\nTypedDict\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlanggraph\n.\nconstants\nimport\nSend\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\nmap_template\n=\n\"Write a concise summary of the following: {context}.\"\nreduce_template\n=\n\"\"\"\nThe following is a set of summaries:\n{docs}\nTake these and distill it into a final, consolidated summary\nof the main themes.\n\"\"\"\nmap_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\nmap_template\n)\n]\n)\nreduce_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\nreduce_template\n)\n]\n)\nmap_chain\n=\nmap_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\nreduce_chain\n=\nreduce_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\n# Graph components: define the components that will make up the graph\n# This will be the overall state of the main graph.\n# It will contain the input document contents, corresponding\n# summaries, and a final summary.\nclass\nOverallState\n(\nTypedDict\n)\n:\n# Notice here we use the operator.add\n# This is because we want combine all the summaries we generate\n# from individual nodes back into one list - this is essentially\n# the \"reduce\" part\ncontents\n:\nList\n[\nstr\n]\nsummaries\n:\nAnnotated\n[\nlist\n,\noperator\n.\nadd\n]\nfinal_summary\n:\nstr\n# This will be the state of the node that we will \"map\" all\n# documents to in order to generate summaries\nclass\nSummaryState\n(\nTypedDict\n)\n:\ncontent\n:\nstr\n# Here we generate a summary, given a document\nasync\ndef\ngenerate_summary\n(\nstate\n:\nSummaryState\n)\n:\nresponse\n=\nawait\nmap_chain\n.\nainvoke\n(\nstate\n[\n\"content\"\n]\n)\nreturn\n{\n\"summaries\"\n:\n[\nresponse\n]\n}\n# Here we define the logic to map out over the documents\n# We will use this an edge in the graph\ndef\nmap_summaries\n(\nstate\n:\nOverallState\n)\n:\n# We will return a list of `Send` objects\n# Each `Send` object consists of the name of a node in the graph\n# as well as the state to send to that node\nreturn\n[\nSend\n(\n\"generate_summary\"\n,\n{\n\"content\"\n:\ncontent\n}\n)\nfor\ncontent\nin\nstate\n[\n\"contents\"\n]\n]\n# Here we will generate the final summary\nasync\ndef\ngenerate_final_summary\n(\nstate\n:\nOverallState\n)\n:\nresponse\n=\nawait\nreduce_chain\n.\nainvoke\n(\nstate\n[\n\"summaries\"\n]\n)\nreturn\n{\n\"final_summary\"\n:\nresponse\n}\n# Construct the graph: here we put everything together to construct our graph\ngraph\n=\nStateGraph\n(\nOverallState\n)\ngraph\n.\nadd_node\n(\n\"generate_summary\"\n,\ngenerate_summary\n)\ngraph\n.\nadd_node\n(\n\"generate_final_summary\"\n,\ngenerate_final_summary\n)\ngraph\n.\nadd_conditional_edges\n(\nSTART\n,\nmap_summaries\n,\n[\n\"generate_summary\"\n]\n)\ngraph\n.\nadd_edge\n(\n\"generate_summary\"\n,\n\"generate_final_summary\"\n)\ngraph\n.\nadd_edge\n(\n\"generate_final_summary\"\n,\nEND\n)\napp\n=\ngraph\n.\ncompile\n(\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nSend\n|\nStateGraph\nfrom\nIPython\n.\ndisplay\nimport\nImage\nImage\n(\napp\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\nNote that calling the graph in streaming mode allows us to monitor steps and potentially take action on them during execution.\n# Call the graph:\nasync\nfor\nstep\nin\napp\n.\nastream\n(\n{\n\"contents\"\n:\n[\ndoc\n.\npage_content\nfor\ndoc\nin\ndocuments\n]\n}\n)\n:\nprint\n(\nstep\n)\n{'generate_summary': {'summaries': ['Apples are typically red in color.']}}\n{'generate_summary': {'summaries': ['Bananas are yellow in color.']}}\n{'generate_summary': {'summaries': ['Blueberries are a type of fruit that are blue in color.']}}\n{'generate_final_summary': {'final_summary': 'The main themes are the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'}}\nIn the\nLangSmith trace\nwe recover the same four LLM calls as before.\nSummarizing long documents\nâ€‹\nMap-reduce flows are particularly useful when texts are long compared to the context window of a LLM.\nMapReduceDocumentsChain\nsupports a recursive \"collapsing\" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.\nThis \"collapse\" step is implemented as a\nwhile\nloop within\nMapReduceDocumentsChain\n. We can demonstrate this step on a longer text, a\nLLM Powered Autonomous Agents\nblog post by Lilian Weng (as featured in the\nRAG tutorial\nand other documentation).\nFirst we load the post and chunk it into smaller \"sub documents\":\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_text_splitters\nimport\nCharacterTextSplitter\nloader\n=\nWebBaseLoader\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n)\ndocuments\n=\nloader\n.\nload\n(\n)\ntext_splitter\n=\nCharacterTextSplitter\n.\nfrom_tiktoken_encoder\n(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n0\n)\nsplit_docs\n=\ntext_splitter\n.\nsplit_documents\n(\ndocuments\n)\nprint\n(\nf\"Generated\n{\nlen\n(\nsplit_docs\n)\n}\ndocuments.\"\n)\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\nCreated a chunk of size 1003, which is longer than the specified 1000\n``````output\nGenerated 14 documents.\nLegacy\nâ€‹\nDetails\nWe can invoke\nMapReduceDocumentsChain\nas before:\nresult\n=\nmap_reduce_chain\n.\ninvoke\n(\nsplit_docs\n)\nprint\n(\nresult\n[\n\"output_text\"\n]\n)\nThe article discusses the use of Large Language Models (LLMs) to power autonomous agents in various tasks, showcasing their capabilities in problem-solving beyond generating written content. Key components such as planning, memory optimization, and tool use are explored, with proof-of-concept demos like AutoGPT and GPT-Engineer demonstrating the potential of LLM-powered agents. Challenges include limitations in historical information retention and natural language interface reliability, while the potential of LLMs in enhancing reasoning, problem-solving, and planning proficiency for autonomous agents is highlighted. Overall, the article emphasizes the versatility and power of LLMs in creating intelligent agents for tasks like scientific discovery and experiment design.\nConsider the\nLangSmith trace\nfor the above invocation. When instantiating our\nReduceDocumentsChain\n, we set a\ntoken_max\nof 1,000 tokens. This results in a total of 17 LLM calls:\n14 calls are for summarizing the 14 sub-documents generated by our text splitter.\nThis generated summaries that totaled about 1,000 - 2,000 tokens. Because we set a\ntoken_max\nof 1,000, there are two more calls to summarize (or \"collapse\") these summaries.\nOne final call is for generating a final summary of the two \"collapsed\" summaries.\nLangGraph\nâ€‹\nDetails\nWe can extend our original map-reduce implementation in LangGraph to implement the same recursive collapsing step. We make the following changes:\nAdd a\ncollapsed_summaries\nkey to the state to store the collapsed summaries;\nUpdate the final summarization node to summarize the collapsed summaries;\nAdd a\ncollapse_summaries\nnode that partitions a list of documents based on a token length (1,000 tokens here, as before) and generates summaries of each partition and stores the result in\ncollapsed_summaries\n.\nWe add a conditional edge from\ncollapse_summaries\nto itself to form a loop: if the collapsed summaries total more than the\ntoken_max\n, we re-run the node.\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\n.\nreduce\nimport\n(\nacollapse_docs\n,\nsplit_list_of_docs\n,\n)\ndef\nlength_function\n(\ndocuments\n:\nList\n[\nDocument\n]\n)\n-\n>\nint\n:\n\"\"\"Get number of tokens for input contents.\"\"\"\nreturn\nsum\n(\nllm\n.\nget_num_tokens\n(\ndoc\n.\npage_content\n)\nfor\ndoc\nin\ndocuments\n)\ntoken_max\n=\n1000\nclass\nOverallState\n(\nTypedDict\n)\n:\ncontents\n:\nList\n[\nstr\n]\nsummaries\n:\nAnnotated\n[\nlist\n,\noperator\n.\nadd\n]\ncollapsed_summaries\n:\nList\n[\nDocument\n]\n# add key for collapsed summaries\nfinal_summary\n:\nstr\n# Add node to store summaries for collapsing\ndef\ncollect_summaries\n(\nstate\n:\nOverallState\n)\n:\nreturn\n{\n\"collapsed_summaries\"\n:\n[\nDocument\n(\nsummary\n)\nfor\nsummary\nin\nstate\n[\n\"summaries\"\n]\n]\n}\n# Modify final summary to read off collapsed summaries\nasync\ndef\ngenerate_final_summary\n(\nstate\n:\nOverallState\n)\n:\nresponse\n=\nawait\nreduce_chain\n.\nainvoke\n(\nstate\n[\n\"collapsed_summaries\"\n]\n)\nreturn\n{\n\"final_summary\"\n:\nresponse\n}\ngraph\n=\nStateGraph\n(\nOverallState\n)\ngraph\n.\nadd_node\n(\n\"generate_summary\"\n,\ngenerate_summary\n)\n# same as before\ngraph\n.\nadd_node\n(\n\"collect_summaries\"\n,\ncollect_summaries\n)\ngraph\n.\nadd_node\n(\n\"generate_final_summary\"\n,\ngenerate_final_summary\n)\n# Add node to collapse summaries\nasync\ndef\ncollapse_summaries\n(\nstate\n:\nOverallState\n)\n:\ndoc_lists\n=\nsplit_list_of_docs\n(\nstate\n[\n\"collapsed_summaries\"\n]\n,\nlength_function\n,\ntoken_max\n)\nresults\n=\n[\n]\nfor\ndoc_list\nin\ndoc_lists\n:\nresults\n.\nappend\n(\nawait\nacollapse_docs\n(\ndoc_list\n,\nreduce_chain\n.\nainvoke\n)\n)\nreturn\n{\n\"collapsed_summaries\"\n:\nresults\n}\ngraph\n.\nadd_node\n(\n\"collapse_summaries\"\n,\ncollapse_summaries\n)\ndef\nshould_collapse\n(\nstate\n:\nOverallState\n,\n)\n-\n>\nLiteral\n[\n\"collapse_summaries\"\n,\n\"generate_final_summary\"\n]\n:\nnum_tokens\n=\nlength_function\n(\nstate\n[\n\"collapsed_summaries\"\n]\n)\nif\nnum_tokens\n>\ntoken_max\n:\nreturn\n\"collapse_summaries\"\nelse\n:\nreturn\n\"generate_final_summary\"\ngraph\n.\nadd_conditional_edges\n(\nSTART\n,\nmap_summaries\n,\n[\n\"generate_summary\"\n]\n)\ngraph\n.\nadd_edge\n(\n\"generate_summary\"\n,\n\"collect_summaries\"\n)\ngraph\n.\nadd_conditional_edges\n(\n\"collect_summaries\"\n,\nshould_collapse\n)\ngraph\n.\nadd_conditional_edges\n(\n\"collapse_summaries\"\n,\nshould_collapse\n)\ngraph\n.\nadd_edge\n(\n\"generate_final_summary\"\n,\nEND\n)\napp\n=\ngraph\n.\ncompile\n(\n)\nLangGraph allows the graph structure to be plotted to help visualize its function:\nfrom\nIPython\n.\ndisplay\nimport\nImage\nImage\n(\napp\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\nAs before, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.\nNote that because we have a loop in the graph, it can be helpful to specify a\nrecursion_limit\non its execution. This is analogous to\nReduceDocumentsChain.token_max\nto will raise a specific error when the specified limit is exceeded.\nasync\nfor\nstep\nin\napp\n.\nastream\n(\n{\n\"contents\"\n:\n[\ndoc\n.\npage_content\nfor\ndoc\nin\nsplit_docs\n]\n}\n,\n{\n\"recursion_limit\"\n:\n10\n}\n,\n)\n:\nprint\n(\nlist\n(\nstep\n.\nkeys\n(\n)\n)\n)\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['generate_summary']\n['collect_summaries']\n['collapse_summaries']\n['generate_final_summary']\nprint\n(\nstep\n)\n{'generate_final_summary': {'final_summary': 'The summaries discuss the use of Large Language Models (LLMs) to power autonomous agents in various tasks such as problem-solving, planning, and tool use. Key components like planning, memory, and task decomposition are highlighted, along with challenges such as inefficient planning and hallucination. Techniques like Algorithm Distillation and Maximum Inner Product Search are explored for optimization, while frameworks like ReAct and Reflexion show improvements in knowledge-intensive tasks. The importance of accurate interpretation of user input and well-structured code for functional autonomy is emphasized, along with the potential of LLMs in prompting, reasoning, and emergent social behavior in simulation environments. Challenges in real-world scenarios and the use of LLMs with expert-designed tools for tasks like organic synthesis and drug discovery are also discussed.'}}\nIn the corresponding\nLangSmith trace\nwe can see the same 17 LLM calls as before, this time grouped under their respective nodes.\nNext steps\nâ€‹\nCheck out the\nLangGraph documentation\nfor detail on building with LangGraph, including\nthis guide\non the details of map-reduce in LangGraph.\nSee\nthis tutorial\nfor more LLM-based summarization strategies.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from MapRerankDocumentsChain\nOn this page\nMigrating from MapRerankDocumentsChain\nMapRerankDocumentsChain\nimplements a strategy for analyzing long texts. The strategy is as follows:\nSplit a text into smaller documents;\nMap a process to the set of documents, where the process includes generating a score;\nRank the results by score and return the maximum.\nA common process in this scenario is question-answering using pieces of context from a document. Forcing the model to generate a score along with its answer helps to select for answers generated only by relevant context.\nAn\nLangGraph\nimplementation allows for the incorporation of\ntool calling\nand other features for this problem. Below we will go through both\nMapRerankDocumentsChain\nand a corresponding LangGraph implementation on a simple example for illustrative purposes.\nExample\nâ€‹\nLet's go through an example where we analyze a set of documents. Let's use the following 3 documents:\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\ndocuments\n=\n[\nDocument\n(\npage_content\n=\n\"Alice has blue eyes\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"book_chapter_2\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Bob has brown eyes\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"book_chapter_1\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Charlie has green eyes\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"book_chapter_3\"\n}\n)\n,\n]\nAPI Reference:\nDocument\nLegacy\nâ€‹\nDetails\nBelow we show an implementation with\nMapRerankDocumentsChain\n. We define the prompt template for a question-answering task and instantiate a\nLLMChain\nobject for this purpose. We define how documents are formatted into the prompt and ensure consistency among the keys in the various prompts.\nfrom\nlangchain\n.\nchains\nimport\nLLMChain\n,\nMapRerankDocumentsChain\nfrom\nlangchain\n.\noutput_parsers\n.\nregex\nimport\nRegexParser\nfrom\nlangchain_core\n.\nprompts\nimport\nPromptTemplate\nfrom\nlangchain_openai\nimport\nOpenAI\ndocument_variable_name\n=\n\"context\"\nllm\n=\nOpenAI\n(\n)\n# The prompt here should take as an input variable the\n# `document_variable_name`\n# The actual prompt will need to be a lot more complex, this is just\n# an example.\nprompt_template\n=\n(\n\"What color are Bob's eyes? \"\n\"Output both your answer and a score (1-10) of how confident \"\n\"you are in the format: <Answer>\\nScore: <Score>.\\n\\n\"\n\"Provide no other commentary.\\n\\n\"\n\"Context: {context}\"\n)\noutput_parser\n=\nRegexParser\n(\nregex\n=\nr\"(.*?)\\nScore: (.*)\"\n,\noutput_keys\n=\n[\n\"answer\"\n,\n\"score\"\n]\n,\n)\nprompt\n=\nPromptTemplate\n(\ntemplate\n=\nprompt_template\n,\ninput_variables\n=\n[\n\"context\"\n]\n,\noutput_parser\n=\noutput_parser\n,\n)\nllm_chain\n=\nLLMChain\n(\nllm\n=\nllm\n,\nprompt\n=\nprompt\n)\nchain\n=\nMapRerankDocumentsChain\n(\nllm_chain\n=\nllm_chain\n,\ndocument_variable_name\n=\ndocument_variable_name\n,\nrank_key\n=\n\"score\"\n,\nanswer_key\n=\n\"answer\"\n,\n)\nAPI Reference:\nPromptTemplate\nresponse\n=\nchain\n.\ninvoke\n(\ndocuments\n)\nresponse\n[\n\"output_text\"\n]\n/langchain/libs/langchain/langchain/chains/llm.py:369: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\nwarnings.warn(\n'Brown'\nInspecting the\nLangSmith trace\nfor the above run, we can see three LLM calls-- one for each document-- and that the scoring mechanism mitigated against hallucinations.\nLangGraph\nâ€‹\nDetails\nBelow we show a LangGraph implementation of this process. Note that our template is simplified, as we delegate the formatting instructions to the chat model's tool-calling features via the\n.with_structured_output\nmethod.\nHere we follow a basic\nmap-reduce\nworkflow to execute the LLM calls in parallel.\nWe will need to install\nlanggraph\n:\npip install\n-\nqU langgraph\nimport\noperator\nfrom\ntyping\nimport\nAnnotated\n,\nList\n,\nTypedDict\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph\n.\nconstants\nimport\nSend\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\nclass\nAnswerWithScore\n(\nTypedDict\n)\n:\nanswer\n:\nstr\nscore\n:\nAnnotated\n[\nint\n,\n.\n.\n.\n,\n\"Score from 1-10.\"\n]\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\nprompt_template\n=\n\"What color are Bob's eyes?\\n\\nContext: {context}\"\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\nprompt_template\n)\n# The below chain formats context from a document into a prompt, then\n# generates a response structured according to the AnswerWithScore schema.\nmap_chain\n=\nprompt\n|\nllm\n.\nwith_structured_output\n(\nAnswerWithScore\n)\n# Below we define the components that will make up the graph\n# This will be the overall state of the graph.\n# It will contain the input document contents, corresponding\n# answers with scores, and a final answer.\nclass\nState\n(\nTypedDict\n)\n:\ncontents\n:\nList\n[\nstr\n]\nanswers_with_scores\n:\nAnnotated\n[\nlist\n,\noperator\n.\nadd\n]\nanswer\n:\nstr\n# This will be the state of the node that we will \"map\" all\n# documents to in order to generate answers with scores\nclass\nMapState\n(\nTypedDict\n)\n:\ncontent\n:\nstr\n# Here we define the logic to map out over the documents\n# We will use this an edge in the graph\ndef\nmap_analyses\n(\nstate\n:\nState\n)\n:\n# We will return a list of `Send` objects\n# Each `Send` object consists of the name of a node in the graph\n# as well as the state to send to that node\nreturn\n[\nSend\n(\n\"generate_analysis\"\n,\n{\n\"content\"\n:\ncontent\n}\n)\nfor\ncontent\nin\nstate\n[\n\"contents\"\n]\n]\n# Here we generate an answer with score, given a document\nasync\ndef\ngenerate_analysis\n(\nstate\n:\nMapState\n)\n:\nresponse\n=\nawait\nmap_chain\n.\nainvoke\n(\nstate\n[\n\"content\"\n]\n)\nreturn\n{\n\"answers_with_scores\"\n:\n[\nresponse\n]\n}\n# Here we will select the top answer\ndef\npick_top_ranked\n(\nstate\n:\nState\n)\n:\nranked_answers\n=\nsorted\n(\nstate\n[\n\"answers_with_scores\"\n]\n,\nkey\n=\nlambda\nx\n:\n-\nint\n(\nx\n[\n\"score\"\n]\n)\n)\nreturn\n{\n\"answer\"\n:\nranked_answers\n[\n0\n]\n}\n# Construct the graph: here we put everything together to construct our graph\ngraph\n=\nStateGraph\n(\nState\n)\ngraph\n.\nadd_node\n(\n\"generate_analysis\"\n,\ngenerate_analysis\n)\ngraph\n.\nadd_node\n(\n\"pick_top_ranked\"\n,\npick_top_ranked\n)\ngraph\n.\nadd_conditional_edges\n(\nSTART\n,\nmap_analyses\n,\n[\n\"generate_analysis\"\n]\n)\ngraph\n.\nadd_edge\n(\n\"generate_analysis\"\n,\n\"pick_top_ranked\"\n)\ngraph\n.\nadd_edge\n(\n\"pick_top_ranked\"\n,\nEND\n)\napp\n=\ngraph\n.\ncompile\n(\n)\nAPI Reference:\nChatPromptTemplate\n|\nSend\n|\nStateGraph\nfrom\nIPython\n.\ndisplay\nimport\nImage\nImage\n(\napp\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\nresult\n=\nawait\napp\n.\nainvoke\n(\n{\n\"contents\"\n:\n[\ndoc\n.\npage_content\nfor\ndoc\nin\ndocuments\n]\n}\n)\nresult\n[\n\"answer\"\n]\n{'answer': 'Bob has brown eyes.', 'score': 10}\nInspecting the\nLangSmith trace\nfor the above run, we can see three LLM calls as before. Using the model's tool-calling features have also enabled us to remove the parsing step.\nNext steps\nâ€‹\nSee these\nhow-to guides\nfor more on question-answering tasks with RAG.\nCheck out the\nLangGraph documentation\nfor detail on building with LangGraph, including\nthis guide\non the details of map-reduce in LangGraph.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from MultiPromptChain\nOn this page\nMigrating from MultiPromptChain\nThe\nMultiPromptChain\nrouted an input query to one of multiple LLMChains-- that is, given an input query, it used a LLM to select from a list of prompts, formatted the query into the prompt, and generated a response.\nMultiPromptChain\ndoes not support common\nchat model\nfeatures, such as message roles and\ntool calling\n.\nA\nLangGraph\nimplementation confers a number of advantages for this problem:\nSupports chat prompt templates, including messages with\nsystem\nand other roles;\nSupports the use of tool calling for the routing step;\nSupports streaming of both individual steps and output tokens.\nNow let's look at them side-by-side. Note that for this guide we will\nlangchain-openai >= 0.1.20\n%\npip install\n-\nqU langchain\n-\ncore langchain\n-\nopenai\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nLegacy\nâ€‹\nDetails\nfrom\nlangchain\n.\nchains\n.\nrouter\n.\nmulti_prompt\nimport\nMultiPromptChain\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\nprompt_1_template\n=\n\"\"\"\nYou are an expert on animals. Please answer the below query:\n{input}\n\"\"\"\nprompt_2_template\n=\n\"\"\"\nYou are an expert on vegetables. Please answer the below query:\n{input}\n\"\"\"\nprompt_infos\n=\n[\n{\n\"name\"\n:\n\"animals\"\n,\n\"description\"\n:\n\"prompt for an animal expert\"\n,\n\"prompt_template\"\n:\nprompt_1_template\n,\n}\n,\n{\n\"name\"\n:\n\"vegetables\"\n,\n\"description\"\n:\n\"prompt for a vegetable expert\"\n,\n\"prompt_template\"\n:\nprompt_2_template\n,\n}\n,\n]\nchain\n=\nMultiPromptChain\n.\nfrom_prompts\n(\nllm\n,\nprompt_infos\n)\nchain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What color are carrots?\"\n}\n)\n{'input': 'What color are carrots?',\n'text': 'Carrots are most commonly orange, but they can also be found in a variety of other colors including purple, yellow, white, and red. The orange variety is the most popular and widely recognized.'}\nIn the\nLangSmith trace\nwe can see the two steps of this process, including the prompts for routing the query and the final selected prompt.\nLangGraph\nâ€‹\nDetails\npip install\n-\nqU langgraph\nfrom\noperator\nimport\nitemgetter\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\nfrom\ntyping_extensions\nimport\nTypedDict\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)\n# Define the prompts we will route to\nprompt_1\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are an expert on animals.\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\nprompt_2\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are an expert on vegetables.\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\n# Construct the chains we will route to. These format the input query\n# into the respective prompt, run it through a chat model, and cast\n# the result to a string.\nchain_1\n=\nprompt_1\n|\nllm\n|\nStrOutputParser\n(\n)\nchain_2\n=\nprompt_2\n|\nllm\n|\nStrOutputParser\n(\n)\n# Next: define the chain that selects which branch to route to.\n# Here we will take advantage of tool-calling features to force\n# the output to select one of two desired branches.\nroute_system\n=\n\"Route the user's query to either the animal or vegetable expert.\"\nroute_prompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\nroute_system\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n]\n)\n# Define schema for output:\nclass\nRouteQuery\n(\nTypedDict\n)\n:\n\"\"\"Route query to destination expert.\"\"\"\ndestination\n:\nLiteral\n[\n\"animal\"\n,\n\"vegetable\"\n]\nroute_chain\n=\nroute_prompt\n|\nllm\n.\nwith_structured_output\n(\nRouteQuery\n)\n# For LangGraph, we will define the state of the graph to hold the query,\n# destination, and final answer.\nclass\nState\n(\nTypedDict\n)\n:\nquery\n:\nstr\ndestination\n:\nRouteQuery\nanswer\n:\nstr\n# We define functions for each node, including routing the query:\nasync\ndef\nroute_query\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n:\ndestination\n=\nawait\nroute_chain\n.\nainvoke\n(\nstate\n[\n\"query\"\n]\n,\nconfig\n)\nreturn\n{\n\"destination\"\n:\ndestination\n}\n# And one node for each prompt\nasync\ndef\nprompt_1\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n:\nreturn\n{\n\"answer\"\n:\nawait\nchain_1\n.\nainvoke\n(\nstate\n[\n\"query\"\n]\n,\nconfig\n)\n}\nasync\ndef\nprompt_2\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n:\nreturn\n{\n\"answer\"\n:\nawait\nchain_2\n.\nainvoke\n(\nstate\n[\n\"query\"\n]\n,\nconfig\n)\n}\n# We then define logic that selects the prompt based on the classification\ndef\nselect_node\n(\nstate\n:\nState\n)\n-\n>\nLiteral\n[\n\"prompt_1\"\n,\n\"prompt_2\"\n]\n:\nif\nstate\n[\n\"destination\"\n]\n==\n\"animal\"\n:\nreturn\n\"prompt_1\"\nelse\n:\nreturn\n\"prompt_2\"\n# Finally, assemble the multi-prompt chain. This is a sequence of two steps:\n# 1) Select \"animal\" or \"vegetable\" via the route_chain, and collect the answer\n# alongside the input query.\n# 2) Route the input query to chain_1 or chain_2, based on the\n# selection.\ngraph\n=\nStateGraph\n(\nState\n)\ngraph\n.\nadd_node\n(\n\"route_query\"\n,\nroute_query\n)\ngraph\n.\nadd_node\n(\n\"prompt_1\"\n,\nprompt_1\n)\ngraph\n.\nadd_node\n(\n\"prompt_2\"\n,\nprompt_2\n)\ngraph\n.\nadd_edge\n(\nSTART\n,\n\"route_query\"\n)\ngraph\n.\nadd_conditional_edges\n(\n\"route_query\"\n,\nselect_node\n)\ngraph\n.\nadd_edge\n(\n\"prompt_1\"\n,\nEND\n)\ngraph\n.\nadd_edge\n(\n\"prompt_2\"\n,\nEND\n)\napp\n=\ngraph\n.\ncompile\n(\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnableConfig\n|\nStateGraph\nfrom\nIPython\n.\ndisplay\nimport\nImage\nImage\n(\napp\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\nWe can invoke the chain as follows:\nstate\n=\nawait\napp\n.\nainvoke\n(\n{\n\"query\"\n:\n\"what color are carrots\"\n}\n)\nprint\n(\nstate\n[\n\"destination\"\n]\n)\nprint\n(\nstate\n[\n\"answer\"\n]\n)\n{'destination': 'vegetable'}\nCarrots are most commonly orange, but they can also come in a variety of other colors, including purple, red, yellow, and white. The different colors often indicate varying flavors and nutritional profiles. For example, purple carrots contain anthocyanins, while orange carrots are rich in beta-carotene, which is converted to vitamin A in the body.\nIn the\nLangSmith trace\nwe can see the tool call that routed the query and the prompt that was selected to generate the answer.\nOverview:\nâ€‹\nUnder the hood,\nMultiPromptChain\nrouted the query by instructing the LLM to generate JSON-formatted text, and parses out the intended destination. It took a registry of string prompt templates as input.\nThe LangGraph implementation, implemented above via lower-level primitives, uses tool-calling to route to arbitrary chains. In this example, the chains include chat model templates and chat models.\nNext steps\nâ€‹\nSee\nthis tutorial\nfor more detail on building with prompt templates, LLMs, and output parsers.\nCheck out the\nLangGraph documentation\nfor detail on building with LangGraph.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/refine_docs_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from RefineDocumentsChain\nOn this page\nMigrating from RefineDocumentsChain\nRefineDocumentsChain\nimplements a strategy for analyzing long texts. The strategy is as follows:\nSplit a text into smaller documents;\nApply a process to the first document;\nRefine or update the result based on the next document;\nRepeat through the sequence of documents until finished.\nA common process applied in this context is summarization, in which a running summary is modified as we proceed through chunks of a long text. This is particularly useful for texts that are large compared to the context window of a given LLM.\nA\nLangGraph\nimplementation confers a number of advantages for this problem:\nWhere\nRefineDocumentsChain\nrefines the summary via a\nfor\nloop inside the class, a LangGraph implementation lets you step through the execution to monitor or otherwise steer it if needed.\nThe LangGraph implementation supports streaming of both execution steps and individual tokens.\nBecause it is assembled from modular components, it is also simple to extend or modify (e.g., to incorporate\ntool calling\nor other behavior).\nBelow we will go through both\nRefineDocumentsChain\nand a corresponding LangGraph implementation on a simple example for illustrative purposes.\nLet's first load a chat model:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nExample\nâ€‹\nLet's go through an example where we summarize a sequence of documents. We first generate some simple documents for illustrative purposes:\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\ndocuments\n=\n[\nDocument\n(\npage_content\n=\n\"Apples are red\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"apple_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Blueberries are blue\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"blueberry_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Bananas are yelow\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"banana_book\"\n}\n)\n,\n]\nAPI Reference:\nDocument\nLegacy\nâ€‹\nDetails\nBelow we show an implementation with\nRefineDocumentsChain\n. We define the prompt templates for the initial summarization and successive refinements, instantiate separate\nLLMChain\nobjects for these two purposes, and instantiate\nRefineDocumentsChain\nwith these components.\nfrom\nlangchain\n.\nchains\nimport\nLLMChain\n,\nRefineDocumentsChain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nPromptTemplate\nfrom\nlangchain_openai\nimport\nChatOpenAI\n# This controls how each document will be formatted. Specifically,\n# it will be passed to `format_document` - see that function for more\n# details.\ndocument_prompt\n=\nPromptTemplate\n(\ninput_variables\n=\n[\n\"page_content\"\n]\n,\ntemplate\n=\n\"{page_content}\"\n)\ndocument_variable_name\n=\n\"context\"\n# The prompt here should take as an input variable the\n# `document_variable_name`\nsummarize_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\n\"Write a concise summary of the following: {context}\"\n)\n,\n]\n)\ninitial_llm_chain\n=\nLLMChain\n(\nllm\n=\nllm\n,\nprompt\n=\nsummarize_prompt\n)\ninitial_response_name\n=\n\"existing_answer\"\n# The prompt here should take as an input variable the\n# `document_variable_name` as well as `initial_response_name`\nrefine_template\n=\n\"\"\"\nProduce a final summary.\nExisting summary up to this point:\n{existing_answer}\nNew context:\n------------\n{context}\n------------\nGiven the new context, refine the original summary.\n\"\"\"\nrefine_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\nrefine_template\n)\n]\n)\nrefine_llm_chain\n=\nLLMChain\n(\nllm\n=\nllm\n,\nprompt\n=\nrefine_prompt\n)\nchain\n=\nRefineDocumentsChain\n(\ninitial_llm_chain\n=\ninitial_llm_chain\n,\nrefine_llm_chain\n=\nrefine_llm_chain\n,\ndocument_prompt\n=\ndocument_prompt\n,\ndocument_variable_name\n=\ndocument_variable_name\n,\ninitial_response_name\n=\ninitial_response_name\n,\n)\nAPI Reference:\nChatPromptTemplate\n|\nPromptTemplate\nWe can now invoke our chain:\nresult\n=\nchain\n.\ninvoke\n(\ndocuments\n)\nresult\n[\n\"output_text\"\n]\n'Apples are typically red in color, blueberries are blue, and bananas are yellow.'\nThe\nLangSmith trace\nis composed of three LLM calls: one for the initial summary, and two more updates of that summary. The process completes when we update the summary with content from the final document.\nLangGraph\nâ€‹\nDetails\nBelow we show a LangGraph implementation of this process:\nWe use the same two templates as before.\nWe generate a simple chain for the initial summary that plucks out the first document, formats it into a prompt and runs inference with our LLM.\nWe generate a second\nrefine_summary_chain\nthat operates on each successive document, refining the initial summary.\nWe will need to install\nlanggraph\n:\npip install\n-\nqU langgraph\nimport\noperator\nfrom\ntyping\nimport\nList\n,\nLiteral\n,\nTypedDict\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph\n.\nconstants\nimport\nSend\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nStateGraph\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n,\ntemperature\n=\n0\n)\n# Initial summary\nsummarize_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\n\"Write a concise summary of the following: {context}\"\n)\n,\n]\n)\ninitial_summary_chain\n=\nsummarize_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\n# Refining the summary with new docs\nrefine_template\n=\n\"\"\"\nProduce a final summary.\nExisting summary up to this point:\n{existing_answer}\nNew context:\n------------\n{context}\n------------\nGiven the new context, refine the original summary.\n\"\"\"\nrefine_prompt\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\nrefine_template\n)\n]\n)\nrefine_summary_chain\n=\nrefine_prompt\n|\nllm\n|\nStrOutputParser\n(\n)\n# For LangGraph, we will define the state of the graph to hold the query,\n# destination, and final answer.\nclass\nState\n(\nTypedDict\n)\n:\ncontents\n:\nList\n[\nstr\n]\nindex\n:\nint\nsummary\n:\nstr\n# We define functions for each node, including a node that generates\n# the initial summary:\nasync\ndef\ngenerate_initial_summary\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n:\nsummary\n=\nawait\ninitial_summary_chain\n.\nainvoke\n(\nstate\n[\n\"contents\"\n]\n[\n0\n]\n,\nconfig\n,\n)\nreturn\n{\n\"summary\"\n:\nsummary\n,\n\"index\"\n:\n1\n}\n# And a node that refines the summary based on the next document\nasync\ndef\nrefine_summary\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n:\ncontent\n=\nstate\n[\n\"contents\"\n]\n[\nstate\n[\n\"index\"\n]\n]\nsummary\n=\nawait\nrefine_summary_chain\n.\nainvoke\n(\n{\n\"existing_answer\"\n:\nstate\n[\n\"summary\"\n]\n,\n\"context\"\n:\ncontent\n}\n,\nconfig\n,\n)\nreturn\n{\n\"summary\"\n:\nsummary\n,\n\"index\"\n:\nstate\n[\n\"index\"\n]\n+\n1\n}\n# Here we implement logic to either exit the application or refine\n# the summary.\ndef\nshould_refine\n(\nstate\n:\nState\n)\n-\n>\nLiteral\n[\n\"refine_summary\"\n,\nEND\n]\n:\nif\nstate\n[\n\"index\"\n]\n>=\nlen\n(\nstate\n[\n\"contents\"\n]\n)\n:\nreturn\nEND\nelse\n:\nreturn\n\"refine_summary\"\ngraph\n=\nStateGraph\n(\nState\n)\ngraph\n.\nadd_node\n(\n\"generate_initial_summary\"\n,\ngenerate_initial_summary\n)\ngraph\n.\nadd_node\n(\n\"refine_summary\"\n,\nrefine_summary\n)\ngraph\n.\nadd_edge\n(\nSTART\n,\n\"generate_initial_summary\"\n)\ngraph\n.\nadd_conditional_edges\n(\n\"generate_initial_summary\"\n,\nshould_refine\n)\ngraph\n.\nadd_conditional_edges\n(\n\"refine_summary\"\n,\nshould_refine\n)\napp\n=\ngraph\n.\ncompile\n(\n)\nAPI Reference:\nStrOutputParser\n|\nChatPromptTemplate\n|\nRunnableConfig\n|\nSend\n|\nStateGraph\nfrom\nIPython\n.\ndisplay\nimport\nImage\nImage\n(\napp\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\nWe can step through the execution as follows, printing out the summary as it is refined:\nasync\nfor\nstep\nin\napp\n.\nastream\n(\n{\n\"contents\"\n:\n[\ndoc\n.\npage_content\nfor\ndoc\nin\ndocuments\n]\n}\n,\nstream_mode\n=\n\"values\"\n,\n)\n:\nif\nsummary\n:=\nstep\n.\nget\n(\n\"summary\"\n)\n:\nprint\n(\nsummary\n)\nApples are typically red in color.\nApples are typically red in color, while blueberries are blue.\nApples are typically red in color, blueberries are blue, and bananas are yellow.\nIn the\nLangSmith trace\nwe again recover three LLM calls, performing the same functions as before.\nNote that we can stream tokens from the application, including from intermediate steps:\nasync\nfor\nevent\nin\napp\n.\nastream_events\n(\n{\n\"contents\"\n:\n[\ndoc\n.\npage_content\nfor\ndoc\nin\ndocuments\n]\n}\n,\nversion\n=\n\"v2\"\n)\n:\nkind\n=\nevent\n[\n\"event\"\n]\nif\nkind\n==\n\"on_chat_model_stream\"\n:\ncontent\n=\nevent\n[\n\"data\"\n]\n[\n\"chunk\"\n]\n.\ncontent\nif\ncontent\n:\nprint\n(\ncontent\n,\nend\n=\n\"|\"\n)\nelif\nkind\n==\n\"on_chat_model_end\"\n:\nprint\n(\n\"\\n\\n\"\n)\nAp|ples| are| characterized| by| their| red| color|.|\nAp|ples| are| characterized| by| their| red| color|,| while| blueberries| are| known| for| their| blue| hue|.|\nAp|ples| are| characterized| by| their| red| color|,| blueberries| are| known| for| their| blue| hue|,| and| bananas| are| recognized| for| their| yellow| color|.|\nNext steps\nâ€‹\nSee\nthis tutorial\nfor more LLM-based summarization strategies.\nCheck out the\nLangGraph documentation\nfor detail on building with LangGraph.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/",
    "Versions\nMigrating from v0.0 chains\nMigrating from RetrievalQA\nOn this page\nMigrating from RetrievalQA\nThe\nRetrievalQA\nchain\nperformed natural-language question answering over a data source using retrieval-augmented generation.\nSome advantages of switching to the LCEL implementation are:\nEasier customizability. Details such as the prompt and how documents are formatted are only configurable via specific parameters in the\nRetrievalQA\nchain.\nMore easily return source documents.\nSupport for runnable methods like streaming and async operations.\nNow let's look at them side-by-side. We'll use the following ingestion code to load a\nblog post by Lilian Weng\non autonomous agents into a local vector store:\nShared setup\nâ€‹\nFor both versions, we'll need to load the data with the\nWebBaseLoader\ndocument loader, split it with\nRecursiveCharacterTextSplitter\n, and add it to an in-memory\nFAISS\nvector store.\nWe will also instantiate a chat model to use.\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\ncommunity langchain langchain\n-\nopenai faiss\n-\ncpu beautifulsoup4\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\n# Load docs\nfrom\nlangchain_community\n.\ndocument_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_community\n.\nvectorstores\nimport\nFAISS\nfrom\nlangchain_openai\n.\nchat_models\nimport\nChatOpenAI\nfrom\nlangchain_openai\n.\nembeddings\nimport\nOpenAIEmbeddings\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\nloader\n=\nWebBaseLoader\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n)\ndata\n=\nloader\n.\nload\n(\n)\n# Split\ntext_splitter\n=\nRecursiveCharacterTextSplitter\n(\nchunk_size\n=\n500\n,\nchunk_overlap\n=\n0\n)\nall_splits\n=\ntext_splitter\n.\nsplit_documents\n(\ndata\n)\n# Store splits\nvectorstore\n=\nFAISS\n.\nfrom_documents\n(\ndocuments\n=\nall_splits\n,\nembedding\n=\nOpenAIEmbeddings\n(\n)\n)\n# LLM\nllm\n=\nChatOpenAI\n(\n)\nLegacy\nâ€‹\nDetails\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain\n.\nchains\nimport\nRetrievalQA\n# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\nprompt\n=\nhub\n.\npull\n(\n\"rlm/rag-prompt\"\n)\nqa_chain\n=\nRetrievalQA\n.\nfrom_llm\n(\nllm\n,\nretriever\n=\nvectorstore\n.\nas_retriever\n(\n)\n,\nprompt\n=\nprompt\n)\nqa_chain\n(\n\"What are autonomous agents?\"\n)\n{'query': 'What are autonomous agents?',\n'result': 'Autonomous agents are LLM-empowered agents capable of handling autonomous design, planning, and performance of complex scientific experiments. These agents can browse the Internet, read documentation, execute code, call robotics experimentation APIs, and leverage other LLMs. They can generate reasoning steps, such as developing a novel anticancer drug, based on requested tasks.'}\nLCEL\nâ€‹\nDetails\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain_core\n.\noutput_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnablePassthrough\n# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\nprompt\n=\nhub\n.\npull\n(\n\"rlm/rag-prompt\"\n)\ndef\nformat_docs\n(\ndocs\n)\n:\nreturn\n\"\\n\\n\"\n.\njoin\n(\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n)\nqa_chain\n=\n(\n{\n\"context\"\n:\nvectorstore\n.\nas_retriever\n(\n)\n|\nformat_docs\n,\n\"question\"\n:\nRunnablePassthrough\n(\n)\n,\n}\n|\nprompt\n|\nllm\n|\nStrOutputParser\n(\n)\n)\nqa_chain\n.\ninvoke\n(\n\"What are autonomous agents?\"\n)\nAPI Reference:\nStrOutputParser\n|\nRunnablePassthrough\n'Autonomous agents are agents empowered by large language models (LLMs) that can handle autonomous design, planning, and performance of complex tasks such as scientific experiments. These agents can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs, and leverage other LLMs for their tasks. The model can come up with reasoning steps when given a specific task, such as developing a novel anticancer drug.'\nThe LCEL implementation exposes the internals of what's happening around retrieving, formatting documents, and passing them through a prompt to the LLM, but it is more verbose. You can customize and wrap this composition logic in a helper function, or use the higher-level\ncreate_retrieval_chain\nand\ncreate_stuff_documents_chain\nhelper method:\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain\n.\nchains\nimport\ncreate_retrieval_chain\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\nimport\ncreate_stuff_documents_chain\n# See full prompt at https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\nretrieval_qa_chat_prompt\n=\nhub\n.\npull\n(\n\"langchain-ai/retrieval-qa-chat\"\n)\ncombine_docs_chain\n=\ncreate_stuff_documents_chain\n(\nllm\n,\nretrieval_qa_chat_prompt\n)\nrag_chain\n=\ncreate_retrieval_chain\n(\nvectorstore\n.\nas_retriever\n(\n)\n,\ncombine_docs_chain\n)\nrag_chain\n.\ninvoke\n(\n{\n\"input\"\n:\n\"What are autonomous agents?\"\n}\n)\n{'input': 'What are autonomous agents?',\n'context': [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:'),\nDocument(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Weng, Lilian. (Jun 2023). â€œLLM-powered Autonomous Agentsâ€. Lilâ€™Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),\nDocument(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#'),\nDocument(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. â€œChain of thought prompting elicits reasoning in large language models.â€ NeurIPS 2022\\n[2] Yao et al. â€œTree of Thoughts: Dliberate Problem Solving with Large Language Models.â€ arXiv preprint arXiv:2305.10601 (2023).')],\n'answer': 'Autonomous agents are entities capable of operating independently to perform tasks or make decisions without direct human intervention. In the context provided, autonomous agents empowered by Large Language Models (LLMs) are used for scientific discovery, including tasks like autonomous design, planning, and executing complex scientific experiments.'}\nNext steps\nâ€‹\nCheck out the\nLCEL conceptual docs\nfor more background information on the LangChain expression language.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/",
    "Versions\nMigrating from v0.0 chains\nMigrating from StuffDocumentsChain\nOn this page\nMigrating from StuffDocumentsChain\nStuffDocumentsChain\ncombines documents by concatenating them into a single context window. It is a straightforward and effective strategy for combining documents for question-answering, summarization, and other purposes.\ncreate_stuff_documents_chain\nis the recommended alternative. It functions the same as\nStuffDocumentsChain\n, with better support for streaming and batch functionality. Because it is a simple combination of\nLCEL primitives\n, it is also easier to extend and incorporate into other LangChain applications.\nBelow we will go through both\nStuffDocumentsChain\nand\ncreate_stuff_documents_chain\non a simple example for illustrative purposes.\nLet's first load a chat model:\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nllm\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nExample\nâ€‹\nLet's go through an example where we analyze a set of documents. We first generate some simple documents for illustrative purposes:\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\ndocuments\n=\n[\nDocument\n(\npage_content\n=\n\"Apples are red\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"apple_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Blueberries are blue\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"blueberry_book\"\n}\n)\n,\nDocument\n(\npage_content\n=\n\"Bananas are yelow\"\n,\nmetadata\n=\n{\n\"title\"\n:\n\"banana_book\"\n}\n)\n,\n]\nAPI Reference:\nDocument\nLegacy\nâ€‹\nDetails\nBelow we show an implementation with\nStuffDocumentsChain\n. We define the prompt template for a summarization task and instantiate a\nLLMChain\nobject for this purpose. We define how documents are formatted into the prompt and ensure consistency among the keys in the various prompts.\nfrom\nlangchain\n.\nchains\nimport\nLLMChain\n,\nStuffDocumentsChain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\n,\nPromptTemplate\n# This controls how each document will be formatted. Specifically,\n# it will be passed to `format_document` - see that function for more\n# details.\ndocument_prompt\n=\nPromptTemplate\n(\ninput_variables\n=\n[\n\"page_content\"\n]\n,\ntemplate\n=\n\"{page_content}\"\n)\ndocument_variable_name\n=\n\"context\"\n# The prompt here should take as an input variable the\n# `document_variable_name`\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"Summarize this content: {context}\"\n)\nllm_chain\n=\nLLMChain\n(\nllm\n=\nllm\n,\nprompt\n=\nprompt\n)\nchain\n=\nStuffDocumentsChain\n(\nllm_chain\n=\nllm_chain\n,\ndocument_prompt\n=\ndocument_prompt\n,\ndocument_variable_name\n=\ndocument_variable_name\n,\n)\nAPI Reference:\nChatPromptTemplate\n|\nPromptTemplate\nWe can now invoke our chain:\nresult\n=\nchain\n.\ninvoke\n(\ndocuments\n)\nresult\n[\n\"output_text\"\n]\n'This content describes the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'\nfor\nchunk\nin\nchain\n.\nstream\n(\ndocuments\n)\n:\nprint\n(\nchunk\n)\n{'input_documents': [Document(metadata={'title': 'apple_book'}, page_content='Apples are red'), Document(metadata={'title': 'blueberry_book'}, page_content='Blueberries are blue'), Document(metadata={'title': 'banana_book'}, page_content='Bananas are yelow')], 'output_text': 'This content describes the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'}\nLCEL\nâ€‹\nDetails\nBelow we show an implementation using\ncreate_stuff_documents_chain\n:\nfrom\nlangchain\n.\nchains\n.\ncombine_documents\nimport\ncreate_stuff_documents_chain\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nprompt\n=\nChatPromptTemplate\n.\nfrom_template\n(\n\"Summarize this content: {context}\"\n)\nchain\n=\ncreate_stuff_documents_chain\n(\nllm\n,\nprompt\n)\nAPI Reference:\nChatPromptTemplate\nInvoking the chain, we obtain a similar result as before:\nresult\n=\nchain\n.\ninvoke\n(\n{\n\"context\"\n:\ndocuments\n}\n)\nresult\n'This content describes the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'\nNote that this implementation supports streaming of output tokens:\nfor\nchunk\nin\nchain\n.\nstream\n(\n{\n\"context\"\n:\ndocuments\n}\n)\n:\nprint\n(\nchunk\n,\nend\n=\n\" | \"\n)\n| This |  content |  describes |  the |  colors |  of |  different |  fruits | : |  apples |  are |  red | , |  blue | berries |  are |  blue | , |  and |  bananas |  are |  yellow | . |  |\nNext steps\nâ€‹\nCheck out the\nLCEL conceptual docs\nfor more background information.\nSee these\nhow-to guides\nfor more on question-answering tasks with RAG.\nSee\nthis tutorial\nfor more LLM-based summarization strategies.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_memory/",
    "Versions\nUpgrading to LangGraph memory\nOn this page\nHow to migrate to LangGraph memory\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of\nLangGraph persistence\nto incorporate\nmemory\ninto their LangChain application.\nUsers that rely on\nRunnableWithMessageHistory\nor\nBaseChatMessageHistory\ndo\nnot\nneed to make any changes, but are encouraged to consider using LangGraph for more complex use cases.\nUsers that rely on deprecated memory abstractions from LangChain 0.0.x should follow this guide to upgrade to the new LangGraph persistence feature in LangChain 0.3.x.\nWhy use LangGraph for memory?\nâ€‹\nThe main advantages of persistence in LangGraph are:\nBuilt-in support for multiple users and conversations, which is a typical requirement for real-world conversational AI applications.\nAbility to save and resume complex conversations at any point. This helps with:\nError recovery\nAllowing human intervention in AI workflows\nExploring different conversation paths (\"time travel\")\nFull compatibility with both traditional\nlanguage models\nand modern\nchat models\n. Early memory implementations in LangChain weren't designed for newer chat model APIs, causing issues with features like tool-calling. LangGraph memory can persist any custom state.\nHighly customizable, allowing you to fully control how memory works and use different storage backends.\nEvolution of memory in LangChain\nâ€‹\nThe concept of memory has evolved significantly in LangChain since its initial release.\nLangChain 0.0.x memory\nâ€‹\nBroadly speaking, LangChain 0.0.x memory was used to handle three main use cases:\nUse Case\nExample\nManaging conversation history\nKeep only the last\nn\nturns of the conversation between the user and the AI.\nExtraction of structured information\nExtract structured information from the conversation history, such as a list of facts learned about the user.\nComposite memory implementations\nCombine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation.\nWhile the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.\nMost of these implementations have been officially deprecated in LangChain 0.3.x in favor of LangGraph persistence.\nRunnableWithMessageHistory and BaseChatMessageHistory\nâ€‹\nnote\nPlease see\nHow to use BaseChatMessageHistory with LangGraph\n, if you would like to use\nBaseChatMessageHistory\n(with or without\nRunnableWithMessageHistory\n) in LangGraph.\nAs of LangChain v0.1, we started recommending that users rely primarily on\nBaseChatMessageHistory\n.\nBaseChatMessageHistory\nserves\nas a simple persistence for storing and retrieving messages in a conversation.\nAt that time, the only option for orchestrating LangChain chains was via\nLCEL\n. To incorporate memory with\nLCEL\n, users had to use the\nRunnableWithMessageHistory\ninterface. While sufficient for basic chat applications, many users found the API unintuitive and challenging to use.\nAs of LangChain v0.3, we recommend that\nnew\ncode takes advantage of LangGraph for both orchestration and persistence:\nOrchestration: In LangGraph, users define\ngraphs\nthat specify the flow of the application. This allows users to keep using\nLCEL\nwithin individual nodes when\nLCEL\nis needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\nPersistence: Users can rely on LangGraph's\npersistence\nto store and retrieve data. LangGraph persistence is extremely flexible and can support a much wider range of use cases than the\nRunnableWithMessageHistory\ninterface.\nimportant\nIf you have been using\nRunnableWithMessageHistory\nor\nBaseChatMessageHistory\n, you do not need to make any changes. We do not plan on deprecating either functionality in the near future. This functionality is sufficient for simple chat applications and any code that uses\nRunnableWithMessageHistory\nwill continue to work as expected.\nMigrations\nâ€‹\nPrerequisites\nThese guides assume some familiarity with the following concepts:\nLangGraph\nv0.0.x Memory\nHow to add persistence (\"memory\") to your graph\n1. Managing conversation history\nâ€‹\nThe goal of managing conversation history is to store and retrieve the history in a way that is optimal for a chat model to use.\nOften this involves trimming and / or summarizing the conversation history to keep the most relevant parts of the conversation while having the conversation fit inside the context window of the chat model.\nMemory classes that fall into this category include:\nMemory Type\nHow to Migrate\nDescription\nConversationBufferMemory\nLink to Migration Guide\nA basic memory implementation that simply stores the conversation history.\nConversationStringBufferMemory\nLink to Migration Guide\nA special case of\nConversationBufferMemory\ndesigned for LLMs and no longer relevant.\nConversationBufferWindowMemory\nLink to Migration Guide\nKeeps the last\nn\nturns of the conversation. Drops the oldest turn when the buffer is full.\nConversationTokenBufferMemory\nLink to Migration Guide\nKeeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.\nConversationSummaryMemory\nLink to Migration Guide\nContinually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.\nConversationSummaryBufferMemory\nLink to Migration Guide\nProvides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.\nVectorStoreRetrieverMemory\nSee related\nlong-term memory agent tutorial\nStores the conversation history in a vector store and retrieves the most relevant parts of past conversation based on the input.\n2. Extraction of structured information from the conversation history\nâ€‹\nPlease see\nlong-term memory agent tutorial\nimplements an agent that can extract structured information from the conversation history.\nMemory classes that fall into this category include:\nMemory Type\nDescription\nBaseEntityStore\nAn abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as a dictionary of key-value pairs.\nConversationEntityMemory\nCombines the ability to summarize the conversation while extracting structured information from the conversation history.\nAnd specific backend implementations of abstractions:\nMemory Type\nDescription\nInMemoryEntityStore\nAn implementation of\nBaseEntityStore\nthat stores the information in the literal computer memory (RAM).\nRedisEntityStore\nA specific implementation of\nBaseEntityStore\nthat uses Redis as the backend.\nSQLiteEntityStore\nA specific implementation of\nBaseEntityStore\nthat uses SQLite as the backend.\nUpstashRedisEntityStore\nA specific implementation of\nBaseEntityStore\nthat uses Upstash as the backend.\nThese abstractions have received limited development since their initial release. This is because they generally require significant customization for a specific application to be effective, making\nthem less widely used than the conversation history management abstractions.\nFor this reason, there are no migration guides for these abstractions. If you're struggling to migrate an application\nthat relies on these abstractions, please:\nPlease review this\nLong-term memory agent tutorial\nwhich should provide a good starting point for how to extract structured information from the conversation history.\nIf you're still struggling, please open an issue on the LangChain GitHub repository, explain your use case, and we'll try to provide more guidance on how to migrate these abstractions.\nThe general strategy for extracting structured information from the conversation history is to use a chat model with tool calling capabilities to extract structured information from the conversation history.\nThe extracted information can then be saved into an appropriate data structure (e.g., a dictionary), and information from it can be retrieved and added into the prompt as needed.\n3. Implementations that provide composite logic on top of one or more memory implementations\nâ€‹\nMemory classes that fall into this category include:\nMemory Type\nDescription\nCombinedMemory\nThis abstraction accepted a list of\nBaseMemory\nand fetched relevant memory information from each of them based on the input.\nSimpleMemory\nUsed to add read-only hard-coded context. Users can simply write this information into the prompt.\nReadOnlySharedMemory\nProvided a read-only view of an existing\nBaseMemory\nimplementation.\nThese implementations did not seem to be used widely or provide significant value. Users should be able\nto re-implement these without too much difficulty in custom code.\nRelated Resources\nâ€‹\nExplore persistence with LangGraph:\nLangGraph quickstart tutorial\nHow to add persistence (\"memory\") to your graph\nHow to manage conversation history\nHow to add summary of the conversation history\nAdd persistence with simple LCEL (favor langgraph for more complex use cases):\nHow to add message history\nWorking with message history:\nHow to trim messages\nHow to filter messages\nHow to merge message runs\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_memory/chat_history/",
    "Versions\nUpgrading to LangGraph memory\nHow to use BaseChatMessageHistory with LangGraph\nOn this page\nHow to use BaseChatMessageHistory with LangGraph\nPrerequisites\nThis guide assumes familiarity with the following concepts:\nChat History\nRunnableWithMessageHistory\nLangGraph\nMemory\nWe recommend that new LangChain applications take advantage of the\nbuilt-in LangGraph persistence\nto implement memory.\nIn some situations, users may need to keep using an existing persistence solution for chat message history.\nHere, we will show how to use\nLangChain chat message histories\n(implementations of\nBaseChatMessageHistory\n) with LangGraph.\nSet up\nâ€‹\n%\n%\ncapture\n-\n-\nno\n-\nstderr\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\nanthropic langgraph\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"ANTHROPIC_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"ANTHROPIC_API_KEY\"\n]\n=\ngetpass\n(\n)\nChatMessageHistory\nâ€‹\nA message history needs to be parameterized by a conversation ID or maybe by the 2-tuple of (user ID, conversation ID).\nMany of the\nLangChain chat message histories\nwill have either a\nsession_id\nor some\nnamespace\nto allow keeping track of different conversations. Please refer to the specific implementations to check how it is parameterized.\nThe built-in\nInMemoryChatMessageHistory\ndoes not contains such a parameterization, so we'll create a dictionary to keep track of the message histories.\nimport\nuuid\nfrom\nlangchain_core\n.\nchat_history\nimport\nInMemoryChatMessageHistory\nchats_by_session_id\n=\n{\n}\ndef\nget_chat_history\n(\nsession_id\n:\nstr\n)\n-\n>\nInMemoryChatMessageHistory\n:\nchat_history\n=\nchats_by_session_id\n.\nget\n(\nsession_id\n)\nif\nchat_history\nis\nNone\n:\nchat_history\n=\nInMemoryChatMessageHistory\n(\n)\nchats_by_session_id\n[\nsession_id\n]\n=\nchat_history\nreturn\nchat_history\nAPI Reference:\nInMemoryChatMessageHistory\nUse with LangGraph\nâ€‹\nNext, we'll set up a basic chat bot using LangGraph. If you're not familiar with LangGraph, you should look at the following\nQuick Start Tutorial\n.\nWe'll create a\nLangGraph node\nfor the chat model, and manually manage the conversation history, taking into account the conversation ID passed as part of the RunnableConfig.\nThe conversation ID can be passed as either part of the RunnableConfig (as we'll do here), or as part of the\ngraph state\n.\nimport\nuuid\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core\n.\nmessages\nimport\nBaseMessage\n,\nHumanMessage\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nMessagesState\n,\nStateGraph\n# Define a new graph\nbuilder\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define a chat model\nmodel\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\n# Define the function that calls the model\ndef\ncall_model\n(\nstate\n:\nMessagesState\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nlist\n[\nBaseMessage\n]\n:\n# Make sure that config is populated with the session id\nif\n\"configurable\"\nnot\nin\nconfig\nor\n\"session_id\"\nnot\nin\nconfig\n[\n\"configurable\"\n]\n:\nraise\nValueError\n(\n\"Make sure that the config includes the following information: {'configurable': {'session_id': 'some_value'}}\"\n)\n# Fetch the history of messages and append to it any new messages.\nchat_history\n=\nget_chat_history\n(\nconfig\n[\n\"configurable\"\n]\n[\n\"session_id\"\n]\n)\nmessages\n=\nlist\n(\nchat_history\n.\nmessages\n)\n+\nstate\n[\n\"messages\"\n]\nai_message\n=\nmodel\n.\ninvoke\n(\nmessages\n)\n# Finally, update the chat message history to include\n# the new input message from the user together with the\n# response from the model.\nchat_history\n.\nadd_messages\n(\nstate\n[\n\"messages\"\n]\n+\n[\nai_message\n]\n)\nreturn\n{\n\"messages\"\n:\nai_message\n}\n# Define the two nodes we will cycle between\nbuilder\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nbuilder\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\ngraph\n=\nbuilder\n.\ncompile\n(\n)\n# Here, we'll create a unique session ID to identify the conversation\nsession_id\n=\nuuid\n.\nuuid4\n(\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"session_id\"\n:\nsession_id\n}\n}\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"hi! I'm bob\"\n)\nfor\nevent\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n# Here, let's confirm that the AI remembers our name!\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"what was my name?\"\n)\nfor\nevent\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nAPI Reference:\nBaseMessage\n|\nHumanMessage\n|\nRunnableConfig\n|\nStateGraph\n================================\u001b[1m Human Message \u001b[0m=================================\nhi! I'm bob\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello Bob! It's nice to meet you. I'm Claude, an AI assistant created by Anthropic. How are you doing today?\n================================\u001b[1m Human Message \u001b[0m=================================\nwhat was my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYou introduced yourself as Bob when you said \"hi! I'm bob\".\ntip\nThis also supports streaming LLM content token by token if using langgraph >= 0.2.28.\nfrom\nlangchain_core\n.\nmessages\nimport\nAIMessageChunk\nfirst\n=\nTrue\nfor\nmsg\n,\nmetadata\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\ninput_message\n}\n,\nconfig\n,\nstream_mode\n=\n\"messages\"\n)\n:\nif\nmsg\n.\ncontent\nand\nnot\nisinstance\n(\nmsg\n,\nHumanMessage\n)\n:\nprint\n(\nmsg\n.\ncontent\n,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nAPI Reference:\nAIMessageChunk\nYou| sai|d your| name was Bob.|\nUsing With RunnableWithMessageHistory\nâ€‹\nThis how-to guide used the\nmessages\nand\nadd_messages\ninterface of\nBaseChatMessageHistory\ndirectly.\nAlternatively, you can use\nRunnableWithMessageHistory\n, as\nLCEL\ncan be used inside any\nLangGraph node\n.\nTo do that replace the following code:\ndef\ncall_model\n(\nstate\n:\nMessagesState\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nlist\n[\nBaseMessage\n]\n:\n# Make sure that config is populated with the session id\nif\n\"configurable\"\nnot\nin\nconfig\nor\n\"session_id\"\nnot\nin\nconfig\n[\n\"configurable\"\n]\n:\nraise\nValueError\n(\n\"You make sure that the config includes the following information: {'configurable': {'session_id': 'some_value'}}\"\n)\n# Fetch the history of messages and append to it any new messages.\nchat_history\n=\nget_chat_history\n(\nconfig\n[\n\"configurable\"\n]\n[\n\"session_id\"\n]\n)\nmessages\n=\nlist\n(\nchat_history\n.\nmessages\n)\n+\nstate\n[\n\"messages\"\n]\nai_message\n=\nmodel\n.\ninvoke\n(\nmessages\n)\n# Finally, update the chat message history to include\n# the new input message from the user together with the\n# response from the model.\nchat_history\n.\nadd_messages\n(\nstate\n[\n\"messages\"\n]\n+\n[\nai_message\n]\n)\n# hilight-end\nreturn\n{\n\"messages\"\n:\nai_message\n}\nWith the corresponding instance of\nRunnableWithMessageHistory\ndefined in your current application.\nrunnable\n=\nRunnableWithMessageHistory\n(\n.\n.\n.\n)\n# From existing code\ndef\ncall_model\n(\nstate\n:\nMessagesState\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nlist\n[\nBaseMessage\n]\n:\n# RunnableWithMessageHistory takes care of reading the message history\n# and updating it with the new human message and ai response.\nai_message\n=\nrunnable\n.\ninvoke\n(\nstate\n[\n'messages'\n]\n,\nconfig\n)\nreturn\n{\n\"messages\"\n:\nai_message\n}\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_memory/",
    "Versions\nUpgrading to LangGraph memory\nMigrating off ConversationBufferMemory or ConversationStringBufferMemory\nOn this page\nMigrating off ConversationBufferMemory or ConversationStringBufferMemory\nConversationBufferMemory\nand\nConversationStringBufferMemory\nwere used to keep track of a conversation between a human and an ai asstistant without any additional processing.\nnote\nThe\nConversationStringBufferMemory\nis equivalent to\nConversationBufferMemory\nbut was targeting LLMs that were not chat models.\nThe methods for handling conversation history using existing modern primitives are:\nUsing\nLangGraph persistence\nalong with appropriate processing of the message history\nUsing LCEL with\nRunnableWithMessageHistory\ncombined with appropriate processing of the message history.\nMost users will find\nLangGraph persistence\nboth easier to use and configure than the equivalent LCEL, especially for more complex use cases.\nSet up\nâ€‹\n%\n%\ncapture\n-\n-\nno\n-\nstderr\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\nopenai langchain\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nUsage with LLMChain / ConversationChain\nâ€‹\nThis section shows how to migrate off\nConversationBufferMemory\nor\nConversationStringBufferMemory\nthat's used together with either an\nLLMChain\nor a\nConversationChain\n.\nLegacy\nâ€‹\nBelow is example usage of\nConversationBufferMemory\nwith an\nLLMChain\nor an equivalent\nConversationChain\n.\nDetails\nfrom\nlangchain\n.\nchains\nimport\nLLMChain\nfrom\nlangchain\n.\nmemory\nimport\nConversationBufferMemory\nfrom\nlangchain_core\n.\nmessages\nimport\nSystemMessage\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nprompts\n.\nchat\nimport\n(\nChatPromptTemplate\n,\nHumanMessagePromptTemplate\n,\nMessagesPlaceholder\n,\n)\nfrom\nlangchain_openai\nimport\nChatOpenAI\nprompt\n=\nChatPromptTemplate\n(\n[\nMessagesPlaceholder\n(\nvariable_name\n=\n\"chat_history\"\n)\n,\nHumanMessagePromptTemplate\n.\nfrom_template\n(\n\"{text}\"\n)\n,\n]\n)\nmemory\n=\nConversationBufferMemory\n(\nmemory_key\n=\n\"chat_history\"\n,\nreturn_messages\n=\nTrue\n)\nlegacy_chain\n=\nLLMChain\n(\nllm\n=\nChatOpenAI\n(\n)\n,\nprompt\n=\nprompt\n,\nmemory\n=\nmemory\n,\n)\nlegacy_result\n=\nlegacy_chain\n.\ninvoke\n(\n{\n\"text\"\n:\n\"my name is bob\"\n}\n)\nprint\n(\nlegacy_result\n)\nlegacy_result\n=\nlegacy_chain\n.\ninvoke\n(\n{\n\"text\"\n:\n\"what was my name\"\n}\n)\nAPI Reference:\nSystemMessage\n|\nChatPromptTemplate\n|\nChatPromptTemplate\n|\nHumanMessagePromptTemplate\n|\nMessagesPlaceholder\n{'text': 'Hello Bob! How can I assist you today?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}\nlegacy_result\n[\n\"text\"\n]\n'Your name is Bob. How can I assist you today, Bob?'\nnote\nNote that there is no support for separating conversation threads in a single memory object\nLangGraph\nâ€‹\nThe example below shows how to use LangGraph to implement a\nConversationChain\nor\nLLMChain\nwith\nConversationBufferMemory\n.\nThis example assumes that you're already somewhat familiar with\nLangGraph\n. If you're not, then please see the\nLangGraph Quickstart Guide\nfor more details.\nLangGraph\noffers a lot of additional functionality (e.g., time-travel and interrupts) and will work well for other more complex (and realistic) architectures.\nDetails\nimport\nuuid\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nMessagesState\n,\nStateGraph\n# Define a new graph\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define a chat model\nmodel\n=\nChatOpenAI\n(\n)\n# Define the function that calls the model\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\nresponse\n=\nmodel\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\n# We return a list, because this will get added to the existing list\nreturn\n{\n\"messages\"\n:\nresponse\n}\n# Define the two nodes we will cycle between\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\n# Adding memory is straight forward in langgraph!\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\n# The thread id is a unique key that identifies\n# this particular conversation.\n# We'll just generate a random uuid here.\n# This enables a single application to manage conversations among multiple users.\nthread_id\n=\nuuid\n.\nuuid4\n(\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\nthread_id\n}\n}\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"hi! I'm bob\"\n)\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n# Here, let's confirm that the AI remembers our name!\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"what was my name?\"\n)\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nAPI Reference:\nHumanMessage\n|\nMemorySaver\n|\nStateGraph\n================================\u001b[1m Human Message \u001b[0m=================================\nhi! I'm bob\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello Bob! How can I assist you today?\n================================\u001b[1m Human Message \u001b[0m=================================\nwhat was my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYour name is Bob. How can I help you today, Bob?\nLCEL RunnableWithMessageHistory\nâ€‹\nAlternatively, if you have a simple chain, you can wrap the chat model of the chain within a\nRunnableWithMessageHistory\n.\nPlease refer to the following\nmigration guide\nfor more information.\nUsage with a pre-built agent\nâ€‹\nThis example shows usage of an Agent Executor with a pre-built agent constructed using the\ncreate_tool_calling_agent\nfunction.\nIf you are using one of the\nold LangChain pre-built agents\n, you should be able\nto replace that code with the new\nlanggraph pre-built agent\nwhich leverages\nnative tool calling capabilities of chat models and will likely work better out of the box.\nLegacy Usage\nâ€‹\nDetails\nfrom\nlangchain\nimport\nhub\nfrom\nlangchain\n.\nagents\nimport\nAgentExecutor\n,\ncreate_tool_calling_agent\nfrom\nlangchain\n.\nmemory\nimport\nConversationBufferMemory\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\ntemperature\n=\n0\n)\n@tool\ndef\nget_user_age\n(\nname\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Use this tool to find the user's age.\"\"\"\n# This is a placeholder for the actual implementation\nif\n\"bob\"\nin\nname\n.\nlower\n(\n)\n:\nreturn\n\"42 years old\"\nreturn\n\"41 years old\"\ntools\n=\n[\nget_user_age\n]\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"placeholder\"\n,\n\"{chat_history}\"\n)\n,\n(\n\"human\"\n,\n\"{input}\"\n)\n,\n(\n\"placeholder\"\n,\n\"{agent_scratchpad}\"\n)\n,\n]\n)\n# Construct the Tools agent\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n)\n# Instantiate memory\nmemory\n=\nConversationBufferMemory\n(\nmemory_key\n=\n\"chat_history\"\n,\nreturn_messages\n=\nTrue\n)\n# Create an agent\nagent\n=\ncreate_tool_calling_agent\n(\nmodel\n,\ntools\n,\nprompt\n)\nagent_executor\n=\nAgentExecutor\n(\nagent\n=\nagent\n,\ntools\n=\ntools\n,\nmemory\n=\nmemory\n,\n# Pass the memory to the executor\n)\n# Verify that the agent can use tools\nprint\n(\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"hi! my name is bob what is my age?\"\n}\n)\n)\nprint\n(\n)\n# Verify that the agent has access to conversation history.\n# The agent should be able to answer that the user's name is bob.\nprint\n(\nagent_executor\n.\ninvoke\n(\n{\n\"input\"\n:\n\"do you remember my name?\"\n}\n)\n)\nAPI Reference:\ntool\n{'input': 'hi! my name is bob what is my age?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Bob, you are 42 years old.', additional_kwargs={}, response_metadata={})], 'output': 'Bob, you are 42 years old.'}\n{'input': 'do you remember my name?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Bob, you are 42 years old.', additional_kwargs={}, response_metadata={}), HumanMessage(content='do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, your name is Bob.', additional_kwargs={}, response_metadata={})], 'output': 'Yes, your name is Bob.'}\nLangGraph\nâ€‹\nYou can follow the standard LangChain tutorial for\nbuilding an agent\nan in depth explanation of how this works.\nThis example is shown here explicitly to make it easier for users to compare the legacy implementation vs. the corresponding langgraph implementation.\nThis example shows how to add memory to the\npre-built react agent\nin langgraph.\nFor more details, please see the\nhow to add memory to the prebuilt ReAct agent\nguide in langgraph.\nDetails\nimport\nuuid\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\n@tool\ndef\nget_user_age\n(\nname\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Use this tool to find the user's age.\"\"\"\n# This is a placeholder for the actual implementation\nif\n\"bob\"\nin\nname\n.\nlower\n(\n)\n:\nreturn\n\"42 years old\"\nreturn\n\"41 years old\"\nmemory\n=\nMemorySaver\n(\n)\nmodel\n=\nChatOpenAI\n(\n)\napp\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n=\n[\nget_user_age\n]\n,\ncheckpointer\n=\nmemory\n,\n)\n# The thread id is a unique key that identifies\n# this particular conversation.\n# We'll just generate a random uuid here.\n# This enables a single application to manage conversations among multiple users.\nthread_id\n=\nuuid\n.\nuuid4\n(\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\nthread_id\n}\n}\n# Tell the AI that our name is Bob, and ask it to use a tool to confirm\n# that it's capable of working like an agent.\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"hi! I'm bob. What is my age?\"\n)\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n# Confirm that the chat bot has access to previous conversation\n# and can respond to the user saying that the user's name is Bob.\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"do you remember my name?\"\n)\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nAPI Reference:\nHumanMessage\n|\ntool\n|\nMemorySaver\n|\ncreate_react_agent\n================================\u001b[1m Human Message \u001b[0m=================================\nhi! I'm bob. What is my age?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nget_user_age (call_oEDwEbIDNdokwqhAV6Azn47c)\nCall ID: call_oEDwEbIDNdokwqhAV6Azn47c\nArgs:\nname: bob\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: get_user_age\n42 years old\n==================================\u001b[1m Ai Message \u001b[0m==================================\nBob, you are 42 years old! If you need any more assistance or information, feel free to ask.\n================================\u001b[1m Human Message \u001b[0m=================================\ndo you remember my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYes, your name is Bob. If you have any other questions or need assistance, feel free to ask!\nIf we use a different thread ID, it'll start a new conversation and the bot will not know our name!\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"123456789\"\n}\n}\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"hi! do you remember my name?\"\n)\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n================================\u001b[1m Human Message \u001b[0m=================================\nhi! do you remember my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello! Yes, I remember your name. It's great to see you again! How can I assist you today?\nNext steps\nâ€‹\nExplore persistence with LangGraph:\nLangGraph quickstart tutorial\nHow to add persistence (\"memory\") to your graph\nHow to manage conversation history\nHow to add summary of the conversation history\nAdd persistence with simple LCEL (favor langgraph for more complex use cases):\nHow to add message history\nWorking with message history:\nHow to trim messages\nHow to filter messages\nHow to merge message runs\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_window_memory/",
    "Versions\nUpgrading to LangGraph memory\nMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory\nOn this page\nMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory\nFollow this guide if you're trying to migrate off one of the old memory classes listed below:\nMemory Type\nDescription\nConversationBufferWindowMemory\nKeeps the last\nn\nmessages of the conversation. Drops the oldest messages when there are more than\nn\nmessages.\nConversationTokenBufferMemory\nKeeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.\nConversationBufferWindowMemory\nand\nConversationTokenBufferMemory\napply additional processing on top of the raw conversation history to trim the conversation history to a size that fits inside the context window of a chat model.\nThis processing functionality can be accomplished using LangChain's built-in\ntrim_messages\nfunction.\nimportant\nWeâ€™ll begin by exploring a straightforward method that involves applying processing logic to the entire conversation history.\nWhile this approach is easy to implement, it has a downside: as the conversation grows, so does the latency, since the logic is re-applied to all previous exchanges in the conversation at each turn.\nMore advanced strategies focus on incrementally updating the conversation history to avoid redundant processing.\nFor instance, the langgraph\nhow-to guide on summarization\ndemonstrates\nhow to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.\nSet up\nâ€‹\n%\n%\ncapture\n-\n-\nno\n-\nstderr\n%\npip install\n-\n-\nupgrade\n-\n-\nquiet langchain\n-\nopenai langchain\nimport\nos\nfrom\ngetpass\nimport\ngetpass\nif\n\"OPENAI_API_KEY\"\nnot\nin\nos\n.\nenviron\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n(\n)\nLegacy usage with LLMChain / Conversation Chain\nâ€‹\nDetails\nfrom\nlangchain\n.\nchains\nimport\nLLMChain\nfrom\nlangchain\n.\nmemory\nimport\nConversationBufferWindowMemory\nfrom\nlangchain_core\n.\nmessages\nimport\nSystemMessage\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nprompts\n.\nchat\nimport\n(\nChatPromptTemplate\n,\nHumanMessagePromptTemplate\n,\nMessagesPlaceholder\n,\n)\nfrom\nlangchain_openai\nimport\nChatOpenAI\nprompt\n=\nChatPromptTemplate\n(\n[\nSystemMessage\n(\ncontent\n=\n\"You are a helpful assistant.\"\n)\n,\nMessagesPlaceholder\n(\nvariable_name\n=\n\"chat_history\"\n)\n,\nHumanMessagePromptTemplate\n.\nfrom_template\n(\n\"{text}\"\n)\n,\n]\n)\nmemory\n=\nConversationBufferWindowMemory\n(\nmemory_key\n=\n\"chat_history\"\n,\nreturn_messages\n=\nTrue\n)\nlegacy_chain\n=\nLLMChain\n(\nllm\n=\nChatOpenAI\n(\n)\n,\nprompt\n=\nprompt\n,\nmemory\n=\nmemory\n,\n)\nlegacy_result\n=\nlegacy_chain\n.\ninvoke\n(\n{\n\"text\"\n:\n\"my name is bob\"\n}\n)\nprint\n(\nlegacy_result\n)\nlegacy_result\n=\nlegacy_chain\n.\ninvoke\n(\n{\n\"text\"\n:\n\"what was my name\"\n}\n)\nprint\n(\nlegacy_result\n)\nAPI Reference:\nSystemMessage\n|\nChatPromptTemplate\n|\nChatPromptTemplate\n|\nHumanMessagePromptTemplate\n|\nMessagesPlaceholder\n{'text': 'Nice to meet you, Bob! How can I assist you today?', 'chat_history': []}\n{'text': 'Your name is Bob. How can I assist you further, Bob?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}\nReimplementing ConversationBufferWindowMemory logic\nâ€‹\nLet's first create appropriate logic to process the conversation history, and then we'll see how to integrate it into an application. You can later replace this basic setup with more advanced logic tailored to your specific needs.\nWe'll use\ntrim_messages\nto implement logic that keeps the last\nn\nmessages of the conversation. It will drop the oldest messages when the number of messages exceeds\nn\n.\nIn addition, we will also keep the system message if it's present -- when present, it's the first message in a conversation that includes instructions for the chat model.\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nBaseMessage\n,\nHumanMessage\n,\nSystemMessage\n,\ntrim_messages\n,\n)\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmessages\n=\n[\nSystemMessage\n(\n\"you're a good assistant, you always respond with a joke.\"\n)\n,\nHumanMessage\n(\n\"i wonder why it's called langchain\"\n)\n,\nAIMessage\n(\n'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n)\n,\nHumanMessage\n(\n\"and who is harrison chasing anyways\"\n)\n,\nAIMessage\n(\n\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n)\n,\nHumanMessage\n(\n\"why is 42 always the answer?\"\n)\n,\nAIMessage\n(\n\"Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!\"\n)\n,\nHumanMessage\n(\n\"What did the cow say?\"\n)\n,\n]\nAPI Reference:\nAIMessage\n|\nBaseMessage\n|\nHumanMessage\n|\nSystemMessage\n|\ntrim_messages\nfrom\nlangchain_core\n.\nmessages\nimport\ntrim_messages\nselected_messages\n=\ntrim_messages\n(\nmessages\n,\ntoken_counter\n=\nlen\n,\n# <-- len will simply count the number of messages rather than tokens\nmax_tokens\n=\n5\n,\n# <-- allow up to 5 messages.\nstrategy\n=\n\"last\"\n,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\n# start_on=\"human\" makes sure we produce a valid chat history\nstart_on\n=\n\"human\"\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\nallow_partial\n=\nFalse\n,\n)\nfor\nmsg\nin\nselected_messages\n:\nmsg\n.\npretty_print\n(\n)\nAPI Reference:\ntrim_messages\n================================\u001b[1m System Message \u001b[0m================================\nyou're a good assistant, you always respond with a joke.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHmmm let me think.\nWhy, he's probably chasing after the last cup of coffee in the office!\n================================\u001b[1m Human Message \u001b[0m=================================\nwhy is 42 always the answer?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nBecause itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat did the cow say?\nReimplementing ConversationTokenBufferMemory logic\nâ€‹\nHere, we'll use\ntrim_messages\nto keeps the system message and the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.\nfrom\nlangchain_core\n.\nmessages\nimport\ntrim_messages\nselected_messages\n=\ntrim_messages\n(\nmessages\n,\n# Please see API reference for trim_messages for other ways to specify a token counter.\ntoken_counter\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\n,\nmax_tokens\n=\n80\n,\n# <-- token limit\n# The start_on is specified\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\n# start_on=\"human\" makes sure we produce a valid chat history\nstart_on\n=\n\"human\"\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\nstrategy\n=\n\"last\"\n,\n)\nfor\nmsg\nin\nselected_messages\n:\nmsg\n.\npretty_print\n(\n)\nAPI Reference:\ntrim_messages\n================================\u001b[1m System Message \u001b[0m================================\nyou're a good assistant, you always respond with a joke.\n================================\u001b[1m Human Message \u001b[0m=================================\nwhy is 42 always the answer?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nBecause itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!\n================================\u001b[1m Human Message \u001b[0m=================================\nWhat did the cow say?\nModern usage with LangGraph\nâ€‹\nThe example below shows how to use LangGraph to add simple conversation pre-processing logic.\nnote\nIf you want to avoid running the computation on the entire conversation history each time, you can follow\nthe\nhow-to guide on summarization\nthat demonstrates\nhow to discard older messages, ensuring they aren't re-processed during later turns.\nDetails\nimport\nuuid\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\nfrom\nlangchain_core\n.\nmessages\nimport\nHumanMessage\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nSTART\n,\nMessagesState\n,\nStateGraph\n# Define a new graph\nworkflow\n=\nStateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define a chat model\nmodel\n=\nChatOpenAI\n(\n)\n# Define the function that calls the model\ndef\ncall_model\n(\nstate\n:\nMessagesState\n)\n:\nselected_messages\n=\ntrim_messages\n(\nstate\n[\n\"messages\"\n]\n,\ntoken_counter\n=\nlen\n,\n# <-- len will simply count the number of messages rather than tokens\nmax_tokens\n=\n5\n,\n# <-- allow up to 5 messages.\nstrategy\n=\n\"last\"\n,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\n# start_on=\"human\" makes sure we produce a valid chat history\nstart_on\n=\n\"human\"\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\nallow_partial\n=\nFalse\n,\n)\nresponse\n=\nmodel\n.\ninvoke\n(\nselected_messages\n)\n# We return a list, because this will get added to the existing list\nreturn\n{\n\"messages\"\n:\nresponse\n}\n# Define the two nodes we will cycle between\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\nworkflow\n.\nadd_node\n(\n\"model\"\n,\ncall_model\n)\n# Adding memory is straight forward in langgraph!\nmemory\n=\nMemorySaver\n(\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\n# The thread id is a unique key that identifies\n# this particular conversation.\n# We'll just generate a random uuid here.\nthread_id\n=\nuuid\n.\nuuid4\n(\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\nthread_id\n}\n}\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"hi! I'm bob\"\n)\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n# Here, let's confirm that the AI remembers our name!\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\nthread_id\n}\n}\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"what was my name?\"\n)\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nAPI Reference:\nHumanMessage\n|\nMemorySaver\n|\nStateGraph\n================================\u001b[1m Human Message \u001b[0m=================================\nhi! I'm bob\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello Bob! How can I assist you today?\n================================\u001b[1m Human Message \u001b[0m=================================\nwhat was my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYour name is Bob. How can I help you, Bob?\nUsage with a pre-built langgraph agent\nâ€‹\nThis example shows usage of an Agent Executor with a pre-built agent constructed using the\ncreate_tool_calling_agent\nfunction.\nIf you are using one of the\nold LangChain pre-built agents\n, you should be able\nto replace that code with the new\nlanggraph pre-built agent\nwhich leverages\nnative tool calling capabilities of chat models and will likely work better out of the box.\nDetails\nimport\nuuid\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nBaseMessage\n,\nHumanMessage\n,\nSystemMessage\n,\ntrim_messages\n,\n)\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\nprebuilt\nimport\ncreate_react_agent\n@tool\ndef\nget_user_age\n(\nname\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Use this tool to find the user's age.\"\"\"\n# This is a placeholder for the actual implementation\nif\n\"bob\"\nin\nname\n.\nlower\n(\n)\n:\nreturn\n\"42 years old\"\nreturn\n\"41 years old\"\nmemory\n=\nMemorySaver\n(\n)\nmodel\n=\nChatOpenAI\n(\n)\ndef\nprompt\n(\nstate\n)\n-\n>\nlist\n[\nBaseMessage\n]\n:\n\"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n# We're using the message processor defined above.\nreturn\ntrim_messages\n(\nstate\n[\n\"messages\"\n]\n,\ntoken_counter\n=\nlen\n,\n# <-- len will simply count the number of messages rather than tokens\nmax_tokens\n=\n5\n,\n# <-- allow up to 5 messages.\nstrategy\n=\n\"last\"\n,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\n# start_on=\"human\" makes sure we produce a valid chat history\nstart_on\n=\n\"human\"\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\nallow_partial\n=\nFalse\n,\n)\napp\n=\ncreate_react_agent\n(\nmodel\n,\ntools\n=\n[\nget_user_age\n]\n,\ncheckpointer\n=\nmemory\n,\nprompt\n=\nprompt\n,\n)\n# The thread id is a unique key that identifies\n# this particular conversation.\n# We'll just generate a random uuid here.\nthread_id\n=\nuuid\n.\nuuid4\n(\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\nthread_id\n}\n}\n# Tell the AI that our name is Bob, and ask it to use a tool to confirm\n# that it's capable of working like an agent.\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"hi! I'm bob. What is my age?\"\n)\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n# Confirm that the chat bot has access to previous conversation\n# and can respond to the user saying that the user's name is Bob.\ninput_message\n=\nHumanMessage\n(\ncontent\n=\n\"do you remember my name?\"\n)\nfor\nevent\nin\napp\n.\nstream\n(\n{\n\"messages\"\n:\n[\ninput_message\n]\n}\n,\nconfig\n,\nstream_mode\n=\n\"values\"\n)\n:\nevent\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nAPI Reference:\nAIMessage\n|\nBaseMessage\n|\nHumanMessage\n|\nSystemMessage\n|\ntrim_messages\n|\ntool\n|\nMemorySaver\n|\ncreate_react_agent\n================================\u001b[1m Human Message \u001b[0m=================================\nhi! I'm bob. What is my age?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nget_user_age (call_jsMvoIFv970DhqqLCJDzPKsp)\nCall ID: call_jsMvoIFv970DhqqLCJDzPKsp\nArgs:\nname: bob\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: get_user_age\n42 years old\n==================================\u001b[1m Ai Message \u001b[0m==================================\nBob, you are 42 years old.\n================================\u001b[1m Human Message \u001b[0m=================================\ndo you remember my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nYes, your name is Bob.\nLCEL: Add a preprocessing step\nâ€‹\nThe simplest way to add complex conversation management is by introducing a pre-processing step in front of the chat model and pass the full conversation history to the pre-processing step.\nThis approach is conceptually simple and will work in many situations; for example, if using a\nRunnableWithMessageHistory\ninstead of wrapping the chat model, wrap the chat model with the pre-processor.\nThe obvious downside of this approach is that latency starts to increase as the conversation history grows because of two reasons:\nAs the conversation gets longer, more data may need to be fetched from whatever store your'e using to store the conversation history (if not storing it in memory).\nThe pre-processing logic will end up doing a lot of redundant computation, repeating computation from previous steps of the conversation.\ncaution\nIf you want to use a chat model's tool calling capabilities, remember to bind the tools to the model before adding the history pre-processing step to it!\nDetails\nfrom\nlangchain_core\n.\nmessages\nimport\n(\nAIMessage\n,\nBaseMessage\n,\nHumanMessage\n,\nSystemMessage\n,\ntrim_messages\n,\n)\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\n)\n@tool\ndef\nwhat_did_the_cow_say\n(\n)\n-\n>\nstr\n:\n\"\"\"Check to see what the cow said.\"\"\"\nreturn\n\"foo\"\nmessage_processor\n=\ntrim_messages\n(\n# Returns a Runnable if no messages are provided\ntoken_counter\n=\nlen\n,\n# <-- len will simply count the number of messages rather than tokens\nmax_tokens\n=\n5\n,\n# <-- allow up to 5 messages.\nstrategy\n=\n\"last\"\n,\n# The start_on is specified\n# to make sure we do not generate a sequence where\n# a ToolMessage that contains the result of a tool invocation\n# appears before the AIMessage that requested a tool invocation\n# as this will cause some chat models to raise an error.\nstart_on\n=\n(\n\"human\"\n,\n\"ai\"\n)\n,\ninclude_system\n=\nTrue\n,\n# <-- Keep the system message\nallow_partial\n=\nFalse\n,\n)\n# Note that we bind tools to the model first!\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\n[\nwhat_did_the_cow_say\n]\n)\nmodel_with_preprocessor\n=\nmessage_processor\n|\nmodel_with_tools\nfull_history\n=\n[\nSystemMessage\n(\n\"you're a good assistant, you always respond with a joke.\"\n)\n,\nHumanMessage\n(\n\"i wonder why it's called langchain\"\n)\n,\nAIMessage\n(\n'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n)\n,\nHumanMessage\n(\n\"and who is harrison chasing anyways\"\n)\n,\nAIMessage\n(\n\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n)\n,\nHumanMessage\n(\n\"why is 42 always the answer?\"\n)\n,\nAIMessage\n(\n\"Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!\"\n)\n,\nHumanMessage\n(\n\"What did the cow say?\"\n)\n,\n]\n# We pass it explicity to the model_with_preprocesor for illustrative purposes.\n# If you're using `RunnableWithMessageHistory` the history will be automatically\n# read from the source the you configure.\nmodel_with_preprocessor\n.\ninvoke\n(\nfull_history\n)\n.\npretty_print\n(\n)\nAPI Reference:\nAIMessage\n|\nBaseMessage\n|\nHumanMessage\n|\nSystemMessage\n|\ntrim_messages\n|\ntool\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nwhat_did_the_cow_say (call_urHTB5CShhcKz37QiVzNBlIS)\nCall ID: call_urHTB5CShhcKz37QiVzNBlIS\nArgs:\nIf you need to implement more efficient logic and want to use\nRunnableWithMessageHistory\nfor now the way to achieve this\nis to subclass from\nBaseChatMessageHistory\nand\ndefine appropriate logic for\nadd_messages\n(that doesn't simply append the history, but instead re-writes it).\nUnless you have a good reason to implement this solution, you should instead use LangGraph.\nNext steps\nâ€‹\nExplore persistence with LangGraph:\nLangGraph quickstart tutorial\nHow to add persistence (\"memory\") to your graph\nHow to manage conversation history\nHow to add summary of the conversation history\nAdd persistence with simple LCEL (favor langgraph for more complex use cases):\nHow to add message history\nWorking with message history:\nHow to trim messages\nHow to filter messages\nHow to merge message runs\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_memory/conversation_summary_memory/",
    "Versions\nUpgrading to LangGraph memory\nMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemory\nMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemory\nFollow this guide if you're trying to migrate off one of the old memory classes listed below:\nMemory Type\nDescription\nConversationSummaryMemory\nContinually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.\nConversationSummaryBufferMemory\nProvides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.\nPlease follow the following\nhow-to guide on summarization\nin LangGraph.\nThis guide shows how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/migrating_memory/long_term_memory_agent/",
    "Versions\nUpgrading to LangGraph memory\nA Long-Term Memory Agent\nOn this page\nA Long-Term Memory Agent\nThis tutorial shows how to implement an agent with long-term memory capabilities using LangGraph. The agent can store, retrieve, and use memories to enhance its interactions with users.\nInspired by papers like\nMemGPT\nand distilled from our own works on long-term memory, the graph extracts memories from chat interactions and persists them to a database. \"Memory\" in this tutorial will be represented in two ways:\na piece of text information that is generated by the agent\nstructured information about entities extracted by the agent in the shape of\n(subject, predicate, object)\nknowledge triples.\nThis information can later be read or queried semantically to provide personalized context when your bot is responding to a particular user.\nThe KEY idea is that by saving memories, the agent persists information about users that is SHARED across multiple conversations (threads), which is different from memory of a single conversation that is already enabled by LangGraph's\npersistence\n.\nYou can also check out a full implementation of this agent in\nthis repo\n.\nInstall dependencies\nâ€‹\n%\npip install\n-\nU\n-\n-\nquiet langgraph langchain\n-\nopenai langchain\n-\ntavily tiktoken\nimport\ngetpass\nimport\nos\ndef\n_set_env\n(\nvar\n:\nstr\n)\n:\nif\nnot\nos\n.\nenviron\n.\nget\n(\nvar\n)\n:\nos\n.\nenviron\n[\nvar\n]\n=\ngetpass\n.\ngetpass\n(\nf\"\n{\nvar\n}\n: \"\n)\n_set_env\n(\n\"OPENAI_API_KEY\"\n)\n_set_env\n(\n\"TAVILY_API_KEY\"\n)\nOPENAI_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·\nTAVILY_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·\nimport\njson\nfrom\ntyping\nimport\nList\n,\nLiteral\n,\nOptional\nimport\ntiktoken\nfrom\nlangchain_core\n.\ndocuments\nimport\nDocument\nfrom\nlangchain_core\n.\nembeddings\nimport\nEmbeddings\nfrom\nlangchain_core\n.\nmessages\nimport\nget_buffer_string\nfrom\nlangchain_core\n.\nprompts\nimport\nChatPromptTemplate\nfrom\nlangchain_core\n.\nrunnables\nimport\nRunnableConfig\nfrom\nlangchain_core\n.\ntools\nimport\ntool\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlangchain_openai\n.\nembeddings\nimport\nOpenAIEmbeddings\nfrom\nlangchain_tavily\nimport\nTavilySearch\nfrom\nlanggraph\n.\ncheckpoint\n.\nmemory\nimport\nMemorySaver\nfrom\nlanggraph\n.\ngraph\nimport\nEND\n,\nSTART\n,\nMessagesState\n,\nStateGraph\nfrom\nlanggraph\n.\nprebuilt\nimport\nToolNode\nAPI Reference:\nDocument\n|\nEmbeddings\n|\nget_buffer_string\n|\nChatPromptTemplate\n|\nRunnableConfig\n|\ntool\n|\nInMemoryVectorStore\n|\nMemorySaver\n|\nStateGraph\n|\nToolNode\nDefine vectorstore for memories\nâ€‹\nFirst, let's define the vectorstore where we will be storing our memories. Memories will be stored as embeddings and later looked up based on the conversation context. We will be using an in-memory vectorstore.\nrecall_vector_store\n=\nInMemoryVectorStore\n(\nOpenAIEmbeddings\n(\n)\n)\nDefine tools\n\u0000â€‹\nNext, let's define our memory tools. We will need a tool to store the memories and another tool to search them to find the most relevant memory.\nimport\nuuid\ndef\nget_user_id\n(\nconfig\n:\nRunnableConfig\n)\n-\n>\nstr\n:\nuser_id\n=\nconfig\n[\n\"configurable\"\n]\n.\nget\n(\n\"user_id\"\n)\nif\nuser_id\nis\nNone\n:\nraise\nValueError\n(\n\"User ID needs to be provided to save a memory.\"\n)\nreturn\nuser_id\n@tool\ndef\nsave_recall_memory\n(\nmemory\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nstr\n:\n\"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\nuser_id\n=\nget_user_id\n(\nconfig\n)\ndocument\n=\nDocument\n(\npage_content\n=\nmemory\n,\nid\n=\nstr\n(\nuuid\n.\nuuid4\n(\n)\n)\n,\nmetadata\n=\n{\n\"user_id\"\n:\nuser_id\n}\n)\nrecall_vector_store\n.\nadd_documents\n(\n[\ndocument\n]\n)\nreturn\nmemory\n@tool\ndef\nsearch_recall_memories\n(\nquery\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nList\n[\nstr\n]\n:\n\"\"\"Search for relevant memories.\"\"\"\nuser_id\n=\nget_user_id\n(\nconfig\n)\ndef\n_filter_function\n(\ndoc\n:\nDocument\n)\n-\n>\nbool\n:\nreturn\ndoc\n.\nmetadata\n.\nget\n(\n\"user_id\"\n)\n==\nuser_id\ndocuments\n=\nrecall_vector_store\n.\nsimilarity_search\n(\nquery\n,\nk\n=\n3\n,\nfilter\n=\n_filter_function\n)\nreturn\n[\ndocument\n.\npage_content\nfor\ndocument\nin\ndocuments\n]\nAdditionally, let's give our agent ability to search the web using\nTavily\n.\nsearch\n=\nTavilySearch\n(\nmax_results\n=\n1\n)\ntools\n=\n[\nsave_recall_memory\n,\nsearch_recall_memories\n,\nsearch\n]\nDefine state, nodes and edges\nâ€‹\nOur graph state will contain just two channels --\nmessages\nfor keeping track of the chat history and\nrecall_memories\n-- contextual memories that will be pulled in before calling the agent and passed to the agent's system prompt.\nclass\nState\n(\nMessagesState\n)\n:\n# add memories that will be retrieved based on the conversation context\nrecall_memories\n:\nList\n[\nstr\n]\n# Define the prompt template for the agent\nprompt\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are a helpful assistant with advanced long-term memory\"\n\" capabilities. Powered by a stateless LLM, you must rely on\"\n\" external memory to store information between conversations.\"\n\" Utilize the available memory tools to store and retrieve\"\n\" important details that will help you better attend to the user's\"\n\" needs and understand their context.\\n\\n\"\n\"Memory Usage Guidelines:\\n\"\n\"1. Actively use memory tools (save_core_memory, save_recall_memory)\"\n\" to build a comprehensive understanding of the user.\\n\"\n\"2. Make informed suppositions and extrapolations based on stored\"\n\" memories.\\n\"\n\"3. Regularly reflect on past interactions to identify patterns and\"\n\" preferences.\\n\"\n\"4. Update your mental model of the user with each new piece of\"\n\" information.\\n\"\n\"5. Cross-reference new information with existing memories for\"\n\" consistency.\\n\"\n\"6. Prioritize storing emotional context and personal values\"\n\" alongside facts.\\n\"\n\"7. Use memory to anticipate needs and tailor responses to the\"\n\" user's style.\\n\"\n\"8. Recognize and acknowledge changes in the user's situation or\"\n\" perspectives over time.\\n\"\n\"9. Leverage memories to provide personalized examples and\"\n\" analogies.\\n\"\n\"10. Recall past challenges or successes to inform current\"\n\" problem-solving.\\n\\n\"\n\"## Recall Memories\\n\"\n\"Recall memories are contextually retrieved based on the current\"\n\" conversation:\\n{recall_memories}\\n\\n\"\n\"## Instructions\\n\"\n\"Engage with the user naturally, as a trusted colleague or friend.\"\n\" There's no need to explicitly mention your memory capabilities.\"\n\" Instead, seamlessly incorporate your understanding of the user\"\n\" into your responses. Be attentive to subtle cues and underlying\"\n\" emotions. Adapt your communication style to match the user's\"\n\" preferences and current emotional state. Use tools to persist\"\n\" information you want to retain in the next conversation. If you\"\n\" do call tools, all text preceding the tool call is an internal\"\n\" message. Respond AFTER calling the tool, once you have\"\n\" confirmation that the tool completed successfully.\\n\\n\"\n,\n)\n,\n(\n\"placeholder\"\n,\n\"{messages}\"\n)\n,\n]\n)\nmodel\n=\nChatOpenAI\n(\nmodel_name\n=\n\"gpt-4o\"\n)\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\ntools\n)\ntokenizer\n=\ntiktoken\n.\nencoding_for_model\n(\n\"gpt-4o\"\n)\ndef\nagent\n(\nstate\n:\nState\n)\n-\n>\nState\n:\n\"\"\"Process the current state and generate a response using the LLM.\nArgs:\nstate (schemas.State): The current state of the conversation.\nReturns:\nschemas.State: The updated state with the agent's response.\n\"\"\"\nbound\n=\nprompt\n|\nmodel_with_tools\nrecall_str\n=\n(\n\"<recall_memory>\\n\"\n+\n\"\\n\"\n.\njoin\n(\nstate\n[\n\"recall_memories\"\n]\n)\n+\n\"\\n</recall_memory>\"\n)\nprediction\n=\nbound\n.\ninvoke\n(\n{\n\"messages\"\n:\nstate\n[\n\"messages\"\n]\n,\n\"recall_memories\"\n:\nrecall_str\n,\n}\n)\nreturn\n{\n\"messages\"\n:\n[\nprediction\n]\n,\n}\ndef\nload_memories\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nState\n:\n\"\"\"Load memories for the current conversation.\nArgs:\nstate (schemas.State): The current state of the conversation.\nconfig (RunnableConfig): The runtime configuration for the agent.\nReturns:\nState: The updated state with loaded memories.\n\"\"\"\nconvo_str\n=\nget_buffer_string\n(\nstate\n[\n\"messages\"\n]\n)\nconvo_str\n=\ntokenizer\n.\ndecode\n(\ntokenizer\n.\nencode\n(\nconvo_str\n)\n[\n:\n2048\n]\n)\nrecall_memories\n=\nsearch_recall_memories\n.\ninvoke\n(\nconvo_str\n,\nconfig\n)\nreturn\n{\n\"recall_memories\"\n:\nrecall_memories\n,\n}\ndef\nroute_tools\n(\nstate\n:\nState\n)\n:\n\"\"\"Determine whether to use tools or end the conversation based on the last message.\nArgs:\nstate (schemas.State): The current state of the conversation.\nReturns:\nLiteral[\"tools\", \"__end__\"]: The next step in the graph.\n\"\"\"\nmsg\n=\nstate\n[\n\"messages\"\n]\n[\n-\n1\n]\nif\nmsg\n.\ntool_calls\n:\nreturn\n\"tools\"\nreturn\nEND\nBuild the graph\nâ€‹\nOur agent graph is going to be very similar to simple\nReAct agent\n. The only important modification is adding a node to load memories BEFORE calling the agent for the first time.\n# Create the graph and add nodes\nbuilder\n=\nStateGraph\n(\nState\n)\nbuilder\n.\nadd_node\n(\nload_memories\n)\nbuilder\n.\nadd_node\n(\nagent\n)\nbuilder\n.\nadd_node\n(\n\"tools\"\n,\nToolNode\n(\ntools\n)\n)\n# Add edges to the graph\nbuilder\n.\nadd_edge\n(\nSTART\n,\n\"load_memories\"\n)\nbuilder\n.\nadd_edge\n(\n\"load_memories\"\n,\n\"agent\"\n)\nbuilder\n.\nadd_conditional_edges\n(\n\"agent\"\n,\nroute_tools\n,\n[\n\"tools\"\n,\nEND\n]\n)\nbuilder\n.\nadd_edge\n(\n\"tools\"\n,\n\"agent\"\n)\n# Compile the graph\nmemory\n=\nMemorySaver\n(\n)\ngraph\n=\nbuilder\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nfrom\nIPython\n.\ndisplay\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n(\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\nRun the agent!\nâ€‹\nLet's run the agent for the first time and tell it some information about the user!\ndef\npretty_print_stream_chunk\n(\nchunk\n)\n:\nfor\nnode\n,\nupdates\nin\nchunk\n.\nitems\n(\n)\n:\nprint\n(\nf\"Update from node:\n{\nnode\n}\n\"\n)\nif\n\"messages\"\nin\nupdates\n:\nupdates\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\nelse\n:\nprint\n(\nupdates\n)\nprint\n(\n\"\\n\"\n)\n# NOTE: we're specifying `user_id` to save memories for a given user\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"user_id\"\n:\n\"1\"\n,\n\"thread_id\"\n:\n\"1\"\n}\n}\nfor\nchunk\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"my name is John\"\n)\n]\n}\n,\nconfig\n=\nconfig\n)\n:\npretty_print_stream_chunk\n(\nchunk\n)\nUpdate from node: load_memories\n{'recall_memories': []}\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsave_recall_memory (call_OqfbWodmrywjMnB1v3p19QLt)\nCall ID: call_OqfbWodmrywjMnB1v3p19QLt\nArgs:\nmemory: User's name is John.\nUpdate from node: tools\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: save_recall_memory\nUser's name is John.\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nNice to meet you, John! How can I assist you today?\nYou can see that the agent saved the memory about user's name. Let's add some more information about the user!\nfor\nchunk\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"i love pizza\"\n)\n]\n}\n,\nconfig\n=\nconfig\n)\n:\npretty_print_stream_chunk\n(\nchunk\n)\nUpdate from node: load_memories\n{'recall_memories': [\"User's name is John.\"]}\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsave_recall_memory (call_xxEivMuWCURJrGxMZb02Eh31)\nCall ID: call_xxEivMuWCURJrGxMZb02Eh31\nArgs:\nmemory: John loves pizza.\nUpdate from node: tools\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: save_recall_memory\nJohn loves pizza.\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nPizza is amazing! Do you have a favorite type or topping?\nfor\nchunk\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"yes -- pepperoni!\"\n)\n]\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"user_id\"\n:\n\"1\"\n,\n\"thread_id\"\n:\n\"1\"\n}\n}\n,\n)\n:\npretty_print_stream_chunk\n(\nchunk\n)\nUpdate from node: load_memories\n{'recall_memories': [\"User's name is John.\", 'John loves pizza.']}\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsave_recall_memory (call_AFrtCVwIEr48Fim80zlhe6xg)\nCall ID: call_AFrtCVwIEr48Fim80zlhe6xg\nArgs:\nmemory: John's favorite pizza topping is pepperoni.\nUpdate from node: tools\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: save_recall_memory\nJohn's favorite pizza topping is pepperoni.\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nPepperoni is a classic choice! Do you have a favorite pizza place, or do you enjoy making it at home?\nfor\nchunk\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"i also just moved to new york\"\n)\n]\n}\n,\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"user_id\"\n:\n\"1\"\n,\n\"thread_id\"\n:\n\"1\"\n}\n}\n,\n)\n:\npretty_print_stream_chunk\n(\nchunk\n)\nUpdate from node: load_memories\n{'recall_memories': [\"User's name is John.\", 'John loves pizza.', \"John's favorite pizza topping is pepperoni.\"]}\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsave_recall_memory (call_Na86uY9eBzaJ0sS0GM4Z9tSf)\nCall ID: call_Na86uY9eBzaJ0sS0GM4Z9tSf\nArgs:\nmemory: John just moved to New York.\nUpdate from node: tools\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: save_recall_memory\nJohn just moved to New York.\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nWelcome to New York! That's a fantastic place for a pizza lover. Have you had a chance to explore any of the famous pizzerias there yet?\nNow we can use the saved information about our user on a different thread. Let's try it out:\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"user_id\"\n:\n\"1\"\n,\n\"thread_id\"\n:\n\"2\"\n}\n}\nfor\nchunk\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"where should i go for dinner?\"\n)\n]\n}\n,\nconfig\n=\nconfig\n)\n:\npretty_print_stream_chunk\n(\nchunk\n)\nUpdate from node: load_memories\n{'recall_memories': ['John loves pizza.', \"User's name is John.\", 'John just moved to New York.']}\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nConsidering you just moved to New York and love pizza, I'd recommend checking out some of the iconic pizza places in the city. Some popular spots include:\n1. **Di Fara Pizza** in Brooklyn â€“ Known for its classic New York-style pizza.\n2. **Joe's Pizza** in Greenwich Village â€“ A historic pizzeria with a great reputation.\n3. **Lucali** in Carroll Gardens, Brooklyn â€“ Often ranked among the best for its delicious thin-crust pies.\nWould you like more recommendations or information about any of these places?\nNotice how the agent is loading the most relevant memories before answering, and in our case suggests the dinner recommendations based on both the food preferences as well as location.\nFinally, let's use the search tool together with the rest of the conversation context and memory to find location of a pizzeria:\nfor\nchunk\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"what's the address for joe's in greenwich village?\"\n)\n]\n}\n,\nconfig\n=\nconfig\n,\n)\n:\npretty_print_stream_chunk\n(\nchunk\n)\nUpdate from node: load_memories\n{'recall_memories': ['John loves pizza.', 'John just moved to New York.', \"John's favorite pizza topping is pepperoni.\"]}\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\ntavily_search_results_json (call_aespiB28jpTFvaC4d0qpfY6t)\nCall ID: call_aespiB28jpTFvaC4d0qpfY6t\nArgs:\nquery: Joe's Pizza Greenwich Village NYC address\nUpdate from node: tools\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: tavily_search_results_json\n[{\"url\": \"https://www.joespizzanyc.com/locations-1-1\", \"content\": \"Joe's Pizza Greenwich Village (Original Location) 7 Carmine Street New York, NY 10014 (212) 366-1182ï»¿ Joe's Pizza Times Square 1435 Broadway New York, NY 10018 (646) 559-4878. TIMES SQUARE MENU. ORDER JOE'S TIMES SQUARE Joe's Pizza Williamsburg 216 Bedford Avenue Brooklyn, NY 11249\"}]\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe address for Joe's Pizza in Greenwich Village is:\n**7 Carmine Street, New York, NY 10014**\nEnjoy your pizza!\nIf you were to pass a different user ID, the agent's response will not be personalized as we haven't saved any information about the other user:\nAdding structured memories\nâ€‹\nSo far we've represented memories as strings, e.g.,\n\"John loves pizza\"\n. This is a natural representation when persisting memories to a vector store. If your use-case would benefit from other persistence backends-- such as a graph database-- we can update our application to generate memories with additional structure.\nBelow, we update the\nsave_recall_memory\ntool to accept a list of \"knowledge triples\", or 3-tuples with a\nsubject\n,\npredicate\n, and\nobject\n, suitable for storage in a knolwedge graph. Our model will then generate these representations as part of its tool calls.\nFor simplicity, we use the same vector database as before, but the\nsave_recall_memory\nand\nsearch_recall_memories\ntools could be further updated to interact with a graph database. For now, we only need to update the\nsave_recall_memory\ntool:\nrecall_vector_store\n=\nInMemoryVectorStore\n(\nOpenAIEmbeddings\n(\n)\n)\nfrom\ntyping_extensions\nimport\nTypedDict\nclass\nKnowledgeTriple\n(\nTypedDict\n)\n:\nsubject\n:\nstr\npredicate\n:\nstr\nobject_\n:\nstr\n@tool\ndef\nsave_recall_memory\n(\nmemories\n:\nList\n[\nKnowledgeTriple\n]\n,\nconfig\n:\nRunnableConfig\n)\n-\n>\nstr\n:\n\"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\nuser_id\n=\nget_user_id\n(\nconfig\n)\nfor\nmemory\nin\nmemories\n:\nserialized\n=\n\" \"\n.\njoin\n(\nmemory\n.\nvalues\n(\n)\n)\ndocument\n=\nDocument\n(\nserialized\n,\nid\n=\nstr\n(\nuuid\n.\nuuid4\n(\n)\n)\n,\nmetadata\n=\n{\n\"user_id\"\n:\nuser_id\n,\n**\nmemory\n,\n}\n,\n)\nrecall_vector_store\n.\nadd_documents\n(\n[\ndocument\n]\n)\nreturn\nmemories\nWe can then compile the graph exactly as before:\ntools\n=\n[\nsave_recall_memory\n,\nsearch_recall_memories\n,\nsearch\n]\nmodel_with_tools\n=\nmodel\n.\nbind_tools\n(\ntools\n)\n# Create the graph and add nodes\nbuilder\n=\nStateGraph\n(\nState\n)\nbuilder\n.\nadd_node\n(\nload_memories\n)\nbuilder\n.\nadd_node\n(\nagent\n)\nbuilder\n.\nadd_node\n(\n\"tools\"\n,\nToolNode\n(\ntools\n)\n)\n# Add edges to the graph\nbuilder\n.\nadd_edge\n(\nSTART\n,\n\"load_memories\"\n)\nbuilder\n.\nadd_edge\n(\n\"load_memories\"\n,\n\"agent\"\n)\nbuilder\n.\nadd_conditional_edges\n(\n\"agent\"\n,\nroute_tools\n,\n[\n\"tools\"\n,\nEND\n]\n)\nbuilder\n.\nadd_edge\n(\n\"tools\"\n,\n\"agent\"\n)\n# Compile the graph\nmemory\n=\nMemorySaver\n(\n)\ngraph\n=\nbuilder\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"user_id\"\n:\n\"3\"\n,\n\"thread_id\"\n:\n\"1\"\n}\n}\nfor\nchunk\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"Hi, I'm Alice.\"\n)\n]\n}\n,\nconfig\n=\nconfig\n)\n:\npretty_print_stream_chunk\n(\nchunk\n)\nUpdate from node: load_memories\n{'recall_memories': []}\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nHello, Alice! How can I assist you today?\nNote that the application elects to extract knowledge-triples from the user's statements:\nfor\nchunk\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"My friend John likes Pizza.\"\n)\n]\n}\n,\nconfig\n=\nconfig\n)\n:\npretty_print_stream_chunk\n(\nchunk\n)\nUpdate from node: load_memories\n{'recall_memories': []}\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\nsave_recall_memory (call_EQSZlvZLZpPa0OGS5Kyzy2Yz)\nCall ID: call_EQSZlvZLZpPa0OGS5Kyzy2Yz\nArgs:\nmemories: [{'subject': 'Alice', 'predicate': 'has a friend', 'object_': 'John'}, {'subject': 'John', 'predicate': 'likes', 'object_': 'Pizza'}]\nUpdate from node: tools\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: save_recall_memory\n[{\"subject\": \"Alice\", \"predicate\": \"has a friend\", \"object_\": \"John\"}, {\"subject\": \"John\", \"predicate\": \"likes\", \"object_\": \"Pizza\"}]\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nGot it! If you need any suggestions related to pizza or anything else, feel free to ask. What else is on your mind today?\nAs before, the memories generated from one thread are accessed in another thread from the same user:\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"user_id\"\n:\n\"3\"\n,\n\"thread_id\"\n:\n\"2\"\n}\n}\nfor\nchunk\nin\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"What food should I bring to John's party?\"\n)\n]\n}\n,\nconfig\n=\nconfig\n)\n:\npretty_print_stream_chunk\n(\nchunk\n)\nUpdate from node: load_memories\n{'recall_memories': ['John likes Pizza', 'Alice has a friend John']}\nUpdate from node: agent\n==================================\u001b[1m Ai Message \u001b[0m==================================\nSince John likes pizza, bringing some delicious pizza would be a great choice for the party. You might also consider asking if there are any specific toppings he prefers or if there are any dietary restrictions among the guests. This way, you can ensure everyone enjoys the food!\nOptionally, for illustrative purposes we can visualize the knowledge graph extracted by the model:\n%\npip install\n-\nU\n-\n-\nquiet matplotlib networkx\nimport\nmatplotlib\n.\npyplot\nas\nplt\nimport\nnetworkx\nas\nnx\n# Fetch records\nrecords\n=\nrecall_vector_store\n.\nsimilarity_search\n(\n\"Alice\"\n,\nk\n=\n2\n,\nfilter\n=\nlambda\ndoc\n:\ndoc\n.\nmetadata\n[\n\"user_id\"\n]\n==\n\"3\"\n)\n# Plot graph\nplt\n.\nfigure\n(\nfigsize\n=\n(\n6\n,\n4\n)\n,\ndpi\n=\n80\n)\nG\n=\nnx\n.\nDiGraph\n(\n)\nfor\nrecord\nin\nrecords\n:\nG\n.\nadd_edge\n(\nrecord\n.\nmetadata\n[\n\"subject\"\n]\n,\nrecord\n.\nmetadata\n[\n\"object_\"\n]\n,\nlabel\n=\nrecord\n.\nmetadata\n[\n\"predicate\"\n]\n,\n)\npos\n=\nnx\n.\nspring_layout\n(\nG\n)\nnx\n.\ndraw\n(\nG\n,\npos\n,\nwith_labels\n=\nTrue\n,\nnode_size\n=\n3000\n,\nnode_color\n=\n\"lightblue\"\n,\nfont_size\n=\n10\n,\nfont_weight\n=\n\"bold\"\n,\narrows\n=\nTrue\n,\n)\nedge_labels\n=\nnx\n.\nget_edge_attributes\n(\nG\n,\n\"label\"\n)\nnx\n.\ndraw_networkx_edge_labels\n(\nG\n,\npos\n,\nedge_labels\n=\nedge_labels\n,\nfont_color\n=\n\"red\"\n)\nplt\n.\nshow\n(\n)\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/versions/release_policy/",
    "Versions\nRelease policy\nOn this page\nLangChain release policy\nThe LangChain ecosystem is composed of different component packages (e.g.,\nlangchain-core\n,\nlangchain\n,\nlangchain-community\n,\nlanggraph\n,\nlangserve\n, partner packages etc.)\nVersioning\nâ€‹\nlangchain\n,\nlangchain-core\n, and integration packages\nâ€‹\nlangchain\n,\nlangchain-core\n,\nlangchain-text-splitters\n, and integration packages (\nlangchain-openai\n,\nlangchain-anthropic\n, etc.) follow\nsemantic versioning\nin the format of 0.\nY\n.\nZ\n. The packages are under rapid development, and so are currently versioning the packages with a major version of 0.\nMinor version increases will occur for:\nBreaking changes for any public interfaces\nnot\nmarked as\nbeta\n.\nPatch version increases will occur for:\nBug fixes,\nNew features,\nAny changes to private interfaces,\nAny changes to\nbeta\nfeatures.\nWhen upgrading between minor versions, users should review the list of breaking changes and deprecations.\nFrom time to time, we will version packages as\nrelease candidates\n. These are versions that are intended to be released as stable versions, but we want to get feedback from the community before doing so. Release candidates will be versioned as 0.\nY\n.\nZ\nrc\nN\n. For example, 0.2.0rc1. If no issues are found, the release candidate will be released as a stable version with the same version number. If issues are found, we will release a new release candidate with an incremented\nN\nvalue (e.g., 0.2.0rc2).\nlangchain-community\nâ€‹\nMinor version increases will occur for:\nUpdates to the major/minor versions of required\nlangchain-x\ndependencies. E.g., when updating the required version of\nlangchain-core\nfrom\n^0.2.x\nto\n0.3.0\n.\nPatch version increases will occur for:\nBug fixes,\nNew features,\nAny changes to private interfaces,\nAny changes to\nbeta\nfeatures,\nBreaking changes to integrations to reflect breaking changes in the third-party service.\nWhenever possible we will avoid making breaking changes in patch versions.\nHowever, if an external API makes a breaking change then breaking changes to the corresponding\nlangchain-community\nintegration can occur in a patch version.\nlangchain-experimental\nâ€‹\nAll changes will be accompanied with patch version increases.\nRelease cadence\nâ€‹\nWe expect to space out\nminor\nreleases (e.g., from 0.2.x to 0.3.0) of\nlangchain\nand\nlangchain-core\nby at least 2-3 months, as such releases may contain breaking changes.\nPatch versions are released frequently, up to a few times per week, as they contain bug fixes and new features.\nAPI stability\nâ€‹\nThe development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in\nlangchain\nand\nlangchain-core\nwill continue to evolve to better serve the needs of our users.\nEven though both\nlangchain\nand\nlangchain-core\nare currently in a pre-1.0 state, we are committed to maintaining API stability in these packages.\nBreaking changes to the public API will result in a minor version bump (the second digit)\nAny bug fixes or new features will result in a patch version bump (the third digit)\nWe will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.\nStability of other packages\nâ€‹\nThe stability of other packages in the LangChain ecosystem may vary:\nlangchain-community\nis a community maintained package that contains 3rd party integrations. While we do our best to review and test changes in\nlangchain-community\n,\nlangchain-community\nis expected to experience more breaking changes than\nlangchain\nand\nlangchain-core\nas it contains many community contributions.\nPartner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information; however, in general these packages are expected to be stable.\nWhat is a \"API stability\"?\nâ€‹\nAPI stability means:\nAll the public APIs (everything in this documentation) will not be moved or renamed without providing backwards-compatible aliases.\nIf new features are added to these APIs â€“ which is quite possible â€“ they will not break or change the meaning of existing methods. In other words, \"stable\" does not (necessarily) mean \"complete.\"\nIf, for some reason, an API declared stable must be removed or replaced, it will be declared deprecated but will remain in the API for at least two minor releases. Warnings will be issued when the deprecated method is called.\nAPIs marked as internal\nâ€‹\nCertain APIs are explicitly marked as â€œinternalâ€ in a couple of ways:\nSome documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.\nFunctions, methods, and other objects prefixed by a leading underscore (\n_\n). This is the standard Python convention of indicating that something is private; if any method starts with a single\n_\n, itâ€™s an internal API.\nException:\nCertain methods are prefixed with\n_\n, but do not contain an implementation. These methods are\nmeant\nto be overridden by sub-classes that provide the implementation. Such methods are generally part of the\nPublic API\nof LangChain.\nDeprecation policy\nâ€‹\nWe will generally avoid deprecating features until a better alternative is available.\nWhen a feature is deprecated, it will continue to work in the current and next minor version of\nlangchain\nand\nlangchain-core\n. After that, the feature will be removed.\nSince we're expecting to space out minor releases by at least 2-3 months, this means that a feature can be removed within 2-6 months of being deprecated.\nIn some situations, we may allow the feature to remain in the code base for longer periods of time, if it's not causing issues in the packages, to reduce the burden on users.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/security/",
    "Security Policy\nOn this page\nSecurity Policy\nLangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.\nBest practices\nâ€‹\nWhen building such applications, developers should remember to follow good security practices:\nLimit Permissions\n: Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), specifying proxy configurations to control external requests, etc., as appropriate for your application.\nAnticipate Potential Misuse\n: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it's safest to assume that any LLM able to use those credentials may in fact delete data.\nDefense in Depth\n: No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It's best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.\nRisks of not doing so include, but are not limited to:\nData corruption or loss.\nUnauthorized access to confidential information.\nCompromised performance or availability of critical resources.\nExample scenarios with mitigation strategies:\nA user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.\nA user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.\nA user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.\nIf you're building applications that access external resources like file systems, APIs\nor databases, consider speaking with your company's security team to determine how to best\ndesign and secure your applications.\nReporting OSS Vulnerabilities\nâ€‹\nLangChain is partnered with\nhuntr by Protect AI\nto provide\na bounty program for our open source projects.\nPlease report security vulnerabilities associated with the LangChain\nopen source projects at\nhuntr\n.\nBefore reporting a vulnerability, please review:\nIn-Scope Targets and Out-of-Scope Targets below.\nThe\nlangchain-ai/langchain\nmonorepo structure.\nThe\nBest Practices\nabove to\nunderstand what we consider to be a security vulnerability vs. developer\nresponsibility.\nIn-Scope Targets\nâ€‹\nThe following packages and repositories are eligible for bug bounties:\nlangchain-core\nlangchain (see exceptions)\nlangchain-community (see exceptions)\nlanggraph\nlangserve\nOut of Scope Targets\nâ€‹\nAll out of scope targets defined by huntr as well as:\nlangchain-experimental\n: This repository is for experimental code and is not\neligible for bug bounties (see\npackage warning\n), bug reports to it will be marked as interesting or waste of\ntime and published with no bounty attached.\ntools\n: Tools in either langchain or langchain-community are not eligible for bug\nbounties. This includes the following directories\nlibs/langchain/langchain/tools\nlibs/community/langchain_community/tools\nPlease review the\nBest Practices\nfor more details, but generally tools interact with the real world. Developers are\nexpected to understand the security implications of their code and are responsible\nfor the security of their tools.\nCode documented with security notices. This will be decided on a case-by-case basis, but likely will not be eligible for a bounty as the code is already\ndocumented with guidelines for developers that should be followed for making their\napplication secure.\nAny LangSmith related repositories or APIs (see\nReporting LangSmith Vulnerabilities\n).\nReporting LangSmith Vulnerabilities\nâ€‹\nPlease report security vulnerabilities associated with LangSmith by email to\nsecurity@langchain.dev\n.\nLangSmith site:\nhttps://smith.langchain.com\nSDK client:\nhttps://github.com/langchain-ai/langsmith-sdk\nOther Security Concerns\nâ€‹\nFor any other security concerns, please contact us at\nsecurity@langchain.dev\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/concepts/architecture/#langgraph",
    "Conceptual guide\nArchitecture\nOn this page\nArchitecture\nLangChain is a framework that consists of a number of packages.\nlangchain-core\nâ€‹\nThis package contains base abstractions for different components and ways to compose them together.\nThe interfaces for core components like chat models, vector stores, tools and more are defined here.\nNo third-party integrations are defined here.\nThe dependencies are kept purposefully very lightweight.\nlangchain\nâ€‹\nThe main\nlangchain\npackage contains chains and retrieval strategies that make up an application's cognitive architecture.\nThese are NOT third-party integrations.\nAll chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.\nIntegration packages\nâ€‹\nPopular integrations have their own packages (e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc) so that they can be properly versioned and appropriately lightweight.\nFor more information see:\nA list\nintegrations packages\nThe\nAPI Reference\nwhere you can find detailed information about each of the integration package.\nlangchain-community\nâ€‹\nThis package contains third-party integrations that are maintained by the LangChain community.\nKey integration packages are separated out (see above).\nThis contains integrations for various components (chat models, vector stores, tools, etc).\nAll dependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph\nâ€‹\nlanggraph\nis an extension of\nlangchain\naimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.\nFurther reading\nSee our LangGraph overview\nhere\n.\nSee our LangGraph Academy Course\nhere\n.\nlangserve\nâ€‹\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nimportant\nLangServe is designed to primarily deploy simple Runnables and work with well-known primitives in langchain-core.\nIf you need a deployment option for LangGraph, you should instead be looking at LangGraph Platform (beta) which will be better suited for deploying LangGraph applications.\nFor more information, see the\nLangServe documentation\n.\nLangSmith\nâ€‹\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nFor more information, see the\nLangSmith documentation\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/integrations/chat/",
    "Components\nChat models\nOn this page\nChat models\nChat models\nare language models that use a sequence of\nmessages\nas inputs and return messages as outputs (as opposed to using plain text). These are generally newer models.\ninfo\nIf you'd like to write your own chat model, see\nthis how-to\n.\nIf you'd like to contribute an integration, see\nContributing integrations\n.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nFeatured Providers\nâ€‹\ninfo\nWhile all these LangChain classes support the indicated advanced feature, you may have\nto open the provider-specific documentation to learn which hosted models or backends support\nthe feature.\nProvider\nTool calling\nStructured output\nJSON mode\nLocal\nMultimodal\nPackage\nChatAnthropic\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nlangchain-anthropic\nChatMistralAI\nâœ…\nâœ…\nâŒ\nâŒ\nâŒ\nlangchain-mistralai\nChatFireworks\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nlangchain-fireworks\nAzureChatOpenAI\nâœ…\nâœ…\nâœ…\nâŒ\nâœ…\nlangchain-openai\nChatOpenAI\nâœ…\nâœ…\nâœ…\nâŒ\nâœ…\nlangchain-openai\nChatTogether\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nlangchain-together\nChatVertexAI\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nlangchain-google-vertexai\nChatGoogleGenerativeAI\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nlangchain-google-genai\nChatGroq\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nlangchain-groq\nChatCohere\nâœ…\nâœ…\nâŒ\nâŒ\nâŒ\nlangchain-cohere\nChatBedrock\nâœ…\nâœ…\nâŒ\nâŒ\nâŒ\nlangchain-aws\nChatHuggingFace\nâœ…\nâœ…\nâŒ\nâœ…\nâŒ\nlangchain-huggingface\nChatNVIDIA\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nlangchain-nvidia-ai-endpoints\nChatOllama\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nlangchain-ollama\nChatLlamaCpp\nâœ…\nâœ…\nâŒ\nâœ…\nâŒ\nlangchain-community\nChatAI21\nâœ…\nâœ…\nâŒ\nâŒ\nâŒ\nlangchain-ai21\nChatUpstage\nâœ…\nâœ…\nâŒ\nâŒ\nâŒ\nlangchain-upstage\nChatDatabricks\nâœ…\nâœ…\nâŒ\nâŒ\nâŒ\ndatabricks-langchain\nChatWatsonx\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nlangchain-ibm\nChatXAI\nâœ…\nâœ…\nâŒ\nâŒ\nâŒ\nlangchain-xai\nChatPerplexity\nâŒ\nâœ…\nâœ…\nâŒ\nâœ…\nlangchain-perplexity\nAll chat models\nâ€‹\nName\nDescription\nAbso\nThis will help you get started with ChatAbso chat models. For detaile...\nAI21 Labs\nThis notebook covers how to get started with AI21 chat models.\nAlibaba Cloud PAI EAS\nAlibaba Cloud PAI (Platform for AI) is a lightweight and cost-efficie...\nAnthropic\nThis notebook provides a quick overview for getting started with Anth...\nAnyscale\nThis notebook demonstrates the use of langchain.chat_models.ChatAnysc...\nAzureAIChatCompletionsModel\nThis will help you get started with AzureAIChatCompletionsModel chat ...\nAzure OpenAI\nThis guide will help you get started with AzureOpenAI chat models. Fo...\nAzure ML Endpoint\nAzure Machine Learning is a platform used to build, train, and deploy...\nBaichuan Chat\nBaichuan chat models API by Baichuan Intelligent Technology. For more...\nBaidu Qianfan\nBaidu AI Cloud Qianfan Platform is a one-stop large model development...\nAWS Bedrock\nThis doc will help you get started with AWS Bedrock chat models. Amaz...\nCerebras\nThis notebook provides a quick overview for getting started with Cere...\nCloudflareWorkersAI\nThis will help you get started with CloudflareWorkersAI chat models. ...\nCohere\nThis notebook covers how to get started with Cohere chat models.\nContextualAI\nThis will help you get started with Contextual AI's Grounded Language...\nCoze Chat\nChatCoze chat models API by coze.com. For more information, see https...\nDappier AI\nDappier: Powering AI with Dynamic, Real-Time Data Models\nDatabricks\nDatabricks Lakehouse Platform unifies data, analytics, and AI on one ...\nDeepInfra\nDeepInfra is a serverless inference as a service that provides access...\nDeepSeek\nThis will help you get started with DeepSeek's hosted chat models. Fo...\nEden AI\nEden AI is revolutionizing the AI landscape by uniting the best AI pr...\nEverlyAI\nEverlyAI allows you to run your ML models at scale in the cloud. It a...\nFeatherless AI\nThis will help you get started with FeatherlessAi chat models. For de...\nFireworks\nThis doc helps you get started with Fireworks AI chat models. For det...\nChatFriendli\nFriendli enhances AI application performance and optimizes cost savin...\nGoodfire\nThis will help you get started with Goodfire chat models. For detaile...\nGoogle Gemini\nAccess Google's Generative AI models, including the Gemini family, di...\nGoogle Cloud Vertex AI\nThis page provides a quick overview for getting started with VertexAI...\nGPTRouter\nGPTRouter is an open source LLM API Gateway that offers a universal A...\nDigitalOcean Gradient\nThis will help you getting started with DigitalOcean Gradient Chat Mo...\nGreenNode\nGreenNode is a global AI solutions provider and a NVIDIA Preferred Pa...\nGroq\nThis will help you get started with Groq chat models. For detailed do...\nChatHuggingFace\nThis will help you get started with langchainhuggingface chat models....\nIBM watsonx.ai\nChatWatsonx is a wrapper for IBM watsonx.ai foundation models.\nJinaChat\nThis notebook covers how to get started with JinaChat chat models.\nKinetica\nThis notebook demonstrates how to use Kinetica to transform natural l...\nKonko\nKonko API is a fully managed Web API designed to help application dev...\nLiteLLM\nLiteLLM is a library that simplifies calling Anthropic, Azure, Huggin...\nLlama 2 Chat\nThis notebook shows how to augment Llama-2 LLMs with the Llama2Chat w...\nLlama API\nThis notebook shows how to use LangChain with LlamaAPI - a hosted ver...\nLlamaEdge\nLlamaEdge allows you to chat with LLMs of GGUF format both locally an...\nLlama.cpp\nllama.cpp python library is a simple Python bindings for @ggerganov\nmaritalk\nMariTalk is an assistant developed by the Brazilian company Maritaca ...\nMiniMax\nMinimax is a Chinese startup that provides LLM service for companies ...\nMistralAI\nThis will help you get started with Mistral chat models. For detailed...\nMLX\nThis notebook shows how to get started using MLX LLM's as chat models.\nModelScope\nModelScope (Home | GitHub) is built upon the notion of â€œModel-as-a-Se...\nMoonshot\nMoonshot is a Chinese startup that provides LLM service for companies...\nNaver\nThis notebook provides a quick overview for getting started with Nave...\nNebius\nThis page will help you get started with Nebius AI Studio chat models...\nNetmind\nThis will help you get started with Netmind chat models. For detailed...\nNVIDIA AI Endpoints\nThis will help you get started with NVIDIA chat models. For detailed ...\nChatOCIModelDeployment\nThis will help you get started with OCIModelDeployment chat models. F...\nOCIGenAI\nThis notebook provides a quick overview for getting started with OCIG...\nChatOctoAI\nOctoAI offers easy access to efficient compute and enables users to i...\nOllama\nOllama allows you to run open-source large language models, such as g...\nOpenAI\nThis notebook provides a quick overview for getting started with Open...\nOutlines\nThis will help you get started with Outlines chat models. For detaile...\nPerplexity\nThis page will help you get started with Perplexity chat models. For ...\nPipeshift\nThis will help you get started with Pipeshift chat models. For detail...\nChatPredictionGuard\nPrediction Guard is a secure, scalable GenAI platform that safeguards...\nPremAI\nPremAI is an all-in-one platform that simplifies the creation of robu...\nPromptLayer ChatOpenAI\nThis example showcases how to connect to PromptLayer to start recordi...\nQwen QwQ\nThis will help you get started with QwQ chat models. For detailed doc...\nReka\nThis notebook provides a quick overview for getting started with Reka...\nRunPod Chat Model\nGet started with RunPod chat models.\nSambaNovaCloud\nThis will help you get started with SambaNovaCloud chat models. For d...\nSambaStudio\nThis will help you get started with SambaStudio chat models. For deta...\nChatSeekrFlow\nSeekr provides AI-powered solutions for structured, explainable, and ...\nSnowflake Cortex\nSnowflake Cortex gives you instant access to industry-leading large l...\nsolar\nDeprecated since version 0.0.34: Use langchain_upstage.ChatUpstage in...\nSparkLLM Chat\nSparkLLM chat models API by iFlyTek. For more information, see iFlyTe...\nNebula (Symbl.ai)\nThis notebook covers how to get started with Nebula - Symbl.ai's chat...\nTencent Hunyuan\nTencent's hybrid model API (Hunyuan API)\nTogether\nThis page will help you get started with Together AI chat models. For...\nTongyi Qwen\nTongyi Qwen is a large language model developed by Alibaba's Damo Aca...\nUpstage\nThis notebook covers how to get started with Upstage chat models.\nvectara\nVectara is the trusted AI Assistant and Agent platform, which focuses...\nvLLM Chat\nvLLM can be deployed as a server that mimics the OpenAI API protocol....\nVolc Engine Maas\nThis notebook provides you with a guide on how to get started with vo...\nChat Writer\nThis notebook provides a quick overview for getting started with Writ...\nxAI\nThis page will help you get started with xAI chat models. For detaile...\nXinference\nXinference is a powerful and versatile library designed to serve LLMs,\nYandexGPT\nThis notebook goes over how to use Langchain with YandexGPT chat mode...\nChatYI\nThis will help you get started with Yi chat models. For detailed docu...\nYuan2.0\nThis notebook shows how to use YUAN2 API in LangChain with the langch...\nZHIPU AI\nThis notebook shows how to use ZHIPU AI API in LangChain with the lan...\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#architecture",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#guides",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#tutorials",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#how-to-guides",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/#chat-models",
    "How-to guides\nOn this page\nHow-to guides\nHere youâ€™ll find answers to \"How do Iâ€¦.?\" types of questions.\nThese guides are\ngoal-oriented\nand\nconcrete\n; they're meant to help you complete a specific task.\nFor conceptual explanations see the\nConceptual guide\n.\nFor end-to-end walkthroughs see\nTutorials\n.\nFor comprehensive descriptions of every class and function see the\nAPI Reference\n.\nInstallation\nâ€‹\nHow to: install LangChain packages\nHow to: use LangChain with different Pydantic versions\nKey features\nâ€‹\nThis highlights functionality that is core to using LangChain.\nHow to: return structured data from a model\nHow to: use a model to call tools\nHow to: stream runnables\nHow to: debug your LLM apps\nComponents\nâ€‹\nThese are the core building blocks you can use when building applications.\nChat models\nâ€‹\nChat Models\nare newer forms of language models that take messages in and output a message.\nSee\nsupported integrations\nfor details on getting started with chat models from a specific provider.\nHow to: initialize any model in one line\nHow to: work with local models\nHow to: do function/tool calling\nHow to: get models to return structured output\nHow to: cache model responses\nHow to: get log probabilities\nHow to: create a custom chat model class\nHow to: stream a response back\nHow to: track token usage\nHow to: track response metadata across providers\nHow to: use chat model to call tools\nHow to: stream tool calls\nHow to: handle rate limits\nHow to: few-shot prompt tool behavior\nHow to: bind model-specific formatted tools\nHow to: force a specific tool call\nHow to: pass multimodal data directly to models\nMessages\nâ€‹\nMessages\nare the input and output of chat models. They have some\ncontent\nand a\nrole\n, which describes the source of the message.\nHow to: trim messages\nHow to: filter messages\nHow to: merge consecutive messages of the same type\nPrompt templates\nâ€‹\nPrompt Templates\nare responsible for formatting user input into a format that can be passed to a language model.\nHow to: use few-shot examples\nHow to: use few-shot examples in chat models\nHow to: partially format prompt templates\nHow to: compose prompts together\nHow to: use multimodal prompts\nExample selectors\nâ€‹\nExample Selectors\nare responsible for selecting the correct few shot examples to pass to the prompt.\nHow to: use example selectors\nHow to: select examples by length\nHow to: select examples by semantic similarity\nHow to: select examples by semantic ngram overlap\nHow to: select examples by maximal marginal relevance\nHow to: select examples from LangSmith few-shot datasets\nLLMs\nâ€‹\nWhat LangChain calls\nLLMs\nare older forms of language models that take a string in and output a string.\nHow to: cache model responses\nHow to: create a custom LLM class\nHow to: stream a response back\nHow to: track token usage\nHow to: work with local models\nOutput parsers\nâ€‹\nOutput Parsers\nare responsible for taking the output of an LLM and parsing into more structured format.\nHow to: parse text from message objects\nHow to: use output parsers to parse an LLM response into structured format\nHow to: parse JSON output\nHow to: parse XML output\nHow to: parse YAML output\nHow to: retry when output parsing errors occur\nHow to: try to fix errors in output parsing\nHow to: write a custom output parser class\nDocument loaders\nâ€‹\nDocument Loaders\nare responsible for loading documents from a variety of sources.\nHow to: load PDF files\nHow to: load web pages\nHow to: load CSV data\nHow to: load data from a directory\nHow to: load HTML data\nHow to: load JSON data\nHow to: load Markdown data\nHow to: load Microsoft Office data\nHow to: write a custom document loader\nText splitters\nâ€‹\nText Splitters\ntake a document and split into chunks that can be used for retrieval.\nHow to: recursively split text\nHow to: split HTML\nHow to: split by character\nHow to: split code\nHow to: split Markdown by headers\nHow to: recursively split JSON\nHow to: split text into semantic chunks\nHow to: split by tokens\nEmbedding models\nâ€‹\nEmbedding Models\ntake a piece of text and create a numerical representation of it.\nSee\nsupported integrations\nfor details on getting started with embedding models from a specific provider.\nHow to: embed text data\nHow to: cache embedding results\nHow to: create a custom embeddings class\nVector stores\nâ€‹\nVector stores\nare databases that can efficiently store and retrieve embeddings.\nSee\nsupported integrations\nfor details on getting started with vector stores from a specific provider.\nHow to: use a vector store to retrieve data\nRetrievers\nâ€‹\nRetrievers\nare responsible for taking a query and returning relevant documents.\nHow to: use a vector store to retrieve data\nHow to: generate multiple queries to retrieve data for\nHow to: use contextual compression to compress the data retrieved\nHow to: write a custom retriever class\nHow to: add similarity scores to retriever results\nHow to: combine the results from multiple retrievers\nHow to: reorder retrieved results to mitigate the \"lost in the middle\" effect\nHow to: generate multiple embeddings per document\nHow to: retrieve the whole document for a chunk\nHow to: generate metadata filters\nHow to: create a time-weighted retriever\nHow to: use hybrid vector and keyword retrieval\nIndexing\nâ€‹\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\nHow to: reindex data to keep your vectorstore in sync with the underlying data source\nTools\nâ€‹\nLangChain\nTools\ncontain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer\nhere\nfor a list of pre-built tools.\nHow to: create tools\nHow to: use built-in tools and toolkits\nHow to: use chat models to call tools\nHow to: pass tool outputs to chat models\nHow to: pass runtime values to tools\nHow to: add a human-in-the-loop for tools\nHow to: handle tool errors\nHow to: force models to call a tool\nHow to: disable parallel tool calling\nHow to: access the\nRunnableConfig\nfrom a tool\nHow to: stream events from a tool\nHow to: return artifacts from a tool\nHow to: convert Runnables to tools\nHow to: add ad-hoc tool calling capability to models\nHow to: pass in runtime secrets\nMultimodal\nâ€‹\nHow to: pass multimodal data directly to models\nHow to: use multimodal prompts\nAgents\nâ€‹\nnote\nFor in depth how-to guides for agents, please check out\nLangGraph\ndocumentation.\nHow to: use legacy LangChain Agents (AgentExecutor)\nHow to: migrate from legacy LangChain agents to LangGraph\nCallbacks\nâ€‹\nCallbacks\nallow you to hook into the various stages of your LLM application's execution.\nHow to: pass in callbacks at runtime\nHow to: attach callbacks to a module\nHow to: pass callbacks into a module constructor\nHow to: create custom callback handlers\nHow to: use callbacks in async environments\nHow to: dispatch custom callback events\nCustom\nâ€‹\nAll of LangChain components can easily be extended to support your own versions.\nHow to: create a custom chat model class\nHow to: create a custom LLM class\nHow to: create a custom embeddings class\nHow to: write a custom retriever class\nHow to: write a custom document loader\nHow to: write a custom output parser class\nHow to: create custom callback handlers\nHow to: define a custom tool\nHow to: dispatch custom callback events\nSerialization\nâ€‹\nHow to: save and load LangChain objects\nUse cases\nâ€‹\nThese guides cover use-case specific details.\nQ&A with RAG\nâ€‹\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\nFor a high-level tutorial on RAG, check out\nthis guide\n.\nHow to: add chat history\nHow to: stream\nHow to: return sources\nHow to: return citations\nHow to: do per-user retrieval\nExtraction\nâ€‹\nExtraction is when you use LLMs to extract structured information from unstructured text.\nFor a high level tutorial on extraction, check out\nthis guide\n.\nHow to: use reference examples\nHow to: handle long text\nHow to: do extraction without using function calling\nChatbots\nâ€‹\nChatbots involve using an LLM to have a conversation.\nFor a high-level tutorial on building chatbots, check out\nthis guide\n.\nHow to: manage memory\nHow to: do retrieval\nHow to: use tools\nHow to: manage large chat history\nQuery analysis\nâ€‹\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\nFor a high-level tutorial on query analysis, check out\nthis guide\n.\nHow to: add examples to the prompt\nHow to: handle cases where no queries are generated\nHow to: handle multiple queries\nHow to: handle multiple retrievers\nHow to: construct filters\nHow to: deal with high cardinality categorical variables\nQ&A over SQL + CSV\nâ€‹\nYou can use LLMs to do question answering over tabular data.\nFor a high-level tutorial, check out\nthis guide\n.\nHow to: use prompting to improve results\nHow to: do query validation\nHow to: deal with large databases\nHow to: deal with CSV files\nQ&A over graph databases\nâ€‹\nYou can use an LLM to do question answering over graph databases.\nFor a high-level tutorial, check out\nthis guide\n.\nHow to: add a semantic layer over a database\nHow to: construct knowledge graphs\nSummarization\nâ€‹\nLLMs can summarize and otherwise distill desired information from text, including\nlarge volumes of text. For a high-level tutorial, check out\nthis guide\n.\nHow to: summarize text in a single LLM call\nHow to: summarize text through parallelization\nHow to: summarize text through iterative refinement\nLangChain Expression Language (LCEL)\nâ€‹\nShould I use LCEL?\nLCEL is an orchestration solution. See our\nconcepts page\nfor recommendations on when to\nuse LCEL.\nLangChain Expression Language\nis a way to create arbitrary custom chains. It is built on the\nRunnable\nprotocol.\nLCEL cheatsheet\n: For a quick overview of how to use the main LCEL primitives.\nMigration guide\n: For migrating legacy chain abstractions to LCEL.\nHow to: chain runnables\nHow to: stream runnables\nHow to: invoke runnables in parallel\nHow to: add default invocation args to runnables\nHow to: turn any function into a runnable\nHow to: pass through inputs from one chain step to the next\nHow to: configure runnable behavior at runtime\nHow to: add message history (memory) to a chain\nHow to: route between sub-chains\nHow to: create a dynamic (self-constructing) chain\nHow to: inspect runnables\nHow to: add fallbacks to a runnable\nHow to: pass runtime secrets to a runnable\nLangGraph\nâ€‹\nLangGraph is an extension of LangChain aimed at\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\nLangGraph documentation is currently hosted on a separate site.\nYou can find the\nLangGraph guides here\n.\nLangSmith\nâ€‹\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\nLangSmith documentation is hosted on a separate site.\nYou can peruse\nLangSmith how-to guides here\n, but we'll highlight a few sections that are particularly\nrelevant to LangChain below:\nEvaluation\nâ€‹\nEvaluating performance is a vital part of building LLM-powered applications.\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\nTo learn more, check out the\nLangSmith evaluation how-to guides\n.\nTracing\nâ€‹\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\nHow to: trace with LangChain\nHow to: add metadata and tags to traces\nYou can see general tracing-related how-tos\nin this section of the LangSmith docs\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/how_to/#vector-stores",
    "How-to guides\nOn this page\nHow-to guides\nHere youâ€™ll find answers to \"How do Iâ€¦.?\" types of questions.\nThese guides are\ngoal-oriented\nand\nconcrete\n; they're meant to help you complete a specific task.\nFor conceptual explanations see the\nConceptual guide\n.\nFor end-to-end walkthroughs see\nTutorials\n.\nFor comprehensive descriptions of every class and function see the\nAPI Reference\n.\nInstallation\nâ€‹\nHow to: install LangChain packages\nHow to: use LangChain with different Pydantic versions\nKey features\nâ€‹\nThis highlights functionality that is core to using LangChain.\nHow to: return structured data from a model\nHow to: use a model to call tools\nHow to: stream runnables\nHow to: debug your LLM apps\nComponents\nâ€‹\nThese are the core building blocks you can use when building applications.\nChat models\nâ€‹\nChat Models\nare newer forms of language models that take messages in and output a message.\nSee\nsupported integrations\nfor details on getting started with chat models from a specific provider.\nHow to: initialize any model in one line\nHow to: work with local models\nHow to: do function/tool calling\nHow to: get models to return structured output\nHow to: cache model responses\nHow to: get log probabilities\nHow to: create a custom chat model class\nHow to: stream a response back\nHow to: track token usage\nHow to: track response metadata across providers\nHow to: use chat model to call tools\nHow to: stream tool calls\nHow to: handle rate limits\nHow to: few-shot prompt tool behavior\nHow to: bind model-specific formatted tools\nHow to: force a specific tool call\nHow to: pass multimodal data directly to models\nMessages\nâ€‹\nMessages\nare the input and output of chat models. They have some\ncontent\nand a\nrole\n, which describes the source of the message.\nHow to: trim messages\nHow to: filter messages\nHow to: merge consecutive messages of the same type\nPrompt templates\nâ€‹\nPrompt Templates\nare responsible for formatting user input into a format that can be passed to a language model.\nHow to: use few-shot examples\nHow to: use few-shot examples in chat models\nHow to: partially format prompt templates\nHow to: compose prompts together\nHow to: use multimodal prompts\nExample selectors\nâ€‹\nExample Selectors\nare responsible for selecting the correct few shot examples to pass to the prompt.\nHow to: use example selectors\nHow to: select examples by length\nHow to: select examples by semantic similarity\nHow to: select examples by semantic ngram overlap\nHow to: select examples by maximal marginal relevance\nHow to: select examples from LangSmith few-shot datasets\nLLMs\nâ€‹\nWhat LangChain calls\nLLMs\nare older forms of language models that take a string in and output a string.\nHow to: cache model responses\nHow to: create a custom LLM class\nHow to: stream a response back\nHow to: track token usage\nHow to: work with local models\nOutput parsers\nâ€‹\nOutput Parsers\nare responsible for taking the output of an LLM and parsing into more structured format.\nHow to: parse text from message objects\nHow to: use output parsers to parse an LLM response into structured format\nHow to: parse JSON output\nHow to: parse XML output\nHow to: parse YAML output\nHow to: retry when output parsing errors occur\nHow to: try to fix errors in output parsing\nHow to: write a custom output parser class\nDocument loaders\nâ€‹\nDocument Loaders\nare responsible for loading documents from a variety of sources.\nHow to: load PDF files\nHow to: load web pages\nHow to: load CSV data\nHow to: load data from a directory\nHow to: load HTML data\nHow to: load JSON data\nHow to: load Markdown data\nHow to: load Microsoft Office data\nHow to: write a custom document loader\nText splitters\nâ€‹\nText Splitters\ntake a document and split into chunks that can be used for retrieval.\nHow to: recursively split text\nHow to: split HTML\nHow to: split by character\nHow to: split code\nHow to: split Markdown by headers\nHow to: recursively split JSON\nHow to: split text into semantic chunks\nHow to: split by tokens\nEmbedding models\nâ€‹\nEmbedding Models\ntake a piece of text and create a numerical representation of it.\nSee\nsupported integrations\nfor details on getting started with embedding models from a specific provider.\nHow to: embed text data\nHow to: cache embedding results\nHow to: create a custom embeddings class\nVector stores\nâ€‹\nVector stores\nare databases that can efficiently store and retrieve embeddings.\nSee\nsupported integrations\nfor details on getting started with vector stores from a specific provider.\nHow to: use a vector store to retrieve data\nRetrievers\nâ€‹\nRetrievers\nare responsible for taking a query and returning relevant documents.\nHow to: use a vector store to retrieve data\nHow to: generate multiple queries to retrieve data for\nHow to: use contextual compression to compress the data retrieved\nHow to: write a custom retriever class\nHow to: add similarity scores to retriever results\nHow to: combine the results from multiple retrievers\nHow to: reorder retrieved results to mitigate the \"lost in the middle\" effect\nHow to: generate multiple embeddings per document\nHow to: retrieve the whole document for a chunk\nHow to: generate metadata filters\nHow to: create a time-weighted retriever\nHow to: use hybrid vector and keyword retrieval\nIndexing\nâ€‹\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\nHow to: reindex data to keep your vectorstore in sync with the underlying data source\nTools\nâ€‹\nLangChain\nTools\ncontain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer\nhere\nfor a list of pre-built tools.\nHow to: create tools\nHow to: use built-in tools and toolkits\nHow to: use chat models to call tools\nHow to: pass tool outputs to chat models\nHow to: pass runtime values to tools\nHow to: add a human-in-the-loop for tools\nHow to: handle tool errors\nHow to: force models to call a tool\nHow to: disable parallel tool calling\nHow to: access the\nRunnableConfig\nfrom a tool\nHow to: stream events from a tool\nHow to: return artifacts from a tool\nHow to: convert Runnables to tools\nHow to: add ad-hoc tool calling capability to models\nHow to: pass in runtime secrets\nMultimodal\nâ€‹\nHow to: pass multimodal data directly to models\nHow to: use multimodal prompts\nAgents\nâ€‹\nnote\nFor in depth how-to guides for agents, please check out\nLangGraph\ndocumentation.\nHow to: use legacy LangChain Agents (AgentExecutor)\nHow to: migrate from legacy LangChain agents to LangGraph\nCallbacks\nâ€‹\nCallbacks\nallow you to hook into the various stages of your LLM application's execution.\nHow to: pass in callbacks at runtime\nHow to: attach callbacks to a module\nHow to: pass callbacks into a module constructor\nHow to: create custom callback handlers\nHow to: use callbacks in async environments\nHow to: dispatch custom callback events\nCustom\nâ€‹\nAll of LangChain components can easily be extended to support your own versions.\nHow to: create a custom chat model class\nHow to: create a custom LLM class\nHow to: create a custom embeddings class\nHow to: write a custom retriever class\nHow to: write a custom document loader\nHow to: write a custom output parser class\nHow to: create custom callback handlers\nHow to: define a custom tool\nHow to: dispatch custom callback events\nSerialization\nâ€‹\nHow to: save and load LangChain objects\nUse cases\nâ€‹\nThese guides cover use-case specific details.\nQ&A with RAG\nâ€‹\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\nFor a high-level tutorial on RAG, check out\nthis guide\n.\nHow to: add chat history\nHow to: stream\nHow to: return sources\nHow to: return citations\nHow to: do per-user retrieval\nExtraction\nâ€‹\nExtraction is when you use LLMs to extract structured information from unstructured text.\nFor a high level tutorial on extraction, check out\nthis guide\n.\nHow to: use reference examples\nHow to: handle long text\nHow to: do extraction without using function calling\nChatbots\nâ€‹\nChatbots involve using an LLM to have a conversation.\nFor a high-level tutorial on building chatbots, check out\nthis guide\n.\nHow to: manage memory\nHow to: do retrieval\nHow to: use tools\nHow to: manage large chat history\nQuery analysis\nâ€‹\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\nFor a high-level tutorial on query analysis, check out\nthis guide\n.\nHow to: add examples to the prompt\nHow to: handle cases where no queries are generated\nHow to: handle multiple queries\nHow to: handle multiple retrievers\nHow to: construct filters\nHow to: deal with high cardinality categorical variables\nQ&A over SQL + CSV\nâ€‹\nYou can use LLMs to do question answering over tabular data.\nFor a high-level tutorial, check out\nthis guide\n.\nHow to: use prompting to improve results\nHow to: do query validation\nHow to: deal with large databases\nHow to: deal with CSV files\nQ&A over graph databases\nâ€‹\nYou can use an LLM to do question answering over graph databases.\nFor a high-level tutorial, check out\nthis guide\n.\nHow to: add a semantic layer over a database\nHow to: construct knowledge graphs\nSummarization\nâ€‹\nLLMs can summarize and otherwise distill desired information from text, including\nlarge volumes of text. For a high-level tutorial, check out\nthis guide\n.\nHow to: summarize text in a single LLM call\nHow to: summarize text through parallelization\nHow to: summarize text through iterative refinement\nLangChain Expression Language (LCEL)\nâ€‹\nShould I use LCEL?\nLCEL is an orchestration solution. See our\nconcepts page\nfor recommendations on when to\nuse LCEL.\nLangChain Expression Language\nis a way to create arbitrary custom chains. It is built on the\nRunnable\nprotocol.\nLCEL cheatsheet\n: For a quick overview of how to use the main LCEL primitives.\nMigration guide\n: For migrating legacy chain abstractions to LCEL.\nHow to: chain runnables\nHow to: stream runnables\nHow to: invoke runnables in parallel\nHow to: add default invocation args to runnables\nHow to: turn any function into a runnable\nHow to: pass through inputs from one chain step to the next\nHow to: configure runnable behavior at runtime\nHow to: add message history (memory) to a chain\nHow to: route between sub-chains\nHow to: create a dynamic (self-constructing) chain\nHow to: inspect runnables\nHow to: add fallbacks to a runnable\nHow to: pass runtime secrets to a runnable\nLangGraph\nâ€‹\nLangGraph is an extension of LangChain aimed at\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\nLangGraph documentation is currently hosted on a separate site.\nYou can find the\nLangGraph guides here\n.\nLangSmith\nâ€‹\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\nLangSmith documentation is hosted on a separate site.\nYou can peruse\nLangSmith how-to guides here\n, but we'll highlight a few sections that are particularly\nrelevant to LangChain below:\nEvaluation\nâ€‹\nEvaluating performance is a vital part of building LLM-powered applications.\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\nTo learn more, check out the\nLangSmith evaluation how-to guides\n.\nTracing\nâ€‹\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\nHow to: trace with LangChain\nHow to: add metadata and tags to traces\nYou can see general tracing-related how-tos\nin this section of the LangSmith docs\n.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#conceptual-guide",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#integrations",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/integrations/vectorstores/",
    "Components\nVector stores\nOn this page\nVector stores\nA\nvector store\nstores\nembedded\ndata and performs similarity search.\nSelect embedding model:\nSelect\nembeddings model\n:\nOpenAI\nâ–¾\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\npip install -qU langchain-openai\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings\n(\nmodel\n=\n\"text-embedding-3-large\"\n)\nSelect vector store:\nSelect\nvector store\n:\nIn-memory\nâ–¾\nIn-memory\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\npip install -qU langchain-core\nfrom\nlangchain_core\n.\nvectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore\n(\nembeddings\n)\nVectorstore\nDelete by ID\nFiltering\nSearch by Vector\nSearch with score\nAsync\nPasses Standard Tests\nMulti Tenancy\nIDs in add Documents\nAstraDBVectorStore\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nChroma\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nClickhouse\nâœ…\nâœ…\nâŒ\nâœ…\nâŒ\nâŒ\nâŒ\nâœ…\nCouchbaseSearchVectorStore\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâœ…\nâœ…\nDatabricksVectorSearch\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nElasticsearchStore\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nFAISS\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nInMemoryVectorStore\nâœ…\nâœ…\nâŒ\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nMilvus\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nMongoDBAtlasVectorSearch\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nopenGauss\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâœ…\nâŒ\nâœ…\nPGVector\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nPGVectorStore\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâœ…\nPineconeVectorStore\nâœ…\nâœ…\nâœ…\nâŒ\nâœ…\nâŒ\nâŒ\nâœ…\nQdrantVectorStore\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâœ…\nâœ…\nRedis\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nâœ…\nWeaviate\nâœ…\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâœ…\nâœ…\nSQLServer\nâœ…\nâœ…\nâœ…\nâœ…\nâŒ\nâŒ\nâŒ\nâœ…\nAll Vectorstores\nâ€‹\nName\nDescription\nActiveloop Deep Lake\nActiveloop Deep Lake as a Multi-Modal Vector Store that stores embedd...\nAerospike\nAerospike Vector Search (AVS) is an\nAlibaba Cloud OpenSearch\nAlibaba Cloud Opensearch is a one-stop platform to develop intelligen...\nAnalyticDB\nAnalyticDB for PostgreSQL is a massively parallel processing (MPP) da...\nAnnoy\nAnnoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with P...\nApache Doris\nApache Doris is a modern data warehouse for real-time analytics.\nApertureDB\nApertureDB is a database that stores, indexes, and manages multi-moda...\nAstra DB Vector Store\nThis page provides a quickstart for using Astra DB as a Vector Store.\nAtlas\nAtlas is a platform by Nomic made for interacting with both small and...\nAwaDB\nAwaDB is an AI Native database for the search and storage of embeddin...\nAzure Cosmos DB Mongo vCore\nThis notebook shows you how to leverage this integrated vector databa...\nAzure Cosmos DB No SQL\nThis notebook shows you how to leverage this integrated vector databa...\nAzure AI Search\nAzure AI Search (formerly known as Azure Search and Azure Cognitive S...\nBagel\nBagel (Open Inference platform for AI), is like GitHub for AI data.\nBagelDB\nBagelDB (Open Vector Database for AI), is like GitHub for AI data.\nBaidu Cloud ElasticSearch VectorSearch\nBaidu Cloud VectorSearch is a fully managed, enterprise-level distrib...\nBaidu VectorDB\nBaidu VectorDB is a robust, enterprise-level distributed database ser...\nApache Cassandra\nThis page provides a quickstart for using Apache CassandraÂ® as a Vect...\nChroma\nThis notebook covers how to get started with the Chroma vector store.\nClarifai\nClarifai is an AI Platform that provides the full AI lifecycle rangin...\nClickHouse\nClickHouse is the fastest and most resource efficient open-source dat...\nCloudflareVectorize\nThis notebook covers how to get started with the CloudflareVectorize ...\nCouchbase\nCouchbase is an award-winning distributed NoSQL cloud database that d...\nDashVector\nDashVector is a fully-managed vectorDB service that supports high-dim...\nDatabricks\nDatabricks Vector Search is a serverless similarity search engine tha...\nIBM Db2 Vector Store and Vector Search\nLangChain's Db2 integration (langchain-db2) provides vector store and...\nDingoDB\nDingoDB is a distributed multi-mode vector database, which combines t...\nDocArray HnswSearch\nDocArrayHnswSearch is a lightweight Document Index implementation pro...\nDocArray InMemorySearch\nDocArrayInMemorySearch is a document index provided by Docarray that ...\nAmazon Document DB\nAmazon DocumentDB (with MongoDB Compatibility) makes it easy to set u...\nDuckDB\nThis notebook shows how to use DuckDB as a vector store.\nChina Mobile ECloud ElasticSearch VectorSearch\nChina Mobile ECloud VectorSearch is a fully managed, enterprise-level...\nElasticsearch\nElasticsearch is a distributed, RESTful search and analytics engine, ...\nEpsilla\nEpsilla is an open-source vector database that leverages the advanced...\nFaiss\nFacebook AI Similarity Search (FAISS) is a library for efficient simi...\nFaiss (Async)\nFacebook AI Similarity Search (Faiss) is a library for efficient simi...\nFalkorDBVectorStore\nFalkorDB is an open-source graph database with integrated support for...\nGel\nAn implementation of LangChain vectorstore abstraction using gel as t...\nGoogle AlloyDB for PostgreSQL\nAlloyDB is a fully managed relational database service that offers hi...\nGoogle BigQuery Vector Search\nGoogle Cloud BigQuery Vector Search lets you use GoogleSQL to do sema...\nGoogle Cloud SQL for MySQL\nCloud SQL is a fully managed relational database service that offers ...\nGoogle Cloud SQL for PostgreSQL\nCloud SQL is a fully managed relational database service that offers ...\nFirestore\nFirestore is a serverless document-oriented database that scales to m...\nGoogle Memorystore for Redis\nGoogle Memorystore for Redis is a fully-managed service that is power...\nGoogle Spanner\nSpanner is a highly scalable database that combines unlimited scalabi...\nGoogle Vertex AI Feature Store\nGoogle Cloud Vertex Feature Store streamlines your ML feature managem...\nGoogle Vertex AI Vector Search\nThis notebook shows how to use functionality related to the Google Cl...\nHippo\nTranswarp Hippo is an enterprise-level cloud-native distributed vecto...\nHologres\nHologres is a unified real-time data warehousing service developed by...\nInfinispan\nInfinispan is an open-source key-value data grid, it can work as sing...\nJaguar Vector Database\n1. It is a distributed vector database\nKDB.AI\nKDB.AI is a powerful knowledge-based vector database and search engin...\nKinetica\nKinetica is a database with integrated support for vector similarity ...\nLanceDB\nLanceDB is an open-source database for vector-search built with persi...\nLantern\nLantern is an open-source vector similarity search for Postgres\nLindorm\nThis notebook covers how to get started with the Lindorm vector store.\nLLMRails\nLLMRails is a API platform for building GenAI applications. It provid...\nManticoreSearch VectorStore\nManticoreSearch is an open-source search engine that offers fast, sca...\nMariaDB\nLangChain's MariaDB integration (langchain-mariadb) provides vector c...\nMarqo\nThis notebook shows how to use functionality related to the Marqo vec...\nMeilisearch\nMeilisearch is an open-source, lightning-fast, and hyper relevant sea...\nAmazon MemoryDB\nVector Search introduction and langchain integration guide.\nMilvus\nMilvus is a database that stores, indexes, and manages massive embedd...\nMomento Vector Index (MVI)\nMVI: the most productive, easiest to use, serverless vector index for...\nMongoDB Atlas\nThis notebook covers how to MongoDB Atlas vector search in LangChain,...\nMyScale\nMyScale is a cloud-based database optimized for AI applications and s...\nNeo4j Vector Index\nNeo4j is an open-source graph database with integrated support for ve...\nNucliaDB\nYou can use a local NucliaDB instance or use Nuclia Cloud.\nOceanbase\nThis notebook covers how to get started with the Oceanbase vector sto...\nopenGauss\nThis notebook covers how to get started with the openGauss VectorStor...\nOpenSearch\nOpenSearch is a scalable, flexible, and extensible open-source softwa...\nOracle AI Vector Search: Vector Store\nOracle AI Vector Search is designed for Artificial Intelligence (AI) ...\nPathway\nPathway is an open data processing framework. It allows you to easily...\nPostgres Embedding\nPostgres Embedding is an open-source vector similarity search for Pos...\nPGVecto.rs\nThis notebook shows how to use functionality related to the Postgres ...\nPGVector\nAn implementation of LangChain vectorstore abstraction using postgres...\nPGVectorStore\nPGVectorStore is an implementation of a LangChain vectorstore using p...\nPinecone\nPinecone is a vector database with broad functionality.\nPinecone (sparse)\nPinecone is a vector database with broad functionality.\nQdrant\nQdrant (read: quadrant) is a vector similarity search engine. It prov...\nRedis\nThis notebook covers how to get started with the Redis vector store.\nRelyt\nRelyt is a cloud native data warehousing service that is designed to ...\nRockset\nRockset is a real-time search and analytics database built for the cl...\nSAP HANA Cloud Vector Engine\nSAP HANA Cloud Vector Engine is a vector store fully integrated into ...\nScaNN\nScaNN (Scalable Nearest Neighbors) is a method for efficient vector s...\nSemaDB\nSemaDB from SemaFind is a no fuss vector similarity database for buil...\nSingleStore\nSingleStore is a robust, high-performance distributed SQL database so...\nscikit-learn\nscikit-learn is an open-source collection of machine learning algorit...\nSQLiteVec\nThis notebook covers how to get started with the SQLiteVec vector sto...\nSQLite-VSS\nSQLite-VSS is an SQLite extension designed for vector search, emphasi...\nSQLServer\nAzure SQL provides a dedicatedÂ Vector data type that simplifies the c...\nStarRocks\nStarRocks is a High-Performance Analytical Database.\nSupabase (Postgres)\nSupabase is an open-source Firebase alternative. Supabase is built on...\nSurrealDBVectorStore\nSurrealDB is a unified, multi-model database purpose-built for AI sys...\nTablestore\nTablestore is a fully managed NoSQL cloud database service.\nTair\nTair is a cloud native in-memory database service developed by Alibab...\nTencent Cloud VectorDB\nTencent Cloud VectorDB is a fully managed, self-developed, enterprise...\nThirdAI NeuralDB\nNeuralDB is a CPU-friendly and fine-tunable vector store developed by...\nTiDB Vector\nTiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution...\nTigris\nTigris is an open-source Serverless NoSQL Database and Search Platfor...\nTileDB\nTileDB is a powerful engine for indexing and querying dense and spars...\nTimescale Vector (Postgres)\nTimescale Vector is PostgreSQL++ vector database for AI applications.\nTypesense\nTypesense is an open-source, in-memory search engine, that you can ei...\nUpstash Vector\nUpstash Vector is a serverless vector database designed for working w...\nUSearch\nUSearch is a Smaller & Faster Single-File Vector Search Engine\nVald\nVald is a highly scalable distributed fast approximate nearest neighb...\nVDMS\nThis notebook covers how to get started with VDMS as a vector store.\nVearch\nVearch is the vector search infrastructure for deeping learning and A...\nVectara\nVectara is the trusted AI Assistant and Agent platform which focuses ...\nVespa\nVespa is a fully featured search engine and vector database. It suppo...\nviking DB\nviking DB is a database that stores, indexes, and manages massive emb...\nvlite\nVLite is a simple and blazing fast vector database that allows you to...\nWeaviate\nThis notebook covers how to get started with the Weaviate vector stor...\nXata\nXata is a serverless data platform, based on PostgreSQL. It provides ...\nYDB\nYDB is a versatile open source Distributed SQL Database that combines...\nYellowbrick\nYellowbrick is an elastic, massively parallel processing (MPP) SQL da...\nZep\nRecall, understand, and extract data from chat histories. Power perso...\nZep Cloud\nRecall, understand, and extract data from chat histories. Power perso...\nZilliz\nZilliz Cloud is a fully managed service on cloud for LF AI MilvusÂ®,\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#api-reference",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#ecosystem",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#ï¸-langsmith",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#ï¸-langgraph",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#additional-resources",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#versions",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#security",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ],
  [
    "https://python.langchain.com/docs/#contributing",
    "Introduction\nOn this page\nIntroduction\nLangChain\nis a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment\n: Build your applications using LangChain's open-source\ncomponents\nand\nthird-party integrations\n.\nUse\nLangGraph\nto build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization\n: Use\nLangSmith\nto inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment\n: Turn your LangGraph applications into production-ready APIs and Assistants with\nLangGraph Platform\n.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the\nintegrations\npage for\nmore.\nSelect\nchat model\n:\nGoogle Gemini\nâ–¾\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nGroq\nCohere\nNVIDIA\nFireworks AI\nMistral AI\nTogether AI\nIBM watsonx\nDatabricks\nxAI\nPerplexity\nDeepSeek\npip install -qU \"langchain[google-genai]\"\nimport\ngetpass\nimport\nos\nif\nnot\nos\n.\nenviron\n.\nget\n(\n\"GOOGLE_API_KEY\"\n)\n:\nos\n.\nenviron\n[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass\n.\ngetpass\n(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain\n.\nchat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model\n(\n\"gemini-2.5-flash\"\n,\nmodel_provider\n=\n\"google_genai\"\n)\nmodel\n.\ninvoke\n(\n\"Hello, world!\"\n)\nnote\nThese docs focus on the Python LangChain library.\nHead here\nfor docs on the JavaScript LangChain library.\nArchitecture\nâ€‹\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture\npage.\nlangchain-core\n: Base abstractions for chat models and other components.\nIntegration packages\n(e.g.\nlangchain-openai\n,\nlangchain-anthropic\n, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain\n: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community\n: Third-party integrations that are community maintained.\nlanggraph\n: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See\nLangGraph documentation\n.\nGuides\nâ€‹\nTutorials\nâ€‹\nIf you're looking to build something specific or are more of a hands-on learner, check out our\ntutorials section\n.\nThis is the best place to get started.\nThese are the best ones to get started with:\nBuild a Simple LLM Application\nBuild a Chatbot\nBuild an Agent\nIntroduction to LangGraph\nExplore the full list of LangChain tutorials\nhere\n, and check out other\nLangGraph tutorials here\n. To learn more about LangGraph, check out our first LangChain Academy course,\nIntroduction to LangGraph\n, available\nhere\n.\nHow-to guides\nâ€‹\nHere\nyouâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the\nTutorials\nand the\nAPI Reference\n.\nHowever, these guides will help you quickly accomplish common tasks using\nchat models\n,\nvector stores\n, and other common LangChain components.\nCheck out\nLangGraph-specific how-tos here\n.\nConceptual guide\nâ€‹\nIntroductions to all the key parts of LangChain youâ€™ll need to know!\nHere\nyou'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out\nthis page\n.\nIntegrations\nâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with\nchat models\n,\nvector stores\n,\nor other LangChain components from a specific provider, check out our growing list of\nintegrations\n.\nAPI reference\nâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem\nâ€‹\nðŸ¦œðŸ› ï¸ LangSmith\nâ€‹\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\nðŸ¦œðŸ•¸ï¸ LangGraph\nâ€‹\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by LinkedIn, Uber, Klarna, GitLab, and many more.\nAdditional resources\nâ€‹\nVersions\nâ€‹\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity\nâ€‹\nRead up on\nsecurity\nbest practices to make sure you're developing safely with LangChain.\nContributing\nâ€‹\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\nEdit this page"
  ]
]